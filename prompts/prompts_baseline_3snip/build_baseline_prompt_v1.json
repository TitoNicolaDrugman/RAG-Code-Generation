{"meta": {"builder": "build_baseline_prompt_v1", "top_k": 3, "num_queries": 150, "timestamp": 1757769215.0488088, "source_retrieval_file": "BM25/retrieved_k3_samples.json"}, "prompts": [{"idx": 0, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n\nThe base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n\nThe domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n\nThe eBGP layer should add private peerings between different autonomous systems. \n\nFinally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file.", "prompt": "### Task:\nGenerate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n\nThe base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n\nThe domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n\nThe eBGP layer should add private peerings between different autonomous systems. \n\nFinally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file.\n\n### Code:\n", "top_k": 3}, {"idx": 1, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that solves the heat conduction equation using the adaptive finite element method on a triangular mesh. The code should allow the user to specify the number of spatial and temporal divisions, the adaptive refinement stop threshold, and the adaptive refinement and coarsening parameters. The code should use the 2D equation model from the fealpy library for the heat conduction equation, and the lagrange finite element space. The code should also handle Dirichlet boundary conditions. The code should iteratively refine the mesh based on the recovery estimate until the error is below the specified threshold. The code should also coarsen the mesh after each time step. The code should save a plot of the mesh at each refinement and coarsening step. The code should also print the error at each time step and plot the numerical solution at specified time steps.", "prompt": "### Task:\nGenerate code that solves the heat conduction equation using the adaptive finite element method on a triangular mesh. The code should allow the user to specify the number of spatial and temporal divisions, the adaptive refinement stop threshold, and the adaptive refinement and coarsening parameters. The code should use the 2D equation model from the fealpy library for the heat conduction equation, and the lagrange finite element space. The code should also handle Dirichlet boundary conditions. The code should iteratively refine the mesh based on the recovery estimate until the error is below the specified threshold. The code should also coarsen the mesh after each time step. The code should save a plot of the mesh at each refinement and coarsening step. The code should also print the error at each time step and plot the numerical solution at specified time steps.\n\n### Code:\n", "top_k": 3}, {"idx": 2, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that performs the following tasks:\n\n1. Import necessary libraries and modules including numpy, argparse, matplotlib, scipy, and fealpy.\n2. Define command line arguments for time division, initial spatial division, and number of spatial iterations.\n3. Create an initial 2D triangular mesh using the fealpy library.\n4. Define the parameters for the PDE (Partial Differential Equation) using the ADI_2d class from fealpy.\n5. Initialize the electric and magnetic fields using the FirstKindNedelecFiniteElementSpace2d and ScaledMonomialSpace2d classes from fealpy.\n6. Define a function to get the phi curl matrix.\n7. Create mass and curl matrices using the fealpy library.\n8. Iterate over the number of spatial iterations, in each iteration:\n   - Compute the right-hand side of the equation for the next time layer.\n   - Handle the boundary conditions for the next time layer.\n   - Compute the electric and magnetic fields for the next time layer.\n   - Calculate the error between the computed and actual solutions for the electric and magnetic fields.\n9. If not the last iteration, refine the mesh uniformly.\n10. Finally, display the error matrix and plot the error rates using the fealpy library.", "prompt": "### Task:\nGenerate code that performs the following tasks:\n\n1. Import necessary libraries and modules including numpy, argparse, matplotlib, scipy, and fealpy.\n2. Define command line arguments for time division, initial spatial division, and number of spatial iterations.\n3. Create an initial 2D triangular mesh using the fealpy library.\n4. Define the parameters for the PDE (Partial Differential Equation) using the ADI_2d class from fealpy.\n5. Initialize the electric and magnetic fields using the FirstKindNedelecFiniteElementSpace2d and ScaledMonomialSpace2d classes from fealpy.\n6. Define a function to get the phi curl matrix.\n7. Create mass and curl matrices using the fealpy library.\n8. Iterate over the number of spatial iterations, in each iteration:\n   - Compute the right-hand side of the equation for the next time layer.\n   - Handle the boundary conditions for the next time layer.\n   - Compute the electric and magnetic fields for the next time layer.\n   - Calculate the error between the computed and actual solutions for the electric and magnetic fields.\n9. If not the last iteration, refine the mesh uniformly.\n10. Finally, display the error matrix and plot the error rates using the fealpy library.\n\n### Code:\n", "top_k": 3}, {"idx": 3, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that calculates the effective electronic coupling based on single determinant diabatic states using the pyscf library. The code should first define a molecule with specific atoms and basis. Then, it should perform two state calculations with DFT, storing molecular orbital information into separate chkfiles. The code should then read the MO coefficients and occupation numbers from these chkfiles. Afterwards, it should calculate the overlap between two determinants, construct density matrices, calculate one-electron and two-electron part contributions, and calculate new total energy. Finally, the code should calculate the effective electronic coupling and print the results. The code should also remove the chkfiles at the end.", "prompt": "### Task:\nGenerate code that calculates the effective electronic coupling based on single determinant diabatic states using the pyscf library. The code should first define a molecule with specific atoms and basis. Then, it should perform two state calculations with DFT, storing molecular orbital information into separate chkfiles. The code should then read the MO coefficients and occupation numbers from these chkfiles. Afterwards, it should calculate the overlap between two determinants, construct density matrices, calculate one-electron and two-electron part contributions, and calculate new total energy. Finally, the code should calculate the effective electronic coupling and print the results. The code should also remove the chkfiles at the end.\n\n### Code:\n", "top_k": 3}, {"idx": 4, "repo_full_name": "capytaine__capytaine", "instruction": "Generate code that performs the following tasks using the capytaine library:\n\n1. Set up logging with a specific level and format.\n2. Create a mesh of a sphere with a specified radius, center, and resolution using capytaine's mesh_sphere function. Create a floating body from this mesh and add a translation degree of freedom to it.\n3. Extract the immersed part of the mesh.\n4. Set up a BEMSolver.\n5. Define and solve a diffraction problem for the immersed part of the sphere, with a specified wave direction and omega.\n6. Define and solve a radiation problem for the immersed part of the sphere, with a specified radiating degree of freedom and omega.\n7. Define a free surface with specified x and y ranges and number of points in each direction.\n8. Compute the free surface elevation for both the diffraction and radiation results.\n9. Add incoming waves to the diffraction elevation.\n10. Create and run two animations: one for the diffraction result and one for the radiation result. Each animation should include the full sphere and the free surface, with specified face motions and elevations. The animations should run with a specified camera position.", "prompt": "### Task:\nGenerate code that performs the following tasks using the capytaine library:\n\n1. Set up logging with a specific level and format.\n2. Create a mesh of a sphere with a specified radius, center, and resolution using capytaine's mesh_sphere function. Create a floating body from this mesh and add a translation degree of freedom to it.\n3. Extract the immersed part of the mesh.\n4. Set up a BEMSolver.\n5. Define and solve a diffraction problem for the immersed part of the sphere, with a specified wave direction and omega.\n6. Define and solve a radiation problem for the immersed part of the sphere, with a specified radiating degree of freedom and omega.\n7. Define a free surface with specified x and y ranges and number of points in each direction.\n8. Compute the free surface elevation for both the diffraction and radiation results.\n9. Add incoming waves to the diffraction elevation.\n10. Create and run two animations: one for the diffraction result and one for the radiation result. Each animation should include the full sphere and the free surface, with specified face motions and elevations. The animations should run with a specified camera position.\n\n### Code:\n", "top_k": 3}, {"idx": 5, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that initializes an emulator and several layers using the seedemu library. The code should create an Internet Exchange with a specific ID and set its display name and description. Then, it should create three Autonomous Systems with different IDs. For each Autonomous System, the code should create a network, a router that joins two networks, and a host that joins a network. It should also install a web service on a virtual node and bind this node to a host. The code should set display names and descriptions for the networks, routers, and Autonomous Systems. After creating the Autonomous Systems, the code should peer them with the Internet Exchange. Finally, the code should add all the layers to the emulator, render the emulator, and compile it with Docker, enabling the internet map.", "prompt": "### Task:\nGenerate code that initializes an emulator and several layers using the seedemu library. The code should create an Internet Exchange with a specific ID and set its display name and description. Then, it should create three Autonomous Systems with different IDs. For each Autonomous System, the code should create a network, a router that joins two networks, and a host that joins a network. It should also install a web service on a virtual node and bind this node to a host. The code should set display names and descriptions for the networks, routers, and Autonomous Systems. After creating the Autonomous Systems, the code should peer them with the Internet Exchange. Finally, the code should add all the layers to the emulator, render the emulator, and compile it with Docker, enabling the internet map.\n\n### Code:\n", "top_k": 3}, {"idx": 6, "repo_full_name": "urwid__urwid", "instruction": "Generate code that creates a user interface for a crystalfontz 635 LCD display using the urwid library in Python. The interface should include a menu with options for adjusting display settings, cursor settings, LED settings, and an 'About this Demo' section. The display settings should allow the user to adjust brightness and contrast. The cursor settings should allow the user to choose between different cursor styles. The LED settings should allow the user to adjust the color of each LED. The 'About this Demo' section should display a text about the demo. The interface should also include custom characters for a check box, a radio button, and a progress bar. The check box and radio button should use only one character, including a custom character. The progress bar should use custom characters to represent different levels of progress. The interface should also include a horizontal slider control using custom characters. The slider should be able to move in response to user input. The interface should also include a menu option indicated with a single arrow character. The menu should be able to go back to the previous menu when the cancel button is pressed. The interface should be able to connect to the LCD display using the provided command line argument.", "prompt": "### Task:\nGenerate code that creates a user interface for a crystalfontz 635 LCD display using the urwid library in Python. The interface should include a menu with options for adjusting display settings, cursor settings, LED settings, and an 'About this Demo' section. The display settings should allow the user to adjust brightness and contrast. The cursor settings should allow the user to choose between different cursor styles. The LED settings should allow the user to adjust the color of each LED. The 'About this Demo' section should display a text about the demo. The interface should also include custom characters for a check box, a radio button, and a progress bar. The check box and radio button should use only one character, including a custom character. The progress bar should use custom characters to represent different levels of progress. The interface should also include a horizontal slider control using custom characters. The slider should be able to move in response to user input. The interface should also include a menu option indicated with a single arrow character. The menu should be able to go back to the previous menu when the cancel button is pressed. The interface should be able to connect to the LCD display using the provided command line argument.\n\n### Code:\n", "top_k": 3}, {"idx": 7, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that performs the following tasks using the avalanche library:\n\n1. Parse command line arguments to determine the device to use for computations.\n2. Define transformations for training and testing data.\n3. Create a benchmark using the MNIST dataset with the defined transformations.\n4. Create a simple MLP model with the number of classes equal to the number of classes in the benchmark.\n5. Define various loggers including a text logger, an interactive logger, a CSV logger, and a Tensorboard logger.\n6. Define an evaluation plugin that computes a wide range of metrics including accuracy, loss, class accuracy, AMCA, forgetting, backward transfer, forward transfer, CPU usage, timing, RAM usage, GPU usage, disk usage, MAC, and labels repartition metrics. The plugin should log these metrics using the defined loggers.\n7. Create a Naive continual learning strategy using the defined model, an SGD optimizer, a CrossEntropyLoss loss function, and the defined evaluation plugin.\n8. Train the model on the benchmark's training stream and evaluate it on the benchmark's test stream, printing the results after each experience.\n9. After all experiences, print all the metrics stored by the evaluation plugin.", "prompt": "### Task:\nGenerate code that performs the following tasks using the avalanche library:\n\n1. Parse command line arguments to determine the device to use for computations.\n2. Define transformations for training and testing data.\n3. Create a benchmark using the MNIST dataset with the defined transformations.\n4. Create a simple MLP model with the number of classes equal to the number of classes in the benchmark.\n5. Define various loggers including a text logger, an interactive logger, a CSV logger, and a Tensorboard logger.\n6. Define an evaluation plugin that computes a wide range of metrics including accuracy, loss, class accuracy, AMCA, forgetting, backward transfer, forward transfer, CPU usage, timing, RAM usage, GPU usage, disk usage, MAC, and labels repartition metrics. The plugin should log these metrics using the defined loggers.\n7. Create a Naive continual learning strategy using the defined model, an SGD optimizer, a CrossEntropyLoss loss function, and the defined evaluation plugin.\n8. Train the model on the benchmark's training stream and evaluate it on the benchmark's test stream, printing the results after each experience.\n9. After all experiences, print all the metrics stored by the evaluation plugin.\n\n### Code:\n", "top_k": 3}, {"idx": 8, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that initializes a scene in Blender using the blenderproc library. The code should parse command line arguments for paths to scene, texture, and material files, as well as an output directory. It should load a scene and label its objects based on a mapping from a CSV file. The code should also load materials and randomly assign them to 40% of the objects' materials. \n\nThe code should then load textures for the materials that were assigned to at least one object. It should extract floors and ceilings from wall objects and assign them appropriate category IDs. It should make all lamp objects emit light and make all ceiling objects emit a bit of light to brighten the room. \n\nThe code should then create a BVH tree containing all mesh objects and sample camera locations above the floor, ensuring there are no obstacles in front of the camera and that the scene coverage score is not too low. \n\nFinally, the code should enable normal, depth, and segmentation rendering, render the scene, and write the data to a .hdf5 file in the specified output directory.", "prompt": "### Task:\nGenerate code that initializes a scene in Blender using the blenderproc library. The code should parse command line arguments for paths to scene, texture, and material files, as well as an output directory. It should load a scene and label its objects based on a mapping from a CSV file. The code should also load materials and randomly assign them to 40% of the objects' materials. \n\nThe code should then load textures for the materials that were assigned to at least one object. It should extract floors and ceilings from wall objects and assign them appropriate category IDs. It should make all lamp objects emit light and make all ceiling objects emit a bit of light to brighten the room. \n\nThe code should then create a BVH tree containing all mesh objects and sample camera locations above the floor, ensuring there are no obstacles in front of the camera and that the scene coverage score is not too low. \n\nFinally, the code should enable normal, depth, and segmentation rendering, render the scene, and write the data to a .hdf5 file in the specified output directory.\n\n### Code:\n", "top_k": 3}, {"idx": 9, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a simulation using the dd4hep library. The code should import necessary modules and set up logging. It should define a function that runs the simulation. This function should import additional modules, set up command line arguments, and load a geometry file. If help is requested, it should print a help message and exit. The function should then load constants, set up Geant4, and print detectors. It should configure the UI, tracking field, and event actions. It should also set up a particle gun and a tracker. Finally, it should build a physics list and execute Geant4. If the script is run as the main program, it should call the function to run the simulation.", "prompt": "### Task:\nGenerate code that sets up a simulation using the dd4hep library. The code should import necessary modules and set up logging. It should define a function that runs the simulation. This function should import additional modules, set up command line arguments, and load a geometry file. If help is requested, it should print a help message and exit. The function should then load constants, set up Geant4, and print detectors. It should configure the UI, tracking field, and event actions. It should also set up a particle gun and a tracker. Finally, it should build a physics list and execute Geant4. If the script is run as the main program, it should call the function to run the simulation.\n\n### Code:\n", "top_k": 3}, {"idx": 10, "repo_full_name": "simpeg__simpeg", "instruction": "Generate code that performs a 1D inversion of Magnetic Susceptibility from Frequency-Domain Electromagnetic (FDEM) data, assuming a fixed electrical conductivity. The code should set up a cylindrically symmetric mesh, define geologic parameters and electrical conductivity, and set up the relative magnetic permeability. It should also define mappings and set up the FDEM problem and survey. The code should then perform the FDEM inversion, set up inversion directives, and run the inversion. If a flag is set to true, the code should plot the conductivity model, the permeability model, and the data misfits. The code should be able to handle the absence of the PardisoSolver by falling back to the SolverLU.", "prompt": "### Task:\nGenerate code that performs a 1D inversion of Magnetic Susceptibility from Frequency-Domain Electromagnetic (FDEM) data, assuming a fixed electrical conductivity. The code should set up a cylindrically symmetric mesh, define geologic parameters and electrical conductivity, and set up the relative magnetic permeability. It should also define mappings and set up the FDEM problem and survey. The code should then perform the FDEM inversion, set up inversion directives, and run the inversion. If a flag is set to true, the code should plot the conductivity model, the permeability model, and the data misfits. The code should be able to handle the absence of the PardisoSolver by falling back to the SolverLU.\n\n### Code:\n", "top_k": 3}, {"idx": 11, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seed-emulator library. The emulation should include three layers: base, routing, and eBGP. The base layer should create three autonomous systems with specific routers and networks. The first autonomous system should create five hosts and a router, all of which join a network. The second autonomous system should create three routers, each joining a different network. The third autonomous system should create two routers, both joining the same network. The eBGP layer should add private peering between different autonomous systems. Finally, the code should add all layers to the emulator and dump the emulator state to a binary file.", "prompt": "### Task:\nGenerate code that creates an emulation using the seed-emulator library. The emulation should include three layers: base, routing, and eBGP. The base layer should create three autonomous systems with specific routers and networks. The first autonomous system should create five hosts and a router, all of which join a network. The second autonomous system should create three routers, each joining a different network. The third autonomous system should create two routers, both joining the same network. The eBGP layer should add private peering between different autonomous systems. Finally, the code should add all layers to the emulator and dump the emulator state to a binary file.\n\n### Code:\n", "top_k": 3}, {"idx": 12, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that imports necessary libraries and creates a TetrahedronMesh using the fealpy library. The code should define nodes and cells for the mesh, calculate the number of nodes, edges, faces, and cells, and store these entities. It should also calculate and store the barycenter coordinates for each entity, the measure of each entity, and the relationships between each entity (cell to cell, cell to face, etc.). The code should also identify boundary flags for each entity and the indices of boundary nodes, edges, faces, and cells. Finally, the code should plot the mesh using matplotlib, showing the indices of nodes, edges, and cells.", "prompt": "### Task:\nGenerate code that imports necessary libraries and creates a TetrahedronMesh using the fealpy library. The code should define nodes and cells for the mesh, calculate the number of nodes, edges, faces, and cells, and store these entities. It should also calculate and store the barycenter coordinates for each entity, the measure of each entity, and the relationships between each entity (cell to cell, cell to face, etc.). The code should also identify boundary flags for each entity and the indices of boundary nodes, edges, faces, and cells. Finally, the code should plot the mesh using matplotlib, showing the indices of nodes, edges, and cells.\n\n### Code:\n", "top_k": 3}, {"idx": 13, "repo_full_name": "microsoft__qlib", "instruction": "Generate code that creates a class for simulating the OnlineManager based on rolling tasks using the qlib library. The class should initialize with parameters such as provider_uri, region, experiment name, task URL, task database name, task pool name, rolling step, start time, end time, tasks, and trainer. The class should have methods to reset the experiment, run the entire workflow automatically, and train tasks by other processes or machines for multiprocessing. The main method should include steps to reset, simulate, collect results, get signals, perform backtesting, and risk analysis. The class should be executable from the command line with user-defined parameters.", "prompt": "### Task:\nGenerate code that creates a class for simulating the OnlineManager based on rolling tasks using the qlib library. The class should initialize with parameters such as provider_uri, region, experiment name, task URL, task database name, task pool name, rolling step, start time, end time, tasks, and trainer. The class should have methods to reset the experiment, run the entire workflow automatically, and train tasks by other processes or machines for multiprocessing. The main method should include steps to reset, simulate, collect results, get signals, perform backtesting, and risk analysis. The class should be executable from the command line with user-defined parameters.\n\n### Code:\n", "top_k": 3}, {"idx": 14, "repo_full_name": "smdogroup__tacs", "instruction": "Generate code that performs the following tasks using the TACS library:\n\n1. Import necessary libraries including numpy, os, MPI from mpi4py, and several modules from tacs.\n2. Load a structural mesh from a BDF file.\n3. Set constitutive properties such as density, elastic modulus, poisson's ratio, shear correction factor, yield stress, and thickness.\n4. Loop over components of the mesh, creating stiffness and element object for each.\n5. Create a TACS assembler object from the mesh loader.\n6. Create a KS function and get the design variable values.\n7. Get the node locations and create the forces.\n8. Set up and solve the analysis problem by creating vectors, assembling the Jacobian, factoring, and solving the linear system.\n9. Evaluate the function and solve for the adjoint variables.\n10. Compute the total derivative with respect to material design variables and nodal locations.\n11. Create a random direction along which to perturb the nodes and compute the total derivative with respect to nodal locations.\n12. Set the complex step and compute the perturbed solution.\n13. Evaluate the function for the perturbed solution and compute the projected derivative.\n14. Output the results for visualization.", "prompt": "### Task:\nGenerate code that performs the following tasks using the TACS library:\n\n1. Import necessary libraries including numpy, os, MPI from mpi4py, and several modules from tacs.\n2. Load a structural mesh from a BDF file.\n3. Set constitutive properties such as density, elastic modulus, poisson's ratio, shear correction factor, yield stress, and thickness.\n4. Loop over components of the mesh, creating stiffness and element object for each.\n5. Create a TACS assembler object from the mesh loader.\n6. Create a KS function and get the design variable values.\n7. Get the node locations and create the forces.\n8. Set up and solve the analysis problem by creating vectors, assembling the Jacobian, factoring, and solving the linear system.\n9. Evaluate the function and solve for the adjoint variables.\n10. Compute the total derivative with respect to material design variables and nodal locations.\n11. Create a random direction along which to perturb the nodes and compute the total derivative with respect to nodal locations.\n12. Set the complex step and compute the perturbed solution.\n13. Evaluate the function for the perturbed solution and compute the projected derivative.\n14. Output the results for visualization.\n\n### Code:\n", "top_k": 3}, {"idx": 15, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that solves the time-harmonic equation using adaptive methods. The code should include the following functionalities:\n\n1. Define functions to recover the curl of a given solution and to calculate the least squares matrix for each node of a mesh.\n2. Parse command-line arguments to set the degree of the first kind Nedelec element, the initial mesh size, the maximum number of adaptive iterations, and the theta parameter for adaptive iteration.\n3. Initialize the problem using the CosSinData function from the fealpy library.\n4. Create a 2D box mesh using the MeshFactory class from the fealpy library and remove the fourth quadrant of the mesh.\n5. Iterate over the maximum number of adaptive iterations, during each iteration:\n   - Define the function space using the FirstKindNedelecFiniteElementSpace2d class from the fealpy library.\n   - Apply Dirichlet boundary conditions using the DirichletBC class from the fealpy library.\n   - Solve the system of equations using the scipy library's spsolve function.\n   - Calculate the L2 error between the solution and the exact solution, the curl of the solution and the exact curl, and the curl of the solution and the recovered curl.\n   - If not the last iteration, mark the cells for refinement based on the recovery error and refine the mesh.\n6. Plot the error rates using the showmultirate function from the fealpy library.", "prompt": "### Task:\nGenerate code that solves the time-harmonic equation using adaptive methods. The code should include the following functionalities:\n\n1. Define functions to recover the curl of a given solution and to calculate the least squares matrix for each node of a mesh.\n2. Parse command-line arguments to set the degree of the first kind Nedelec element, the initial mesh size, the maximum number of adaptive iterations, and the theta parameter for adaptive iteration.\n3. Initialize the problem using the CosSinData function from the fealpy library.\n4. Create a 2D box mesh using the MeshFactory class from the fealpy library and remove the fourth quadrant of the mesh.\n5. Iterate over the maximum number of adaptive iterations, during each iteration:\n   - Define the function space using the FirstKindNedelecFiniteElementSpace2d class from the fealpy library.\n   - Apply Dirichlet boundary conditions using the DirichletBC class from the fealpy library.\n   - Solve the system of equations using the scipy library's spsolve function.\n   - Calculate the L2 error between the solution and the exact solution, the curl of the solution and the exact curl, and the curl of the solution and the recovered curl.\n   - If not the last iteration, mark the cells for refinement based on the recovery error and refine the mesh.\n6. Plot the error rates using the showmultirate function from the fealpy library.\n\n### Code:\n", "top_k": 3}, {"idx": 16, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that uses the SIRF library to create multiplicative sinograms from normalisation and/or attenuation data. The code should accept command-line options for the path to data files, template sinogram, attenuation image file, ECAT8 bin normalisation file, output file, transform for attenuation image, transform type, and an option for non-interactive mode. The code should check if the provided files exist, and if not, use default files. It should also handle different types of transformations for the attenuation image. The main function of the code should create an acquisition model, check if norm and attenuation are present, and based on that, create an acquisition sensitivity model. It should then project the data if normalisation is added, and finally write the multiplicative sinogram to the specified output file.", "prompt": "### Task:\nGenerate code that uses the SIRF library to create multiplicative sinograms from normalisation and/or attenuation data. The code should accept command-line options for the path to data files, template sinogram, attenuation image file, ECAT8 bin normalisation file, output file, transform for attenuation image, transform type, and an option for non-interactive mode. The code should check if the provided files exist, and if not, use default files. It should also handle different types of transformations for the attenuation image. The main function of the code should create an acquisition model, check if norm and attenuation are present, and based on that, create an acquisition sensitivity model. It should then project the data if normalisation is added, and finally write the multiplicative sinogram to the specified output file.\n\n### Code:\n", "top_k": 3}, {"idx": 17, "repo_full_name": "fusion-power-plant-framework__bluemira", "instruction": "Generate code that uses the bluemira library to solve a 2D magnetostatic problem for a single coil. The code should first import necessary modules and define parameters for the coil and enclosure. Then, it should create the coil and enclosure using the bluemira library's geometry tools and set the mesh options for each. The code should also create components for the universe, enclosure, and coil. \n\nNext, the code should create a mesh and convert it for use in the FEniCS library. After setting up the mesh, the code should instantiate a magnetostatic solver and define the source term for the problem. The source term should be plotted for visualization. \n\nThe code should then solve the magnetostatic problem and calculate the magnetic field. Finally, the code should compare the calculated magnetic field with the theoretical value along the z-axis and along a radial path at a certain z-offset. The differences between the calculated and theoretical values should be plotted for each comparison.", "prompt": "### Task:\nGenerate code that uses the bluemira library to solve a 2D magnetostatic problem for a single coil. The code should first import necessary modules and define parameters for the coil and enclosure. Then, it should create the coil and enclosure using the bluemira library's geometry tools and set the mesh options for each. The code should also create components for the universe, enclosure, and coil. \n\nNext, the code should create a mesh and convert it for use in the FEniCS library. After setting up the mesh, the code should instantiate a magnetostatic solver and define the source term for the problem. The source term should be plotted for visualization. \n\nThe code should then solve the magnetostatic problem and calculate the magnetic field. Finally, the code should compare the calculated magnetic field with the theoretical value along the z-axis and along a radial path at a certain z-offset. The differences between the calculated and theoretical values should be plotted for each comparison.\n\n### Code:\n", "top_k": 3}, {"idx": 18, "repo_full_name": "silx-kit__silx", "instruction": "Generate code that creates a simple GUI application using the silx library in Python. The application should have a main window that displays a variety of widgets provided by the silx library. These widgets should include a WaitingPushButton, ThreadPoolPushButton, RangeSlider, LegendIconWidget, and ElidedLabel. Each widget should be labeled and have specific functionalities. For instance, the WaitingPushButton should swap its waiting state when clicked, the ThreadPoolPushButton should compute a power operation, and the RangeSlider should print events when its value or position changes. The LegendIconWidget should display different styles of lines, symbols, and colormaps, and the ElidedLabel should display long texts with different elide modes. The application should handle exceptions using the silx library's exception handler and should clean up after execution.", "prompt": "### Task:\nGenerate code that creates a simple GUI application using the silx library in Python. The application should have a main window that displays a variety of widgets provided by the silx library. These widgets should include a WaitingPushButton, ThreadPoolPushButton, RangeSlider, LegendIconWidget, and ElidedLabel. Each widget should be labeled and have specific functionalities. For instance, the WaitingPushButton should swap its waiting state when clicked, the ThreadPoolPushButton should compute a power operation, and the RangeSlider should print events when its value or position changes. The LegendIconWidget should display different styles of lines, symbols, and colormaps, and the ElidedLabel should display long texts with different elide modes. The application should handle exceptions using the silx library's exception handler and should clean up after execution.\n\n### Code:\n", "top_k": 3}, {"idx": 19, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that sets up a simple runaway scenario simulation using the DREAM library. The simulation should use constant temperature, density, and electric field to generate a runaway current. The physical parameters should be set to an electric field strength of 6 V/m, an electron density of 5e19 m^-3, and a temperature of 100 eV. The grid parameters should be set to a maximum momentum of 1 m_e*c, 300 momentum grid points, 20 pitch grid points, a simulation time of 1e-3 seconds, and 20 time steps. The code should also set up the radial grid, solver type, and time stepper. The output should be saved to an HDF5 file named 'output.h5'.", "prompt": "### Task:\nGenerate code that sets up a simple runaway scenario simulation using the DREAM library. The simulation should use constant temperature, density, and electric field to generate a runaway current. The physical parameters should be set to an electric field strength of 6 V/m, an electron density of 5e19 m^-3, and a temperature of 100 eV. The grid parameters should be set to a maximum momentum of 1 m_e*c, 300 momentum grid points, 20 pitch grid points, a simulation time of 1e-3 seconds, and 20 time steps. The code should also set up the radial grid, solver type, and time stepper. The output should be saved to an HDF5 file named 'output.h5'.\n\n### Code:\n", "top_k": 3}, {"idx": 20, "repo_full_name": "avslab__basilisk", "instruction": "Generate code that sets up a Monte Carlo simulation using the Basilisk library, specifically using the Python Spice setup. The simulation should create a simple spacecraft with specific initial conditions. The code should also include the loading of Spice kernels within Python to pull the Hubble states from Spice. This Python Spice call should be performed within each Monte Carlo thread. The Hubble states should then be printed to the terminal. The Monte Carlo scenario should be set up to run 12 times. The code should also include a Controller class with Spice kernel loading code that is commented out. The simulation should be set up within a class called \"MySimulation\". The code should also include a function to access the Spice Kernel and print out the state. Finally, the code should include a main function that sets up and executes the Monte Carlo simulation, and cleans up the data after the test.", "prompt": "### Task:\nGenerate code that sets up a Monte Carlo simulation using the Basilisk library, specifically using the Python Spice setup. The simulation should create a simple spacecraft with specific initial conditions. The code should also include the loading of Spice kernels within Python to pull the Hubble states from Spice. This Python Spice call should be performed within each Monte Carlo thread. The Hubble states should then be printed to the terminal. The Monte Carlo scenario should be set up to run 12 times. The code should also include a Controller class with Spice kernel loading code that is commented out. The simulation should be set up within a class called \"MySimulation\". The code should also include a function to access the Spice Kernel and print out the state. Finally, the code should include a main function that sets up and executes the Monte Carlo simulation, and cleans up the data after the test.\n\n### Code:\n", "top_k": 3}, {"idx": 21, "repo_full_name": "rlberry-py__rlberry", "instruction": "Generate code that compares several bandit agents using the rlberry library. The code should define the parameters of the problem, construct the experiment, define several classes of agents (UCB, UCBV, ETC, MOSS, IMED, NPTS, EXP3), and train these agents. After training, the code should compute and plot the cumulative pseudo-regret and cumulative regret for each agent. Finally, the code should compute and plot the number of times each arm was selected. The agents should be wrapped with a WriterWrapper to track the action and reward. The experiment should be managed using the ExperimentManager class. The plots should be created using matplotlib and seaborn, with varying line styles for each agent.", "prompt": "### Task:\nGenerate code that compares several bandit agents using the rlberry library. The code should define the parameters of the problem, construct the experiment, define several classes of agents (UCB, UCBV, ETC, MOSS, IMED, NPTS, EXP3), and train these agents. After training, the code should compute and plot the cumulative pseudo-regret and cumulative regret for each agent. Finally, the code should compute and plot the number of times each arm was selected. The agents should be wrapped with a WriterWrapper to track the action and reward. The experiment should be managed using the ExperimentManager class. The plots should be created using matplotlib and seaborn, with varying line styles for each agent.\n\n### Code:\n", "top_k": 3}, {"idx": 22, "repo_full_name": "burnysc2__python-sc2", "instruction": "Generate code that creates a StarCraft II bot using the python-sc2 library. The bot should be a Zerg race bot that performs a rush strategy. The bot should have methods to handle the start of the game, each step of the game, and the end of the game. \n\nOn start, the bot should set the game step to 2. On each step, the bot should perform a series of actions such as sending a chat message, attacking enemy structures, injecting hatcheries with larva, managing vespene gas and mineral resources, researching upgrades, training units, and building structures. \n\nThe bot should also have a method to draw a creep pixelmap for debugging purposes. At the end of the game, the bot should log that the game has ended. \n\nFinally, the bot should be run on a specific map against a computer opponent of the Terran race with medium difficulty. The game should not be run in real time and a replay of the game should be saved.", "prompt": "### Task:\nGenerate code that creates a StarCraft II bot using the python-sc2 library. The bot should be a Zerg race bot that performs a rush strategy. The bot should have methods to handle the start of the game, each step of the game, and the end of the game. \n\nOn start, the bot should set the game step to 2. On each step, the bot should perform a series of actions such as sending a chat message, attacking enemy structures, injecting hatcheries with larva, managing vespene gas and mineral resources, researching upgrades, training units, and building structures. \n\nThe bot should also have a method to draw a creep pixelmap for debugging purposes. At the end of the game, the bot should log that the game has ended. \n\nFinally, the bot should be run on a specific map against a computer opponent of the Terran race with medium difficulty. The game should not be run in real time and a replay of the game should be saved.\n\n### Code:\n", "top_k": 3}, {"idx": 23, "repo_full_name": "kivy__kivy", "instruction": "Generate code that creates a custom class named \"SelectableGrid\" that inherits from \"FocusBehavior\", \"CompoundSelectionBehavior\", and \"GridLayout\" from the Kivy library. This class should have methods to handle key down and key up events, navigate to a node by typing its number, select and deselect nodes, and handle touch events. The selection of a node should change its background color. The class should also print the selected nodes when the selection changes. After defining the class, create an instance of it with specific parameters and add 40 buttons to it, each with a unique number as its text. Bind the touch down event of each button to the touch handling method of the grid. Finally, run the application with the grid as the root widget.", "prompt": "### Task:\nGenerate code that creates a custom class named \"SelectableGrid\" that inherits from \"FocusBehavior\", \"CompoundSelectionBehavior\", and \"GridLayout\" from the Kivy library. This class should have methods to handle key down and key up events, navigate to a node by typing its number, select and deselect nodes, and handle touch events. The selection of a node should change its background color. The class should also print the selected nodes when the selection changes. After defining the class, create an instance of it with specific parameters and add 40 buttons to it, each with a unique number as its text. Bind the touch down event of each button to the touch handling method of the grid. Finally, run the application with the grid as the root widget.\n\n### Code:\n", "top_k": 3}, {"idx": 24, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that imports necessary libraries and modules such as argparse, sys, numpy, matplotlib, scipy, mumps, and fealpy. The code should define a Poisuille PDE from the navier_stokes_mold_2d module and set up a command-line argument parser to accept parameters like the degree of motion finite element space, degree of pressure finite element space, number of time divisions, evolution end time, output directory, steps, and non-linearization method. \n\nThe code should then parse these arguments and use them to define variables. It should also create a TriangleMesh from a unit square and a UniformTimeLine for time evolution. The code should define LagrangeFiniteElementSpace for motion and pressure, and calculate the number of global degrees of freedom for both. \n\nThe code should then set up a BilinearForm and MixedBilinearForm, and add domain integrators to them. It should assemble these forms and get their matrices. The code should also calculate the mass matrix of the motion space and initialize an error matrix. \n\nIn a loop, the code should advance to the next time level, add a ScalarConvectionIntegrator to a new BilinearForm, assemble it, and get its matrix. It should calculate a divergence matrix and a new matrix M. It should also calculate a source vector and set up boundary conditions. \n\nThe code should then solve the system of equations, update the motion and pressure functions, calculate the L2 error and maximum error, and advance to the next time level. Finally, the code should print the sum of absolute values of the motion function.", "prompt": "### Task:\nGenerate code that imports necessary libraries and modules such as argparse, sys, numpy, matplotlib, scipy, mumps, and fealpy. The code should define a Poisuille PDE from the navier_stokes_mold_2d module and set up a command-line argument parser to accept parameters like the degree of motion finite element space, degree of pressure finite element space, number of time divisions, evolution end time, output directory, steps, and non-linearization method. \n\nThe code should then parse these arguments and use them to define variables. It should also create a TriangleMesh from a unit square and a UniformTimeLine for time evolution. The code should define LagrangeFiniteElementSpace for motion and pressure, and calculate the number of global degrees of freedom for both. \n\nThe code should then set up a BilinearForm and MixedBilinearForm, and add domain integrators to them. It should assemble these forms and get their matrices. The code should also calculate the mass matrix of the motion space and initialize an error matrix. \n\nIn a loop, the code should advance to the next time level, add a ScalarConvectionIntegrator to a new BilinearForm, assemble it, and get its matrix. It should calculate a divergence matrix and a new matrix M. It should also calculate a source vector and set up boundary conditions. \n\nThe code should then solve the system of equations, update the motion and pressure functions, calculate the L2 error and maximum error, and advance to the next time level. Finally, the code should print the sum of absolute values of the motion function.\n\n### Code:\n", "top_k": 3}, {"idx": 25, "repo_full_name": "hiddensymmetries__simsopt", "instruction": "Generate code that solves a coil optimization problem using the simsopt library. The goal is to find coils that generate a specific target normal field on a given surface. The objective function includes terms for the magnetic field, coil length, coil-to-coil distance, coil-to-surface distance, curvature, and mean squared curvature. The code should initialize the boundary magnetic surface, create initial coils, define the individual terms of the objective function, and form the total objective function. It should then perform a Taylor test and run the optimization. After the optimization, the code should use the result as the initial guess for a subsequent optimization with reduced penalty for the coil length. Finally, the code should save the optimized coil shapes and currents.", "prompt": "### Task:\nGenerate code that solves a coil optimization problem using the simsopt library. The goal is to find coils that generate a specific target normal field on a given surface. The objective function includes terms for the magnetic field, coil length, coil-to-coil distance, coil-to-surface distance, curvature, and mean squared curvature. The code should initialize the boundary magnetic surface, create initial coils, define the individual terms of the objective function, and form the total objective function. It should then perform a Taylor test and run the optimization. After the optimization, the code should use the result as the initial guess for a subsequent optimization with reduced penalty for the coil length. Finally, the code should save the optimized coil shapes and currents.\n\n### Code:\n", "top_k": 3}, {"idx": 26, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation environment using the seedemu library. The environment should include multiple layers such as Base, Routing, Ebgp, Ibgp, Ospf, and WebService. It should create several Internet Exchanges with custom display names. It should also create Transit Autonomous Systems and single-homed stub Autonomous Systems with various services. The code should also add a host with a customized IP address to one of the Autonomous Systems and create a real-world Autonomous System. It should enable remote access to one of the Autonomous System's network. The code should also set up peering via a route server and private peering with different peer relationships. Finally, the code should add all the layers to the emulator, save the emulator to a component file, and render and compile the emulator.", "prompt": "### Task:\nGenerate code that creates an emulation environment using the seedemu library. The environment should include multiple layers such as Base, Routing, Ebgp, Ibgp, Ospf, and WebService. It should create several Internet Exchanges with custom display names. It should also create Transit Autonomous Systems and single-homed stub Autonomous Systems with various services. The code should also add a host with a customized IP address to one of the Autonomous Systems and create a real-world Autonomous System. It should enable remote access to one of the Autonomous System's network. The code should also set up peering via a route server and private peering with different peer relationships. Finally, the code should add all the layers to the emulator, save the emulator to a component file, and render and compile the emulator.\n\n### Code:\n", "top_k": 3}, {"idx": 27, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that simulates the energy balance in a plasma by plotting ohmic heating and radiative losses as a function of temperature at equilibrium ionization. The simulation should be performed using the DREAM library. The simulation should include setting up a radial grid, setting time steps, adding ions, setting up temperature and electric field, and disabling runaway and hot-tail grid. The simulation should be run in four stages: initialization, ionization, equilibration, and radiation. After each stage, the settings should be saved and the simulation should be run. Finally, the results should be plotted.", "prompt": "### Task:\nGenerate code that simulates the energy balance in a plasma by plotting ohmic heating and radiative losses as a function of temperature at equilibrium ionization. The simulation should be performed using the DREAM library. The simulation should include setting up a radial grid, setting time steps, adding ions, setting up temperature and electric field, and disabling runaway and hot-tail grid. The simulation should be run in four stages: initialization, ionization, equilibration, and radiation. After each stage, the settings should be saved and the simulation should be run. Finally, the results should be plotted.\n\n### Code:\n", "top_k": 3}, {"idx": 28, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that implements a user-defined Ordered Subset Maximum A Posteriori One Step Late (OSMAPOSL) reconstruction algorithm using the SIRF library. The code should accept command-line options for the raw data file, path to data files, number of subsets, number of sub-iterations, reconstruction engine, and an option to disable plots. The reconstruction algorithm should be implemented in a function that takes an image, objective function, prior, filter, number of subsets, and number of sub-iterations as parameters. The main function should handle the creation of the acquisition model, acquisition data, filter, initial image estimate, prior, and objective function. It should also call the reconstruction function and display the reconstructed image if the non-interactive option is not set. The code should handle errors and print an error message if an exception is thrown.", "prompt": "### Task:\nGenerate code that implements a user-defined Ordered Subset Maximum A Posteriori One Step Late (OSMAPOSL) reconstruction algorithm using the SIRF library. The code should accept command-line options for the raw data file, path to data files, number of subsets, number of sub-iterations, reconstruction engine, and an option to disable plots. The reconstruction algorithm should be implemented in a function that takes an image, objective function, prior, filter, number of subsets, and number of sub-iterations as parameters. The main function should handle the creation of the acquisition model, acquisition data, filter, initial image estimate, prior, and objective function. It should also call the reconstruction function and display the reconstructed image if the non-interactive option is not set. The code should handle errors and print an error message if an exception is thrown.\n\n### Code:\n", "top_k": 3}, {"idx": 29, "repo_full_name": "pyvista__pyvista", "instruction": "Generate code that uses the PyVista library to create a 3D visualization of the solar system. The code should load models of the planets, apply textures to them, and position them in a 3D space. It should also create a light source to simulate the sun. The code should then add these models to a plotter and display them. Additionally, the code should create subplots for individual planets, showing their textures. Finally, the code should create a visualization of Venus with and without its atmosphere.", "prompt": "### Task:\nGenerate code that uses the PyVista library to create a 3D visualization of the solar system. The code should load models of the planets, apply textures to them, and position them in a 3D space. It should also create a light source to simulate the sun. The code should then add these models to a plotter and display them. Additionally, the code should create subplots for individual planets, showing their textures. Finally, the code should create a visualization of Venus with and without its atmosphere.\n\n### Code:\n", "top_k": 3}, {"idx": 30, "repo_full_name": "explosion__thinc", "instruction": "Generate code that trains a transformer tagging model using Huggingface's Transformers and Thinc libraries. The code should include a configuration for the model, optimizer, learning rate, and training parameters. It should define a main function that checks for GPU usage, loads the configuration, resolves the configuration to construct objects, loads a dataset, initializes the model, and trains the model over a specified number of epochs. The code should also define a dataclass to hold the output of the Huggingface 'batch_encode_plus' method, and functions to create a transformer tagger model, a transformer tokenizer, and a transformer model. It should also include functions to convert transformer inputs and outputs, evaluate sequences, and group pairs of sequences into minibatches. Finally, the code should run the main function if the script is run as the main program.", "prompt": "### Task:\nGenerate code that trains a transformer tagging model using Huggingface's Transformers and Thinc libraries. The code should include a configuration for the model, optimizer, learning rate, and training parameters. It should define a main function that checks for GPU usage, loads the configuration, resolves the configuration to construct objects, loads a dataset, initializes the model, and trains the model over a specified number of epochs. The code should also define a dataclass to hold the output of the Huggingface 'batch_encode_plus' method, and functions to create a transformer tagger model, a transformer tokenizer, and a transformer model. It should also include functions to convert transformer inputs and outputs, evaluate sequences, and group pairs of sequences into minibatches. Finally, the code should run the main function if the script is run as the main program.\n\n### Code:\n", "top_k": 3}, {"idx": 31, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that uses the pyaedt library to create a flex cable CPWG (coplanar waveguide with ground) in HFSS. The code should first import necessary libraries and set the non-graphical mode. Then, it should launch AEDT in a specified version and solution type, and set some properties such as material override, automatically use causal materials, open region, model units, and initial mesh. \n\nNext, the code should define variables for the flex cable CPWG, including total length, theta, radius, width, height, spacing, ground width, and ground thickness. It should also define a function to create a bending based on the curvature radius and extension.\n\nThe code should then draw a signal line and a ground line to create a bent signal wire and two bent ground wires respectively. It should also draw a dielectric to create a dielectric cable, and create bottom metals. \n\nAfterwards, the code should create port interfaces (PEC enclosures) and a Perfect E boundary condition. It should also create ports and a setup and sweep with specified properties. \n\nFinally, the code should plot the model and release AEDT.", "prompt": "### Task:\nGenerate code that uses the pyaedt library to create a flex cable CPWG (coplanar waveguide with ground) in HFSS. The code should first import necessary libraries and set the non-graphical mode. Then, it should launch AEDT in a specified version and solution type, and set some properties such as material override, automatically use causal materials, open region, model units, and initial mesh. \n\nNext, the code should define variables for the flex cable CPWG, including total length, theta, radius, width, height, spacing, ground width, and ground thickness. It should also define a function to create a bending based on the curvature radius and extension.\n\nThe code should then draw a signal line and a ground line to create a bent signal wire and two bent ground wires respectively. It should also draw a dielectric to create a dielectric cable, and create bottom metals. \n\nAfterwards, the code should create port interfaces (PEC enclosures) and a Perfect E boundary condition. It should also create ports and a setup and sweep with specified properties. \n\nFinally, the code should plot the model and release AEDT.\n\n### Code:\n", "top_k": 3}, {"idx": 32, "repo_full_name": "scikit-learn__scikit-learn", "instruction": "Generate code that compares different clustering algorithms on various toy datasets. The code should generate several datasets, including noisy circles, noisy moons, blobs, anisotropicly distributed data, blobs with varied variances, and a dataset with no structure. It should then set up parameters for clustering and apply a variety of clustering algorithms to each dataset, including MeanShift, MiniBatchKMeans, AgglomerativeClustering, SpectralClustering, DBSCAN, HDBSCAN, OPTICS, AffinityPropagation, Birch, and GaussianMixture. The code should also handle warnings related to kneighbors_graph and measure the time taken for each algorithm to fit the data. Finally, it should visualize the results of each clustering algorithm on each dataset, displaying the time taken for each algorithm in the plot.", "prompt": "### Task:\nGenerate code that compares different clustering algorithms on various toy datasets. The code should generate several datasets, including noisy circles, noisy moons, blobs, anisotropicly distributed data, blobs with varied variances, and a dataset with no structure. It should then set up parameters for clustering and apply a variety of clustering algorithms to each dataset, including MeanShift, MiniBatchKMeans, AgglomerativeClustering, SpectralClustering, DBSCAN, HDBSCAN, OPTICS, AffinityPropagation, Birch, and GaussianMixture. The code should also handle warnings related to kneighbors_graph and measure the time taken for each algorithm to fit the data. Finally, it should visualize the results of each clustering algorithm on each dataset, displaying the time taken for each algorithm in the plot.\n\n### Code:\n", "top_k": 3}, {"idx": 33, "repo_full_name": "pyqtgraph__pyqtgraph", "instruction": "Generate code that creates a PyQtGraph application with a main window that displays three plots. The first plot should be non-interactive and display an image with an integrated vertical color bar. The second plot should be interactive and display a noisy image with an integrated horizontal color bar. The third and fourth plots should display noisy images and share a separate color bar. The color bars should be created using the ColorBarItem class from the PyQtGraph library. The images should be created using the ImageItem class from the PyQtGraph library. The plots should be created using the addPlot method of a GraphicsLayoutWidget instance. The application should start the Qt event loop if it is the main module.", "prompt": "### Task:\nGenerate code that creates a PyQtGraph application with a main window that displays three plots. The first plot should be non-interactive and display an image with an integrated vertical color bar. The second plot should be interactive and display a noisy image with an integrated horizontal color bar. The third and fourth plots should display noisy images and share a separate color bar. The color bars should be created using the ColorBarItem class from the PyQtGraph library. The images should be created using the ImageItem class from the PyQtGraph library. The plots should be created using the addPlot method of a GraphicsLayoutWidget instance. The application should start the Qt event loop if it is the main module.\n\n### Code:\n", "top_k": 3}, {"idx": 34, "repo_full_name": "federatedai__fate", "instruction": "Generate code that creates a pipeline for a machine learning task using the FATE library. The pipeline should include the following steps: reading data, data transformation, intersection, feature scaling, feature binning, data statistics, Pearson correlation, one-hot encoding, feature selection, logistic regression, and evaluation. The pipeline should be set up with roles for guest, host, and arbiter. The data reading, data transformation, intersection, and feature scaling steps should be performed for both the guest and host. The feature binning parameters, feature selection parameters, and logistic regression parameters should be defined as dictionaries. The pipeline should be compiled and fitted. The script should accept a configuration file as an argument from the command line. If no configuration file is provided, a default one should be used.", "prompt": "### Task:\nGenerate code that creates a pipeline for a machine learning task using the FATE library. The pipeline should include the following steps: reading data, data transformation, intersection, feature scaling, feature binning, data statistics, Pearson correlation, one-hot encoding, feature selection, logistic regression, and evaluation. The pipeline should be set up with roles for guest, host, and arbiter. The data reading, data transformation, intersection, and feature scaling steps should be performed for both the guest and host. The feature binning parameters, feature selection parameters, and logistic regression parameters should be defined as dictionaries. The pipeline should be compiled and fitted. The script should accept a configuration file as an argument from the command line. If no configuration file is provided, a default one should be used.\n\n### Code:\n", "top_k": 3}, {"idx": 35, "repo_full_name": "burnysc2__python-sc2", "instruction": "Generate code that creates a StarCraft II bot using the python-sc2 library. The bot should be able to manage resources, build structures, train units, and engage in combat. It should be able to distribute workers, build pylons when on low supply, train probes, build gateways, build gas, research warp gate, morph to warp gate when research is complete, warp new units, make stalkers attack either closest enemy unit or enemy spawn location, build proxy pylon, and chrono boost nexus or cybercore. The bot should also be able to handle situations when there are no nexuses left. The bot should be run on the \"(2)CatalystLE\" map against a Protoss computer opponent with easy difficulty.", "prompt": "### Task:\nGenerate code that creates a StarCraft II bot using the python-sc2 library. The bot should be able to manage resources, build structures, train units, and engage in combat. It should be able to distribute workers, build pylons when on low supply, train probes, build gateways, build gas, research warp gate, morph to warp gate when research is complete, warp new units, make stalkers attack either closest enemy unit or enemy spawn location, build proxy pylon, and chrono boost nexus or cybercore. The bot should also be able to handle situations when there are no nexuses left. The bot should be run on the \"(2)CatalystLE\" map against a Protoss computer opponent with easy difficulty.\n\n### Code:\n", "top_k": 3}, {"idx": 36, "repo_full_name": "avslab__basilisk", "instruction": "Generate code that imports necessary libraries and modules, including the 'Controller', 'RetentionPolicy', and various 'Dispersions' from the 'Basilisk.utilities.MonteCarlo' module, as well as a scenario module named 'scenario_AttFeedback'. The code should define a function 'run' that takes a boolean parameter 'show_plots'. Inside this function, it should create a Monte Carlo simulation controller, set its simulation and execution functions, execution count, archive directory, seed dispersion, thread count, verbosity, variable casting, and dispersion magnitude file. It should also define a list of dispersions and add them to the Monte Carlo controller. Then, it should create a retention policy, add message logs to it, set its data callback, and add it to the Monte Carlo controller. The function should execute the simulations, execute callbacks if 'show_plots' is True, and return. The code should also define a function 'displayPlots' that takes 'data' and 'retentionPolicy' as parameters, extracts time and states from the data, and plots the states against time. Finally, it should call the 'run' function with 'True' as the argument if the script is run as the main program.", "prompt": "### Task:\nGenerate code that imports necessary libraries and modules, including the 'Controller', 'RetentionPolicy', and various 'Dispersions' from the 'Basilisk.utilities.MonteCarlo' module, as well as a scenario module named 'scenario_AttFeedback'. The code should define a function 'run' that takes a boolean parameter 'show_plots'. Inside this function, it should create a Monte Carlo simulation controller, set its simulation and execution functions, execution count, archive directory, seed dispersion, thread count, verbosity, variable casting, and dispersion magnitude file. It should also define a list of dispersions and add them to the Monte Carlo controller. Then, it should create a retention policy, add message logs to it, set its data callback, and add it to the Monte Carlo controller. The function should execute the simulations, execute callbacks if 'show_plots' is True, and return. The code should also define a function 'displayPlots' that takes 'data' and 'retentionPolicy' as parameters, extracts time and states from the data, and plots the states against time. Finally, it should call the 'run' function with 'True' as the argument if the script is run as the main program.\n\n### Code:\n", "top_k": 3}, {"idx": 37, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that performs the following tasks using the blenderproc library:\n\n1. Parse command line arguments for a house.json file path, a chair object path, and an optional output directory.\n2. Initialize blenderproc.\n3. Load objects from the house.json file into the scene using a label mapping from a csv file.\n4. Load a chair object from the provided path and replace all chair objects in the scene with this chair object. The replacement should ignore collisions with floor objects and copy properties from the original objects. The pose of the new chair objects should be randomly rotated around the z-axis.\n5. Filter out invalid objects from the scene.\n6. Make all Suncg objects in the scene emit light.\n7. Initialize a point sampler for sampling locations inside the loaded house and a bvh tree containing all mesh objects.\n8. Sample camera poses inside the house, ensuring that obstacles are at least 1 meter away from the camera and the view covers at least 40% of the scene. Add these camera poses to the scene.\n9. Enable normal, depth, and segmentation rendering. Add an alpha channel to textures.\n10. Render the scene and write the rendered data to a .hdf5 file in the specified output directory.", "prompt": "### Task:\nGenerate code that performs the following tasks using the blenderproc library:\n\n1. Parse command line arguments for a house.json file path, a chair object path, and an optional output directory.\n2. Initialize blenderproc.\n3. Load objects from the house.json file into the scene using a label mapping from a csv file.\n4. Load a chair object from the provided path and replace all chair objects in the scene with this chair object. The replacement should ignore collisions with floor objects and copy properties from the original objects. The pose of the new chair objects should be randomly rotated around the z-axis.\n5. Filter out invalid objects from the scene.\n6. Make all Suncg objects in the scene emit light.\n7. Initialize a point sampler for sampling locations inside the loaded house and a bvh tree containing all mesh objects.\n8. Sample camera poses inside the house, ensuring that obstacles are at least 1 meter away from the camera and the view covers at least 40% of the scene. Add these camera poses to the scene.\n9. Enable normal, depth, and segmentation rendering. Add an alpha channel to textures.\n10. Render the scene and write the rendered data to a .hdf5 file in the specified output directory.\n\n### Code:\n", "top_k": 3}, {"idx": 38, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation environment using the seed-emulator library. The environment should include three types of autonomous systems (AS): transit, stub, and utility. \n\nFor the transit AS, create two internet exchanges with specific display names, three internal networks, and four routers linked in a linear structure. \n\nFor the stub AS, create three different systems. The first two should have an internal network, a router, and two host nodes. The first host node should have additional software installed and a new account created. The third system should be created using a utility function and further customized.\n\nEstablish BGP peering by creating an Ebgp layer and setting up the transit AS as the internet service provider for all the stub ASes. Also, set up direct peering between two of the stub ASes.\n\nCreate a web service layer with two web service nodes and bind these virtual nodes to physical nodes. \n\nAdd all the created layers to the emulator and save it to a component file. Render the emulator and change the display names for the nodes hosting the web services.\n\nFinally, compile the emulator using Docker, specifying custom images from DockerHub and local sources. Generate Docker files and copy the base container image to the output folder.", "prompt": "### Task:\nGenerate code that creates an emulation environment using the seed-emulator library. The environment should include three types of autonomous systems (AS): transit, stub, and utility. \n\nFor the transit AS, create two internet exchanges with specific display names, three internal networks, and four routers linked in a linear structure. \n\nFor the stub AS, create three different systems. The first two should have an internal network, a router, and two host nodes. The first host node should have additional software installed and a new account created. The third system should be created using a utility function and further customized.\n\nEstablish BGP peering by creating an Ebgp layer and setting up the transit AS as the internet service provider for all the stub ASes. Also, set up direct peering between two of the stub ASes.\n\nCreate a web service layer with two web service nodes and bind these virtual nodes to physical nodes. \n\nAdd all the created layers to the emulator and save it to a component file. Render the emulator and change the display names for the nodes hosting the web services.\n\nFinally, compile the emulator using Docker, specifying custom images from DockerHub and local sources. Generate Docker files and copy the base container image to the output folder.\n\n### Code:\n", "top_k": 3}, {"idx": 39, "repo_full_name": "geodynamics__burnman", "instruction": "Generate code that demonstrates how to create different minerals, compute seismic velocities, and compare them to a seismic reference model using the BurnMan library. The code should include four examples of creating composite minerals: two minerals mixed in simple mole fractions, a mix of three minerals, using preset solutions, and defining a custom solution. The code should also include the computation of seismic velocities and other properties by supplying BurnMan with a list of minerals and their molar abundances. The results should be compared to a seismic model, and the misfit between the computed and reference values should be calculated. Finally, the code should plot the computed and reference values of Vs, Vphi, density, and geotherm against pressure. The plots should be saved as an image file.", "prompt": "### Task:\nGenerate code that demonstrates how to create different minerals, compute seismic velocities, and compare them to a seismic reference model using the BurnMan library. The code should include four examples of creating composite minerals: two minerals mixed in simple mole fractions, a mix of three minerals, using preset solutions, and defining a custom solution. The code should also include the computation of seismic velocities and other properties by supplying BurnMan with a list of minerals and their molar abundances. The results should be compared to a seismic model, and the misfit between the computed and reference values should be calculated. Finally, the code should plot the computed and reference values of Vs, Vphi, density, and geotherm against pressure. The plots should be saved as an image file.\n\n### Code:\n", "top_k": 3}, {"idx": 40, "repo_full_name": "microsoft__nni", "instruction": "Generate code that performs the following tasks using the nni library:\n\n1. Fine-tunes a ResNet18 model on the Cifar10 dataset for 30 epochs and evaluates its accuracy.\n2. Creates a teacher model by duplicating the fine-tuned model.\n3. Creates a pruner using the TaylorPruner and AGPPruner classes, with a configuration list that targets Conv2d operations and has a sparse ratio of 0.5. The pruner is set to run for 100 training steps and 30 total times.\n4. Creates a quantizer using the QATQuantizer class, with a configuration list that targets Conv2d and BatchNorm2d operations and uses int8 quantization. The quantizer starts at the 100th step.\n5. Creates a distiller using the DynamicLayerwiseDistiller class, with a configuration list that targets Conv2d operations and uses the mean squared error method. The distiller uses the teacher model for predictions.\n6. Compresses the model using the distiller for 60 iterations of 100 steps each.\n7. Speeds up the model using the ModelSpeedup class and the masks from the pruner.\n8. Evaluates the accuracy of the compressed model.\n9. Simulates quantization by updating the calibration configuration of the model using the Quantizer class.\n10. Evaluates the accuracy of the compressed and quantized model.", "prompt": "### Task:\nGenerate code that performs the following tasks using the nni library:\n\n1. Fine-tunes a ResNet18 model on the Cifar10 dataset for 30 epochs and evaluates its accuracy.\n2. Creates a teacher model by duplicating the fine-tuned model.\n3. Creates a pruner using the TaylorPruner and AGPPruner classes, with a configuration list that targets Conv2d operations and has a sparse ratio of 0.5. The pruner is set to run for 100 training steps and 30 total times.\n4. Creates a quantizer using the QATQuantizer class, with a configuration list that targets Conv2d and BatchNorm2d operations and uses int8 quantization. The quantizer starts at the 100th step.\n5. Creates a distiller using the DynamicLayerwiseDistiller class, with a configuration list that targets Conv2d operations and uses the mean squared error method. The distiller uses the teacher model for predictions.\n6. Compresses the model using the distiller for 60 iterations of 100 steps each.\n7. Speeds up the model using the ModelSpeedup class and the masks from the pruner.\n8. Evaluates the accuracy of the compressed model.\n9. Simulates quantization by updating the calibration configuration of the model using the Quantizer class.\n10. Evaluates the accuracy of the compressed and quantized model.\n\n### Code:\n", "top_k": 3}, {"idx": 41, "repo_full_name": "unidata__metpy", "instruction": "Generate code that uses the MetPy library to create an advanced sounding plot. The code should first import necessary libraries and modules. Then, it should load a sample dataset, clean it by dropping rows with NaN values in specific columns, and assign units to the data. After preparing the data, the code should create a new figure with a specific aspect ratio and plot the data using normal plotting functions. It should also set custom labels for the x and y axes. The code should calculate the lifted condensation level (LCL) and plot it as a black dot, and calculate the full parcel profile and add it to the plot as a black line. It should shade areas of Convective Available Potential Energy (CAPE) and Convective Inhibition (CIN). Finally, the code should add special lines to the plot and display it.", "prompt": "### Task:\nGenerate code that uses the MetPy library to create an advanced sounding plot. The code should first import necessary libraries and modules. Then, it should load a sample dataset, clean it by dropping rows with NaN values in specific columns, and assign units to the data. After preparing the data, the code should create a new figure with a specific aspect ratio and plot the data using normal plotting functions. It should also set custom labels for the x and y axes. The code should calculate the lifted condensation level (LCL) and plot it as a black dot, and calculate the full parcel profile and add it to the plot as a black line. It should shade areas of Convective Available Potential Energy (CAPE) and Convective Inhibition (CIN). Finally, the code should add special lines to the plot and display it.\n\n### Code:\n", "top_k": 3}, {"idx": 42, "repo_full_name": "deepmind__acme", "instruction": "Generate code that creates a CRR agent for a specified environment using the Acme library. The agent should be configured with various parameters such as batch size, evaluation period, number of demonstration episodes, random seed, learning rates, discount, target update period, and whether to use SARSA target or not. The environment and dataset names should also be configurable. The code should also include a function to add next action extras to the transitions. The main function should create the environment, get the demonstrations dataset, create the networks to optimize, create the learner, define the evaluator network, create the actor and the environment loop, and finally run the environment loop.", "prompt": "### Task:\nGenerate code that creates a CRR agent for a specified environment using the Acme library. The agent should be configured with various parameters such as batch size, evaluation period, number of demonstration episodes, random seed, learning rates, discount, target update period, and whether to use SARSA target or not. The environment and dataset names should also be configurable. The code should also include a function to add next action extras to the transitions. The main function should create the environment, get the demonstrations dataset, create the networks to optimize, create the learner, define the evaluator network, create the actor and the environment loop, and finally run the environment loop.\n\n### Code:\n", "top_k": 3}, {"idx": 43, "repo_full_name": "pysteps__pysteps", "instruction": "Generate code that performs ensemble verification of a probabilistic extrapolation nowcast using MeteoSwiss radar data. The code should read precipitation field data, upscale it to 2 km resolution, and convert it to rain rate. It should also log-transform the data and handle missing values. The code should estimate the motion field and perform an ensemble nowcast using the STEPS approach. It should then back-transform the nowcast to rain rates and plot some of the realizations. Finally, the code should verify the probabilistic forecasts using the ROC curve, reliability diagrams, and rank histograms.", "prompt": "### Task:\nGenerate code that performs ensemble verification of a probabilistic extrapolation nowcast using MeteoSwiss radar data. The code should read precipitation field data, upscale it to 2 km resolution, and convert it to rain rate. It should also log-transform the data and handle missing values. The code should estimate the motion field and perform an ensemble nowcast using the STEPS approach. It should then back-transform the nowcast to rain rates and plot some of the realizations. Finally, the code should verify the probabilistic forecasts using the ROC curve, reliability diagrams, and rank histograms.\n\n### Code:\n", "top_k": 3}, {"idx": 44, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that performs a PCB analysis using the PyAEDT library. The code should start by importing necessary libraries and setting up project files and paths. It should download a specific project file and set up a temporary project directory. Then, it should open an EDB project, create a cutout on selected nets, and export it to Q3D. The code should identify pin locations on the components to define where to assign sources and sinks for Q3D and append Z elevation. After that, it should save and close the EDB, open it in Hfss 3D Layout to generate the 3D model, and export the layout to Q3D. The code should then launch the newly created Q3D project, plot it, and assign sources and sinks on nets using the previously calculated positions. It should create a setup and a frequency sweep from DC to 2GHz, analyze the project, compute ACL and ACR solutions, plot them, and finally, release the desktop.", "prompt": "### Task:\nGenerate code that performs a PCB analysis using the PyAEDT library. The code should start by importing necessary libraries and setting up project files and paths. It should download a specific project file and set up a temporary project directory. Then, it should open an EDB project, create a cutout on selected nets, and export it to Q3D. The code should identify pin locations on the components to define where to assign sources and sinks for Q3D and append Z elevation. After that, it should save and close the EDB, open it in Hfss 3D Layout to generate the 3D model, and export the layout to Q3D. The code should then launch the newly created Q3D project, plot it, and assign sources and sinks on nets using the previously calculated positions. It should create a setup and a frequency sweep from DC to 2GHz, analyze the project, compute ACL and ACR solutions, plot them, and finally, release the desktop.\n\n### Code:\n", "top_k": 3}, {"idx": 45, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that initializes a parser with arguments for paths to the bop datasets parent directory, cc textures, output directory, and the number of scenes to generate. The code should initialize the blenderproc library and load bop objects into the scene from the 'itodd' and 'tless' datasets. It should also load BOP dataset intrinsics and set shading and hide objects. \n\nThe code should create a room using primitive planes and enable rigidbody for these planes. It should also create a light plane and a point light. The code should load cc textures and define a function to sample 6-DoF poses. \n\nThe code should enable depth rendering without antialiasing and set the maximum amount of samples for color rendering. For each scene, the code should sample bop objects, randomize materials, set physics, sample two light sources, assign a random cc texture to room planes, sample object poses, check collisions, simulate physics and fix final poses. \n\nThe code should create a BVH tree for camera obstacle checks and generate camera poses while ensuring that obstacles are at least 0.3 meter away from the camera. The code should render the pipeline and write data in bop format. After each scene, the code should disable rigidbody and hide objects.", "prompt": "### Task:\nGenerate code that initializes a parser with arguments for paths to the bop datasets parent directory, cc textures, output directory, and the number of scenes to generate. The code should initialize the blenderproc library and load bop objects into the scene from the 'itodd' and 'tless' datasets. It should also load BOP dataset intrinsics and set shading and hide objects. \n\nThe code should create a room using primitive planes and enable rigidbody for these planes. It should also create a light plane and a point light. The code should load cc textures and define a function to sample 6-DoF poses. \n\nThe code should enable depth rendering without antialiasing and set the maximum amount of samples for color rendering. For each scene, the code should sample bop objects, randomize materials, set physics, sample two light sources, assign a random cc texture to room planes, sample object poses, check collisions, simulate physics and fix final poses. \n\nThe code should create a BVH tree for camera obstacle checks and generate camera poses while ensuring that obstacles are at least 0.3 meter away from the camera. The code should render the pipeline and write data in bop format. After each scene, the code should disable rigidbody and hide objects.\n\n### Code:\n", "top_k": 3}, {"idx": 46, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that performs two tasks using the pyscf library. The first task is to transform a Full Configuration Interaction (FCI) wavefunction with respect to orbital rotation/transformation. This involves creating two molecules with different atomic configurations, calculating their FCI energies, and then transforming the wavefunction of the first molecule to match the second one. The second task is to transfer a FCI wavefunction from a smaller orbital space to a larger one. This involves creating a molecule with a specific atomic configuration, calculating its FCI energy, and then expanding the wavefunction to a larger orbital space. The code should also compare the transformed wavefunction with the one obtained from the FCI solver and check if they are close. Finally, the code should transform the FCI wavefunction using a different method and compare the results with the previous transformation.", "prompt": "### Task:\nGenerate code that performs two tasks using the pyscf library. The first task is to transform a Full Configuration Interaction (FCI) wavefunction with respect to orbital rotation/transformation. This involves creating two molecules with different atomic configurations, calculating their FCI energies, and then transforming the wavefunction of the first molecule to match the second one. The second task is to transfer a FCI wavefunction from a smaller orbital space to a larger one. This involves creating a molecule with a specific atomic configuration, calculating its FCI energy, and then expanding the wavefunction to a larger orbital space. The code should also compare the transformed wavefunction with the one obtained from the FCI solver and check if they are close. Finally, the code should transform the FCI wavefunction using a different method and compare the results with the previous transformation.\n\n### Code:\n", "top_k": 3}, {"idx": 47, "repo_full_name": "simpeg__simpeg", "instruction": "Generate code that performs a 3D DC inversion of a dipole-dipole array using the SimPEG library. The model should consist of two spheres, one conductive and the other resistive, compared to the background. The inversion should be restrained to the Core Mesh using an Active Cells mapping combined with an exponential mapping to invert in log conductivity space. The code should also include the creation of a synthetic Dipole-Dipole Survey and a Tikhonov Inversion. Finally, the code should generate a plot of the ground truth and the inverted model, both vertically and horizontally.", "prompt": "### Task:\nGenerate code that performs a 3D DC inversion of a dipole-dipole array using the SimPEG library. The model should consist of two spheres, one conductive and the other resistive, compared to the background. The inversion should be restrained to the Core Mesh using an Active Cells mapping combined with an exponential mapping to invert in log conductivity space. The code should also include the creation of a synthetic Dipole-Dipole Survey and a Tikhonov Inversion. Finally, the code should generate a plot of the ground truth and the inverted model, both vertically and horizontally.\n\n### Code:\n", "top_k": 3}, {"idx": 48, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that estimates randoms from a list mode file and compares the result with the original delayed coincidences. The code should be able to handle command-line options for the path to data files, listmode file, sinograms file prefix, randoms file, raw data template, scanning time interval, reconstruction engine, acquisition data storage scheme, and an option for non-interactive mode. The code should import necessary modules and set up the listmode-to-sinograms converter object with the appropriate input, output, and template files. It should also set the time interval and flags for storing delayed coincidences. After setting up the converter, the code should process the data, get access to the sinograms, and estimate the randoms from the delayeds via Maximum Likelihood estimation. The estimated randoms should be written to a file. The code should also copy the acquisition data into Python arrays and print out the acquisition data dimensions, total number of delayed coincidences and estimated randoms, and max values. If not in non-interactive mode, the code should display a single sinogram. The code should be wrapped in a main function and handle any errors that occur during execution.", "prompt": "### Task:\nGenerate code that estimates randoms from a list mode file and compares the result with the original delayed coincidences. The code should be able to handle command-line options for the path to data files, listmode file, sinograms file prefix, randoms file, raw data template, scanning time interval, reconstruction engine, acquisition data storage scheme, and an option for non-interactive mode. The code should import necessary modules and set up the listmode-to-sinograms converter object with the appropriate input, output, and template files. It should also set the time interval and flags for storing delayed coincidences. After setting up the converter, the code should process the data, get access to the sinograms, and estimate the randoms from the delayeds via Maximum Likelihood estimation. The estimated randoms should be written to a file. The code should also copy the acquisition data into Python arrays and print out the acquisition data dimensions, total number of delayed coincidences and estimated randoms, and max values. If not in non-interactive mode, the code should display a single sinogram. The code should be wrapped in a main function and handle any errors that occur during execution.\n\n### Code:\n", "top_k": 3}, {"idx": 49, "repo_full_name": "pyqtgraph__pyqtgraph", "instruction": "Generate code that creates a custom graph using the pyqtgraph library. The graph should be displayed in a GraphicsLayoutWidget with the title 'pyqtgraph example: CustomGraphItem'. The graph should be a subclass of GraphItem and include methods for setting data, updating the graph, handling mouse drag events, and responding to clicks on the graph. The graph should be populated with nodes at specified positions, connected by lines with specified styles, and each node should be labeled with a text. The nodes should be draggable, and the graph should update in real time as nodes are dragged. The program should print a message to the console when a node is clicked. The graph should be displayed when the script is run.", "prompt": "### Task:\nGenerate code that creates a custom graph using the pyqtgraph library. The graph should be displayed in a GraphicsLayoutWidget with the title 'pyqtgraph example: CustomGraphItem'. The graph should be a subclass of GraphItem and include methods for setting data, updating the graph, handling mouse drag events, and responding to clicks on the graph. The graph should be populated with nodes at specified positions, connected by lines with specified styles, and each node should be labeled with a text. The nodes should be draggable, and the graph should update in real time as nodes are dragged. The program should print a message to the console when a node is clicked. The graph should be displayed when the script is run.\n\n### Code:\n", "top_k": 3}, {"idx": 50, "repo_full_name": "deepmind__acme", "instruction": "Generate code that sets up and runs a behavioral cloning (BC) experiment on a specified environment using the Acme library. The code should define several flags for configuring the experiment, such as the environment name, number of demonstrations, learning steps, batch size, learning rate, dropout rate, and network parameters. It should also include functions to create a demonstration dataset factory, an environment factory, and a network factory. The network should be a multi-layer perceptron (MLP) with ReLU activation and dropout. The code should also include a function to build the experiment configuration, which uses the previously defined factories and the BC builder. Finally, the code should include a main function that builds the experiment configuration and runs the experiment, either in a distributed or single-threaded manner.", "prompt": "### Task:\nGenerate code that sets up and runs a behavioral cloning (BC) experiment on a specified environment using the Acme library. The code should define several flags for configuring the experiment, such as the environment name, number of demonstrations, learning steps, batch size, learning rate, dropout rate, and network parameters. It should also include functions to create a demonstration dataset factory, an environment factory, and a network factory. The network should be a multi-layer perceptron (MLP) with ReLU activation and dropout. The code should also include a function to build the experiment configuration, which uses the previously defined factories and the BC builder. Finally, the code should include a main function that builds the experiment configuration and runs the experiment, either in a distributed or single-threaded manner.\n\n### Code:\n", "top_k": 3}, {"idx": 51, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that performs a CCSD (Coupled Cluster with Single and Double excitations) calculation with k-point sampling for a system of two carbon atoms in a cell using the pyscf library. The code should first build the cell with the given atomic coordinates, basis, pseudopotential, lattice vectors, and unit. Then, it should perform KHF and KCCSD calculations with 2x2x2 k-points and print the total energy per unit cell. Next, it should perform KHF and KCCSD calculations for a single k-point and print the total energy per unit cell. \n\nAfterwards, the code should perform a single k-point calculation using the RHF method, run RCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the RCCSD energy based on CCSD density matrices. \n\nNext, the code should convert the RHF object to a UHF object, run UCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the UCCSD energy based on CCSD density matrices. \n\nFinally, the code should convert the UHF object to a GHF object, run GCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the GCCSD energy based on CCSD density matrices.", "prompt": "### Task:\nGenerate code that performs a CCSD (Coupled Cluster with Single and Double excitations) calculation with k-point sampling for a system of two carbon atoms in a cell using the pyscf library. The code should first build the cell with the given atomic coordinates, basis, pseudopotential, lattice vectors, and unit. Then, it should perform KHF and KCCSD calculations with 2x2x2 k-points and print the total energy per unit cell. Next, it should perform KHF and KCCSD calculations for a single k-point and print the total energy per unit cell. \n\nAfterwards, the code should perform a single k-point calculation using the RHF method, run RCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the RCCSD energy based on CCSD density matrices. \n\nNext, the code should convert the RHF object to a UHF object, run UCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the UCCSD energy based on CCSD density matrices. \n\nFinally, the code should convert the UHF object to a GHF object, run GCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the GCCSD energy based on CCSD density matrices.\n\n### Code:\n", "top_k": 3}, {"idx": 52, "repo_full_name": "vispy__vispy", "instruction": "Generate code that simulates fireworks using the vispy library in Python. The simulation should create a series of explosions that last one second, with each explosion being unique. The visualization during the explosion should be highly optimized using a Vertex Buffer Object (VBO). The code should include a class named 'Canvas' that inherits from 'app.Canvas' and includes methods for initializing the simulation, drawing the simulation, resizing the simulation, timing the simulation, and creating a new explosion. The code should also include vertex and fragment shaders written in GLSL. The simulation should be interactive and the window size should be 800x600 pixels. The code should run the simulation when executed.", "prompt": "### Task:\nGenerate code that simulates fireworks using the vispy library in Python. The simulation should create a series of explosions that last one second, with each explosion being unique. The visualization during the explosion should be highly optimized using a Vertex Buffer Object (VBO). The code should include a class named 'Canvas' that inherits from 'app.Canvas' and includes methods for initializing the simulation, drawing the simulation, resizing the simulation, timing the simulation, and creating a new explosion. The code should also include vertex and fragment shaders written in GLSL. The simulation should be interactive and the window size should be 800x600 pixels. The code should run the simulation when executed.\n\n### Code:\n", "top_k": 3}, {"idx": 53, "repo_full_name": "imsy-dkfz__simpa", "instruction": "Generate code that uses the simpa library to create a simulation of a tissue structure. The tissue structure should include a background, a muscle layer, an epidermis layer, and two blood vessels. The simulation should be set up with specific global parameters such as volume dimensions, spacing, and wavelengths. The code should also include a function to create the tissue structure with specific properties for each component. The simulation should be run for all wavelengths specified and include a linear unmixing component. The results of the simulation and the linear unmixing should be loaded and visualized.", "prompt": "### Task:\nGenerate code that uses the simpa library to create a simulation of a tissue structure. The tissue structure should include a background, a muscle layer, an epidermis layer, and two blood vessels. The simulation should be set up with specific global parameters such as volume dimensions, spacing, and wavelengths. The code should also include a function to create the tissue structure with specific properties for each component. The simulation should be run for all wavelengths specified and include a linear unmixing component. The results of the simulation and the linear unmixing should be loaded and visualized.\n\n### Code:\n", "top_k": 3}, {"idx": 54, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that uses the avalanche library to implement an online continual learning scenario with the Replay strategy. The code should first set up the necessary configurations and transformations. Then, it should create an online continual learning scenario using the MNIST dataset for training and testing. A SimpleMLP model should be created and some metrics for evaluation should be chosen. The code should then create a Replay strategy instance with a ReservoirSamplingBuffer storage policy. Finally, the code should implement a training loop where it trains on the online train stream of the scenario and evaluates on the test stream. The results of the evaluation should be stored in a list. The code should also include an argument parser to select the cuda device to use.", "prompt": "### Task:\nGenerate code that uses the avalanche library to implement an online continual learning scenario with the Replay strategy. The code should first set up the necessary configurations and transformations. Then, it should create an online continual learning scenario using the MNIST dataset for training and testing. A SimpleMLP model should be created and some metrics for evaluation should be chosen. The code should then create a Replay strategy instance with a ReservoirSamplingBuffer storage policy. Finally, the code should implement a training loop where it trains on the online train stream of the scenario and evaluates on the test stream. The results of the evaluation should be stored in a list. The code should also include an argument parser to select the cuda device to use.\n\n### Code:\n", "top_k": 3}, {"idx": 55, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a simulation using the dd4hep library in Python. The code should include a function to display help information and a main function to run the simulation. The simulation should include setting up the logger, parsing command line arguments, setting up the Geant4 kernel and detector description, configuring the user interface, loading a specific geometry, setting up the magnetic field tracking, random generator, event actions, I/O, and various generator actions. The code should also handle simulation particles, setup detectors, build the physics list, add special particle types and a global range cut. If visualization is enabled, the code should include commands for visualization. Finally, the code should configure, initialize, run, and terminate the kernel.", "prompt": "### Task:\nGenerate code that sets up a simulation using the dd4hep library in Python. The code should include a function to display help information and a main function to run the simulation. The simulation should include setting up the logger, parsing command line arguments, setting up the Geant4 kernel and detector description, configuring the user interface, loading a specific geometry, setting up the magnetic field tracking, random generator, event actions, I/O, and various generator actions. The code should also handle simulation particles, setup detectors, build the physics list, add special particle types and a global range cut. If visualization is enabled, the code should include commands for visualization. Finally, the code should configure, initialize, run, and terminate the kernel.\n\n### Code:\n", "top_k": 3}, {"idx": 56, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS150, AS151, AS152) with their respective networks and routers. AS150 should be a transit AS with four routers and three networks. AS151 and AS152 should each have a web host and a router, and they should each join a network. AS151 and AS152 should also join an internet exchange. The code should also set up BGP peering between AS150 and AS151, and between AS150 and AS152. Finally, the code should add all the layers to the emulator and dump the emulator's state to a binary file.", "prompt": "### Task:\nGenerate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS150, AS151, AS152) with their respective networks and routers. AS150 should be a transit AS with four routers and three networks. AS151 and AS152 should each have a web host and a router, and they should each join a network. AS151 and AS152 should also join an internet exchange. The code should also set up BGP peering between AS150 and AS151, and between AS150 and AS152. Finally, the code should add all the layers to the emulator and dump the emulator's state to a binary file.\n\n### Code:\n", "top_k": 3}, {"idx": 57, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates a network topology using the seedemu library. The topology should consist of three autonomous systems (AS): AS150, AS2, and AS151. AS150 and AS151 should each have one router and one network, while AS2 should have two routers and one network. AS150 and AS2 should be connected through an internet exchange (IX) 100, and AS2 and AS151 should be connected through IX 101. \n\nAdditionally, create a BGP attacker component that hijacks the prefix of AS151 and joins IX 100. Merge this component with the main simulation. \n\nFinally, establish private peering relationships: between AS150 and AS2 at IX 100, between AS151 and AS2 at IX 101, and between the attacker and AS2 at IX 100. Render and compile the simulation with Docker, managing the network internally.", "prompt": "### Task:\nGenerate code that creates a network topology using the seedemu library. The topology should consist of three autonomous systems (AS): AS150, AS2, and AS151. AS150 and AS151 should each have one router and one network, while AS2 should have two routers and one network. AS150 and AS2 should be connected through an internet exchange (IX) 100, and AS2 and AS151 should be connected through IX 101. \n\nAdditionally, create a BGP attacker component that hijacks the prefix of AS151 and joins IX 100. Merge this component with the main simulation. \n\nFinally, establish private peering relationships: between AS150 and AS2 at IX 100, between AS151 and AS2 at IX 101, and between the attacker and AS2 at IX 100. Render and compile the simulation with Docker, managing the network internally.\n\n### Code:\n", "top_k": 3}, {"idx": 58, "repo_full_name": "pytorch__torchrec", "instruction": "Generate code that imports necessary modules from the torchrec library and defines two functions: `_get_random_dataloader` and `train`. The `_get_random_dataloader` function should take in the number of embeddings, batch size, and a boolean indicating whether to pin memory, and return a DataLoader object. The `train` function should take in parameters for the number of embeddings, embedding dimension, dense architecture layer sizes, over architecture layer sizes, and learning rate. It should initialize the process group, device, rank, and backend, construct a DLRM model, enable optimizer fusion, distribute the model across devices, overlap communication, compute, and device transfer during training, and finally train the model using a training iterator. The script should call the `train` function if it is the main module.", "prompt": "### Task:\nGenerate code that imports necessary modules from the torchrec library and defines two functions: `_get_random_dataloader` and `train`. The `_get_random_dataloader` function should take in the number of embeddings, batch size, and a boolean indicating whether to pin memory, and return a DataLoader object. The `train` function should take in parameters for the number of embeddings, embedding dimension, dense architecture layer sizes, over architecture layer sizes, and learning rate. It should initialize the process group, device, rank, and backend, construct a DLRM model, enable optimizer fusion, distribute the model across devices, overlap communication, compute, and device transfer during training, and finally train the model using a training iterator. The script should call the `train` function if it is the main module.\n\n### Code:\n", "top_k": 3}, {"idx": 59, "repo_full_name": "funkelab__gunpowder", "instruction": "Generate code that imports necessary libraries and sets up logging. Then, define a function to train a model for a specified number of iterations. This function should declare array keys for raw intensities, labelled objects, per-voxel affinities, loss weights, predicted affinities, and gradients of the loss with respect to the predicted affinities. \n\nNext, the function should read a configuration file and calculate the input and output sizes in world units. It should then formulate a request for what a batch should contain and a snapshot request for inspection. \n\nThe function should then assemble a training pipeline that includes reading batches from an HDF5 file, normalizing raw data, choosing a random location for each requested batch, applying various augmentations, growing a boundary between labels, converting labels into affinities, balancing labels, pre-caching batches, performing one training iteration for each passing batch, saving the passing batch as an HDF5 file for inspection, and printing profiling stats. \n\nFinally, the function should print a statement indicating the start of training, build the pipeline, request batches for the specified number of iterations, and print a statement indicating the end of training. \n\nThe code should then call this function with a specified number of iterations when run as a main program.", "prompt": "### Task:\nGenerate code that imports necessary libraries and sets up logging. Then, define a function to train a model for a specified number of iterations. This function should declare array keys for raw intensities, labelled objects, per-voxel affinities, loss weights, predicted affinities, and gradients of the loss with respect to the predicted affinities. \n\nNext, the function should read a configuration file and calculate the input and output sizes in world units. It should then formulate a request for what a batch should contain and a snapshot request for inspection. \n\nThe function should then assemble a training pipeline that includes reading batches from an HDF5 file, normalizing raw data, choosing a random location for each requested batch, applying various augmentations, growing a boundary between labels, converting labels into affinities, balancing labels, pre-caching batches, performing one training iteration for each passing batch, saving the passing batch as an HDF5 file for inspection, and printing profiling stats. \n\nFinally, the function should print a statement indicating the start of training, build the pipeline, request batches for the specified number of iterations, and print a statement indicating the end of training. \n\nThe code should then call this function with a specified number of iterations when run as a main program.\n\n### Code:\n", "top_k": 3}, {"idx": 60, "repo_full_name": "avslab__basilisk", "instruction": "Generate code that imports necessary modules and defines a function to rerun a set or subset of Monte Carlo simulations using the Basilisk library. The function should allow for the specification of the scenario name, the number of processes to spawn, and the run numbers to rerun. It should also allow for the addition of new retention policies. The function should set up the Monte Carlo controller, specify the initial conditions directory, the archive directory, the execution count, and whether to disperse seeds or archive parameters. It should also add the specified retention policy and run the initial conditions. If the function is run as the main program, it should call itself.", "prompt": "### Task:\nGenerate code that imports necessary modules and defines a function to rerun a set or subset of Monte Carlo simulations using the Basilisk library. The function should allow for the specification of the scenario name, the number of processes to spawn, and the run numbers to rerun. It should also allow for the addition of new retention policies. The function should set up the Monte Carlo controller, specify the initial conditions directory, the archive directory, the execution count, and whether to disperse seeds or archive parameters. It should also add the specified retention policy and run the initial conditions. If the function is run as the main program, it should call itself.\n\n### Code:\n", "top_k": 3}, {"idx": 61, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that performs an iterative reconstruction with radial phase encoding (RPE) data using the SIRF library. The code should include command-line options for specifying the raw data file, path to data files, output file for simulated data, reconstruction engine, and whether to run the reconstruction if non-cartesian code was compiled. It should also include an option for specifying the trajectory type (cartesian, radial, goldenangle or grpe) and whether to show plots. The code should import the necessary engine module from the SIRF library based on the specified engine option. It should then process the command-line options and define a symmetrical operator for cg-optimisation. The code should also define a function for performing the Conjugate Gradient method, which includes computing coil sensitivity maps, setting up the acquisition model, performing backward projection, and implementing the iterative reconstruction. Finally, the code should define a main function that locates the k-space raw data file, reads the acquisition data from an HDF file, pre-processes the acquisition data, sets the trajectory, sorts the processed acquisition data, and performs the reconstruction if the relevant option is set. The code should handle any errors that occur during execution and print an appropriate error message.", "prompt": "### Task:\nGenerate code that performs an iterative reconstruction with radial phase encoding (RPE) data using the SIRF library. The code should include command-line options for specifying the raw data file, path to data files, output file for simulated data, reconstruction engine, and whether to run the reconstruction if non-cartesian code was compiled. It should also include an option for specifying the trajectory type (cartesian, radial, goldenangle or grpe) and whether to show plots. The code should import the necessary engine module from the SIRF library based on the specified engine option. It should then process the command-line options and define a symmetrical operator for cg-optimisation. The code should also define a function for performing the Conjugate Gradient method, which includes computing coil sensitivity maps, setting up the acquisition model, performing backward projection, and implementing the iterative reconstruction. Finally, the code should define a main function that locates the k-space raw data file, reads the acquisition data from an HDF file, pre-processes the acquisition data, sets the trajectory, sorts the processed acquisition data, and performs the reconstruction if the relevant option is set. The code should handle any errors that occur during execution and print an appropriate error message.\n\n### Code:\n", "top_k": 3}, {"idx": 62, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that calculates the triplet and quintet energy gap of an Iron-Porphyrin molecule using DMRG-CASSCF and DMRG-NEVPT2 methods from the pyscf library. The code should first define the DMET active space, then calculate the quintet and triplet energies separately. The active space should include the Fe double d-shell, 4s shell, and the ligand N 2pz orbitals to describe metal-ligand pi bond and pi backbond. The code should also output the active space orbitals to molden format.", "prompt": "### Task:\nGenerate code that calculates the triplet and quintet energy gap of an Iron-Porphyrin molecule using DMRG-CASSCF and DMRG-NEVPT2 methods from the pyscf library. The code should first define the DMET active space, then calculate the quintet and triplet energies separately. The active space should include the Fe double d-shell, 4s shell, and the ligand N 2pz orbitals to describe metal-ligand pi bond and pi backbond. The code should also output the active space orbitals to molden format.\n\n### Code:\n", "top_k": 3}, {"idx": 63, "repo_full_name": "ansys__pydpf-core", "instruction": "Generate code that calculates the average elemental stress on a given volume using the pydpf-core library. The code should first create a model targeting a given result file and get all node IDs in the model to find the minimum amount of surrounding elements to get a minimum volume. Then, it should read the volume by element and find the minimum list of elements by node to get the volume check. After that, the code should create a workflow to compute equivalent stress averaged on elements, apply dot product seqv.volume, sum up those on the list of elements, and divide this sum by the total volume on these elements. Finally, the code should plot equivalent elemental stress and volume averaged elemental equivalent stress, and use the operator with the same algorithm that has been implemented.", "prompt": "### Task:\nGenerate code that calculates the average elemental stress on a given volume using the pydpf-core library. The code should first create a model targeting a given result file and get all node IDs in the model to find the minimum amount of surrounding elements to get a minimum volume. Then, it should read the volume by element and find the minimum list of elements by node to get the volume check. After that, the code should create a workflow to compute equivalent stress averaged on elements, apply dot product seqv.volume, sum up those on the list of elements, and divide this sum by the total volume on these elements. Finally, the code should plot equivalent elemental stress and volume averaged elemental equivalent stress, and use the operator with the same algorithm that has been implemented.\n\n### Code:\n", "top_k": 3}, {"idx": 64, "repo_full_name": "ansys__pymapdl", "instruction": "Generate code that creates a pressure vessel using the pymapdl library. The code should start by launching MAPDL and setting the units to US Customary system using inches. Then, it should define the materials and element type, and create the geometry of the pressure vessel. After that, the code should create a mesh, apply boundary conditions and pressure, and solve the problem. The results should be post-processed to obtain the von-mises stress for the single static solution. The code should also plot the results and compare them with the results obtained from the legacy file reader. Finally, the code should stop MAPDL.", "prompt": "### Task:\nGenerate code that creates a pressure vessel using the pymapdl library. The code should start by launching MAPDL and setting the units to US Customary system using inches. Then, it should define the materials and element type, and create the geometry of the pressure vessel. After that, the code should create a mesh, apply boundary conditions and pressure, and solve the problem. The results should be post-processed to obtain the von-mises stress for the single static solution. The code should also plot the results and compare them with the results obtained from the legacy file reader. Finally, the code should stop MAPDL.\n\n### Code:\n", "top_k": 3}, {"idx": 65, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that tests radial transport of n_re with a constant scalar diffusion coefficient using the DREAM library. The code should set up a simulation with specific parameters such as initial and final temperatures, time steps, ion species, electric field, and cold electron temperature. It should also set up a radial grid, time stepper, and ions. The code should then set the E_field and cold electron temperature, and enable the hot tail grid. The code should also set up the transport settings, and finally, run the simulation. The code should also include conditions for different transport modes and whether the hot tail grid is enabled or not.", "prompt": "### Task:\nGenerate code that tests radial transport of n_re with a constant scalar diffusion coefficient using the DREAM library. The code should set up a simulation with specific parameters such as initial and final temperatures, time steps, ion species, electric field, and cold electron temperature. It should also set up a radial grid, time stepper, and ions. The code should then set the E_field and cold electron temperature, and enable the hot tail grid. The code should also set up the transport settings, and finally, run the simulation. The code should also include conditions for different transport modes and whether the hot tail grid is enabled or not.\n\n### Code:\n", "top_k": 3}, {"idx": 66, "repo_full_name": "hewlettpackard__oneview-python", "instruction": "Generate code that interacts with the OneViewClient from the hpeOneView library. The code should establish a connection to the client using a configuration file. It should then create a scope and a user with specific permissions. The code should also create multiple users with different permissions. After creating the users, the code should update the user's password, add a new role to an existing user, update the roles of a user, and remove certain roles from a user. The code should also retrieve a user by their username, retrieve all users, validate if a full name or username is already in use, get the roles associated with a user, and get users by their role. Finally, the code should delete a single user and multiple users.", "prompt": "### Task:\nGenerate code that interacts with the OneViewClient from the hpeOneView library. The code should establish a connection to the client using a configuration file. It should then create a scope and a user with specific permissions. The code should also create multiple users with different permissions. After creating the users, the code should update the user's password, add a new role to an existing user, update the roles of a user, and remove certain roles from a user. The code should also retrieve a user by their username, retrieve all users, validate if a full name or username is already in use, get the roles associated with a user, and get users by their role. Finally, the code should delete a single user and multiple users.\n\n### Code:\n", "top_k": 3}, {"idx": 67, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that uses the pyaedt library to create a 2D Extractor CPWG (coplanar waveguide with ground) design and run a simulation. The code should import the necessary libraries, set the non-graphical mode, launch AEDT and 2D Extractor, define variables, create primitives, create a signal, create a coplanar ground, create a reference ground plane, create a dielectric, create a conformal coating, assign a conductor to the signal, create a reference ground, assign the Huray model on the signal, create the setup, analyze it, plot solution data, save the project and close AEDT.", "prompt": "### Task:\nGenerate code that uses the pyaedt library to create a 2D Extractor CPWG (coplanar waveguide with ground) design and run a simulation. The code should import the necessary libraries, set the non-graphical mode, launch AEDT and 2D Extractor, define variables, create primitives, create a signal, create a coplanar ground, create a reference ground plane, create a dielectric, create a conformal coating, assign a conductor to the signal, create a reference ground, assign the Huray model on the signal, create the setup, analyze it, plot solution data, save the project and close AEDT.\n\n### Code:\n", "top_k": 3}, {"idx": 68, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that performs a restricted AGF2 calculation with density fitting using the PySCF library. The code should define a molecule with a specific atomic structure and basis, and then run a RHF calculation with a specified convergence tolerance and auxiliary basis. After running the AGF2 calculation with a specified convergence tolerance, the code should print the first three ionization potentials and electron affinities. Then, it should calculate the MO-basis density matrix and dipole moments, transforming dipole moment integrals into MO basis and adding the nuclear component. Finally, the code should print the calculated dipole moment.", "prompt": "### Task:\nGenerate code that performs a restricted AGF2 calculation with density fitting using the PySCF library. The code should define a molecule with a specific atomic structure and basis, and then run a RHF calculation with a specified convergence tolerance and auxiliary basis. After running the AGF2 calculation with a specified convergence tolerance, the code should print the first three ionization potentials and electron affinities. Then, it should calculate the MO-basis density matrix and dipole moments, transforming dipole moment integrals into MO basis and adding the nuclear component. Finally, the code should print the calculated dipole moment.\n\n### Code:\n", "top_k": 3}, {"idx": 69, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that imports necessary libraries and modules for solving a Poisson equation using the Virtual Element Method (VEM) on a polygonal mesh. The code should parse command line arguments for the degree of the VEM space, the maximum number of iterations for mesh refinement, and the adaptive parameter. It should initialize the problem with a specific PDE and domain, and set up an error matrix and a mesh.\n\nThe code should then enter a loop for the maximum number of iterations. In each iteration, it should set up the VEM space and the function, assemble the stiffness matrix and the right-hand side, apply Dirichlet boundary conditions, solve the linear system, and compute the error. It should also mark cells for refinement based on the residual estimate and adaptively refine the mesh. The loop should terminate if the number of degrees of freedom exceeds a certain threshold.\n\nAfter the loop, the code should display the error rates, save the number of degrees of freedom and the error matrix to text files, and plot the error. It should also plot the final mesh.", "prompt": "### Task:\nGenerate code that imports necessary libraries and modules for solving a Poisson equation using the Virtual Element Method (VEM) on a polygonal mesh. The code should parse command line arguments for the degree of the VEM space, the maximum number of iterations for mesh refinement, and the adaptive parameter. It should initialize the problem with a specific PDE and domain, and set up an error matrix and a mesh.\n\nThe code should then enter a loop for the maximum number of iterations. In each iteration, it should set up the VEM space and the function, assemble the stiffness matrix and the right-hand side, apply Dirichlet boundary conditions, solve the linear system, and compute the error. It should also mark cells for refinement based on the residual estimate and adaptively refine the mesh. The loop should terminate if the number of degrees of freedom exceeds a certain threshold.\n\nAfter the loop, the code should display the error rates, save the number of degrees of freedom and the error matrix to text files, and plot the error. It should also plot the final mesh.\n\n### Code:\n", "top_k": 3}, {"idx": 70, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that imports necessary modules from numpy, scipy, and porepy. The code should define four functions: add_data, write_network, main, and two test functions. \n\nThe add_data function should take a grid bucket, a domain, and a permeability factor as arguments. It should add parameters to the grid bucket such as permeability, source term, apertures, and boundary conditions. \n\nThe write_network function should take a file name as an argument and write a predefined network string to a file with that name.\n\nThe main function should take a permeability factor, a description, a boolean indicating whether the grid bucket should be coarsened, and a boolean indicating whether the results should be exported. It should define mesh parameters, a domain, and a file name. It should write a network to a file, import a 2D fracture network from the file, compute the geometry of the grid bucket, optionally coarsen the grid bucket, assign an ordering to the nodes of the grid bucket, and add data to the grid bucket. It should define solvers for flow and source, compute the right-hand side and the matrix of the linear systems, solve the linear systems, split the solution, extract the discharge and pressure from the solution, and project the discharge. If the results should be exported, it should export the pressure and the projected discharge to a vtk file.\n\nThe test_vem_blocking function should call the main function with a small permeability factor and the description \"blocking\".\n\nThe test_vem_permeable function should call the main function with a large permeability factor and the description \"permeable\".", "prompt": "### Task:\nGenerate code that imports necessary modules from numpy, scipy, and porepy. The code should define four functions: add_data, write_network, main, and two test functions. \n\nThe add_data function should take a grid bucket, a domain, and a permeability factor as arguments. It should add parameters to the grid bucket such as permeability, source term, apertures, and boundary conditions. \n\nThe write_network function should take a file name as an argument and write a predefined network string to a file with that name.\n\nThe main function should take a permeability factor, a description, a boolean indicating whether the grid bucket should be coarsened, and a boolean indicating whether the results should be exported. It should define mesh parameters, a domain, and a file name. It should write a network to a file, import a 2D fracture network from the file, compute the geometry of the grid bucket, optionally coarsen the grid bucket, assign an ordering to the nodes of the grid bucket, and add data to the grid bucket. It should define solvers for flow and source, compute the right-hand side and the matrix of the linear systems, solve the linear systems, split the solution, extract the discharge and pressure from the solution, and project the discharge. If the results should be exported, it should export the pressure and the projected discharge to a vtk file.\n\nThe test_vem_blocking function should call the main function with a small permeability factor and the description \"blocking\".\n\nThe test_vem_permeable function should call the main function with a large permeability factor and the description \"permeable\".\n\n### Code:\n", "top_k": 3}, {"idx": 71, "repo_full_name": "paddlepaddle__fastdeploy", "instruction": "Generate code that creates a command-line interface for parsing arguments related to model directory, tokenizer vocab path, inference device, runtime backend, batch size, sequence length, logging interval, and usage of FP16 mode and fast tokenizer. The code should also define a function to batchify text data. \n\nThen, create a class for sequence classification prediction using the Ernie model. This class should initialize the tokenizer and runtime, preprocess the input texts, perform inference, postprocess the inference data, and predict the output for given texts. \n\nIn the main function, the code should parse the arguments, instantiate the prediction class, batchify the text data, and predict the output for each batch of texts. The output should include the batch id, example id, input sentences, predicted label, and confidence score.", "prompt": "### Task:\nGenerate code that creates a command-line interface for parsing arguments related to model directory, tokenizer vocab path, inference device, runtime backend, batch size, sequence length, logging interval, and usage of FP16 mode and fast tokenizer. The code should also define a function to batchify text data. \n\nThen, create a class for sequence classification prediction using the Ernie model. This class should initialize the tokenizer and runtime, preprocess the input texts, perform inference, postprocess the inference data, and predict the output for given texts. \n\nIn the main function, the code should parse the arguments, instantiate the prediction class, batchify the text data, and predict the output for each batch of texts. The output should include the batch id, example id, input sentences, predicted label, and confidence score.\n\n### Code:\n", "top_k": 3}, {"idx": 72, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS) with AS numbers 150, 151, and 152. Each AS should have a host named 'web' and a router named 'router0'. The 'web' host in each AS should have a web service installed. Each AS should also have a network named 'net0' which both the 'web' host and 'router0' join. AS150 and AS152 should have a cross connection between their routers. An internet exchange with the number 100 should be created and AS150 and AS151 should be peers on this exchange. AS150 should also be a provider for AS152. The emulation should be rendered and compiled using Docker with self-managed network. The compiled emulation should be saved in the directory './cross-connect'.", "prompt": "### Task:\nGenerate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS) with AS numbers 150, 151, and 152. Each AS should have a host named 'web' and a router named 'router0'. The 'web' host in each AS should have a web service installed. Each AS should also have a network named 'net0' which both the 'web' host and 'router0' join. AS150 and AS152 should have a cross connection between their routers. An internet exchange with the number 100 should be created and AS150 and AS151 should be peers on this exchange. AS150 should also be a provider for AS152. The emulation should be rendered and compiled using Docker with self-managed network. The compiled emulation should be saved in the directory './cross-connect'.\n\n### Code:\n", "top_k": 3}, {"idx": 73, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that performs a listmode-to-sinograms conversion using the SIRF library. The code should accept command-line options for the path to data files, listmode file, output file prefix, raw data template, scanning time interval to convert, reconstruction engine, acquisition data storage scheme, and an option to disable interactive mode. The code should import the necessary modules, process the command-line options, and define a main function. \n\nIn the main function, it should set the acquisition data storage scheme, read the acquisition data template, create a listmode-to-sinograms converter object, set the input, output and template files, set the time interval, set some flags, set up the converter, perform the conversion, get access to the sinograms, copy the acquisition data into a Python array, print the acquisition data dimensions, and optionally show a 2D array of the acquisition data. \n\nThe code should also estimate randoms, convert the randoms to an array, and optionally show a 2D array of the randoms. The main function should be executed in a try-except block to handle any errors.", "prompt": "### Task:\nGenerate code that performs a listmode-to-sinograms conversion using the SIRF library. The code should accept command-line options for the path to data files, listmode file, output file prefix, raw data template, scanning time interval to convert, reconstruction engine, acquisition data storage scheme, and an option to disable interactive mode. The code should import the necessary modules, process the command-line options, and define a main function. \n\nIn the main function, it should set the acquisition data storage scheme, read the acquisition data template, create a listmode-to-sinograms converter object, set the input, output and template files, set the time interval, set some flags, set up the converter, perform the conversion, get access to the sinograms, copy the acquisition data into a Python array, print the acquisition data dimensions, and optionally show a 2D array of the acquisition data. \n\nThe code should also estimate randoms, convert the randoms to an array, and optionally show a 2D array of the randoms. The main function should be executed in a try-except block to handle any errors.\n\n### Code:\n", "top_k": 3}, {"idx": 74, "repo_full_name": "deepmind__acme", "instruction": "Generate code that sets up and runs a Continuous Q-Learning (CQL) agent on a specified environment using the Acme library. The agent should be configured with various parameters such as batch size, evaluation period, number of demonstration episodes, random seed, learning rates, and CQL specific parameters. The environment should be created using the specified environment name and a demonstrations dataset should be obtained from a specified dataset name. The agent's networks should be created and optimized, and an evaluator network should be defined. The agent should then be run in an environment loop, where it learns and evaluates itself periodically.", "prompt": "### Task:\nGenerate code that sets up and runs a Continuous Q-Learning (CQL) agent on a specified environment using the Acme library. The agent should be configured with various parameters such as batch size, evaluation period, number of demonstration episodes, random seed, learning rates, and CQL specific parameters. The environment should be created using the specified environment name and a demonstrations dataset should be obtained from a specified dataset name. The agent's networks should be created and optimized, and an evaluator network should be defined. The agent should then be run in an environment loop, where it learns and evaluates itself periodically.\n\n### Code:\n", "top_k": 3}, {"idx": 75, "repo_full_name": "oarriaga__paz", "instruction": "Generate code that defines several classes for preprocessing, augmenting, and visualizing bounding boxes and images for object detection using the paz library. The classes should include:\n\n1. `PreprocessBoxes` that preprocesses bounding boxes by matching them, encoding them, and converting the box class to a one-hot vector.\n2. `PreprocessImage` that preprocesses an RGB image by resizing it and either subtracting a mean or normalizing it.\n3. `AugmentImage` that augments an RGB image by resizing it, blending it with a random cropped background, and applying random contrast, brightness, saturation, and hue adjustments.\n4. `AugmentBoxes` that augments bounding boxes by converting them to image box coordinates, expanding them, applying random sample cropping and random flipping.\n5. `DrawBoxData2D` that draws 2D box data on an image.\n6. `ShowBoxes` that shows the boxes on an image after resizing the image, decoding the boxes, denormalizing them, and drawing them on the image.\n7. `AugmentDetection` that augments boxes and images for object detection by loading an image, applying image and box augmentation if in training mode, preprocessing the image and boxes, and wrapping the sequence.\n\nFinally, the code should also include a main section that sets up GPU memory growth, loads a model and data, and applies the `AugmentDetection` and `ShowBoxes` processors to each sample in the dataset.", "prompt": "### Task:\nGenerate code that defines several classes for preprocessing, augmenting, and visualizing bounding boxes and images for object detection using the paz library. The classes should include:\n\n1. `PreprocessBoxes` that preprocesses bounding boxes by matching them, encoding them, and converting the box class to a one-hot vector.\n2. `PreprocessImage` that preprocesses an RGB image by resizing it and either subtracting a mean or normalizing it.\n3. `AugmentImage` that augments an RGB image by resizing it, blending it with a random cropped background, and applying random contrast, brightness, saturation, and hue adjustments.\n4. `AugmentBoxes` that augments bounding boxes by converting them to image box coordinates, expanding them, applying random sample cropping and random flipping.\n5. `DrawBoxData2D` that draws 2D box data on an image.\n6. `ShowBoxes` that shows the boxes on an image after resizing the image, decoding the boxes, denormalizing them, and drawing them on the image.\n7. `AugmentDetection` that augments boxes and images for object detection by loading an image, applying image and box augmentation if in training mode, preprocessing the image and boxes, and wrapping the sequence.\n\nFinally, the code should also include a main section that sets up GPU memory growth, loads a model and data, and applies the `AugmentDetection` and `ShowBoxes` processors to each sample in the dataset.\n\n### Code:\n", "top_k": 3}, {"idx": 76, "repo_full_name": "pybamm-team__pybamm", "instruction": "Generate code that creates a custom lithium-ion model using the pybamm library. The model should be named \"my li-ion model\" and should include submodels for the external circuit, current collector, thermal, porosity, electrolyte diffusion, electrolyte conductivity, SEI, SEI on cracks, and lithium plating. For both the negative and positive electrode domains, the model should include submodels for active material, electrode potential, particle, total particle concentration, open-circuit potential, interface, interface utilisation, interface current, surface potential difference, and particle mechanics. After defining the model, build it, create the geometry, process the model and geometry, set the mesh, discretise the model, and solve it. Finally, plot the solution dynamically.", "prompt": "### Task:\nGenerate code that creates a custom lithium-ion model using the pybamm library. The model should be named \"my li-ion model\" and should include submodels for the external circuit, current collector, thermal, porosity, electrolyte diffusion, electrolyte conductivity, SEI, SEI on cracks, and lithium plating. For both the negative and positive electrode domains, the model should include submodels for active material, electrode potential, particle, total particle concentration, open-circuit potential, interface, interface utilisation, interface current, surface potential difference, and particle mechanics. After defining the model, build it, create the geometry, process the model and geometry, set the mesh, discretise the model, and solve it. Finally, plot the solution dynamically.\n\n### Code:\n", "top_k": 3}, {"idx": 77, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulator with 10 stub Autonomous Systems (AS) and hosts. Then, create an Ethereum service with saveState set to True and override set to True. Create two blockchains, one based on Proof of Work (POW) and the other on Proof of Authority (POA). For each blockchain, create four nodes and set the first two nodes of each blockchain as bootnodes and start mining on them. For the third node of each blockchain, create accounts with a certain balance. For the fourth node of each blockchain, set custom geth command options. Enable HTTP and WebSocket connections on certain nodes and set custom geth binary file on one of the nodes. Customize the display names of the nodes for visualization purposes. Bind the virtual nodes to physical nodes using filters. Add the Ethereum layer to the emulator and save the component to a file. Finally, compile the emulator with Docker and save the output to a directory.", "prompt": "### Task:\nGenerate code that creates an emulator with 10 stub Autonomous Systems (AS) and hosts. Then, create an Ethereum service with saveState set to True and override set to True. Create two blockchains, one based on Proof of Work (POW) and the other on Proof of Authority (POA). For each blockchain, create four nodes and set the first two nodes of each blockchain as bootnodes and start mining on them. For the third node of each blockchain, create accounts with a certain balance. For the fourth node of each blockchain, set custom geth command options. Enable HTTP and WebSocket connections on certain nodes and set custom geth binary file on one of the nodes. Customize the display names of the nodes for visualization purposes. Bind the virtual nodes to physical nodes using filters. Add the Ethereum layer to the emulator and save the component to a file. Finally, compile the emulator with Docker and save the output to a directory.\n\n### Code:\n", "top_k": 3}, {"idx": 78, "repo_full_name": "federatedai__fate", "instruction": "Generate code that creates a pipeline for a federated learning task using the FATE library. The pipeline should include components for reading data, transforming data, sampling, feature binning, one-hot encoding, logistic regression, local baseline model, secure boosting, and evaluation. The pipeline should be set up for a guest and a host party, with data read from specified tables for each party. The pipeline should be compiled and fitted, and the summary of the evaluation components should be printed. The main function should accept a configuration file as an argument.", "prompt": "### Task:\nGenerate code that creates a pipeline for a federated learning task using the FATE library. The pipeline should include components for reading data, transforming data, sampling, feature binning, one-hot encoding, logistic regression, local baseline model, secure boosting, and evaluation. The pipeline should be set up for a guest and a host party, with data read from specified tables for each party. The pipeline should be compiled and fitted, and the summary of the evaluation components should be printed. The main function should accept a configuration file as an argument.\n\n### Code:\n", "top_k": 3}, {"idx": 79, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a dd4hep simulation using Python configuration. The code should define a function that initializes a DDG4 kernel, loads a geometry from an XML file located in the 'OpticalSurfaces/compact' directory of the 'DD4hepExamplesINSTALL' environment variable, and imports constants from the kernel's detector description. \n\nThe function should also configure a Geant4 instance with a tracking field, event actions, detector construction, and a particle gun. The Geant4 instance should be set up with a UI, which uses a macro if provided as a command line argument. The particle gun should be set up with a gamma particle, an energy of 5 keV, and a multiplicity of 1. \n\nThe function should also set up a tracker named 'MaterialTester' and a physics list named 'QGSP_BERT'. After all configurations, the function should execute the Geant4 instance. \n\nFinally, the code should call this function if it is the main module being run.", "prompt": "### Task:\nGenerate code that sets up a dd4hep simulation using Python configuration. The code should define a function that initializes a DDG4 kernel, loads a geometry from an XML file located in the 'OpticalSurfaces/compact' directory of the 'DD4hepExamplesINSTALL' environment variable, and imports constants from the kernel's detector description. \n\nThe function should also configure a Geant4 instance with a tracking field, event actions, detector construction, and a particle gun. The Geant4 instance should be set up with a UI, which uses a macro if provided as a command line argument. The particle gun should be set up with a gamma particle, an energy of 5 keV, and a multiplicity of 1. \n\nThe function should also set up a tracker named 'MaterialTester' and a physics list named 'QGSP_BERT'. After all configurations, the function should execute the Geant4 instance. \n\nFinally, the code should call this function if it is the main module being run.\n\n### Code:\n", "top_k": 3}, {"idx": 80, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that adapts the Avalanche library to use Huggingface models and datasets for a machine translation task. The code should include a custom data collator class that pads labels and prepares decoder input ids. It should also include a class that modifies the Avalanche Naive training strategy to handle Huggingface minibatches and adapt the forward and criterion methods for machine translation tasks. The main function should load a tokenizer and a dataset, preprocess the dataset, create a sequence-to-sequence model, and set up a continual learning scenario with the Avalanche library. The model should be trained and evaluated on the continual learning benchmark.", "prompt": "### Task:\nGenerate code that adapts the Avalanche library to use Huggingface models and datasets for a machine translation task. The code should include a custom data collator class that pads labels and prepares decoder input ids. It should also include a class that modifies the Avalanche Naive training strategy to handle Huggingface minibatches and adapt the forward and criterion methods for machine translation tasks. The main function should load a tokenizer and a dataset, preprocess the dataset, create a sequence-to-sequence model, and set up a continual learning scenario with the Avalanche library. The model should be trained and evaluated on the continual learning benchmark.\n\n### Code:\n", "top_k": 3}, {"idx": 81, "repo_full_name": "unidata__metpy", "instruction": "Generate code that performs a number of calculations using sounding data from the MetPy library. The code should define a function to determine the effective inflow layer for a convective sounding, using the default values of Thompison et al. (2004) for CAPE (100 J/kg) and CIN (-250 J/kg). The code should also read in sample data, isolate needed variables and attach units, compute wind components, compute common sounding index parameters, compute the parcel profile for a surface-based parcel, compute corresponding LI, CAPE, CIN values for a surface parcel, determine the LCL, LFC, and EL for the surface parcel, compute characteristics of a mean layer parcel and the most unstable parcel, compute the Bunkers Storm Motion vector and use it to calculate the critical angle, compute the characteristics needed to compute the significant tornado parameter, compute the supercell composite parameter if possible, and finally print out the important sounding parameters.", "prompt": "### Task:\nGenerate code that performs a number of calculations using sounding data from the MetPy library. The code should define a function to determine the effective inflow layer for a convective sounding, using the default values of Thompison et al. (2004) for CAPE (100 J/kg) and CIN (-250 J/kg). The code should also read in sample data, isolate needed variables and attach units, compute wind components, compute common sounding index parameters, compute the parcel profile for a surface-based parcel, compute corresponding LI, CAPE, CIN values for a surface parcel, determine the LCL, LFC, and EL for the surface parcel, compute characteristics of a mean layer parcel and the most unstable parcel, compute the Bunkers Storm Motion vector and use it to calculate the critical angle, compute the characteristics needed to compute the significant tornado parameter, compute the supercell composite parameter if possible, and finally print out the important sounding parameters.\n\n### Code:\n", "top_k": 3}, {"idx": 82, "repo_full_name": "nvidia__nvflare", "instruction": "Generate code that defines a class named `SupervisedMonaiProstateDittoLearner` which inherits from `SupervisedMonaiProstateLearner`. This class should be used for a prostate segmentation task and should include methods for training configuration and training. The class should initialize with parameters for the training configuration filename, the number of training epochs of the global model for a round, the number of training epochs of the personalized model for a round, and the name of the task to train the model. The training configuration method should initialize a `SupervisedPTDittoHelper` with a `UNet` model and an `SGD` optimizer. The training method should include a pipeline for Ditto training, which includes getting global model weights, preparing for fedprox loss, loading Ditto personalized model info, local training of the reference model and personalized model, and returning updated weights of the reference model. The training method should also handle abort signals and exceptions.", "prompt": "### Task:\nGenerate code that defines a class named `SupervisedMonaiProstateDittoLearner` which inherits from `SupervisedMonaiProstateLearner`. This class should be used for a prostate segmentation task and should include methods for training configuration and training. The class should initialize with parameters for the training configuration filename, the number of training epochs of the global model for a round, the number of training epochs of the personalized model for a round, and the name of the task to train the model. The training configuration method should initialize a `SupervisedPTDittoHelper` with a `UNet` model and an `SGD` optimizer. The training method should include a pipeline for Ditto training, which includes getting global model weights, preparing for fedprox loss, loading Ditto personalized model info, local training of the reference model and personalized model, and returning updated weights of the reference model. The training method should also handle abort signals and exceptions.\n\n### Code:\n", "top_k": 3}, {"idx": 83, "repo_full_name": "oarriaga__paz", "instruction": "Generate code that downloads an image and performs various image and box augmentations using the paz library. The code should include the creation of sequential pipelines for image and box augmentation, as well as preprocessing. The image augmentation pipeline should include random contrast, brightness, saturation, and hue adjustments. The box augmentation pipeline should include conversion to image box coordinates, expansion, random sample cropping, and random flipping of boxes left and right. The code should also include a pipeline for drawing boxes and a pipeline for preprocessing boxes, which includes matching boxes to a set of default boxes, encoding them, and expanding the class label to a one-hot vector. Finally, the code should put everything together in a single processor and demonstrate the image and box augmentations. The code should also include a sequence generator for processing batches of data.", "prompt": "### Task:\nGenerate code that downloads an image and performs various image and box augmentations using the paz library. The code should include the creation of sequential pipelines for image and box augmentation, as well as preprocessing. The image augmentation pipeline should include random contrast, brightness, saturation, and hue adjustments. The box augmentation pipeline should include conversion to image box coordinates, expansion, random sample cropping, and random flipping of boxes left and right. The code should also include a pipeline for drawing boxes and a pipeline for preprocessing boxes, which includes matching boxes to a set of default boxes, encoding them, and expanding the class label to a one-hot vector. Finally, the code should put everything together in a single processor and demonstrate the image and box augmentations. The code should also include a sequence generator for processing batches of data.\n\n### Code:\n", "top_k": 3}, {"idx": 84, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that constructs Maximally Localized Wannier Functions (MLWFs) using the pywannier90 tool from the pyscf library. The code should define a unit cell, perform a PBE calculation, save and load the kks object, construct MLWFs, export the MLWFs in xsf format for plotting, export certain matrices and run a wannier90 using these, interpolate the Fock or band structure using the Slater-Koster scheme, print the difference in the eigenvalues interpolated by scf.get_bands function and by pywannier90, and plot the band structure using mcu.", "prompt": "### Task:\nGenerate code that constructs Maximally Localized Wannier Functions (MLWFs) using the pywannier90 tool from the pyscf library. The code should define a unit cell, perform a PBE calculation, save and load the kks object, construct MLWFs, export the MLWFs in xsf format for plotting, export certain matrices and run a wannier90 using these, interpolate the Fock or band structure using the Slater-Koster scheme, print the difference in the eigenvalues interpolated by scf.get_bands function and by pywannier90, and plot the band structure using mcu.\n\n### Code:\n", "top_k": 3}, {"idx": 85, "repo_full_name": "labsn__expyfun", "instruction": "Generate code that uses the expyfun library to prepare and run an experiment using the CRM corpus. The experiment should prepare two talkers with different genders and the same talker number at a sampling rate of 40000 Hz. It should then print the valid callsigns and read a sentence from the hard drive. The code should also preload all the talkers and get a second sentence from memory. The two sentences should be padded and aligned at the start. The experiment should be run with a specific name, window size, participant, session, and version. It should display a text prompt on the screen, load the padded sentences into the buffer, start the stimulus, and wait for a specific duration. The code should also include a response menu and display a prompt based on the response. Finally, it should plot a screenshot of the experiment.", "prompt": "### Task:\nGenerate code that uses the expyfun library to prepare and run an experiment using the CRM corpus. The experiment should prepare two talkers with different genders and the same talker number at a sampling rate of 40000 Hz. It should then print the valid callsigns and read a sentence from the hard drive. The code should also preload all the talkers and get a second sentence from memory. The two sentences should be padded and aligned at the start. The experiment should be run with a specific name, window size, participant, session, and version. It should display a text prompt on the screen, load the padded sentences into the buffer, start the stimulus, and wait for a specific duration. The code should also include a response menu and display a prompt based on the response. Finally, it should plot a screenshot of the experiment.\n\n### Code:\n", "top_k": 3}, {"idx": 86, "repo_full_name": "pyqtgraph__pyqtgraph", "instruction": "Generate code that creates a scatter plot using the pyqtgraph library. The scatter plot should demonstrate a variety of features. The code should create a main window and a graphics layout widget. Four plots should be added to the widget, each demonstrating a different way of drawing scatter plots. \n\nThe first plot should have all spots identical and transform-invariant. The second plot should have spots that are transform-invariant, but not identical. The third plot should have spots that are not transform-invariant and not identical. The fourth plot should test the performance of large scatterplots. \n\nAll plots should be clickable, and the clicked points should be highlighted. The code should also generate random data for the plots. The application should be executed if the script is run as the main program.", "prompt": "### Task:\nGenerate code that creates a scatter plot using the pyqtgraph library. The scatter plot should demonstrate a variety of features. The code should create a main window and a graphics layout widget. Four plots should be added to the widget, each demonstrating a different way of drawing scatter plots. \n\nThe first plot should have all spots identical and transform-invariant. The second plot should have spots that are transform-invariant, but not identical. The third plot should have spots that are not transform-invariant and not identical. The fourth plot should test the performance of large scatterplots. \n\nAll plots should be clickable, and the clicked points should be highlighted. The code should also generate random data for the plots. The application should be executed if the script is run as the main program.\n\n### Code:\n", "top_k": 3}, {"idx": 87, "repo_full_name": "ansys__pymapdl", "instruction": "Generate code that uses the pymapdl library to create contact elements for general contact. The code should first launch MAPDL, enter the pre-processor, create a block and mesh it with tetrahedral elements. Then, it should create a second volume block above the existing one and mesh it with quadratic hexahedral elements, ensuring that the blocks do not touch. The code should then select all the elements at the intersection between the two blocks and generate contact elements. Finally, it should plot the contact element pairs as a wire-frame to show that the contact pairs overlap, and then stop MAPDL.", "prompt": "### Task:\nGenerate code that uses the pymapdl library to create contact elements for general contact. The code should first launch MAPDL, enter the pre-processor, create a block and mesh it with tetrahedral elements. Then, it should create a second volume block above the existing one and mesh it with quadratic hexahedral elements, ensuring that the blocks do not touch. The code should then select all the elements at the intersection between the two blocks and generate contact elements. Finally, it should plot the contact element pairs as a wire-frame to show that the contact pairs overlap, and then stop MAPDL.\n\n### Code:\n", "top_k": 3}, {"idx": 88, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulator base with 10 Stub AS and 3 hosts per stub AS using the seedemu library. Then, create an Ethereum service layer and a sub-layer of it, a blockchain with the name \"pos\" and consensus mechanism set to POS. Set the terminal total difficulty of the blockchain to 30. \n\nFor each host in the AS, create a blockchain virtual node, a Docker container label, and enable Geth to communicate with the geth node via http. Set specific hosts as BeaconSetupNode, BootNode, and validator nodes with different conditions. Also, customize the display names of the nodes and bind the virtual node to the physical node. \n\nFinally, add the Ethereum layer to the emulator, render it, and compile it with Docker with internetMap and etherView enabled. The output should be saved in the './output' directory and existing files should be overridden.", "prompt": "### Task:\nGenerate code that creates an emulator base with 10 Stub AS and 3 hosts per stub AS using the seedemu library. Then, create an Ethereum service layer and a sub-layer of it, a blockchain with the name \"pos\" and consensus mechanism set to POS. Set the terminal total difficulty of the blockchain to 30. \n\nFor each host in the AS, create a blockchain virtual node, a Docker container label, and enable Geth to communicate with the geth node via http. Set specific hosts as BeaconSetupNode, BootNode, and validator nodes with different conditions. Also, customize the display names of the nodes and bind the virtual node to the physical node. \n\nFinally, add the Ethereum layer to the emulator, render it, and compile it with Docker with internetMap and etherView enabled. The output should be saved in the './output' directory and existing files should be overridden.\n\n### Code:\n", "top_k": 3}, {"idx": 89, "repo_full_name": "federatedai__fate", "instruction": "Generate code that creates a pipeline for a machine learning task using the FATE library. The pipeline should include the following steps: reading data, transforming data, scaling features, training a logistic regression model, and evaluating the model. The pipeline should be set up to handle a multi-party computation scenario with a guest, a host, and an arbiter. The data for the guest and host should be read from specified tables. The logistic regression model should be configured with specific parameters including penalty, optimizer, tolerance, alpha, maximum iterations, early stopping criteria, batch size, learning rate, decay, initialization method, and cross-validation parameters. After the pipeline is compiled and fitted, selected components should be deployed. A prediction pipeline should be created by adding the data reader and selected components from the training pipeline. The prediction pipeline should be compiled and used to make predictions. The DSL and configuration of the prediction pipeline should be saved as JSON files. Finally, the summaries of the logistic regression and evaluation components should be printed. The main function should accept a configuration file as an argument.", "prompt": "### Task:\nGenerate code that creates a pipeline for a machine learning task using the FATE library. The pipeline should include the following steps: reading data, transforming data, scaling features, training a logistic regression model, and evaluating the model. The pipeline should be set up to handle a multi-party computation scenario with a guest, a host, and an arbiter. The data for the guest and host should be read from specified tables. The logistic regression model should be configured with specific parameters including penalty, optimizer, tolerance, alpha, maximum iterations, early stopping criteria, batch size, learning rate, decay, initialization method, and cross-validation parameters. After the pipeline is compiled and fitted, selected components should be deployed. A prediction pipeline should be created by adding the data reader and selected components from the training pipeline. The prediction pipeline should be compiled and used to make predictions. The DSL and configuration of the prediction pipeline should be saved as JSON files. Finally, the summaries of the logistic regression and evaluation components should be printed. The main function should accept a configuration file as an argument.\n\n### Code:\n", "top_k": 3}, {"idx": 90, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that sets up a self-consistent fluid DREAM run, where no kinetic equations are solved, but the electric field and temperature are evolved self-consistently. The code should import necessary modules and classes from the DREAM library, set up the simulation parameters, set up the radial grid, set the time stepper, add ions, set the electric field and cold electron temperature, set up the hot tail grid, disable the runaway grid, set the solver type and its parameters, and include other necessary settings. The code should then save these settings to an HDF5 file and run the simulation. After the initial run, the code should restart the simulation twice, each time loading the settings from the output of the previous run, adjusting the time stepper, and saving the new settings to an HDF5 file before running the simulation again.", "prompt": "### Task:\nGenerate code that sets up a self-consistent fluid DREAM run, where no kinetic equations are solved, but the electric field and temperature are evolved self-consistently. The code should import necessary modules and classes from the DREAM library, set up the simulation parameters, set up the radial grid, set the time stepper, add ions, set the electric field and cold electron temperature, set up the hot tail grid, disable the runaway grid, set the solver type and its parameters, and include other necessary settings. The code should then save these settings to an HDF5 file and run the simulation. After the initial run, the code should restart the simulation twice, each time loading the settings from the output of the previous run, adjusting the time stepper, and saving the new settings to an HDF5 file before running the simulation again.\n\n### Code:\n", "top_k": 3}, {"idx": 91, "repo_full_name": "rstudio__py-shiny", "instruction": "Generate code that creates a CPU usage monitoring application using the py-shiny library. The application should be able to run both in a standard Python environment and in a Pyodide environment. It should use the psutil library to get CPU usage data, but if running in Pyodide, it should use a fake version of psutil. The application should display the CPU usage data in a graphical format using matplotlib and in a tabular format using pandas. The user interface should allow the user to select the colormap for the graphs, clear the history of CPU usage data, freeze the output, and specify the number of samples per graph and the number of rows to display in the table. The application should also include a function to hide ticks on a graph's axis.", "prompt": "### Task:\nGenerate code that creates a CPU usage monitoring application using the py-shiny library. The application should be able to run both in a standard Python environment and in a Pyodide environment. It should use the psutil library to get CPU usage data, but if running in Pyodide, it should use a fake version of psutil. The application should display the CPU usage data in a graphical format using matplotlib and in a tabular format using pandas. The user interface should allow the user to select the colormap for the graphs, clear the history of CPU usage data, freeze the output, and specify the number of samples per graph and the number of rows to display in the table. The application should also include a function to hide ticks on a graph's axis.\n\n### Code:\n", "top_k": 3}, {"idx": 92, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that uses the PyAEDT library to create an antenna array in HFSS. The code should perform the following tasks:\n\n1. Import necessary modules.\n2. Set the non-graphical mode.\n3. Download a 3D component needed for the example.\n4. Launch HFSS and save the project with a unique name.\n5. Read the array definition from a JSON file and load a 3D component into the dictionary from a specified path.\n6. Set up a simulation and analyze it.\n7. Get far field data after the simulation completes.\n8. Generate a contour plot.\n9. Generate 2D cutout plots.\n10. Generate 3D polar plots in Matplotlib.\n11. Generate 3D plots in PyVista.\n12. Release AEDT at the end.", "prompt": "### Task:\nGenerate code that uses the PyAEDT library to create an antenna array in HFSS. The code should perform the following tasks:\n\n1. Import necessary modules.\n2. Set the non-graphical mode.\n3. Download a 3D component needed for the example.\n4. Launch HFSS and save the project with a unique name.\n5. Read the array definition from a JSON file and load a 3D component into the dictionary from a specified path.\n6. Set up a simulation and analyze it.\n7. Get far field data after the simulation completes.\n8. Generate a contour plot.\n9. Generate 2D cutout plots.\n10. Generate 3D polar plots in Matplotlib.\n11. Generate 3D plots in PyVista.\n12. Release AEDT at the end.\n\n### Code:\n", "top_k": 3}, {"idx": 93, "repo_full_name": "burnysc2__python-sc2", "instruction": "Generate code that creates a StarCraft II bot using the python-sc2 library. The bot should be designed to play as the Protoss race and follow a strategy of building three bases and three stargates. The bot should manage resources, build structures, train units, and engage in combat. It should use chrono boost on active nexuses, attack with all workers if no nexuses are left, and attack the enemy with void rays when there are more than five. The bot should also manage worker distribution, build pylons when supply is low, train probes on undersaturated nexuses, expand when there are less than three nexuses, build a gateway and a cybernetics core, build gas near completed nexuses, and build stargates when there are less than three but at least three nexuses. The bot should also train void rays at idle stargates when there are at least three townhalls. The bot should be run on the \"(2)CatalystLE\" map against an easy difficulty Protoss computer opponent.", "prompt": "### Task:\nGenerate code that creates a StarCraft II bot using the python-sc2 library. The bot should be designed to play as the Protoss race and follow a strategy of building three bases and three stargates. The bot should manage resources, build structures, train units, and engage in combat. It should use chrono boost on active nexuses, attack with all workers if no nexuses are left, and attack the enemy with void rays when there are more than five. The bot should also manage worker distribution, build pylons when supply is low, train probes on undersaturated nexuses, expand when there are less than three nexuses, build a gateway and a cybernetics core, build gas near completed nexuses, and build stargates when there are less than three but at least three nexuses. The bot should also train void rays at idle stargates when there are at least three townhalls. The bot should be run on the \"(2)CatalystLE\" map against an easy difficulty Protoss computer opponent.\n\n### Code:\n", "top_k": 3}, {"idx": 94, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that performs a SIwave DCIR analysis in HFSS 3D Layout using the pyaedt library. The code should first configure EDB for DCIR analysis by creating a temporary directory and downloading an example board into EDB. Then, it should create pin groups on VRM positive and negative pins, and create a voltage source between these pin groups. Similarly, it should create pin groups on sink component positive and negative pins, and place a current source between these pin groups. After that, it should add a SIwave DCIR analysis, save and close EDB. The code should then launch AEDT, import the configured EDB and analyze DCIR. It should also retrieve and print the loop resistance, current source, and via information from the DCIR element data, as well as the voltage from the DCIR solution data. Finally, the code should close the AEDT project and release the desktop.", "prompt": "### Task:\nGenerate code that performs a SIwave DCIR analysis in HFSS 3D Layout using the pyaedt library. The code should first configure EDB for DCIR analysis by creating a temporary directory and downloading an example board into EDB. Then, it should create pin groups on VRM positive and negative pins, and create a voltage source between these pin groups. Similarly, it should create pin groups on sink component positive and negative pins, and place a current source between these pin groups. After that, it should add a SIwave DCIR analysis, save and close EDB. The code should then launch AEDT, import the configured EDB and analyze DCIR. It should also retrieve and print the loop resistance, current source, and via information from the DCIR element data, as well as the voltage from the DCIR solution data. Finally, the code should close the AEDT project and release the desktop.\n\n### Code:\n", "top_k": 3}, {"idx": 95, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that uses the Avalanche library for a question answering task on the SQuAD dataset using the T5 model from HuggingFace's transformers library. The code should include a custom class that extends Avalanche's Naive class to adapt it for machine translation tasks. The main function should load the SQuAD dataset, preprocess it, and divide it into training and validation sets. It should then initialize the T5 model, set up the continual learning scenario with Avalanche, and train the model on the training set. Finally, the code should test the model by asking it a question and printing the model's answer.", "prompt": "### Task:\nGenerate code that uses the Avalanche library for a question answering task on the SQuAD dataset using the T5 model from HuggingFace's transformers library. The code should include a custom class that extends Avalanche's Naive class to adapt it for machine translation tasks. The main function should load the SQuAD dataset, preprocess it, and divide it into training and validation sets. It should then initialize the T5 model, set up the continual learning scenario with Avalanche, and train the model on the training set. Finally, the code should test the model by asking it a question and printing the model's answer.\n\n### Code:\n", "top_k": 3}, {"idx": 96, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that imports necessary libraries and modules from the porepy library. The code should define two functions, `add_data_darcy` and `add_data_advection`, which add data to a given grid bucket (`gb`) and domain with a specified tolerance (`tol`). The `add_data_darcy` function should add parameters related to Darcy's law, including permeability, source, aperture, and boundary conditions. The `add_data_advection` function should add parameters related to advection, including source, porosity, discharge, and boundary conditions. \n\nThe code should then define variables for tolerance, export folder, time, number of time steps, time step size, export frequency, and a boolean for coarsening. It should also define a dictionary for mesh size and a dictionary for domain boundaries. \n\nThe code should import a grid from a CSV file, compute its geometry, coarsen it if necessary, and assign node ordering. It should then create a Darcy solver, add Darcy data to the grid bucket, solve the Darcy problem, and extract and project the discharge and pressure. It should also compute the total flow rate and export the results to a VTK file.\n\nThe code should then define variables for physics, create advection and mass matrix solvers, add advection data to the grid bucket, and add a time step property to the grid bucket. It should then create matrices and right-hand sides for the advection and mass matrix problems, and perform an LU factorization to speed up the solver. \n\nThe code should then initialize a solution vector and arrays for time and production, and loop over the time steps to update the solution, compute the production, and export the solution to a VTK file every specified number of time steps. Finally, it should export the time steps to a PVD file and save the times and absolute production values to a text file.", "prompt": "### Task:\nGenerate code that imports necessary libraries and modules from the porepy library. The code should define two functions, `add_data_darcy` and `add_data_advection`, which add data to a given grid bucket (`gb`) and domain with a specified tolerance (`tol`). The `add_data_darcy` function should add parameters related to Darcy's law, including permeability, source, aperture, and boundary conditions. The `add_data_advection` function should add parameters related to advection, including source, porosity, discharge, and boundary conditions. \n\nThe code should then define variables for tolerance, export folder, time, number of time steps, time step size, export frequency, and a boolean for coarsening. It should also define a dictionary for mesh size and a dictionary for domain boundaries. \n\nThe code should import a grid from a CSV file, compute its geometry, coarsen it if necessary, and assign node ordering. It should then create a Darcy solver, add Darcy data to the grid bucket, solve the Darcy problem, and extract and project the discharge and pressure. It should also compute the total flow rate and export the results to a VTK file.\n\nThe code should then define variables for physics, create advection and mass matrix solvers, add advection data to the grid bucket, and add a time step property to the grid bucket. It should then create matrices and right-hand sides for the advection and mass matrix problems, and perform an LU factorization to speed up the solver. \n\nThe code should then initialize a solution vector and arrays for time and production, and loop over the time steps to update the solution, compute the production, and export the solution to a VTK file every specified number of time steps. Finally, it should export the time steps to a PVD file and save the times and absolute production values to a text file.\n\n### Code:\n", "top_k": 3}, {"idx": 97, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that performs scatter estimation in PET imaging using the SIRF library. The code should accept command-line arguments for the raw data file, randoms data file, attenuation correction factors file, path to normalization and attenuation files, normalization file, attenuation image file, output prefix for scatter estimates, and a non-interactive mode. The code should then process these options, set up the scatter estimator, and perform the scatter estimation. If the non-interactive mode is not set, the code should display the scatter estimate and plot a sinogram profile. The code should also handle any errors that occur during the process.", "prompt": "### Task:\nGenerate code that performs scatter estimation in PET imaging using the SIRF library. The code should accept command-line arguments for the raw data file, randoms data file, attenuation correction factors file, path to normalization and attenuation files, normalization file, attenuation image file, output prefix for scatter estimates, and a non-interactive mode. The code should then process these options, set up the scatter estimator, and perform the scatter estimation. If the non-interactive mode is not set, the code should display the scatter estimate and plot a sinogram profile. The code should also handle any errors that occur during the process.\n\n### Code:\n", "top_k": 3}, {"idx": 98, "repo_full_name": "pyomo__mpi-sppy", "instruction": "Generate code that imports necessary modules and functions from the 'hydro' and 'mpisppy' libraries. The code should define a function to parse arguments and create a configuration object with various arguments. Then, define a main function that calls the argument parsing function, checks the length of the branching factors, and creates node names from these factors. The main function should also create scenario names, set up a scenario creator and denouement, and prepare the necessary arguments for the 'vanilla' cylinders. Depending on the configuration, the main function should also set up spokes for the Lagrangian bound and xhat looper bound. The function should then create a wheel spinner with the hub and spoke dictionaries, spin the wheel, and print the best inner and outer bounds. If a certain condition is met, the function should write the first stage and full tree solutions. Finally, the code should call the main function if it is the main module.", "prompt": "### Task:\nGenerate code that imports necessary modules and functions from the 'hydro' and 'mpisppy' libraries. The code should define a function to parse arguments and create a configuration object with various arguments. Then, define a main function that calls the argument parsing function, checks the length of the branching factors, and creates node names from these factors. The main function should also create scenario names, set up a scenario creator and denouement, and prepare the necessary arguments for the 'vanilla' cylinders. Depending on the configuration, the main function should also set up spokes for the Lagrangian bound and xhat looper bound. The function should then create a wheel spinner with the hub and spoke dictionaries, spin the wheel, and print the best inner and outer bounds. If a certain condition is met, the function should write the first stage and full tree solutions. Finally, the code should call the main function if it is the main module.\n\n### Code:\n", "top_k": 3}, {"idx": 99, "repo_full_name": "zulko__moviepy", "instruction": "Generate code that performs the following tasks using the moviepy library:\n\n1. Checks if the required video files exist in the current directory. If not, it attempts to download them using the youtube-dl command-line tool. If the download fails, it outputs an error message and exits the program.\n2. Loads an audio file, extracts a subclip from it, and applies fade-in and fade-out effects. It then analyzes the audio to find its period.\n3. Loads a video file, extracts a subclip from it, and crops it. It then analyzes the video to find a segment that loops well.\n4. Extracts the looping segment from the video, slows it down to match the audio tempo, and makes it loop for the duration of the audio. It then creates a mirrored version of this segment.\n5. Combines the original and mirrored video segments side by side, applies fade-in and fade-out effects, and adds the audio to the video.\n6. Creates a title screen with text overlay on the video and a credits screen with text on a black background.\n7. Concatenates the title screen, video, and credits screen into a final video.\n8. Writes the final video to a file with specified fps, audio bitrate, and bitrate.", "prompt": "### Task:\nGenerate code that performs the following tasks using the moviepy library:\n\n1. Checks if the required video files exist in the current directory. If not, it attempts to download them using the youtube-dl command-line tool. If the download fails, it outputs an error message and exits the program.\n2. Loads an audio file, extracts a subclip from it, and applies fade-in and fade-out effects. It then analyzes the audio to find its period.\n3. Loads a video file, extracts a subclip from it, and crops it. It then analyzes the video to find a segment that loops well.\n4. Extracts the looping segment from the video, slows it down to match the audio tempo, and makes it loop for the duration of the audio. It then creates a mirrored version of this segment.\n5. Combines the original and mirrored video segments side by side, applies fade-in and fade-out effects, and adds the audio to the video.\n6. Creates a title screen with text overlay on the video and a credits screen with text on a black background.\n7. Concatenates the title screen, video, and credits screen into a final video.\n8. Writes the final video to a file with specified fps, audio bitrate, and bitrate.\n\n### Code:\n", "top_k": 3}, {"idx": 100, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a simulation using the dd4hep library. The code should initialize a kernel and load a geometry from an XML file located in the environment's 'DD4hepExamplesINSTALL' directory. It should import constants from the kernel's detector description and set up a Geant4 instance with a tracker. The code should also configure the user interface, tracking field, and event actions. It should set up a particle gun with a gamma particle, energy of 5 keV, and multiplicity of 1. The code should also set up a tracker and a physics list, adding various particle groups and processes. Finally, the code should execute the Geant4 instance.", "prompt": "### Task:\nGenerate code that sets up a simulation using the dd4hep library. The code should initialize a kernel and load a geometry from an XML file located in the environment's 'DD4hepExamplesINSTALL' directory. It should import constants from the kernel's detector description and set up a Geant4 instance with a tracker. The code should also configure the user interface, tracking field, and event actions. It should set up a particle gun with a gamma particle, energy of 5 keV, and multiplicity of 1. The code should also set up a tracker and a physics list, adding various particle groups and processes. Finally, the code should execute the Geant4 instance.\n\n### Code:\n", "top_k": 3}, {"idx": 101, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that performs the following tasks using the porepy library:\n\n1. Import necessary modules and define two functions, `add_data` and `plot_over_line`. The `add_data` function should define the permeability, apertures, and boundary conditions for a given grid bucket and domain. The `plot_over_line` function should plot values over a line in a grid bucket.\n\n2. Set a tolerance value and define mesh size parameters and a domain.\n\n3. Import a grid bucket from a CSV file, compute its geometry, coarsen it, and assign node ordering.\n\n4. Use the `add_data` function to assign parameters to the grid bucket.\n\n5. Define a solver using the DualVEMMixDim class for flow, compute the matrix and right-hand side of the system, and solve it.\n\n6. Split the solution, extract the discharge and pressure, and project the discharge.\n\n7. Export the grid bucket to a VTK file, including the pressure and discharge.\n\n8. Define a bounding box and a number of points, and create two sets of points along the x and y axes.\n\n9. Use the `plot_over_line` function to plot the pressure along these lines and save the results to CSV files.\n\n10. Print the diameter of the grid bucket and the number of cells in 2D and 1D.", "prompt": "### Task:\nGenerate code that performs the following tasks using the porepy library:\n\n1. Import necessary modules and define two functions, `add_data` and `plot_over_line`. The `add_data` function should define the permeability, apertures, and boundary conditions for a given grid bucket and domain. The `plot_over_line` function should plot values over a line in a grid bucket.\n\n2. Set a tolerance value and define mesh size parameters and a domain.\n\n3. Import a grid bucket from a CSV file, compute its geometry, coarsen it, and assign node ordering.\n\n4. Use the `add_data` function to assign parameters to the grid bucket.\n\n5. Define a solver using the DualVEMMixDim class for flow, compute the matrix and right-hand side of the system, and solve it.\n\n6. Split the solution, extract the discharge and pressure, and project the discharge.\n\n7. Export the grid bucket to a VTK file, including the pressure and discharge.\n\n8. Define a bounding box and a number of points, and create two sets of points along the x and y axes.\n\n9. Use the `plot_over_line` function to plot the pressure along these lines and save the results to CSV files.\n\n10. Print the diameter of the grid bucket and the number of cells in 2D and 1D.\n\n### Code:\n", "top_k": 3}, {"idx": 102, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seedemu library. The emulation should include three layers: Base, Routing, and Ebgp. It should create multiple autonomous systems, each with their own hosts and routers. The routers should join different networks. The autonomous systems should be connected through internet exchanges. The code should also define a function to create a stub autonomous system with a specified ASN and exchange. The function should create hosts and a router for the autonomous system, and join them to a network. The router should also join the specified exchange. The code should also add private peering relationships between different autonomous systems. Finally, the code should add the layers to the emulator and dump the emulator state to a binary file.", "prompt": "### Task:\nGenerate code that creates an emulation using the seedemu library. The emulation should include three layers: Base, Routing, and Ebgp. It should create multiple autonomous systems, each with their own hosts and routers. The routers should join different networks. The autonomous systems should be connected through internet exchanges. The code should also define a function to create a stub autonomous system with a specified ASN and exchange. The function should create hosts and a router for the autonomous system, and join them to a network. The router should also join the specified exchange. The code should also add private peering relationships between different autonomous systems. Finally, the code should add the layers to the emulator and dump the emulator state to a binary file.\n\n### Code:\n", "top_k": 3}, {"idx": 103, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that creates a project in Maxwell 2D using the PyAEDT library and runs a transient simulation. The code should import necessary libraries, set non-graphical mode, insert a Maxwell 2D design and save the project. It should create a rectangle and duplicate it, create an air region, assign windings to the sheets and a balloon to the air region, and plot the model. The code should also create a transient setup, create a rectangular plot, solve the model, create output and plot it using PyVista, generate the same plot outside AEDT, and finally close AEDT.", "prompt": "### Task:\nGenerate code that creates a project in Maxwell 2D using the PyAEDT library and runs a transient simulation. The code should import necessary libraries, set non-graphical mode, insert a Maxwell 2D design and save the project. It should create a rectangle and duplicate it, create an air region, assign windings to the sheets and a balloon to the air region, and plot the model. The code should also create a transient setup, create a rectangular plot, solve the model, create output and plot it using PyVista, generate the same plot outside AEDT, and finally close AEDT.\n\n### Code:\n", "top_k": 3}, {"idx": 104, "repo_full_name": "dfki-ric__pytransform3d", "instruction": "Generate code that visualizes a wrench applied to a 6-DOF robot arm. The wrench is assumed to be measured by a force/torque sensor at the tool center point (TCP) of the robot arm due to a spherical mass. The code should include a function to plot the transformation about and along a screw axis, which represents the wrench. The wrench is then transformed from the TCP to the robot's base frame using the adjoint representation of the transformation. The transformed wrench has a force component and a torque component, which are also visualized as a screw. The code should also load a robot model from a URDF file, set joint angles, and plot the robot model and the transformations. The visualization should include the robot arm, the TCP, the spherical mass, and the wrench in both the TCP frame and the base frame. The code should be able to save the visualization as an image. The pytransform3d library is used for transformations and visualizations.", "prompt": "### Task:\nGenerate code that visualizes a wrench applied to a 6-DOF robot arm. The wrench is assumed to be measured by a force/torque sensor at the tool center point (TCP) of the robot arm due to a spherical mass. The code should include a function to plot the transformation about and along a screw axis, which represents the wrench. The wrench is then transformed from the TCP to the robot's base frame using the adjoint representation of the transformation. The transformed wrench has a force component and a torque component, which are also visualized as a screw. The code should also load a robot model from a URDF file, set joint angles, and plot the robot model and the transformations. The visualization should include the robot arm, the TCP, the spherical mass, and the wrench in both the TCP frame and the base frame. The code should be able to save the visualization as an image. The pytransform3d library is used for transformations and visualizations.\n\n### Code:\n", "top_k": 3}, {"idx": 105, "repo_full_name": "pyvista__pyvista", "instruction": "Generate code that creates and plots various parametric geometric objects using the pyvista library. The objects to be created and plotted include a Supertoroid, an Ellipsoid, a Partial Parametric Ellipsoid, a Pseudosphere, a Bohemian Dome, a Bour, a Boy's Surface, a Catalan Minimal, a Conic Spiral, a Cross Cap, a Dini, an Enneper, a Figure-8 Klein, a Henneberg, a Klein, a Kuen, a Mobius, a Plucker Conoid, Random Hills, a Roman, a Super Ellipsoid, a Torus, a Circular Arc, and an Extruded Half Arc. The objects should be plotted with light blue color where applicable. For the Partial Parametric Ellipsoid, a specific plotting direction should be used. For the Enneper, the plotting position should be \"yz\". For the Circular Arc and the Extruded Half Arc, specific points and a center should be defined. The Extruded Half Arc should be extruded in the z direction and its edges should be shown in the plot.", "prompt": "### Task:\nGenerate code that creates and plots various parametric geometric objects using the pyvista library. The objects to be created and plotted include a Supertoroid, an Ellipsoid, a Partial Parametric Ellipsoid, a Pseudosphere, a Bohemian Dome, a Bour, a Boy's Surface, a Catalan Minimal, a Conic Spiral, a Cross Cap, a Dini, an Enneper, a Figure-8 Klein, a Henneberg, a Klein, a Kuen, a Mobius, a Plucker Conoid, Random Hills, a Roman, a Super Ellipsoid, a Torus, a Circular Arc, and an Extruded Half Arc. The objects should be plotted with light blue color where applicable. For the Partial Parametric Ellipsoid, a specific plotting direction should be used. For the Enneper, the plotting position should be \"yz\". For the Circular Arc and the Extruded Half Arc, specific points and a center should be defined. The Extruded Half Arc should be extruded in the z direction and its edges should be shown in the plot.\n\n### Code:\n", "top_k": 3}, {"idx": 106, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seed-emulator library. The emulation should include base, routing, eBGP, iBGP, OSPF, and web service layers. It should define a function to create a stub autonomous system with a web server and a router that join a network and an internet exchange. The code should create three internet exchanges and multiple stub autonomous systems that join these exchanges. It should also create two autonomous systems with routers that join different networks and internet exchanges. The code should define private peerings between different autonomous systems. Finally, it should add a BGP attacker component that hijacks certain prefixes and joins an internet exchange. The code should merge the BGP attacker with the emulator and render the new emulator. The code should compile the new emulator using Docker and output the result to a specified directory.", "prompt": "### Task:\nGenerate code that creates an emulation using the seed-emulator library. The emulation should include base, routing, eBGP, iBGP, OSPF, and web service layers. It should define a function to create a stub autonomous system with a web server and a router that join a network and an internet exchange. The code should create three internet exchanges and multiple stub autonomous systems that join these exchanges. It should also create two autonomous systems with routers that join different networks and internet exchanges. The code should define private peerings between different autonomous systems. Finally, it should add a BGP attacker component that hijacks certain prefixes and joins an internet exchange. The code should merge the BGP attacker with the emulator and render the new emulator. The code should compile the new emulator using Docker and output the result to a specified directory.\n\n### Code:\n", "top_k": 3}, {"idx": 107, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that performs the following tasks using the porepy library:\n\n1. Define a function to add data to a given grid bucket. This function should define the permeability, apertures, and boundary conditions for each grid in the bucket. It should also assign coupling permeability for each edge in the grid bucket.\n\n2. Define a function to write a network of points to a CSV file. The network should be defined as a string and written to the file.\n\n3. Define a main function that takes in a permeability factor, a description, and a mesh size. This function should create a grid bucket from the CSV file, compute its geometry, and optionally generate a coarse grid. It should then assign parameters to the grid bucket, solve a system of equations using the DualVEMMixDim solver, and extract and project the solution. The function should also export the results to a VTK file and print out some information about the grid bucket.\n\n4. Define two functions, one for blocking and one for permeable scenarios. Each function should call the main function with different permeability factors and mesh sizes.\n\n5. Finally, call the two functions for blocking and permeable scenarios.", "prompt": "### Task:\nGenerate code that performs the following tasks using the porepy library:\n\n1. Define a function to add data to a given grid bucket. This function should define the permeability, apertures, and boundary conditions for each grid in the bucket. It should also assign coupling permeability for each edge in the grid bucket.\n\n2. Define a function to write a network of points to a CSV file. The network should be defined as a string and written to the file.\n\n3. Define a main function that takes in a permeability factor, a description, and a mesh size. This function should create a grid bucket from the CSV file, compute its geometry, and optionally generate a coarse grid. It should then assign parameters to the grid bucket, solve a system of equations using the DualVEMMixDim solver, and extract and project the solution. The function should also export the results to a VTK file and print out some information about the grid bucket.\n\n4. Define two functions, one for blocking and one for permeable scenarios. Each function should call the main function with different permeability factors and mesh sizes.\n\n5. Finally, call the two functions for blocking and permeable scenarios.\n\n### Code:\n", "top_k": 3}, {"idx": 108, "repo_full_name": "nanophotonics__nplab", "instruction": "Generate code that creates an experiment using the nplab library. The experiment should involve a shutter and a spectrometer. The experiment should open and close the shutter, wait for a specified amount of time, and then take a spectrum. The experiment should also have a user interface that allows the user to control the irradiation time and wait time. The code should also include a GUI for the experiment that includes a data browser, spectrometer controls, and shutter controls. The experiment and its GUI should be tested using dummy spectrometer and shutter.", "prompt": "### Task:\nGenerate code that creates an experiment using the nplab library. The experiment should involve a shutter and a spectrometer. The experiment should open and close the shutter, wait for a specified amount of time, and then take a spectrum. The experiment should also have a user interface that allows the user to control the irradiation time and wait time. The code should also include a GUI for the experiment that includes a data browser, spectrometer controls, and shutter controls. The experiment and its GUI should be tested using dummy spectrometer and shutter.\n\n### Code:\n", "top_k": 3}, {"idx": 109, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that demonstrates the use of the parallelized CCSD with K-point sampling in the pyscf library. The code should create a supercell composed of replicated units and run a molecular Hartree-Fock program using integrals between periodic gaussians. It should then call a molecular CC method for gamma point calculation and perform k-point calculations for the same system. The code should also calculate the differences between gamma/k-point mean-field, ccsd, ip-eomccsd, and ea-eomccsd calculations and print these differences.", "prompt": "### Task:\nGenerate code that demonstrates the use of the parallelized CCSD with K-point sampling in the pyscf library. The code should create a supercell composed of replicated units and run a molecular Hartree-Fock program using integrals between periodic gaussians. It should then call a molecular CC method for gamma point calculation and perform k-point calculations for the same system. The code should also calculate the differences between gamma/k-point mean-field, ccsd, ip-eomccsd, and ea-eomccsd calculations and print these differences.\n\n### Code:\n", "top_k": 3}, {"idx": 110, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that performs a few steps of steepest ascent for the maximization of Poisson log-likelihood objective function using subset gradients. The code should allow the user to specify the reconstruction engine, raw data file, path to data files, number of steepest descent steps, whether to use locally optimal steepest ascent, verbosity, and whether to show plots or not. The code should import the specified engine module from the sirf library, process the command-line options, and define a function to truncate the image. The main function should create an acquisition model, read PET acquisition data from the specified file, create a filter that zeroes the image outside a cylinder of the same diameter as the image xy-section size, create an initial image estimate, create an objective function of Poisson logarithmic likelihood type compatible with the acquisition data type, and perform the steepest descent steps. If anything goes wrong, the code should catch and display the error information.", "prompt": "### Task:\nGenerate code that performs a few steps of steepest ascent for the maximization of Poisson log-likelihood objective function using subset gradients. The code should allow the user to specify the reconstruction engine, raw data file, path to data files, number of steepest descent steps, whether to use locally optimal steepest ascent, verbosity, and whether to show plots or not. The code should import the specified engine module from the sirf library, process the command-line options, and define a function to truncate the image. The main function should create an acquisition model, read PET acquisition data from the specified file, create a filter that zeroes the image outside a cylinder of the same diameter as the image xy-section size, create an initial image estimate, create an objective function of Poisson logarithmic likelihood type compatible with the acquisition data type, and perform the steepest descent steps. If anything goes wrong, the code should catch and display the error information.\n\n### Code:\n", "top_k": 3}, {"idx": 111, "repo_full_name": "hiddensymmetries__simsopt", "instruction": "Generate code that solves a FOCUS-like Stage II coil optimization problem for finite build coils using the simsopt library. The code should approximate each finite build coil using a multifilament approach and model the multifilament pack. The objective function should be defined as a combination of the squared flux, curve length penalty, and coil-to-coil distance penalty. The code should also include the initialization of the boundary magnetic surface, creation of equally spaced curves and multifilament grid, application of stellarator and rotation symmetries, and definition of the Biot-Savart law. The code should perform a Taylor test and run the optimization using the L-BFGS-B method from scipy.optimize. The output should be saved in VTK format.", "prompt": "### Task:\nGenerate code that solves a FOCUS-like Stage II coil optimization problem for finite build coils using the simsopt library. The code should approximate each finite build coil using a multifilament approach and model the multifilament pack. The objective function should be defined as a combination of the squared flux, curve length penalty, and coil-to-coil distance penalty. The code should also include the initialization of the boundary magnetic surface, creation of equally spaced curves and multifilament grid, application of stellarator and rotation symmetries, and definition of the Biot-Savart law. The code should perform a Taylor test and run the optimization using the L-BFGS-B method from scipy.optimize. The output should be saved in VTK format.\n\n### Code:\n", "top_k": 3}, {"idx": 112, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a dd4hep simulation using Python configuration. The code should import necessary modules and set up logging. It should define a function that runs the simulation. In this function, it should import additional modules, set up the kernel, load the geometry from a file, import constants, and configure the Geant4 interface. It should also set up the tracking field, event actions, and the particle gun. The code should handle simulation particles, build the physics list, and start the engine. If the script is run as the main program, it should call the function to run the simulation.", "prompt": "### Task:\nGenerate code that sets up a dd4hep simulation using Python configuration. The code should import necessary modules and set up logging. It should define a function that runs the simulation. In this function, it should import additional modules, set up the kernel, load the geometry from a file, import constants, and configure the Geant4 interface. It should also set up the tracking field, event actions, and the particle gun. The code should handle simulation particles, build the physics list, and start the engine. If the script is run as the main program, it should call the function to run the simulation.\n\n### Code:\n", "top_k": 3}, {"idx": 113, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that initializes a parser with three arguments: the path to a blend file, the path to a haven directory, and the output directory. Then, initialize the blenderproc library and load the blend file into the scene. Set a random hdri from the haven directory as the background. Define a point light, set its location and energy level. Compute a point of interest and sample five camera poses around it. Enable normal and depth rendering, render the pipeline, and write the data to a .hdf5 container in the specified output directory.", "prompt": "### Task:\nGenerate code that initializes a parser with three arguments: the path to a blend file, the path to a haven directory, and the output directory. Then, initialize the blenderproc library and load the blend file into the scene. Set a random hdri from the haven directory as the background. Define a point light, set its location and energy level. Compute a point of interest and sample five camera poses around it. Enable normal and depth rendering, render the pipeline, and write the data to a .hdf5 container in the specified output directory.\n\n### Code:\n", "top_k": 3}, {"idx": 114, "repo_full_name": "nucypher__nucypher", "instruction": "Generate code that sets up a secure data sharing policy using the nucypher library. The code should perform the following tasks:\n\n1. Set up logging and environment variables for the Ethereum RPC endpoint, wallet filepath, and Alice's Ethereum address.\n2. Connect to the Ethereum provider and layer 2 provider.\n3. Unlock Alice's Ethereum wallet using a password.\n4. Set up Alice's payment method using the SubscriptionManagerPayment class.\n5. Create an instance of Alice with her Ethereum address, signer, domain, Ethereum provider URI, and payment method.\n6. Start Alice's learning loop.\n7. Create a policy label and get the policy public key associated with the label.\n8. Generate heart rate samples using a heart monitor and save them as a file.\n9. Get the public keys of the recipient (Doctor) and create an instance of Bob with these keys.\n10. Set up policy details such as the policy expiration date and m-out-of-n shares.\n11. Grant access to Bob by creating a policy and sending it to the NuCypher network.\n12. Store additional information about the policy in a JSON file.", "prompt": "### Task:\nGenerate code that sets up a secure data sharing policy using the nucypher library. The code should perform the following tasks:\n\n1. Set up logging and environment variables for the Ethereum RPC endpoint, wallet filepath, and Alice's Ethereum address.\n2. Connect to the Ethereum provider and layer 2 provider.\n3. Unlock Alice's Ethereum wallet using a password.\n4. Set up Alice's payment method using the SubscriptionManagerPayment class.\n5. Create an instance of Alice with her Ethereum address, signer, domain, Ethereum provider URI, and payment method.\n6. Start Alice's learning loop.\n7. Create a policy label and get the policy public key associated with the label.\n8. Generate heart rate samples using a heart monitor and save them as a file.\n9. Get the public keys of the recipient (Doctor) and create an instance of Bob with these keys.\n10. Set up policy details such as the policy expiration date and m-out-of-n shares.\n11. Grant access to Bob by creating a policy and sending it to the NuCypher network.\n12. Store additional information about the policy in a JSON file.\n\n### Code:\n", "top_k": 3}, {"idx": 115, "repo_full_name": "1200wd__bitcoinlib", "instruction": "Generate code that imports all functions from the encoding module of the bitcoinlib library. The code should then define a list of examples for base conversion, each example being a tuple of values to be converted, the base of the original value, and the base to which it should be converted. The code should then iterate over this list, printing each example and its result after conversion using the change_base function from the imported module.\n\nNext, the code should demonstrate the conversion of Bitcoin addresses to public key hashes. It should define a list of Bitcoin addresses, iterate over this list, and for each address, print the address and its corresponding public key hash obtained using the addr_to_pubkeyhash function.\n\nThe code should then demonstrate the conversion from public key hashes to Bitcoin addresses by calling the pubkeyhash_to_addr function with specific public key hashes and printing the results.\n\nThe code should also demonstrate the creation of a public key hash from a redeem script by calling the hash160 and to_bytes functions on a given redeem script and printing the hexadecimal string representation of the result.\n\nThe code should then convert a DER encoded signature to a different format using the convert_der_sig function and print the result.\n\nNext, the code should demonstrate the conversion of an integer to a varbyte integer and back using the int_to_varbyteint and varbyteint_to_int functions, respectively, and print the results.\n\nThe code should then normalize a list of data using the normalize_string and normalize_var functions and print the results.\n\nFinally, the code should demonstrate the conversion of a Bech32 address to a public key hash using the addr_bech32_to_pubkeyhash function and the conversion of a public key hash to a Bech32 address using the pubkeyhash_to_addr_bech32 function, and print the results.", "prompt": "### Task:\nGenerate code that imports all functions from the encoding module of the bitcoinlib library. The code should then define a list of examples for base conversion, each example being a tuple of values to be converted, the base of the original value, and the base to which it should be converted. The code should then iterate over this list, printing each example and its result after conversion using the change_base function from the imported module.\n\nNext, the code should demonstrate the conversion of Bitcoin addresses to public key hashes. It should define a list of Bitcoin addresses, iterate over this list, and for each address, print the address and its corresponding public key hash obtained using the addr_to_pubkeyhash function.\n\nThe code should then demonstrate the conversion from public key hashes to Bitcoin addresses by calling the pubkeyhash_to_addr function with specific public key hashes and printing the results.\n\nThe code should also demonstrate the creation of a public key hash from a redeem script by calling the hash160 and to_bytes functions on a given redeem script and printing the hexadecimal string representation of the result.\n\nThe code should then convert a DER encoded signature to a different format using the convert_der_sig function and print the result.\n\nNext, the code should demonstrate the conversion of an integer to a varbyte integer and back using the int_to_varbyteint and varbyteint_to_int functions, respectively, and print the results.\n\nThe code should then normalize a list of data using the normalize_string and normalize_var functions and print the results.\n\nFinally, the code should demonstrate the conversion of a Bech32 address to a public key hash using the addr_bech32_to_pubkeyhash function and the conversion of a public key hash to a Bech32 address using the pubkeyhash_to_addr_bech32 function, and print the results.\n\n### Code:\n", "top_k": 3}, {"idx": 116, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that trains and evaluates a model on the CLEAR benchmark using the Avalanche library. The code should define a set of hyperparameters, create a learning rate scheduler, and define a main function. In the main function, it should initialize a ResNet18 model, define normalization and transformation operations for the training and testing data, and set up logging to Tensorboard, a text file, and stdout. It should also define an evaluation plugin with various metrics. Depending on the evaluation protocol, it should set a seed value and create a CLEAR benchmark. The code should then move the model to the appropriate device, define an SGD optimizer, and create a learning rate scheduler. It should also define a continual learning strategy using the Naive method from Avalanche. The code should then run a training loop, saving the model after each experience and evaluating it on the test stream. Finally, it should generate an accuracy matrix and compute the CLEAR metrics, logging these results to a text file.", "prompt": "### Task:\nGenerate code that trains and evaluates a model on the CLEAR benchmark using the Avalanche library. The code should define a set of hyperparameters, create a learning rate scheduler, and define a main function. In the main function, it should initialize a ResNet18 model, define normalization and transformation operations for the training and testing data, and set up logging to Tensorboard, a text file, and stdout. It should also define an evaluation plugin with various metrics. Depending on the evaluation protocol, it should set a seed value and create a CLEAR benchmark. The code should then move the model to the appropriate device, define an SGD optimizer, and create a learning rate scheduler. It should also define a continual learning strategy using the Naive method from Avalanche. The code should then run a training loop, saving the model after each experience and evaluating it on the test stream. Finally, it should generate an accuracy matrix and compute the CLEAR metrics, logging these results to a text file.\n\n### Code:\n", "top_k": 3}, {"idx": 117, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that uses the blenderproc library to load a 3D scene from an .obj file and texture files, specified by command line arguments. The code should initialize the blenderproc library, load the scene, and label its objects based on a provided mapping. It should then separate walls, floors, and ceilings into distinct objects and assign them appropriate labels. The code should also make lamp and ceiling objects emit light. It should then create a bounding volume hierarchy (BVH) tree containing all objects in the scene. The code should sample camera locations and rotations above the floor, ensuring there are no obstacles in front of the camera and that the scene coverage score is not too low. If these conditions are met, the camera pose should be added. The code should enable normal, depth, and segmentation rendering. Finally, it should render the scene and write the data to a .hdf5 file in a specified output directory.", "prompt": "### Task:\nGenerate code that uses the blenderproc library to load a 3D scene from an .obj file and texture files, specified by command line arguments. The code should initialize the blenderproc library, load the scene, and label its objects based on a provided mapping. It should then separate walls, floors, and ceilings into distinct objects and assign them appropriate labels. The code should also make lamp and ceiling objects emit light. It should then create a bounding volume hierarchy (BVH) tree containing all objects in the scene. The code should sample camera locations and rotations above the floor, ensuring there are no obstacles in front of the camera and that the scene coverage score is not too low. If these conditions are met, the camera pose should be added. The code should enable normal, depth, and segmentation rendering. Finally, it should render the scene and write the data to a .hdf5 file in a specified output directory.\n\n### Code:\n", "top_k": 3}, {"idx": 118, "repo_full_name": "stfc__psyclone", "instruction": "Generate code that imports necessary transformations and constants from the 'psyclone' library. The code should define several boolean variables to control the application of different transformations. Then, define a function that applies a series of transformations to a given 'psy' object. The transformations should include redundant computation, asynchronous halo exchanges, OpenMP colouring, and intrinsic inlining. The function should iterate over all invokes in the 'psy' object and apply the transformations according to the defined boolean variables. The function should also handle TransformationErrors. Finally, the function should return the transformed 'psy' object.", "prompt": "### Task:\nGenerate code that imports necessary transformations and constants from the 'psyclone' library. The code should define several boolean variables to control the application of different transformations. Then, define a function that applies a series of transformations to a given 'psy' object. The transformations should include redundant computation, asynchronous halo exchanges, OpenMP colouring, and intrinsic inlining. The function should iterate over all invokes in the 'psy' object and apply the transformations according to the defined boolean variables. The function should also handle TransformationErrors. Finally, the function should return the transformed 'psy' object.\n\n### Code:\n", "top_k": 3}, {"idx": 119, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation environment using the seed-emulator library. The environment should include a ransomware service, a Tor service, and a DNS layer. \n\nFor the ransomware service, create a ransomware attacker and 16 ransomware victims. The attacker should be installed on a host in an autonomous system and should not support botnet or Tor. The victims should be installed on hosts and should not support botnet. \n\nFor the Tor service, create different types of Tor nodes including directory authorities, clients, relays, exits, and a hidden service. The hidden service should be linked to the ransomware attacker. \n\nFor the DNS layer, create a root server, TLD and ccTLD servers, second-level zone servers, and a local DNS server. The servers should have appropriate zones and records. \n\nFinally, compile the emulator using a Docker compiler with custom base images for the victim and attacker nodes. Copy necessary files to the output directory and make a script executable.", "prompt": "### Task:\nGenerate code that creates an emulation environment using the seed-emulator library. The environment should include a ransomware service, a Tor service, and a DNS layer. \n\nFor the ransomware service, create a ransomware attacker and 16 ransomware victims. The attacker should be installed on a host in an autonomous system and should not support botnet or Tor. The victims should be installed on hosts and should not support botnet. \n\nFor the Tor service, create different types of Tor nodes including directory authorities, clients, relays, exits, and a hidden service. The hidden service should be linked to the ransomware attacker. \n\nFor the DNS layer, create a root server, TLD and ccTLD servers, second-level zone servers, and a local DNS server. The servers should have appropriate zones and records. \n\nFinally, compile the emulator using a Docker compiler with custom base images for the victim and attacker nodes. Copy necessary files to the output directory and make a script executable.\n\n### Code:\n", "top_k": 3}, {"idx": 120, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that imports necessary modules and defines variables for maximum iterations, theta, and k from command line arguments. The code should initialize a simplified friction problem on a halfedge polygon mesh using the fealpy library. It should then create a loop for the maximum number of iterations, where in each iteration, it solves the problem, calculates residuals and high order terms, and saves the results and error data to a file. The code should also plot the mesh and save it as an image file. If the current iteration is not the last one, the code should refine the mesh based on the residuals. After the loop, the code should save the final error data to a file and display a multi-rate plot.", "prompt": "### Task:\nGenerate code that imports necessary modules and defines variables for maximum iterations, theta, and k from command line arguments. The code should initialize a simplified friction problem on a halfedge polygon mesh using the fealpy library. It should then create a loop for the maximum number of iterations, where in each iteration, it solves the problem, calculates residuals and high order terms, and saves the results and error data to a file. The code should also plot the mesh and save it as an image file. If the current iteration is not the last one, the code should refine the mesh based on the residuals. After the loop, the code should save the final error data to a file and display a multi-rate plot.\n\n### Code:\n", "top_k": 3}, {"idx": 121, "repo_full_name": "bokeh__bokeh", "instruction": "Generate code that creates a Bokeh application to visualize population data. The application should include two plots: a population pyramid and a line chart showing known and predicted population values. The population pyramid should be divided into male and female sections, and the line chart should differentiate between known and predicted values. The application should allow users to select a year and a location, and the plots should update based on these selections. The application should be served using Bokeh server and the layout should be saved into an HTML file named \"widget.html\". The application should continue running until it is manually stopped.", "prompt": "### Task:\nGenerate code that creates a Bokeh application to visualize population data. The application should include two plots: a population pyramid and a line chart showing known and predicted population values. The population pyramid should be divided into male and female sections, and the line chart should differentiate between known and predicted values. The application should allow users to select a year and a location, and the plots should update based on these selections. The application should be served using Bokeh server and the layout should be saved into an HTML file named \"widget.html\". The application should continue running until it is manually stopped.\n\n### Code:\n", "top_k": 3}, {"idx": 122, "repo_full_name": "federatedai__fate", "instruction": "Generate code that uses the fate library to create a pipeline for a binary classification task using a heterogenous neural network (HeteroNN). The pipeline should include the following components: a Reader to read the training data, a DataTransform to preprocess the data, an Intersection to find the common instances between the guest and host data, a HeteroNN for the model training, and an Evaluation for model evaluation. The HeteroNN should be configured with specific parameters such as epochs, learning rate, batch size, and task type. The neural network should include a guest bottom model, a guest top model, a host bottom model, and an interactive layer. The models should be defined using the torch library and added to the HeteroNN. The HeteroNN should be compiled with a specific optimizer and loss function. The pipeline should be compiled and fitted with the training data. Finally, the summary of the HeteroNN component should be printed. The code should also include a main function that accepts a configuration file as an argument and runs the pipeline.", "prompt": "### Task:\nGenerate code that uses the fate library to create a pipeline for a binary classification task using a heterogenous neural network (HeteroNN). The pipeline should include the following components: a Reader to read the training data, a DataTransform to preprocess the data, an Intersection to find the common instances between the guest and host data, a HeteroNN for the model training, and an Evaluation for model evaluation. The HeteroNN should be configured with specific parameters such as epochs, learning rate, batch size, and task type. The neural network should include a guest bottom model, a guest top model, a host bottom model, and an interactive layer. The models should be defined using the torch library and added to the HeteroNN. The HeteroNN should be compiled with a specific optimizer and loss function. The pipeline should be compiled and fitted with the training data. Finally, the summary of the HeteroNN component should be printed. The code should also include a main function that accepts a configuration file as an argument and runs the pipeline.\n\n### Code:\n", "top_k": 3}, {"idx": 123, "repo_full_name": "manimcommunity__manim", "instruction": "Generate code that creates several scenes using the manim library. The first scene should display a LaTeX title and a mathematical equation, then transform the title and fade out the equation. Afterwards, it should create a grid, display a title for it, apply a non-linear function to the grid, and transform the grid title. The second scene should create a square, transform it into a circle, and then fade it out. The third scene should create a square and apply a pointwise function to it. The fourth scene should display a text and a mathematical equation. The fifth scene should create a square and a decimal number that updates its position and value based on the square's position. The sixth scene should create several shapes and a large pi symbol, and then spiral them in and fade them out. The last scene should create three triangles with different line joints.", "prompt": "### Task:\nGenerate code that creates several scenes using the manim library. The first scene should display a LaTeX title and a mathematical equation, then transform the title and fade out the equation. Afterwards, it should create a grid, display a title for it, apply a non-linear function to the grid, and transform the grid title. The second scene should create a square, transform it into a circle, and then fade it out. The third scene should create a square and apply a pointwise function to it. The fourth scene should display a text and a mathematical equation. The fifth scene should create a square and a decimal number that updates its position and value based on the square's position. The sixth scene should create several shapes and a large pi symbol, and then spiral them in and fade them out. The last scene should create three triangles with different line joints.\n\n### Code:\n", "top_k": 3}, {"idx": 124, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that performs the following tasks using the pyaedt library:\n\n1. Creates a temporary folder and prints its path.\n2. Downloads an example file into the temporary folder.\n3. Sets the non-graphical mode and launches AEDT in graphical mode using SI units.\n4. Initializes AEDT and launches HFSS 3D Layout.\n5. If the AEDT file already exists, it removes it and saves the project in the temporary folder.\n6. Prints the boundaries from the setups object.\n7. Hides all nets and then makes only two specified nets visible.\n8. Plots the two specified nets.\n9. Makes all layers visible.\n10. Changes the color of a specified layer.\n11. Disables the visibility of components for the top and bottom layers.\n12. Fits all so that all can be visualized.\n13. Closes the project and releases the desktop.", "prompt": "### Task:\nGenerate code that performs the following tasks using the pyaedt library:\n\n1. Creates a temporary folder and prints its path.\n2. Downloads an example file into the temporary folder.\n3. Sets the non-graphical mode and launches AEDT in graphical mode using SI units.\n4. Initializes AEDT and launches HFSS 3D Layout.\n5. If the AEDT file already exists, it removes it and saves the project in the temporary folder.\n6. Prints the boundaries from the setups object.\n7. Hides all nets and then makes only two specified nets visible.\n8. Plots the two specified nets.\n9. Makes all layers visible.\n10. Changes the color of a specified layer.\n11. Disables the visibility of components for the top and bottom layers.\n12. Fits all so that all can be visualized.\n13. Closes the project and releases the desktop.\n\n### Code:\n", "top_k": 3}, {"idx": 125, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that creates a cell using the pyscf.pbc.gto.Cell() function, with specified atom positions, basis, pseudo, a, unit, and verbosity. Then, perform KHF and KMP2 calculations with 2x2x2 k-points, and print the KMP2 energy per unit cell. Repeat the KHF and KMP2 calculations for a single k-point calculation. Then, perform a single k-point calculation using the RHF method, and print the RMP2 energy per unit cell at the k-point. Also, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Convert the RHF object to UHF and GHF objects, and for each, perform a UMP2 and GMP2 calculation respectively, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Print the UMP2 and GMP2 energy per unit cell at the k-point, and the total energy based on the MP2 density matrices.", "prompt": "### Task:\nGenerate code that creates a cell using the pyscf.pbc.gto.Cell() function, with specified atom positions, basis, pseudo, a, unit, and verbosity. Then, perform KHF and KMP2 calculations with 2x2x2 k-points, and print the KMP2 energy per unit cell. Repeat the KHF and KMP2 calculations for a single k-point calculation. Then, perform a single k-point calculation using the RHF method, and print the RMP2 energy per unit cell at the k-point. Also, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Convert the RHF object to UHF and GHF objects, and for each, perform a UMP2 and GMP2 calculation respectively, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Print the UMP2 and GMP2 energy per unit cell at the k-point, and the total energy based on the MP2 density matrices.\n\n### Code:\n", "top_k": 3}, {"idx": 126, "repo_full_name": "nvidia__nvflare", "instruction": "Generate code that defines a class named `SupervisedMonaiProstateDittoLearner` which inherits from `SupervisedMonaiProstateLearner`. This class should have an initializer that accepts parameters for the training configuration filename, the number of aggregation epochs, the number of ditto model epochs, and the training task name. The initializer should also set up a `SupervisedPTDittoHelper` instance.\n\nThe class should have a `train_config` method that initializes the superclass and sets up a `UNet` model and an `Adam` optimizer for the `SupervisedPTDittoHelper` instance.\n\nThe class should also have a `train` method that performs a training task pipeline for Ditto. This method should handle abort signals, update local model weights with received weights, load Ditto personalized model, perform local training on the reference model and personalized model, validate the Ditto model each round, compute the delta model, and return a shareable object with the updated local model.", "prompt": "### Task:\nGenerate code that defines a class named `SupervisedMonaiProstateDittoLearner` which inherits from `SupervisedMonaiProstateLearner`. This class should have an initializer that accepts parameters for the training configuration filename, the number of aggregation epochs, the number of ditto model epochs, and the training task name. The initializer should also set up a `SupervisedPTDittoHelper` instance.\n\nThe class should have a `train_config` method that initializes the superclass and sets up a `UNet` model and an `Adam` optimizer for the `SupervisedPTDittoHelper` instance.\n\nThe class should also have a `train` method that performs a training task pipeline for Ditto. This method should handle abort signals, update local model weights with received weights, load Ditto personalized model, perform local training on the reference model and personalized model, validate the Ditto model each round, compute the delta model, and return a shareable object with the updated local model.\n\n### Code:\n", "top_k": 3}, {"idx": 127, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that calculates the coupling matrix for singlet energy transfer (SET) and triplet energy transfer (TET) between two molecules using the pyscf library. The code should perform CIS calculations for the excited states of two molecules, calculate the intermolecular 2e integrals, transform these integrals to MO basis, and compute the J-type and K-type coupling. The code should also include functions to compute the Coulomb integrals and exchange integrals across the two molecules, and to evaluate the coupling term including J, K and DFT XC contributions. The code should finally evaluate the overall coupling term.", "prompt": "### Task:\nGenerate code that calculates the coupling matrix for singlet energy transfer (SET) and triplet energy transfer (TET) between two molecules using the pyscf library. The code should perform CIS calculations for the excited states of two molecules, calculate the intermolecular 2e integrals, transform these integrals to MO basis, and compute the J-type and K-type coupling. The code should also include functions to compute the Coulomb integrals and exchange integrals across the two molecules, and to evaluate the coupling term including J, K and DFT XC contributions. The code should finally evaluate the overall coupling term.\n\n### Code:\n", "top_k": 3}, {"idx": 128, "repo_full_name": "jchanvfx__nodegraphqt", "instruction": "Generate code that defines a set of functions to manipulate a node graph using the nodegraphqt library. The functions should allow to zoom in, zoom out, reset zoom, set layout direction to horizontal or vertical, open, import, save, and clear a session, clear undo history, copy, cut, paste, delete, extract, clear connections of, select all, clear selection of, invert selection of, disable, duplicate, expand group of nodes, fit the zoom level to selected nodes, show undo list, set pipe style to curved, straight or angled, set background grid to none, dots or lines, auto layout nodes downstream or upstream, and toggle node search.", "prompt": "### Task:\nGenerate code that defines a set of functions to manipulate a node graph using the nodegraphqt library. The functions should allow to zoom in, zoom out, reset zoom, set layout direction to horizontal or vertical, open, import, save, and clear a session, clear undo history, copy, cut, paste, delete, extract, clear connections of, select all, clear selection of, invert selection of, disable, duplicate, expand group of nodes, fit the zoom level to selected nodes, show undo list, set pipe style to curved, straight or angled, set background grid to none, dots or lines, auto layout nodes downstream or upstream, and toggle node search.\n\n### Code:\n", "top_k": 3}, {"idx": 129, "repo_full_name": "fortra__impacket", "instruction": "Generate code that performs a simple ICMP6 ping. The code should take source and destination IP addresses as command line arguments. It should create an IP6 packet with the source and destination addresses, and send an ICMP echo request to the destination IP. The code should then wait for an echo reply, decode the reply using the ImpactDecoder, and print the size of the reply, the destination IP, and the echo sequence number if the reply type is an echo reply. The code should continue to send echo requests and listen for replies in an infinite loop.", "prompt": "### Task:\nGenerate code that performs a simple ICMP6 ping. The code should take source and destination IP addresses as command line arguments. It should create an IP6 packet with the source and destination addresses, and send an ICMP echo request to the destination IP. The code should then wait for an echo reply, decode the reply using the ImpactDecoder, and print the size of the reply, the destination IP, and the echo sequence number if the reply type is an echo reply. The code should continue to send echo requests and listen for replies in an infinite loop.\n\n### Code:\n", "top_k": 3}, {"idx": 130, "repo_full_name": "pyomo__mpi-sppy", "instruction": "Generate code that imports necessary modules and functions from the mpi-sppy library and other necessary libraries. The code should define a function to parse arguments from the command line and set up a configuration object with various parameters. Then, it should define a main function that uses the parsed arguments to set up a scenario creator and a list of scenario names. The main function should also check if the number of scenarios is in a predefined list of available scenarios. Depending on the configuration, the main function should set up different extensions and spokes for the scenario. The main function should also create a WheelSpinner object with the hub and spokes, spin the wheel, and write the solution to a file if a solution directory is provided. Finally, the code should call the main function if the script is run as the main program.", "prompt": "### Task:\nGenerate code that imports necessary modules and functions from the mpi-sppy library and other necessary libraries. The code should define a function to parse arguments from the command line and set up a configuration object with various parameters. Then, it should define a main function that uses the parsed arguments to set up a scenario creator and a list of scenario names. The main function should also check if the number of scenarios is in a predefined list of available scenarios. Depending on the configuration, the main function should set up different extensions and spokes for the scenario. The main function should also create a WheelSpinner object with the hub and spokes, spin the wheel, and write the solution to a file if a solution directory is provided. Finally, the code should call the main function if the script is run as the main program.\n\n### Code:\n", "top_k": 3}, {"idx": 131, "repo_full_name": "matplotlib__basemap", "instruction": "Generate code that creates a series of maps using the Basemap library. The maps should be orthographic projections centered at latitude 45 and longitude -100. The maps should have coastlines, country boundaries, and continents filled with color. The maps should also have meridians and parallels drawn every 30 degrees. \n\nThe code should plot the locations of five cities on the map, represented by filled circles, and their names should be displayed next to the circles. \n\nThe code should also generate some data on a regular lat/lon grid and contour this data over the map. \n\nThe maps should be displayed with different backgrounds: filled continent, land-sea mask, blue marble, shaded relief, etopo, and etopo with land areas transparent. \n\nFinally, the code should display all the generated maps.", "prompt": "### Task:\nGenerate code that creates a series of maps using the Basemap library. The maps should be orthographic projections centered at latitude 45 and longitude -100. The maps should have coastlines, country boundaries, and continents filled with color. The maps should also have meridians and parallels drawn every 30 degrees. \n\nThe code should plot the locations of five cities on the map, represented by filled circles, and their names should be displayed next to the circles. \n\nThe code should also generate some data on a regular lat/lon grid and contour this data over the map. \n\nThe maps should be displayed with different backgrounds: filled continent, land-sea mask, blue marble, shaded relief, etopo, and etopo with land areas transparent. \n\nFinally, the code should display all the generated maps.\n\n### Code:\n", "top_k": 3}, {"idx": 132, "repo_full_name": "iovisor__bcc", "instruction": "Generate code that uses the bcc library to create a histogram of system-wide strlen return values. The code should handle the case where strlen is an indirect function. It should define two BPF programs, one for counting the return values and another for submitting the addresses of the resolver and implementation functions. The code should also include functions to get the symbol of the indirect function, set the addresses of the resolver and implementation functions, and find the offset of the implementation function. In the main function, it should get the symbol of the indirect function, find the offset of the implementation function, and attach the counting BPF program to the implementation function. The code should then enter a loop where it sleeps for one second, prints the histogram, and clears the histogram. The loop should continue until a KeyboardInterrupt is received.", "prompt": "### Task:\nGenerate code that uses the bcc library to create a histogram of system-wide strlen return values. The code should handle the case where strlen is an indirect function. It should define two BPF programs, one for counting the return values and another for submitting the addresses of the resolver and implementation functions. The code should also include functions to get the symbol of the indirect function, set the addresses of the resolver and implementation functions, and find the offset of the implementation function. In the main function, it should get the symbol of the indirect function, find the offset of the implementation function, and attach the counting BPF program to the implementation function. The code should then enter a loop where it sleeps for one second, prints the histogram, and clears the histogram. The loop should continue until a KeyboardInterrupt is received.\n\n### Code:\n", "top_k": 3}, {"idx": 133, "repo_full_name": "federatedai__fate", "instruction": "Generate code that uses the fate library to create a pipeline for a binary classification task using a heterogenous neural network (HeteroNN). The pipeline should include components for reading data, transforming data, intersecting data, and evaluating the model. The HeteroNN should be configured with specific parameters including the number of epochs, learning rate, batch size, and callback parameters. The model should also include a guest and host bottom model, an interactive layer, and a guest top model. The pipeline should be compiled and fitted, and the summary of the HeteroNN component should be printed. The code should also include a main function that accepts a configuration file as an argument.", "prompt": "### Task:\nGenerate code that uses the fate library to create a pipeline for a binary classification task using a heterogenous neural network (HeteroNN). The pipeline should include components for reading data, transforming data, intersecting data, and evaluating the model. The HeteroNN should be configured with specific parameters including the number of epochs, learning rate, batch size, and callback parameters. The model should also include a guest and host bottom model, an interactive layer, and a guest top model. The pipeline should be compiled and fitted, and the summary of the HeteroNN component should be printed. The code should also include a main function that accepts a configuration file as an argument.\n\n### Code:\n", "top_k": 3}, {"idx": 134, "repo_full_name": "lightly-ai__lightly", "instruction": "Generate code that creates a custom PyTorch model named SMoGModel. This model should inherit from nn.Module and include a backbone, a projection head, and a prediction head. The model should also have a method to cluster features using KMeans from sklearn, a method to reset group features, and a method to reset momentum weights. The forward method should return encoded and predicted values. \n\nAdditionally, the code should create an instance of the SMoGModel using a backbone derived from a ResNet18 model. It should also create a memory bank using the MemoryBankModule from the lightly library. \n\nThe code should then set up a device for computation, apply a SMoGTransform to a CIFAR10 dataset, and create a DataLoader for the transformed dataset. \n\nFinally, the code should define a CrossEntropyLoss criterion and a SGD optimizer, and then run a training loop for 10 epochs. In each epoch, the code should update the model's momentum, encode inputs, update group features, calculate loss, and update the memory bank. The average loss for each epoch should be printed out.", "prompt": "### Task:\nGenerate code that creates a custom PyTorch model named SMoGModel. This model should inherit from nn.Module and include a backbone, a projection head, and a prediction head. The model should also have a method to cluster features using KMeans from sklearn, a method to reset group features, and a method to reset momentum weights. The forward method should return encoded and predicted values. \n\nAdditionally, the code should create an instance of the SMoGModel using a backbone derived from a ResNet18 model. It should also create a memory bank using the MemoryBankModule from the lightly library. \n\nThe code should then set up a device for computation, apply a SMoGTransform to a CIFAR10 dataset, and create a DataLoader for the transformed dataset. \n\nFinally, the code should define a CrossEntropyLoss criterion and a SGD optimizer, and then run a training loop for 10 epochs. In each epoch, the code should update the model's momentum, encode inputs, update group features, calculate loss, and update the memory bank. The average loss for each epoch should be printed out.\n\n### Code:\n", "top_k": 3}, {"idx": 135, "repo_full_name": "pyqtgraph__pyqtgraph", "instruction": "Generate code that creates a PyQtGraph application with multiple plots demonstrating various features. The plots should include basic array plotting, multiple curves, drawing with points, parametric plot with grid enabled, scatter plot with axis labels and log scale, an updating plot, a filled plot with axis disabled, region selection, and zoom on selected region. The plots should be arranged in a grid layout and the application window should be titled \"Basic plotting examples\". The plots should be interactive, allowing for panning and scaling. The application should also include a timer that updates one of the plots at regular intervals.", "prompt": "### Task:\nGenerate code that creates a PyQtGraph application with multiple plots demonstrating various features. The plots should include basic array plotting, multiple curves, drawing with points, parametric plot with grid enabled, scatter plot with axis labels and log scale, an updating plot, a filled plot with axis disabled, region selection, and zoom on selected region. The plots should be arranged in a grid layout and the application window should be titled \"Basic plotting examples\". The plots should be interactive, allowing for panning and scaling. The application should also include a timer that updates one of the plots at regular intervals.\n\n### Code:\n", "top_k": 3}, {"idx": 136, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that sets up a combined fluid-kinetic simulation using the DREAM library. The simulation should include the following settings: an electric field strength of 0.6 V/m, an electron density of 5e19 m^-3, and a temperature of 1e3 eV. The simulation should include a fully ionized ion species named 'D' with a charge of 1. The hot-tail grid should be disabled and the collision frequency mode should be set to ultra-relativistic. The Dreicer and avalanche should be included with the avalanche mode set to fluid and the Dreicer rate set to neural network. The initial profile should be set to 1e15. If the runaway electron grid is enabled, it should be set with 50 radial points, 100 momentum points, and a maximum momentum of 0.5. The advection interpolation method should be set to use flux limiters and the initialization method should be isotropic. The radial grid should be set with a magnetic field strength of 5, a minor radius of 0.22, a wall radius of 0.22, and one radial point. The solver should be set to nonlinear and verbose with a relative tolerance of 1e-4 for the runaway electron current density. The simulation should include fluid effects. The time stepper should be set with a maximum time of 1e-1 and 20 time steps. The settings should be saved to an HDF5 file named 'dream_settings.h5'.", "prompt": "### Task:\nGenerate code that sets up a combined fluid-kinetic simulation using the DREAM library. The simulation should include the following settings: an electric field strength of 0.6 V/m, an electron density of 5e19 m^-3, and a temperature of 1e3 eV. The simulation should include a fully ionized ion species named 'D' with a charge of 1. The hot-tail grid should be disabled and the collision frequency mode should be set to ultra-relativistic. The Dreicer and avalanche should be included with the avalanche mode set to fluid and the Dreicer rate set to neural network. The initial profile should be set to 1e15. If the runaway electron grid is enabled, it should be set with 50 radial points, 100 momentum points, and a maximum momentum of 0.5. The advection interpolation method should be set to use flux limiters and the initialization method should be isotropic. The radial grid should be set with a magnetic field strength of 5, a minor radius of 0.22, a wall radius of 0.22, and one radial point. The solver should be set to nonlinear and verbose with a relative tolerance of 1e-4 for the runaway electron current density. The simulation should include fluid effects. The time stepper should be set with a maximum time of 1e-1 and 20 time steps. The settings should be saved to an HDF5 file named 'dream_settings.h5'.\n\n### Code:\n", "top_k": 3}, {"idx": 137, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that imports necessary libraries and modules, including numpy, scipy, os, sys, and several modules from the porepy library. The code should define two functions, add_data_darcy and add_data_advection, which add data to a given grid bucket (gb) and domain with a specified tolerance (tol). The add_data_darcy function should add parameters related to Darcy's law, including permeability, source, aperture, and boundary conditions. The add_data_advection function should add parameters related to advection, including source, porosity, discharge, and boundary conditions. \n\nThe code should also append a path to the system path, import a module named soultz_grid, and set up parameters for creating a grid. It should then create a grid using the soultz_grid module, compute its geometry, coarsen it if a certain condition is met, and assign node ordering. \n\nThe code should then solve a Darcy problem using the DualVEMMixDim solver from the porepy library, add parameters to the grid bucket, compute a matrix and right-hand side vector, solve the system of equations, split the solution, extract discharge and pressure, project discharge, and compute the total flow rate. \n\nThe code should then set up parameters for a transport problem, define solvers for advection and mass matrix, add parameters to the grid bucket, compute matrices and right-hand side vectors, perform an LU factorization, initialize a solution vector, and perform a time-stepping loop to update the solution and export it at certain time steps. \n\nFinally, the code should export the solution in PVD format and save the production data to a text file.", "prompt": "### Task:\nGenerate code that imports necessary libraries and modules, including numpy, scipy, os, sys, and several modules from the porepy library. The code should define two functions, add_data_darcy and add_data_advection, which add data to a given grid bucket (gb) and domain with a specified tolerance (tol). The add_data_darcy function should add parameters related to Darcy's law, including permeability, source, aperture, and boundary conditions. The add_data_advection function should add parameters related to advection, including source, porosity, discharge, and boundary conditions. \n\nThe code should also append a path to the system path, import a module named soultz_grid, and set up parameters for creating a grid. It should then create a grid using the soultz_grid module, compute its geometry, coarsen it if a certain condition is met, and assign node ordering. \n\nThe code should then solve a Darcy problem using the DualVEMMixDim solver from the porepy library, add parameters to the grid bucket, compute a matrix and right-hand side vector, solve the system of equations, split the solution, extract discharge and pressure, project discharge, and compute the total flow rate. \n\nThe code should then set up parameters for a transport problem, define solvers for advection and mass matrix, add parameters to the grid bucket, compute matrices and right-hand side vectors, perform an LU factorization, initialize a solution vector, and perform a time-stepping loop to update the solution and export it at certain time steps. \n\nFinally, the code should export the solution in PVD format and save the production data to a text file.\n\n### Code:\n", "top_k": 3}, {"idx": 138, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that creates a 2D box domain for linear elasticity problems using the fealpy library. The code should define a class for the box domain with methods for initializing the mesh, defining displacement, strain, stress, source, Dirichlet and Neumann boundary conditions, and checking if a point is on the Dirichlet, Neumann, or fracture boundary. \n\nThe code should also define a class for counting iterations and a class for a fast solver for linear elasticity problems using Lagrange finite elements. The solver should include methods for preconditioning and solving the system of equations.\n\nThe main part of the code should initialize the box domain, create a mesh, define a Lagrange finite element space, and set up Dirichlet and Neumann boundary conditions. It should then create a function for the solution, compute the stiffness matrix and the linear elasticity matrix, and set up the source vector. The code should apply the boundary conditions to the system of equations and print the shape of the matrix.\n\nFinally, the code should solve the system of equations using the fast solver and print the time it took to solve the system. The code should also plot the original mesh.", "prompt": "### Task:\nGenerate code that creates a 2D box domain for linear elasticity problems using the fealpy library. The code should define a class for the box domain with methods for initializing the mesh, defining displacement, strain, stress, source, Dirichlet and Neumann boundary conditions, and checking if a point is on the Dirichlet, Neumann, or fracture boundary. \n\nThe code should also define a class for counting iterations and a class for a fast solver for linear elasticity problems using Lagrange finite elements. The solver should include methods for preconditioning and solving the system of equations.\n\nThe main part of the code should initialize the box domain, create a mesh, define a Lagrange finite element space, and set up Dirichlet and Neumann boundary conditions. It should then create a function for the solution, compute the stiffness matrix and the linear elasticity matrix, and set up the source vector. The code should apply the boundary conditions to the system of equations and print the shape of the matrix.\n\nFinally, the code should solve the system of equations using the fast solver and print the time it took to solve the system. The code should also plot the original mesh.\n\n### Code:\n", "top_k": 3}, {"idx": 139, "repo_full_name": "ansys__pydpf-core", "instruction": "Generate code that demonstrates the use of the pydpf-core library for multi-stage cyclic symmetry analysis with advanced customization. The code should download a multi-stage cyclic result, create a model from it, and display the model's state. It should then verify that the model is a multi-stage model by checking the result info. The code should also go over the cyclic support, displaying the number of stages, the number of sectors in each stage, and the number of nodes in the first stage's base sector. \n\nNext, the code should expand displacement results on chosen sectors. It should create a displacement cyclic operator, select the sectors to expand on the first stage, and select the sectors to expand stage by stage. The code should then expand the displacements and get a total deformation. It should also get the expanded mesh. \n\nThe code should then plot the expanded result on the expanded mesh. It should also demonstrate how to expand only some sectors for the mesh, and plot the expanded result on the expanded mesh. \n\nFinally, the code should check results precisely. It should print the time frequency support to see the harmonic index, and verify that the displacement values are the same on all nodes.", "prompt": "### Task:\nGenerate code that demonstrates the use of the pydpf-core library for multi-stage cyclic symmetry analysis with advanced customization. The code should download a multi-stage cyclic result, create a model from it, and display the model's state. It should then verify that the model is a multi-stage model by checking the result info. The code should also go over the cyclic support, displaying the number of stages, the number of sectors in each stage, and the number of nodes in the first stage's base sector. \n\nNext, the code should expand displacement results on chosen sectors. It should create a displacement cyclic operator, select the sectors to expand on the first stage, and select the sectors to expand stage by stage. The code should then expand the displacements and get a total deformation. It should also get the expanded mesh. \n\nThe code should then plot the expanded result on the expanded mesh. It should also demonstrate how to expand only some sectors for the mesh, and plot the expanded result on the expanded mesh. \n\nFinally, the code should check results precisely. It should print the time frequency support to see the harmonic index, and verify that the displacement values are the same on all nodes.\n\n### Code:\n", "top_k": 3}, {"idx": 140, "repo_full_name": "paddlepaddle__fastdeploy", "instruction": "Generate code that parses command line arguments for paths of detection, recognition, and table recognition models of PPOCR, recognition model label file, table recognition dictionary path, recognition model inference batch size, test image file path, inference device type, device ID, and inference backend type. Then, based on the parsed arguments, it should build runtime options for detection, recognition, and table recognition models. Depending on the device and backend type, it should set the appropriate backend and device for each model. If TensorRT backend is used, it should set the dynamic shape and save the TRT cache file to disk. After setting the runtime options, it should load the models and set the preprocessor and postprocessor parameters for the detection model. Then, it should create an instance of PPStructureV2Table with the loaded models and set the recognition batch size. Finally, it should read the input image, predict and print the results, visualize the results, and save the visualized image.", "prompt": "### Task:\nGenerate code that parses command line arguments for paths of detection, recognition, and table recognition models of PPOCR, recognition model label file, table recognition dictionary path, recognition model inference batch size, test image file path, inference device type, device ID, and inference backend type. Then, based on the parsed arguments, it should build runtime options for detection, recognition, and table recognition models. Depending on the device and backend type, it should set the appropriate backend and device for each model. If TensorRT backend is used, it should set the dynamic shape and save the TRT cache file to disk. After setting the runtime options, it should load the models and set the preprocessor and postprocessor parameters for the detection model. Then, it should create an instance of PPStructureV2Table with the loaded models and set the recognition batch size. Finally, it should read the input image, predict and print the results, visualize the results, and save the visualized image.\n\n### Code:\n", "top_k": 3}, {"idx": 141, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that calculates the force from Quantum Mechanics (QM) region acting on the background Molecular Mechanics (MM) particles. The code should define a molecule using the pyscf library, generate random coordinates and charges for MM particles, and define a function to calculate the force. The force calculation should include the interaction between QM atoms and MM particles, and the interaction between electron density and MM particles. The code should then calculate the force from Hartree-Fock (HF) electron density and verify it. \n\nNext, the code should consider the response of HF orbitals in the analytical gradients for post-HF methods. As an example, it should use MP2 gradients to demonstrate how to include the orbital response effects in the force for MM particles. The code should define a function to make the reduced density matrix (rdm1) with orbital response, calculate the force from MP2 electron density (including orbital response), and verify it.", "prompt": "### Task:\nGenerate code that calculates the force from Quantum Mechanics (QM) region acting on the background Molecular Mechanics (MM) particles. The code should define a molecule using the pyscf library, generate random coordinates and charges for MM particles, and define a function to calculate the force. The force calculation should include the interaction between QM atoms and MM particles, and the interaction between electron density and MM particles. The code should then calculate the force from Hartree-Fock (HF) electron density and verify it. \n\nNext, the code should consider the response of HF orbitals in the analytical gradients for post-HF methods. As an example, it should use MP2 gradients to demonstrate how to include the orbital response effects in the force for MM particles. The code should define a function to make the reduced density matrix (rdm1) with orbital response, calculate the force from MP2 electron density (including orbital response), and verify it.\n\n### Code:\n", "top_k": 3}, {"idx": 142, "repo_full_name": "urwid__urwid", "instruction": "Generate code that creates a text editor using the urwid library in Python. The text editor should be able to lazily load text files, handle keypresses for saving and quitting the application, deleting and backspacing at the end and beginning of lines respectively, starting new lines, and navigating left and right. It should also be able to combine and split lines of text, and save the edited text back to the original file. The text editor should have a custom list walker for lazily loading the text file and a display that includes a list box for the text and a footer with instructions. The main function should take a filename as an argument and instantiate the text editor with that file.", "prompt": "### Task:\nGenerate code that creates a text editor using the urwid library in Python. The text editor should be able to lazily load text files, handle keypresses for saving and quitting the application, deleting and backspacing at the end and beginning of lines respectively, starting new lines, and navigating left and right. It should also be able to combine and split lines of text, and save the edited text back to the original file. The text editor should have a custom list walker for lazily loading the text file and a display that includes a list box for the text and a footer with instructions. The main function should take a filename as an argument and instantiate the text editor with that file.\n\n### Code:\n", "top_k": 3}, {"idx": 143, "repo_full_name": "rstudio__py-shiny", "instruction": "Generate code that creates a web application using the py-shiny library. The application should have a user interface with three columns. The first column should contain two inputs that control the other inputs on the page. The second column should contain a set of inputs that are controlled by the first two inputs. The third column should contain a set of inputs and a tabset. The server function should update the inputs in the second and third columns based on the values of the first two inputs. The application should be run in debug mode.", "prompt": "### Task:\nGenerate code that creates a web application using the py-shiny library. The application should have a user interface with three columns. The first column should contain two inputs that control the other inputs on the page. The second column should contain a set of inputs that are controlled by the first two inputs. The third column should contain a set of inputs and a tabset. The server function should update the inputs in the second and third columns based on the values of the first two inputs. The application should be run in debug mode.\n\n### Code:\n", "top_k": 3}, {"idx": 144, "repo_full_name": "silnrsi__pysilfont", "instruction": "Generate code that uses the pysilfont library to create an FTML (Font Test Markup Language) document from a UFO (Unified Font Object) and a glyph data CSV file. The script should accept various command line arguments to customize the output, such as input UFO, output file, glyph info CSV file, font code, log file name, list of BCP47 language tags, right-to-left feature enabling, rendering check disabling, test name, font source, text scaling, anchor points regular expression, total width of all string column, and XSL stylesheet. The script should read the input CSV, initialize the FTML document, and add encoded characters, unencoded specials and ligatures, Lam-Alef data, and diacritic attachment data to the document based on the provided arguments. Finally, the script should write the output FTML file.", "prompt": "### Task:\nGenerate code that uses the pysilfont library to create an FTML (Font Test Markup Language) document from a UFO (Unified Font Object) and a glyph data CSV file. The script should accept various command line arguments to customize the output, such as input UFO, output file, glyph info CSV file, font code, log file name, list of BCP47 language tags, right-to-left feature enabling, rendering check disabling, test name, font source, text scaling, anchor points regular expression, total width of all string column, and XSL stylesheet. The script should read the input CSV, initialize the FTML document, and add encoded characters, unencoded specials and ligatures, Lam-Alef data, and diacritic attachment data to the document based on the provided arguments. Finally, the script should write the output FTML file.\n\n### Code:\n", "top_k": 3}, {"idx": 145, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that performs a combined fluid-kinetic simulation using the DREAM library. The simulation should include both the hot-tail and runaway electron grids. Set the electric field strength, electron density, and temperature to specific values. Define the momentum grid and set up initial hot electron Maxwellian. Include Dreicer and avalanche in the equation system. Set up the radial grid and disable the runaway grid. Set the Svensson transport coefficients and use the nonlinear solver. Finally, set the time stepper and save the settings to an HDF5 file.", "prompt": "### Task:\nGenerate code that performs a combined fluid-kinetic simulation using the DREAM library. The simulation should include both the hot-tail and runaway electron grids. Set the electric field strength, electron density, and temperature to specific values. Define the momentum grid and set up initial hot electron Maxwellian. Include Dreicer and avalanche in the equation system. Set up the radial grid and disable the runaway grid. Set the Svensson transport coefficients and use the nonlinear solver. Finally, set the time stepper and save the settings to an HDF5 file.\n\n### Code:\n", "top_k": 3}, {"idx": 146, "repo_full_name": "kubernetes-client__python", "instruction": "Generate code that uses the Kubernetes Python client to create a deployment, a service, and an ingress in a Kubernetes cluster. The deployment should use a container with a specific image and expose a certain port. The service should be associated with the deployment and expose the same port. The ingress should allow external network access to the service on a specified host and path. The code should also include a main function that loads the local Kubernetes configuration and calls the functions to create the deployment, service, and ingress.", "prompt": "### Task:\nGenerate code that uses the Kubernetes Python client to create a deployment, a service, and an ingress in a Kubernetes cluster. The deployment should use a container with a specific image and expose a certain port. The service should be associated with the deployment and expose the same port. The ingress should allow external network access to the service on a specified host and path. The code should also include a main function that loads the local Kubernetes configuration and calls the functions to create the deployment, service, and ingress.\n\n### Code:\n", "top_k": 3}, {"idx": 147, "repo_full_name": "mne-tools__mne-python", "instruction": "Generate code that performs a full pipeline on the SPM Faces dataset using the MNE-Python library. The pipeline should include artifact removal, averaging epochs, forward model computation, and source reconstruction using dSPM on the contrast \"faces - scrambled\". The code should load and filter data, set up epochs, fit ICA, find and remove major artifacts, compute and visualize the contrast, estimate noise covariance, visualize fields on MEG helmet, compute forward and inverse solutions, and finally plot the contrast in 3D.", "prompt": "### Task:\nGenerate code that performs a full pipeline on the SPM Faces dataset using the MNE-Python library. The pipeline should include artifact removal, averaging epochs, forward model computation, and source reconstruction using dSPM on the contrast \"faces - scrambled\". The code should load and filter data, set up epochs, fit ICA, find and remove major artifacts, compute and visualize the contrast, estimate noise covariance, visualize fields on MEG helmet, compute forward and inverse solutions, and finally plot the contrast in 3D.\n\n### Code:\n", "top_k": 3}, {"idx": 148, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that performs a multiphysics analysis using the PyAEDT library. The code should import the necessary libraries and set the graphical mode. It should then download and open a project, and start HFSS with a specified version. The code should also start a Circuit and add the HFSS dynamic link component to it. It should set up dynamic link options and create ports and excitations. The code should then create a setup and solve the circuit, pushing excitations to the HFSS model. It should start Mechanical and copy bodies from the HFSS project. The code should get losses from HFSS and assign the convection to Mechanical. Finally, the code should plot the model, solve and plot the thermal results, and release AEDT.", "prompt": "### Task:\nGenerate code that performs a multiphysics analysis using the PyAEDT library. The code should import the necessary libraries and set the graphical mode. It should then download and open a project, and start HFSS with a specified version. The code should also start a Circuit and add the HFSS dynamic link component to it. It should set up dynamic link options and create ports and excitations. The code should then create a setup and solve the circuit, pushing excitations to the HFSS model. It should start Mechanical and copy bodies from the HFSS project. The code should get losses from HFSS and assign the convection to Mechanical. Finally, the code should plot the model, solve and plot the thermal results, and release AEDT.\n\n### Code:\n", "top_k": 3}, {"idx": 149, "repo_full_name": "hiddensymmetries__simsopt", "instruction": "Generate code that solves a stage-II coil optimization problem using the simsopt library. The goal is to find coils that generate a specific target normal field on a given surface. The target equilibrium is a W7-X configuration with an average beta of 4%. The code should use a virtual casing calculation to compute the target B_{External}n. The objective function is given by J = (1/2)  |B_{BiotSavart}n - B_{External}n|^2 ds + LENGTH_PENALTY *  (CurveLength - L0)^2. The code should also include a Taylor test and run the optimization using the L-BFGS-B method. The results should be saved in the \"output\" directory.", "prompt": "### Task:\nGenerate code that solves a stage-II coil optimization problem using the simsopt library. The goal is to find coils that generate a specific target normal field on a given surface. The target equilibrium is a W7-X configuration with an average beta of 4%. The code should use a virtual casing calculation to compute the target B_{External}n. The objective function is given by J = (1/2)  |B_{BiotSavart}n - B_{External}n|^2 ds + LENGTH_PENALTY *  (CurveLength - L0)^2. The code should also include a Taylor test and run the optimization using the L-BFGS-B method. The results should be saved in the \"output\" directory.\n\n### Code:\n", "top_k": 3}]}