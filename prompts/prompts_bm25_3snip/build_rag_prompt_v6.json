{"meta": {"builder": "build_rag_prompt_v6", "top_k": 3, "separator": "\n\n# --- Snippet Separator ---\n\n", "num_queries": 150, "timestamp": 1757768061.1023273, "source_retrieval_file": "BM25/retrieved_k3_samples.json"}, "prompts": [{"idx": 0, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n\nThe base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n\nThe domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n\nThe eBGP layer should add private peerings between different autonomous systems. \n\nFinally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def makeEmulatorBaseWith5StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int,\n    services: List[Service]):\n    \"\"\"!\n    @brief create a new stub AS.\n\n    @param emu reference to the Emulator object.\n    @param base reference to the base layer.\n    @param asn ASN for the newly created AS.\n    @param exchange IXP ID for new newly created AS to join.\n    @param list of instances of Service to install on hosts. One host will be\n    created for each.\n    \"\"\"\n\n    # Create AS and internal network\n    stub_as = base.createAutonomousSystem(asn)\n    stub_as.createNetwork('net0')\n\n    # Create a BGP router \n    # Attach the router to both the internal and external networks\n    router = stub_as.createRouter('router0')\n    router.joinNetwork('net0')\n    router.joinNetwork('ix{}'.format(exchange))\n\n    # Create a host node for each specified service\n    createHostsOnNetwork(emu, stub_as, 'net0', services)\n\n# --- Snippet Separator ---\n\ndef makeEmulatorBaseWith10StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 160, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 161, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 162, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 163, 104, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 164, 104, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(103, [3],  [160, 161, 162], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(104, [3, 4], [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [4],  [163], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [12], [164], PeerRelationship.Provider)\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n\nThe base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n\nThe domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n\nThe eBGP layer should add private peerings between different autonomous systems. \n\nFinally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 1, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that solves the heat conduction equation using the adaptive finite element method on a triangular mesh. The code should allow the user to specify the number of spatial and temporal divisions, the adaptive refinement stop threshold, and the adaptive refinement and coarsening parameters. The code should use the 2D equation model from the fealpy library for the heat conduction equation, and the lagrange finite element space. The code should also handle Dirichlet boundary conditions. The code should iteratively refine the mesh based on the recovery estimate until the error is below the specified threshold. The code should also coarsen the mesh after each time step. The code should save a plot of the mesh at each refinement and coarsening step. The code should also print the error at each time step and plot the numerical solution at specified time steps.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def ignore_code(self, code):\n        \"\"\"Check if the error code should be ignored.\n\n        If 'options.select' contains a prefix of the error code,\n        return False.  Else, if 'options.ignore' contains a prefix of\n        the error code, return True.\n        \"\"\"\n        if len(code) < 4 and any(s.startswith(code)\n                                 for s in self.options.select):\n            return False\n        return (code.startswith(self.options.ignore) and\n                not code.startswith(self.options.select))\n\n# --- Snippet Separator ---\n\ndef as_r6(m):\n        # When making derivative over t2, r6 should be called on the 6-index\n        # tensor. It gives the equation for lambda2, but not corresponding to\n        # the lambda equation used by RCCSD-lambda code.  A transformation was\n        # applied in RCCSD-lambda equation  F(lambda)_{ijab} = 0:\n        #       2/3 * # F(lambda)_{ijab} + 1/3 * F(lambda)_{jiab} = 0\n        # Combining this transformation with r6 operation, leads to the\n        # transformation code below\n        return m * 2 - m.transpose(0,1,2,5,4,3) - m.transpose(0,1,2,3,5,4)\n\n# --- Snippet Separator ---\n\ndef add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that solves the heat conduction equation using the adaptive finite element method on a triangular mesh. The code should allow the user to specify the number of spatial and temporal divisions, the adaptive refinement stop threshold, and the adaptive refinement and coarsening parameters. The code should use the 2D equation model from the fealpy library for the heat conduction equation, and the lagrange finite element space. The code should also handle Dirichlet boundary conditions. The code should iteratively refine the mesh based on the recovery estimate until the error is below the specified threshold. The code should also coarsen the mesh after each time step. The code should save a plot of the mesh at each refinement and coarsening step. The code should also print the error at each time step and plot the numerical solution at specified time steps.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 2, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that performs the following tasks:\n\n1. Import necessary libraries and modules including numpy, argparse, matplotlib, scipy, and fealpy.\n2. Define command line arguments for time division, initial spatial division, and number of spatial iterations.\n3. Create an initial 2D triangular mesh using the fealpy library.\n4. Define the parameters for the PDE (Partial Differential Equation) using the ADI_2d class from fealpy.\n5. Initialize the electric and magnetic fields using the FirstKindNedelecFiniteElementSpace2d and ScaledMonomialSpace2d classes from fealpy.\n6. Define a function to get the phi curl matrix.\n7. Create mass and curl matrices using the fealpy library.\n8. Iterate over the number of spatial iterations, in each iteration:\n   - Compute the right-hand side of the equation for the next time layer.\n   - Handle the boundary conditions for the next time layer.\n   - Compute the electric and magnetic fields for the next time layer.\n   - Calculate the error between the computed and actual solutions for the electric and magnetic fields.\n9. If not the last iteration, refine the mesh uniformly.\n10. Finally, display the error matrix and plot the error rates using the fealpy library.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Mpfa(Solver):\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells (pressure dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a multi-point flux\n        approximation.\n\n        The name of data in the input dictionary (data) are:\n        k : second_order_tensor\n            Permeability defined cell-wise.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): Whether to discetize prior to matrix\n            assembly. If False, data should already contain discretization.\n            Defaults to True.\n\n        Return\n        ------\n        matrix: sparse csr (g_num_cells, g_num_cells)\n            Discretization matrix.\n        rhs: array (g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        \"\"\"\n        if discretize:\n            self.discretize(g, data)\n\n        div = fvutils.scalar_divergence(g)\n        flux = data['flux']\n        M = div * flux\n\n        bound_flux = data['bound_flux']\n\n        param = data['param']\n\n        bc_val = param.get_bc_val(self)\n\n        return M, self.rhs(g, bound_flux, bc_val)\n\n#------------------------------------------------------------------------------#\n\n    def rhs(self, g, bound_flux, bc_val):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using the MPFA method. See self.matrix_rhs for a detaild\n        description.\n        \"\"\"\n        div = g.cell_faces.T\n\n        return -div * bound_flux * bc_val\n\n#------------------------------------------------------------------------------#\n\n    def discretize(self, g, data):\n        \"\"\"\n        The name of data in the input dictionary (data) are:\n        k : second_order_tensor\n            Permeability defined cell-wise. If not given a identity permeability\n            is assumed and a warning arised.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        \"\"\"\n        param = data['param']\n        k = param.get_tensor(self)\n        bnd = param.get_bc(self)\n        a = param.aperture\n\n        trm, bound_flux = mpfa(g, k, bnd, apertures=a)\n        data['flux'] = trm\n        data['bound_flux'] = bound_flux\n\n# --- Snippet Separator ---\n\ndef mass_matrix(self, symbol, boundary_conditions):\n        \"\"\"\n        Calculates the mass matrix for a spatial method.\n\n        Parameters\n        ----------\n        symbol: :class:`pybamm.Variable`\n            The variable corresponding to the equation for which we are\n            calculating the mass matrix.\n        boundary_conditions : dict\n            The boundary conditions of the model\n            ({symbol.id: {\"left\": left bc, \"right\": right bc}})\n\n        Returns\n        -------\n        :class:`pybamm.Matrix`\n            The (sparse) mass matrix for the spatial method.\n        \"\"\"\n        # NOTE: for different spatial methods the matrix may need to be adjusted\n        # to account for Dirichlet boundary conditions. Here, we just have the default\n        # behaviour that the mass matrix is the identity.\n\n        # Create appropriate submesh by combining submeshes in domain\n        submesh = self.mesh.combine_submeshes(*symbol.domain)\n\n        # Get number of points in primary dimension\n        n = submesh[0].npts\n\n        # Create mass matrix for primary dimension\n        prim_mass = eye(n)\n\n        # Get number of points in secondary dimension\n        sec_pts = len(submesh)\n\n        # Convert to csr_matrix as required by some solvers\n        mass = csr_matrix(kron(eye(sec_pts), prim_mass))\n        return pybamm.Matrix(mass)\n\n# --- Snippet Separator ---\n\nclass Mpsa(Solver):\n\n    def __init__(self, physics='mechanics'):\n        self.physics = physics\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells times dimension (stress dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.dim * g.num_cells\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a multi-point stress\n        approximation.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): default True. Whether to discetize\n            prior to matrix assembly. If False, data should already contain\n            discretization.\n\n        Return\n        ------\n        matrix: sparse csr (g.dim * g_num_cells, g.dim * g_num_cells)\n            Discretization matrix.\n        rhs: array (g.dim * g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n        \"\"\"\n        param = data['param']\n\n        if discretize:\n            self.discretize(g, data)\n        div = fvutils.vector_divergence(g)\n        stress = data['stress']\n        bound_stress = data['bound_stress']\n        M = div * stress\n\n        f = data['param'].get_source(self)\n        bc_val = data['param'].get_bc_val(self)\n\n        return M, self.rhs(g, bound_stress, bc_val, f)\n\n#------------------------------------------------------------------------------#\n\n    def discretize(self, g, data):\n        \"\"\"\n        Discretize the vector elliptic equation by the multi-point stress\n\n        The method computes fluxes over faces in terms of displacements in\n        adjacent cells (defined as the two cells sharing the face).\n\n        The name of data in the input dictionary (data) are:\n        param : Parameter(Class). Contains the following parameters:\n            tensor : fourth_order_tensor\n                Permeability defined cell-wise. If not given a identity permeability\n                is assumed and a warning arised.\n            bc : boundary conditions (optional)\n            bc_val : dictionary (optional)\n                Values of the boundary conditions. The dictionary has at most the\n                following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n                conditions, respectively.\n            apertures : (np.ndarray) (optional) apertures of the cells for scaling of\n                the face normals.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n        \"\"\"\n\n        c = data['param'].get_tensor(self)\n        bnd = data['param'].get_bc(self)\n\n        stress, bound_stress = mpsa(g, c, bnd)\n\n        data['stress'] = stress\n        data['bound_stress'] = bound_stress\n\n#------------------------------------------------------------------------------#\n\n    def rhs(self, g, bound_stress, bc_val, f):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using the MPSA method. See self.matrix_rhs for a detailed\n        description.\n        \"\"\"\n        div = fvutils.vector_divergence(g)\n\n        return -div * bound_stress * bc_val - f\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks:\n\n1. Import necessary libraries and modules including numpy, argparse, matplotlib, scipy, and fealpy.\n2. Define command line arguments for time division, initial spatial division, and number of spatial iterations.\n3. Create an initial 2D triangular mesh using the fealpy library.\n4. Define the parameters for the PDE (Partial Differential Equation) using the ADI_2d class from fealpy.\n5. Initialize the electric and magnetic fields using the FirstKindNedelecFiniteElementSpace2d and ScaledMonomialSpace2d classes from fealpy.\n6. Define a function to get the phi curl matrix.\n7. Create mass and curl matrices using the fealpy library.\n8. Iterate over the number of spatial iterations, in each iteration:\n   - Compute the right-hand side of the equation for the next time layer.\n   - Handle the boundary conditions for the next time layer.\n   - Compute the electric and magnetic fields for the next time layer.\n   - Calculate the error between the computed and actual solutions for the electric and magnetic fields.\n9. If not the last iteration, refine the mesh uniformly.\n10. Finally, display the error matrix and plot the error rates using the fealpy library.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 3, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that calculates the effective electronic coupling based on single determinant diabatic states using the pyscf library. The code should first define a molecule with specific atoms and basis. Then, it should perform two state calculations with DFT, storing molecular orbital information into separate chkfiles. The code should then read the MO coefficients and occupation numbers from these chkfiles. Afterwards, it should calculate the overlap between two determinants, construct density matrices, calculate one-electron and two-electron part contributions, and calculate new total energy. Finally, the code should calculate the effective electronic coupling and print the results. The code should also remove the chkfiles at the end.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def det_ovlp(mo1, mo2, occ1, occ2, ovlp):\n    r''' Calculate the overlap between two different determinants. It is the product\n    of single values of molecular orbital overlap matrix.\n\n    Return:\n        A list:\n            the product of single values: float\n            x_a: :math:`\\mathbf{U} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\dagger`\n            They are used to calculate asymmetric density matrix\n    '''\n\n    if numpy.sum(occ1) != numpy.sum(occ2):\n        raise RuntimeError('Electron numbers are not equal. Electronic coupling does not exist.')\n    s = reduce(numpy.dot, (mo1[:,occ1>0].T.conj(), ovlp, mo2[:,occ2>0]))\n    u, s, vt = numpy.linalg.svd(s)\n    x = numpy.dot(u/s, vt)\n    return numpy.prod(s), x\n\n# --- Snippet Separator ---\n\ndef det_ovlp(mo1, mo2, occ1, occ2, ovlp):\n    r''' Calculate the overlap between two different determinants. It is the product\n    of single values of molecular orbital overlap matrix.\n\n    .. math::\n\n        S_{12} = \\langle \\Psi_A | \\Psi_B \\rangle\n        = (\\mathrm{det}\\mathbf{U}) (\\mathrm{det}\\mathbf{V^\\dagger})\n          \\prod\\limits_{i=1}\\limits^{2N} \\lambda_{ii}\n\n    where :math:`\\mathbf{U}, \\mathbf{V}, \\lambda` are unitary matrices and single\n    values generated by single value decomposition(SVD) of the overlap matrix\n    :math:`\\mathbf{O}` which is the overlap matrix of two sets of molecular orbitals:\n\n    .. math::\n\n        \\mathbf{U}^\\dagger \\mathbf{O} \\mathbf{V} = \\mathbf{\\Lambda}\n\n    Args:\n        mo1, mo2 : 2D ndarrays\n            Molecualr orbital coefficients\n        occ1, occ2: 2D ndarrays\n            occupation numbers\n\n    Return:\n        A list: the product of single values: float\n            x_a, x_b: 1D ndarrays\n            :math:`\\mathbf{U} \\mathbf{\\Lambda}^{-1} \\mathbf{V}^\\dagger`\n            They are used to calculate asymmetric density matrix\n    '''\n\n    if not numpy.array_equal(occ1, occ2):\n        raise RuntimeError('Electron numbers are not equal. Electronic coupling does not exist.')\n\n    c1_a = mo1[0][:, occ1[0]>0]\n    c1_b = mo1[1][:, occ1[1]>0]\n    c2_a = mo2[0][:, occ2[0]>0]\n    c2_b = mo2[1][:, occ2[1]>0]\n    o_a = reduce(numpy.dot, (c1_a.conj().T, ovlp, c2_a))\n    o_b = reduce(numpy.dot, (c1_b.conj().T, ovlp, c2_b))\n    u_a, s_a, vt_a = numpy.linalg.svd(o_a)\n    u_b, s_b, vt_b = numpy.linalg.svd(o_b)\n    x_a = reduce(numpy.dot, (u_a*numpy.reciprocal(s_a), vt_a))\n    x_b = reduce(numpy.dot, (u_b*numpy.reciprocal(s_b), vt_b))\n    return numpy.prod(s_a)*numpy.prod(s_b), numpy.array((x_a, x_b))\n\n# --- Snippet Separator ---\n\ndef create_read_in_code(program, psy_data, read_write_info, postfix):\n        '''This function creates the code that reads in the NetCDF file\n        produced during extraction. For each:\n\n        - variable that is read-only, it will declare the symbol and add code\n          that reads in the variable using the PSyData library.\n        - variable that is read and written, it will create code to read in the\n          variable that is read, and create a new variable with the same name\n          and \"_post\" added which is read in to store the values from the\n          NetCDF file after the instrumented region was executed. In the end,\n          the variable that was read and written should have the same value\n          as the corresponding \"_post\" variable.\n        - variable that is written only, it will create a variable with \"_post\"\n          as postfix that reads in the output data from the NetCDF file. It\n          then also declares a variable without postfix (which will be the\n          parameter to the function), allocates it based on the shape of\n          the corresponding \"_post\" variable, and initialises it with 0.\n\n        :param program: the PSyIR Routine to which any code must \\\n            be added. It also contains the symbol table to be used.\n        :type program: :py:class:`psyclone.psyir.nodes.Routine`\n        :param psy_data: the PSyData symbol to be used.\n        :type psy_data: :py:class:`psyclone.psyir.symbols.DataSymbol`\n        :param read_write_info: information about all input and output \\\n            parameters.\n        :type read_write_info: :py:class:`psyclone.psyir.tools.ReadWriteInfo`\n        :param str postfix: a postfix that is added to a variable to \\\n            create the corresponding variable that stores the output \\\n            value from the kernel data file.\n\n        :returns: a list with all output parameters, i.e. variables that \\\n            need to be verified after executing the kernel. Each entry is \\\n            a 2-tuple containing the symbol of the computed variable, and \\\n            the symbol of the variable that contains the value read from \\\n            the file.\n        :rtype: list of 2-tuples of \\\n            :py:class:`psyclone.psyir.symbols.Symbol`\n\n        '''\n        symbol_table = program.scope.symbol_table\n        read_var = f\"{psy_data.name}%ReadVariable\"\n\n        # Collect all output symbols to later create the tests for\n        # correctness. This list stores 2-tuples: first one the\n        # variable that stores the output from the kernel, the second\n        # one the variable that stores the output values read from the\n        # file. The content of these two variables should be identical\n        # at the end.\n        output_symbols = []\n\n        # First handle variables that are read:\n        # -------------------------------------\n        for signature in read_write_info.signatures_read:\n            # Find the right symbol for the variable. Note that all variables\n            # in the input and output list have been detected as being used\n            # when the variable accesses were analysed. Therefore, these\n            # variables have References, and will already have been declared\n            # in the symbol table (in add_all_kernel_symbols).\n            sig_str = str(signature)\n            sym = symbol_table.lookup_with_tag(sig_str)\n            name_lit = Literal(sig_str, CHARACTER_TYPE)\n            ExtractDriverCreator.add_call(program, read_var,\n                                          [name_lit, Reference(sym)])\n\n        # Then handle all variables that are written (note that some\n        # variables might be read and written)\n        for signature in read_write_info.signatures_written:\n            # Find the right symbol for the variable. Note that all variables\n            # in the input and output list have been detected as being used\n            # when the variable accesses were analysed. Therefore, these\n            # variables have References, and will already have been declared\n            # in the symbol table (in add_all_kernel_symbols).\n            sig_str = str(signature)\n            sym = symbol_table.lookup_with_tag(sig_str)\n\n            # The variable is written (and maybe read as well)\n            # ------------------------------------------------\n            # Declare a 'post' variable of the same type and\n            # read in its value.\n            post_name = sig_str+postfix\n            post_sym = symbol_table.new_symbol(post_name,\n                                               symbol_type=DataSymbol,\n                                               datatype=sym.datatype)\n            ExtractDriverCreator.add_call(program, read_var,\n                                          [Literal(post_name, CHARACTER_TYPE),\n                                           Reference(post_sym)])\n\n            # Now if a variable is written to, but not read, the variable\n            # is not allocated. So we need to allocate it and set it to 0.\n            if not read_write_info.is_read(signature):\n                if isinstance(post_sym.datatype, ArrayType):\n                    alloc = IntrinsicCall.create(\n                        IntrinsicCall.Intrinsic.ALLOCATE,\n                        [Reference(sym), (\"mold\", Reference(post_sym))])\n                    program.addchild(alloc)\n                set_zero = Assignment.create(Reference(sym),\n                                             Literal(\"0\", INTEGER_TYPE))\n                program.addchild(set_zero)\n            output_symbols.append((sym, post_sym))\n        return output_symbols\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that calculates the effective electronic coupling based on single determinant diabatic states using the pyscf library. The code should first define a molecule with specific atoms and basis. Then, it should perform two state calculations with DFT, storing molecular orbital information into separate chkfiles. The code should then read the MO coefficients and occupation numbers from these chkfiles. Afterwards, it should calculate the overlap between two determinants, construct density matrices, calculate one-electron and two-electron part contributions, and calculate new total energy. Finally, the code should calculate the effective electronic coupling and print the results. The code should also remove the chkfiles at the end.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 4, "repo_full_name": "capytaine__capytaine", "instruction": "Generate code that performs the following tasks using the capytaine library:\n\n1. Set up logging with a specific level and format.\n2. Create a mesh of a sphere with a specified radius, center, and resolution using capytaine's mesh_sphere function. Create a floating body from this mesh and add a translation degree of freedom to it.\n3. Extract the immersed part of the mesh.\n4. Set up a BEMSolver.\n5. Define and solve a diffraction problem for the immersed part of the sphere, with a specified wave direction and omega.\n6. Define and solve a radiation problem for the immersed part of the sphere, with a specified radiating degree of freedom and omega.\n7. Define a free surface with specified x and y ranges and number of points in each direction.\n8. Compute the free surface elevation for both the diffraction and radiation results.\n9. Add incoming waves to the diffraction elevation.\n10. Create and run two animations: one for the diffraction result and one for the radiation result. Each animation should include the full sphere and the free surface, with specified face motions and elevations. The animations should run with a specified camera position.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Sphere(FloatingBody):\n    \"\"\"Sphere\n    Deprecated: please prefer capytaine.meshes.predefined.mesh_sphere()\n\n    Parameters\n    ----------\n    radius : float\n        radius of the sphere\n    center : 3-ple or array of shape (3,)\n        position of the geometric center of the sphere\n    ntheta : int\n        number of panels along a meridian (or number of parallels-1)\n    nphi : int\n        number of panels along a parallel (or number of meridians-1)\n    axial_symmetry : bool\n        if True, use the axial symmetry to build the mesh (default: True)\n    clip_free_surface : bool\n        if True, only mesh the part of the sphere where z < 0 (default: False),\n        can be used with center to obtain any clipped sphere,\n        if True, then ntheta is the number of parallel below the free surface.\n    name : string\n        a name identifying the sphere (default: \"sphere_id\" where id is an unique integer).\n    \"\"\"\n\n    def __init__(self, *, radius=1.0, center=(0, 0, 0),\n                 ntheta=10, nphi=10, clip_free_surface=False,\n                 axial_symmetry=True, clever=None,\n                 name=None):\n\n        LOG.warning(\"Deprecation warning: The class Sphere() is deprecated. \"\n                \"Please prefer the function capytaine.meshes.predefined.mesh_sphere()\")\n\n        if clever is not None:\n            LOG.warning(\"Deprecation warning: `clever` argument for Sphere is deprecated. \"\n                        \"Use `axial_symmetry` instead.\")\n\n        if name is None:\n            name = f\"sphere_{next(Mesh._ids)}\"\n\n        if clip_free_surface:\n            if center[2] < -radius:  # fully immersed\n                pass\n            elif center[2] < radius:\n                ntheta = int(ntheta*np.pi/np.arccos(center[2]/radius))\n            else:\n                raise ValueError(\"Impossible to mesh the immersed hull of a sphere completely out of the water\")\n\n        mesh = mesh_sphere(radius=radius, center=center, resolution=(ntheta, nphi), axial_symmetry=axial_symmetry, name=f\"{name}_mesh\")\n\n        if clip_free_surface:\n            mesh.keep_immersed_part()\n\n        self.radius = radius\n        self.geometric_center = np.array(center, dtype=float)\n        FloatingBody.__init__(self, mesh=mesh, name=name)\n\n# --- Snippet Separator ---\n\ndef incoming_waves(self, problem: \"DiffractionProblem\") -> np.ndarray:\n        \"\"\"Free surface elevation of the undisturbed incoming waves\n        for a given diffraction problem.\n        Kept for legacy, but not recommended for use.\n        \"\"\"\n        from capytaine.bem.airy_waves import airy_waves_free_surface_elevation\n        return airy_waves_free_surface_elevation(self, problem)\n\n# --- Snippet Separator ---\n\nclass BEMSolver:\n    \"\"\"\n    Solver for linear potential flow problems.\n\n    Parameters\n    ----------\n    green_function: AbstractGreenFunction, optional\n        Object handling the computation of the Green function.\n        (default: :class:`~capytaine.green_function.delhommeau.Delhommeau`)\n    engine: MatrixEngine, optional\n        Object handling the building of matrices and the resolution of linear systems with these matrices.\n        (default: :class:`~capytaine.bem.engines.BasicMatrixEngine`)\n\n    Attributes\n    ----------\n    exportable_settings : dict\n        Settings of the solver that can be saved to reinit the same solver later.\n    \"\"\"\n\n    def __init__(self, *, green_function=None, engine=None):\n        self.green_function = Delhommeau() if green_function is None else green_function\n        self.engine = BasicMatrixEngine() if engine is None else engine\n\n        try:\n            self.exportable_settings = {\n                **self.green_function.exportable_settings,\n                **self.engine.exportable_settings\n            }\n        except AttributeError:\n            pass\n\n    def __str__(self):\n        return f\"BEMSolver(engine={self.engine}, green_function={self.green_function})\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    @classmethod\n    def from_exported_settings(settings):\n        raise NotImplementedError\n\n    def solve(self, problem, keep_details=True):\n        \"\"\"Solve the linear potential flow problem.\n\n        Parameters\n        ----------\n        problem: LinearPotentialFlowProblem\n            the problem to be solved\n        keep_details: bool, optional\n            if True, store the sources and the potential on the floating body in the output object\n            (default: True)\n\n        Returns\n        -------\n        LinearPotentialFlowResult\n            an object storing the problem data and its results\n        \"\"\"\n        LOG.info(\"Solve %s.\", problem)\n\n        S, K = self.engine.build_matrices(\n            problem.body.mesh, problem.body.mesh,\n            problem.free_surface, problem.water_depth, problem.wavenumber,\n            self.green_function\n        )\n        sources = self.engine.linear_solver(K, problem.boundary_condition)\n        potential = S @ sources\n        pressure = 1j * problem.omega * problem.rho * potential\n\n        forces = problem.body.integrate_pressure(pressure)\n\n        if not keep_details:\n            result = problem.make_results_container(forces)\n        else:\n            result = problem.make_results_container(forces, sources, potential, pressure)\n\n        LOG.debug(\"Done!\")\n\n        return result\n\n    def solve_all(self, problems, *, n_jobs=1, **kwargs):\n        \"\"\"Solve several problems.\n        Optional keyword arguments are passed to `BEMSolver.solve`.\n\n        Parameters\n        ----------\n        problems: list of LinearPotentialFlowProblem\n            several problems to be solved\n        n_jobs: int, optional (default: 1)\n            the number of jobs to run in parallel using the optional dependency `joblib`\n            By defaults: do not use joblib and solve sequentially.\n\n        Returns\n        -------\n        list of LinearPotentialFlowResult\n            the solved problems\n        \"\"\"\n        if n_jobs == 1:  # force sequential resolution\n            return [self.solve(pb, **kwargs) for pb in sorted(problems)]\n        else:\n            joblib = silently_import_optional_dependency(\"joblib\")\n            if joblib is None:\n                raise ImportError(f\"Setting the `n_jobs` argument to {n_jobs} requires the missing optional dependency 'joblib'.\")\n            groups_of_problems = LinearPotentialFlowProblem._group_for_parallel_resolution(problems)\n            groups_of_results = joblib.Parallel(n_jobs=n_jobs)(joblib.delayed(self.solve_all)(grp, n_jobs=1, **kwargs) for grp in groups_of_problems)\n            results = [res for grp in groups_of_results for res in grp]  # flatten the nested list\n            return results\n\n    def fill_dataset(self, dataset, bodies, *, n_jobs=1, **kwargs):\n        \"\"\"Solve a set of problems defined by the coordinates of an xarray dataset.\n\n        Parameters\n        ----------\n        dataset : xarray Dataset\n            dataset containing the problems parameters: frequency, radiating_dof, water_depth, ...\n        bodies : FloatingBody or list of FloatingBody\n            The body or bodies involved in the problems\n            They should all have different names.\n        n_jobs: int, optional (default: 1)\n            the number of jobs to run in parallel using the optional dependency `joblib`\n            By defaults: do not use joblib and solve sequentially.\n\n        Returns\n        -------\n        xarray Dataset\n        \"\"\"\n        attrs = {'start_of_computation': datetime.now().isoformat(),\n                 **self.exportable_settings}\n        problems = problems_from_dataset(dataset, bodies)\n        if 'theta' in dataset.coords:\n            results = self.solve_all(problems, keep_details=True, n_jobs=n_jobs)\n            kochin = kochin_data_array(results, dataset.coords['theta'])\n            dataset = assemble_dataset(results, attrs=attrs, **kwargs)\n            dataset.update(kochin)\n        else:\n            results = self.solve_all(problems, keep_details=False, n_jobs=n_jobs)\n            dataset = assemble_dataset(results, attrs=attrs, **kwargs)\n        return dataset\n\n\n    def compute_potential(self, points, result):\n        \"\"\"Compute the value of the potential at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the potential should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the potential at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_points(points, keep_mesh=True)\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        S, _ = self.green_function.evaluate(points, result.body.mesh, result.free_surface, result.water_depth, result.wavenumber)\n        potential = S @ result.sources  # Sum the contributions of all panels in the mesh\n        return potential.reshape(output_shape)\n\n\n    def compute_velocity(self, points, result):\n        \"\"\"Compute the value of the velocity vector at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the velocity should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (3,) or (N,, 3) or (nx, ny, nz, 3) or (mesh.nb_faces, 3) depending of the kind of input\n            The value of the velocity at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_points(points, keep_mesh=True)\n\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        _, gradG = self.green_function.evaluate(points, result.body.mesh, result.free_surface, result.water_depth, result.wavenumber,\n                                                early_dot_product=False)\n        velocities = np.einsum('ijk,j->ik', gradG, result.sources)  # Sum the contributions of all panels in the mesh\n        return velocities.reshape((*output_shape, 3))\n\n\n    def compute_pressure(self, points, result):\n        \"\"\"Compute the value of the pressure at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the pressure should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the pressure at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        return 1j * result.omega * result.rho * self.compute_potential(points, results)\n\n\n    def compute_free_surface_elevation(self, points, result):\n        \"\"\"Compute the value of the free surface elevation at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (2,) or (N, 2), or 2-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the free surface elevation should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the free surface elevation at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_free_surface_points(points, keep_mesh=True)\n\n        fs_elevation = 1j*result.omega/result.g * self.compute_potential(points, result)\n        return fs_elevation.reshape(output_shape)\n\n\n    ## Legacy\n\n    def get_potential_on_mesh(self, result, mesh, chunk_size=50):\n        \"\"\"Compute the potential on a mesh for the potential field of a previously solved problem.\n        Since the interaction matrix does not need to be computed in full to compute the matrix-vector product,\n        only a few lines are evaluated at a time to reduce the memory cost of the operation.\n\n        The newer method :code:`compute_potential` should be prefered in the future.\n\n        Parameters\n        ----------\n        result : LinearPotentialFlowResult\n            the return of the BEM solver\n        mesh : Mesh or CollectionOfMeshes\n            a mesh\n        chunk_size: int, optional\n            Number of lines to compute in the matrix.\n            (legacy, should be passed as an engine setting instead).\n\n        Returns\n        -------\n        array of shape (mesh.nb_faces,)\n            potential on the faces of the mesh\n\n        Raises\n        ------\n        Exception: if the :code:`Result` object given as input does not contain the source distribution.\n        \"\"\"\n        LOG.info(f\"Compute potential on {mesh.name} for {result}.\")\n\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        if chunk_size > mesh.nb_faces:\n            S = self.engine.build_S_matrix(\n                mesh,\n                result.body.mesh,\n                result.free_surface, result.water_depth, result.wavenumber,\n                self.green_function\n            )\n            phi = S @ result.sources\n\n        else:\n            phi = np.empty((mesh.nb_faces,), dtype=np.complex128)\n            for i in range(0, mesh.nb_faces, chunk_size):\n                faces_to_extract = list(range(i, min(i+chunk_size, mesh.nb_faces)))\n                S = self.engine.build_S_matrix(\n                    mesh.extract_faces(faces_to_extract),\n                    result.body.mesh,\n                    result.free_surface, result.water_depth, result.wavenumber,\n                    self.green_function\n                )\n                phi[i:i+chunk_size] = S @ result.sources\n\n        LOG.debug(f\"Done computing potential on {mesh.name} for {result}.\")\n\n        return phi\n\n    def get_free_surface_elevation(self, result, free_surface, keep_details=False):\n        \"\"\"Compute the elevation of the free surface on a mesh for a previously solved problem.\n\n        The newer method :code:`compute_free_surface_elevation` should be prefered in the future.\n\n        Parameters\n        ----------\n        result : LinearPotentialFlowResult\n            the return of the solver\n        free_surface : FreeSurface\n            a meshed free surface\n        keep_details : bool, optional\n            if True, keep the free surface elevation in the LinearPotentialFlowResult (default:False)\n\n        Returns\n        -------\n        array of shape (free_surface.nb_faces,)\n            the free surface elevation on each faces of the meshed free surface\n\n        Raises\n        ------\n        Exception: if the :code:`Result` object given as input does not contain the source distribution.\n        \"\"\"\n        fs_elevation = 1j*result.omega/result.g * self.get_potential_on_mesh(result, free_surface.mesh)\n        if keep_details:\n            result.fs_elevation[free_surface] = fs_elevation\n        return fs_elevation\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks using the capytaine library:\n\n1. Set up logging with a specific level and format.\n2. Create a mesh of a sphere with a specified radius, center, and resolution using capytaine's mesh_sphere function. Create a floating body from this mesh and add a translation degree of freedom to it.\n3. Extract the immersed part of the mesh.\n4. Set up a BEMSolver.\n5. Define and solve a diffraction problem for the immersed part of the sphere, with a specified wave direction and omega.\n6. Define and solve a radiation problem for the immersed part of the sphere, with a specified radiating degree of freedom and omega.\n7. Define a free surface with specified x and y ranges and number of points in each direction.\n8. Compute the free surface elevation for both the diffraction and radiation results.\n9. Add incoming waves to the diffraction elevation.\n10. Create and run two animations: one for the diffraction result and one for the radiation result. Each animation should include the full sphere and the free surface, with specified face motions and elevations. The animations should run with a specified camera position.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 5, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that initializes an emulator and several layers using the seedemu library. The code should create an Internet Exchange with a specific ID and set its display name and description. Then, it should create three Autonomous Systems with different IDs. For each Autonomous System, the code should create a network, a router that joins two networks, and a host that joins a network. It should also install a web service on a virtual node and bind this node to a host. The code should set display names and descriptions for the networks, routers, and Autonomous Systems. After creating the Autonomous Systems, the code should peer them with the Internet Exchange. Finally, the code should add all the layers to the emulator, render the emulator, and compile it with Docker, enabling the internet map.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int,\n    services: List[Service]):\n    \"\"\"!\n    @brief create a new stub AS.\n\n    @param emu reference to the Emulator object.\n    @param base reference to the base layer.\n    @param asn ASN for the newly created AS.\n    @param exchange IXP ID for new newly created AS to join.\n    @param list of instances of Service to install on hosts. One host will be\n    created for each.\n    \"\"\"\n\n    # Create AS and internal network\n    stub_as = base.createAutonomousSystem(asn)\n    stub_as.createNetwork('net0')\n\n    # Create a BGP router \n    # Attach the router to both the internal and external networks\n    router = stub_as.createRouter('router0')\n    router.joinNetwork('net0')\n    router.joinNetwork('ix{}'.format(exchange))\n\n    # Create a host node for each specified service\n    createHostsOnNetwork(emu, stub_as, 'net0', services)\n\n# --- Snippet Separator ---\n\ndef makeEmulatorBaseWith5StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef __init__(\n        self,\n        namingScheme: str = \"as{asn}{role}-{displayName}-{primaryIp}\",\n        selfManagedNetwork: bool = False,\n        dummyNetworksPool: str = '10.128.0.0/9',\n        dummyNetworksMask: int = 24,\n        internetMapEnabled: bool = False,\n        internetMapPort: int = 8080,\n        etherViewEnabled: bool = False,\n        etherViewPort: int = 5000,\n        clientHideServiceNet: bool = True\n    ):\n        \"\"\"!\n        @brief Docker compiler constructor.\n\n        @param namingScheme (optional) node naming scheme. Available variables\n        are: {asn}, {role} (r - router, h - host, rs - route server), {name},\n        {primaryIp} and {displayName}. {displayName} will automatically fall\n        back to {name} if\n        Default to as{asn}{role}-{displayName}-{primaryIp}.\n        @param selfManagedNetwork (optional) use self-managed network. Enable\n        this to manage the network inside containers instead of using docker's\n        network management. This works by first assigning \"dummy\" prefix and\n        address to containers, then replace those address with \"real\" address\n        when the containers start. This will allow the use of overlapping\n        networks in the emulation and will allow the use of the \".1\" address on\n        nodes. Note this will break port forwarding (except for service nodes\n        like real-world access node and remote access node.) Default to False.\n        @param dummyNetworksPool (optional) dummy networks pool. This should not\n        overlap with any \"real\" networks used in the emulation, including\n        loopback IP addresses. Default to 10.128.0.0/9.\n        @param dummyNetworksMask (optional) mask of dummy networks. Default to\n        24.\n        @param internetMapEnabled (optional) set if seedemu internetMap should be enabled.\n        Default to False. Note that the seedemu internetMap allows unauthenticated\n        access to all nodes, which can potentially allow root access to the\n        emulator host. Only enable seedemu in a trusted network.\n        @param internetMapPort (optional) set seedemu internetMap port. Default to 8080.\n        @param etherViewEnabled (optional) set if seedemu EtherView should be enabled.\n        Default to False.\n        @param etherViewPort (optional) set seedemu EtherView port. Default to 5000.\n        @param clientHideServiceNet (optional) hide service network for the\n        client map by not adding metadata on the net. Default to True.\n        \"\"\"\n        self.__networks = \"\"\n        self.__services = \"\"\n        self.__naming_scheme = namingScheme\n        self.__self_managed_network = selfManagedNetwork\n        self.__dummy_network_pool = IPv4Network(dummyNetworksPool).subnets(new_prefix = dummyNetworksMask)\n\n        self.__internet_map_enabled = internetMapEnabled\n        self.__internet_map_port = internetMapPort\n\n        self.__ether_view_enabled = etherViewEnabled\n        self.__ether_view_port = etherViewPort\n\n        self.__client_hide_svcnet = clientHideServiceNet\n\n        self.__images = {}\n        self.__forced_image = None\n        self.__disable_images = False\n        self._used_images = set()\n        self.__image_per_node_list = {}\n\n        for name, image in BASESYSTEM_DOCKERIMAGE_MAPPING.items():\n            priority = 0\n            if name == BaseSystem.DEFAULT:\n                priority = 1\n            self.addImage(image, priority=priority)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that initializes an emulator and several layers using the seedemu library. The code should create an Internet Exchange with a specific ID and set its display name and description. Then, it should create three Autonomous Systems with different IDs. For each Autonomous System, the code should create a network, a router that joins two networks, and a host that joins a network. It should also install a web service on a virtual node and bind this node to a host. The code should set display names and descriptions for the networks, routers, and Autonomous Systems. After creating the Autonomous Systems, the code should peer them with the Internet Exchange. Finally, the code should add all the layers to the emulator, render the emulator, and compile it with Docker, enabling the internet map.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 6, "repo_full_name": "urwid__urwid", "instruction": "Generate code that creates a user interface for a crystalfontz 635 LCD display using the urwid library in Python. The interface should include a menu with options for adjusting display settings, cursor settings, LED settings, and an 'About this Demo' section. The display settings should allow the user to adjust brightness and contrast. The cursor settings should allow the user to choose between different cursor styles. The LED settings should allow the user to adjust the color of each LED. The 'About this Demo' section should display a text about the demo. The interface should also include custom characters for a check box, a radio button, and a progress bar. The check box and radio button should use only one character, including a custom character. The progress bar should use custom characters to represent different levels of progress. The interface should also include a horizontal slider control using custom characters. The slider should be able to move in response to user input. The interface should also include a menu option indicated with a single arrow character. The menu should be able to go back to the previous menu when the cancel button is pressed. The interface should be able to connect to the LCD display using the provided command line argument.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class InterfaceWithSidebar(BoxLayout):\n    '''The default Settings interface class. It displays a sidebar menu\n    with names of available settings panels, which may be used to switch\n    which one is currently displayed.\n\n    See :meth:`~InterfaceWithSidebar.add_panel` for information on the\n    method you must implement if creating your own interface.\n\n    This class also dispatches an event 'on_close', which is triggered\n    when the sidebar menu's close button is released. If creating your\n    own interface widget, it should also dispatch such an event which\n    will automatically be caught by :class:`Settings` and used to\n    trigger its own 'on_close' event.\n\n    '''\n\n    menu = ObjectProperty()\n    '''(internal) A reference to the sidebar menu widget.\n\n    :attr:`menu` is an :class:`~kivy.properties.ObjectProperty` and\n    defaults to None.\n    '''\n\n    content = ObjectProperty()\n    '''(internal) A reference to the panel display widget (a\n    :class:`ContentPanel`).\n\n    :attr:`content` is an :class:`~kivy.properties.ObjectProperty` and\n    defaults to None.\n\n    '''\n\n    __events__ = ('on_close', )\n\n    def __init__(self, *args, **kwargs):\n        super(InterfaceWithSidebar, self).__init__(*args, **kwargs)\n        self.menu.close_button.bind(\n            on_release=lambda j: self.dispatch('on_close'))\n\n    def add_panel(self, panel, name, uid):\n        '''This method is used by Settings to add new panels for possible\n        display. Any replacement for ContentPanel *must* implement\n        this method.\n\n        :Parameters:\n            `panel`: :class:`SettingsPanel`\n                It should be stored and the interface should provide a way to\n                switch between panels.\n            `name`:\n                The name of the panel as a string. It may be used to represent\n                the panel but isn't necessarily unique.\n            `uid`:\n                A unique int identifying the panel. It should be used to\n                identify and switch between panels.\n\n        '''\n        self.menu.add_item(name, uid)\n        self.content.add_panel(panel, name, uid)\n\n    def on_close(self, *args):\n        pass\n\n# --- Snippet Separator ---\n\nclass InterfaceWithSpinner(BoxLayout):\n    '''A settings interface that displays a spinner at the top for\n    switching between panels.\n\n    The workings of this class are considered internal and are not\n    documented. See :meth:`InterfaceWithSidebar` for\n    information on implementing your own interface class.\n\n    '''\n\n    __events__ = ('on_close', )\n\n    menu = ObjectProperty()\n    '''(internal) A reference to the sidebar menu widget.\n\n    :attr:`menu` is an :class:`~kivy.properties.ObjectProperty` and\n    defaults to None.\n    '''\n\n    content = ObjectProperty()\n    '''(internal) A reference to the panel display widget (a\n    :class:`ContentPanel`).\n\n    :attr:`menu` is an :class:`~kivy.properties.ObjectProperty` and\n    defaults to None.\n\n    '''\n\n    def __init__(self, *args, **kwargs):\n        super(InterfaceWithSpinner, self).__init__(*args, **kwargs)\n        self.menu.close_button.bind(\n            on_release=lambda j: self.dispatch('on_close'))\n\n    def add_panel(self, panel, name, uid):\n        '''This method is used by Settings to add new panels for possible\n        display. Any replacement for ContentPanel *must* implement\n        this method.\n\n        :Parameters:\n            `panel`: :class:`SettingsPanel`\n                It should be stored and the interface should provide a way to\n                switch between panels.\n            `name`:\n                The name of the panel as a string. It may be used to represent\n                the panel but may not be unique.\n            `uid`:\n                A unique int identifying the panel. It should be used to\n                identify and switch between panels.\n\n        '''\n        self.content.add_panel(panel, name, uid)\n        self.menu.add_item(name, uid)\n\n    def on_close(self, *args):\n        pass\n\n# --- Snippet Separator ---\n\ndef add_panel(self, panel, name, uid):\n        '''This method is used by Settings to add new panels for possible\n        display. Any replacement for ContentPanel *must* implement\n        this method.\n\n        :Parameters:\n            `panel`: :class:`SettingsPanel`\n                It should be stored and the interface should provide a way to\n                switch between panels.\n            `name`:\n                The name of the panel as a string. It may be used to represent\n                the panel but isn't necessarily unique.\n            `uid`:\n                A unique int identifying the panel. It should be used to\n                identify and switch between panels.\n\n        '''\n        self.menu.add_item(name, uid)\n        self.content.add_panel(panel, name, uid)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a user interface for a crystalfontz 635 LCD display using the urwid library in Python. The interface should include a menu with options for adjusting display settings, cursor settings, LED settings, and an 'About this Demo' section. The display settings should allow the user to adjust brightness and contrast. The cursor settings should allow the user to choose between different cursor styles. The LED settings should allow the user to adjust the color of each LED. The 'About this Demo' section should display a text about the demo. The interface should also include custom characters for a check box, a radio button, and a progress bar. The check box and radio button should use only one character, including a custom character. The progress bar should use custom characters to represent different levels of progress. The interface should also include a horizontal slider control using custom characters. The slider should be able to move in response to user input. The interface should also include a menu option indicated with a single arrow character. The menu should be able to go back to the previous menu when the cancel button is pressed. The interface should be able to connect to the LCD display using the provided command line argument.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 7, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that performs the following tasks using the avalanche library:\n\n1. Parse command line arguments to determine the device to use for computations.\n2. Define transformations for training and testing data.\n3. Create a benchmark using the MNIST dataset with the defined transformations.\n4. Create a simple MLP model with the number of classes equal to the number of classes in the benchmark.\n5. Define various loggers including a text logger, an interactive logger, a CSV logger, and a Tensorboard logger.\n6. Define an evaluation plugin that computes a wide range of metrics including accuracy, loss, class accuracy, AMCA, forgetting, backward transfer, forward transfer, CPU usage, timing, RAM usage, GPU usage, disk usage, MAC, and labels repartition metrics. The plugin should log these metrics using the defined loggers.\n7. Create a Naive continual learning strategy using the defined model, an SGD optimizer, a CrossEntropyLoss loss function, and the defined evaluation plugin.\n8. Train the model on the benchmark's training stream and evaluate it on the benchmark's test stream, printing the results after each experience.\n9. After all experiences, print all the metrics stored by the evaluation plugin.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def bwt_metrics(*, experience=False, stream=False) \\\n        -> List[PluginMetric]:\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param experience: If True, will return a metric able to log\n        the backward transfer on each evaluation experience.\n    :param stream: If True, will return a metric able to log\n        the backward transfer averaged over the evaluation stream experiences\n        which have been observed during training.\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n\n    if experience:\n        metrics.append(ExperienceBWT())\n\n    if stream:\n        metrics.append(StreamBWT())\n\n    return metrics\n\n# --- Snippet Separator ---\n\ndef forward_transfer_metrics(*, experience=False, stream=False):\n    \"\"\"\n    Helper method that can be used to obtain the desired set of\n    plugin metrics.\n\n    :param experience: If True, will return a metric able to log\n        the forward transfer on each evaluation experience.\n    :param stream: If True, will return a metric able to log\n        the forward transfer averaged over the evaluation stream experiences,\n        which have been observed during training.\n\n    :return: A list of plugin metrics.\n    \"\"\"\n\n    metrics = []\n\n    if experience:\n        metrics.append(ExperienceForwardTransfer())\n\n    if stream:\n        metrics.append(StreamForwardTransfer())\n\n    return metrics\n\n# --- Snippet Separator ---\n\ndef __init__(self,\n                 *metrics: Union['PluginMetric', Sequence['PluginMetric']],\n                 loggers: Union['StrategyLogger',\n                                Sequence['StrategyLogger']] = None,\n                 collect_all=True,\n                 benchmark=None,\n                 strict_checks=False,\n                 suppress_warnings=False):\n        \"\"\"\n        Creates an instance of the evaluation plugin.\n\n        :param metrics: The metrics to compute.\n        :param loggers: The loggers to be used to log the metric values.\n        :param collect_all: if True, collect in a separate dictionary all\n            metric curves values. This dictionary is accessible with\n            `get_all_metrics` method.\n        :param benchmark: continual learning benchmark needed to check stream\n            completeness during evaluation or other kind of properties. If\n            None, no check will be conducted and the plugin will emit a\n            warning to signal this fact.\n        :param strict_checks: if True, `benchmark` has to be provided.\n            In this case, only full evaluation streams are admitted when\n            calling `eval`. An error will be raised otherwise. When False,\n            `benchmark` can be `None` and only warnings will be raised.\n        :param suppress_warnings: if True, warnings and errors will never be\n            raised from the plugin.\n            If False, warnings and errors will be raised following\n            `benchmark` and `strict_checks` behavior.\n        \"\"\"\n        super().__init__()\n        self.collect_all = collect_all\n        self.benchmark = benchmark\n        self.strict_checks = strict_checks\n        self.suppress_warnings = suppress_warnings\n        flat_metrics_list = []\n        for metric in metrics:\n            if isinstance(metric, Sequence):\n                flat_metrics_list += list(metric)\n            else:\n                flat_metrics_list.append(metric)\n        self.metrics = flat_metrics_list\n\n        if loggers is None:\n            loggers = []\n        elif not isinstance(loggers, Sequence):\n            loggers = [loggers]\n\n        if benchmark is None:\n            if not suppress_warnings:\n                if strict_checks:\n                    raise ValueError(\"Benchmark cannot be None \"\n                                     \"in strict mode.\")\n                else:\n                    warnings.warn(\n                        \"No benchmark provided to the evaluation plugin. \"\n                        \"Metrics may be computed on inconsistent portion \"\n                        \"of streams, use at your own risk.\")\n        else:\n            self.complete_test_stream = benchmark.test_stream\n\n        self.loggers: Sequence['StrategyLogger'] = loggers\n\n        if len(self.loggers) == 0:\n            warnings.warn('No loggers specified, metrics will not be logged')\n\n        if self.collect_all:\n            # for each curve collect all emitted values.\n            # dictionary key is full metric name.\n            # Dictionary value is a tuple of two lists.\n            # first list gathers x values (indices representing\n            # time steps at which the corresponding metric value\n            # has been emitted)\n            # second list gathers metric values\n            self.all_metric_results = defaultdict(lambda: ([], []))\n        # Dictionary of last values emitted. Dictionary key\n        # is the full metric name, while dictionary value is\n        # metric value.\n        self.last_metric_results = {}\n\n        self._active = True\n        \"\"\"If True, no metrics will be collected.\"\"\"\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks using the avalanche library:\n\n1. Parse command line arguments to determine the device to use for computations.\n2. Define transformations for training and testing data.\n3. Create a benchmark using the MNIST dataset with the defined transformations.\n4. Create a simple MLP model with the number of classes equal to the number of classes in the benchmark.\n5. Define various loggers including a text logger, an interactive logger, a CSV logger, and a Tensorboard logger.\n6. Define an evaluation plugin that computes a wide range of metrics including accuracy, loss, class accuracy, AMCA, forgetting, backward transfer, forward transfer, CPU usage, timing, RAM usage, GPU usage, disk usage, MAC, and labels repartition metrics. The plugin should log these metrics using the defined loggers.\n7. Create a Naive continual learning strategy using the defined model, an SGD optimizer, a CrossEntropyLoss loss function, and the defined evaluation plugin.\n8. Train the model on the benchmark's training stream and evaluate it on the benchmark's test stream, printing the results after each experience.\n9. After all experiences, print all the metrics stored by the evaluation plugin.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 8, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that initializes a scene in Blender using the blenderproc library. The code should parse command line arguments for paths to scene, texture, and material files, as well as an output directory. It should load a scene and label its objects based on a mapping from a CSV file. The code should also load materials and randomly assign them to 40% of the objects' materials. \n\nThe code should then load textures for the materials that were assigned to at least one object. It should extract floors and ceilings from wall objects and assign them appropriate category IDs. It should make all lamp objects emit light and make all ceiling objects emit a bit of light to brighten the room. \n\nThe code should then create a BVH tree containing all mesh objects and sample camera locations above the floor, ensuring there are no obstacles in front of the camera and that the scene coverage score is not too low. \n\nFinally, the code should enable normal, depth, and segmentation rendering, render the scene, and write the data to a .hdf5 file in the specified output directory.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def light_suncg_scene(lightbulb_emission_strength: float = 15, lampshade_emission_strength: float = 7,\n                      ceiling_emission_strength: float = 1.5):\n    \"\"\" Makes the lamps, windows and ceilings object emit light.\n\n    :param lightbulb_emission_strength: The emission strength that should be used for light bulbs. Default: 15\n    :param lampshade_emission_strength: The emission strength that should be used for lamp shades. Default: 7\n    :param ceiling_emission_strength: The emission strength that should be used for the ceiling. Default: 1.5\n    \"\"\"\n    # Read in the materials for lights and windows\n    lights, windows = Utility.read_suncg_lights_windows_materials()\n\n    collection_of_mats: Dict[str, Dict[str, Material]] = {\"lamp\": {}, \"window\": {}, \"ceiling\": {}}\n\n    # Make some objects emit lights\n    for obj in get_all_mesh_objects():\n        if obj.has_cp(\"modelId\"):\n            obj_id = obj.get_cp(\"modelId\")\n\n            # In the case of the lamp\n            if obj_id in lights:\n                _SuncgLighting.make_lamp_emissive(obj, lights[obj_id], collection_of_mats, lightbulb_emission_strength,\n                                                  lampshade_emission_strength)\n\n            # Make the windows emit light\n            if obj_id in windows:\n                _SuncgLighting.make_window_emissive(obj, collection_of_mats)\n\n            # Also make ceilings slightly emit light\n            if obj.get_name().startswith(\"Ceiling#\"):\n                _SuncgLighting.make_ceiling_emissive(obj, collection_of_mats, ceiling_emission_strength)\n\n# --- Snippet Separator ---\n\nclass _Front3DLoader:\n    \"\"\" Loads the 3D-Front dataset.\n\n    https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset\n\n    Each object gets the name based on the category/type, on top of that you can use a mapping specified in the\n    resources/front_3D folder.\n\n    The dataset already supports semantic segmentation with either the 3D-Front classes or the nyu classes.\n    As we have created this mapping ourselves it might be faulty.\n\n    The Front3DLoader creates automatically lights in the scene, by adding emission shaders to the ceiling and lamps.\n    \"\"\"\n\n    @staticmethod\n    def extract_hash_nr_for_texture(given_url: str, front_3D_texture_path: str) -> str:\n        \"\"\"\n        Constructs the path of the hash folder and checks if the texture is available if not it is downloaded\n\n        :param given_url: The url of the texture\n        :param front_3D_texture_path: The path to where the texture are saved\n        :return: The hash id, which is used in the url\n        \"\"\"\n        # extract the hash nr from the given url\n        hash_nr = given_url.split(\"/\")[-2]\n        hash_folder = os.path.join(front_3D_texture_path, hash_nr)\n        if not os.path.exists(hash_folder):\n            # download the file\n            os.makedirs(hash_folder)\n            warnings.warn(f\"This texture: {hash_nr} could not be found it will be downloaded.\")\n            # replace https with http as ssl connection out of blender are difficult\n            urlretrieve(given_url.replace(\"https://\", \"http://\"), os.path.join(hash_folder, \"texture.png\"))\n            if not os.path.exists(os.path.join(hash_folder, \"texture.png\")):\n                raise Exception(f\"The texture could not be found, the following url was used: \"\n                                f\"{front_3D_texture_path}, this is the extracted hash: {hash_nr}, \"\n                                f\"given url: {given_url}\")\n        return hash_folder\n\n    @staticmethod\n    def get_used_image(hash_folder_path: str, saved_image_dict: Mapping[str, bpy.types.Texture]) -> bpy.types.Texture:\n        \"\"\"\n        Returns a texture object for the given hash_folder_path, the textures are stored in the saved_image_dict,\n        to avoid that texture are loaded multiple times\n\n        :param hash_folder_path: Path to the hash folder\n        :param saved_image_dict: Dict which maps the hash_folder_paths to bpy.types.Texture\n        :return: The loaded texture bpy.types.Texture\n        \"\"\"\n        if hash_folder_path in saved_image_dict:\n            ret_used_image = saved_image_dict[hash_folder_path]\n        else:\n            textures = load_texture(hash_folder_path)\n            if len(textures) != 1:\n                raise Exception(f\"There is not just one texture: {len(textures)}\")\n            ret_used_image = textures[0].image\n            saved_image_dict[hash_folder_path] = ret_used_image\n        return ret_used_image\n\n    @staticmethod\n    def create_mesh_objects_from_file(data: dict, front_3D_texture_path: str, ceiling_light_strength: float,\n                                      label_mapping: LabelIdMapping, json_path: str) -> List[MeshObject]:\n        \"\"\"\n        This creates for a given data json block all defined meshes and assigns the correct materials.\n        This means that the json file contains some mesh, like walls and floors, which have to built up manually.\n\n        It also already adds the lighting for the ceiling\n\n        :param data: json data dir. Must contain \"material\" and \"mesh\"\n        :param front_3D_texture_path: Path to the 3D-FRONT-texture folder.\n        :param ceiling_light_strength: Strength of the emission shader used in the ceiling.\n        :param label_mapping: A dict which maps the names of the objects to ids.\n        :param json_path: Path to the json file, where the house information is stored.\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # extract all used materials -> there are more materials defined than used\n        used_materials = []\n        for mat in data[\"material\"]:\n            used_materials.append({\"uid\": mat[\"uid\"], \"texture\": mat[\"texture\"],\n                                   \"normaltexture\": mat[\"normaltexture\"], \"color\": mat[\"color\"]})\n\n        created_objects = []\n        # maps loaded images from image file path to bpy.type.image\n        saved_images = {}\n        saved_normal_images = {}\n        # materials based on colors to avoid recreating the same material over and over\n        used_materials_based_on_color = {}\n        # materials based on texture to avoid recreating the same material over and over\n        used_materials_based_on_texture = {}\n        for mesh_data in data[\"mesh\"]:\n            # extract the obj name, which also is used as the category_id name\n            used_obj_name = mesh_data[\"type\"].strip()\n            if used_obj_name == \"\":\n                used_obj_name = \"void\"\n            if \"material\" not in mesh_data:\n                warnings.warn(f\"Material is not defined for {used_obj_name} in this file: {json_path}\")\n                continue\n            # create a new mesh\n            obj = create_with_empty_mesh(used_obj_name, used_obj_name + \"_mesh\")\n            created_objects.append(obj)\n\n            # set two custom properties, first that it is a 3D_future object and second the category_id\n            obj.set_cp(\"is_3D_future\", True)\n            obj.set_cp(\"category_id\", label_mapping.id_from_label(used_obj_name.lower()))\n\n            # get the material uid of the current mesh data\n            current_mat = mesh_data[\"material\"]\n            used_mat = None\n            # search in the used materials after this uid\n            for u_mat in used_materials:\n                if u_mat[\"uid\"] == current_mat:\n                    used_mat = u_mat\n                    break\n            # If there should be a material used\n            if used_mat:\n                if used_mat[\"texture\"]:\n                    # extract the has folder is from the url and download it if necessary\n                    hash_folder = _Front3DLoader.extract_hash_nr_for_texture(used_mat[\"texture\"], front_3D_texture_path)\n                    if hash_folder in used_materials_based_on_texture and \"ceiling\" not in used_obj_name.lower():\n                        mat = used_materials_based_on_texture[hash_folder]\n                        obj.add_material(mat)\n                    else:\n                        # Create a new material\n                        mat = MaterialLoaderUtility.create(name=used_obj_name + \"_material\")\n                        principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n                        if used_mat[\"color\"]:\n                            principled_node.inputs[\"Base Color\"].default_value = mathutils.Vector(\n                                used_mat[\"color\"]) / 255.0\n\n                        used_image = _Front3DLoader.get_used_image(hash_folder, saved_images)\n                        mat.set_principled_shader_value(\"Base Color\", used_image)\n\n                        if \"ceiling\" in used_obj_name.lower():\n                            mat.make_emissive(ceiling_light_strength,\n                                              emission_color=mathutils.Vector(used_mat[\"color\"]) / 255.0)\n\n                        if used_mat[\"normaltexture\"]:\n                            # get the used image based on the normal texture path\n                            # extract the has folder is from the url and download it if necessary\n                            hash_folder = _Front3DLoader.extract_hash_nr_for_texture(used_mat[\"normaltexture\"],\n                                                                                     front_3D_texture_path)\n                            used_image = _Front3DLoader.get_used_image(hash_folder, saved_normal_images)\n\n                            # create normal texture\n                            normal_texture = MaterialLoaderUtility.create_image_node(mat.nodes, used_image, True)\n                            normal_map = mat.nodes.new(\"ShaderNodeNormalMap\")\n                            normal_map.inputs[\"Strength\"].default_value = 1.0\n                            mat.links.new(normal_texture.outputs[\"Color\"], normal_map.inputs[\"Color\"])\n                            # connect normal texture to principled shader\n                            mat.set_principled_shader_value(\"Normal\", normal_map.outputs[\"Normal\"])\n\n                        obj.add_material(mat)\n                        used_materials_based_on_texture[hash_folder] = mat\n                # if there is a normal color used\n                elif used_mat[\"color\"]:\n                    used_hash = tuple(used_mat[\"color\"])\n                    if used_hash in used_materials_based_on_color and \"ceiling\" not in used_obj_name.lower():\n                        mat = used_materials_based_on_color[used_hash]\n                    else:\n                        # Create a new material\n                        mat = MaterialLoaderUtility.create(name=used_obj_name + \"_material\")\n                        # create a principled node and set the default color\n                        principled_node = mat.get_the_one_node_with_type(\"BsdfPrincipled\")\n                        principled_node.inputs[\"Base Color\"].default_value = mathutils.Vector(used_mat[\"color\"]) / 255.0\n                        # if the object is a ceiling add some light output\n                        if \"ceiling\" in used_obj_name.lower():\n                            mat.make_emissive(ceiling_light_strength,\n                                              emission_color=mathutils.Vector(used_mat[\"color\"]) / 255.0)\n                        else:\n                            used_materials_based_on_color[used_hash] = mat\n\n                    # as this material was just created the material is just append it to the empty list\n                    obj.add_material(mat)\n\n            # extract the vertices from the mesh_data\n            vert = [float(ele) for ele in mesh_data[\"xyz\"]]\n            # extract the faces from the mesh_data\n            faces = mesh_data[\"faces\"]\n            # extract the normals from the mesh_data\n            normal = [float(ele) for ele in mesh_data[\"normal\"]]\n\n            # map those to the blender coordinate system\n            num_vertices = int(len(vert) / 3)\n            vertices = np.reshape(np.array(vert), [num_vertices, 3])\n            normal = np.reshape(np.array(normal), [num_vertices, 3])\n            # flip the first and second value\n            vertices[:, 1], vertices[:, 2] = vertices[:, 2], vertices[:, 1].copy()\n            normal[:, 1], normal[:, 2] = normal[:, 2], normal[:, 1].copy()\n            # reshape back to a long list\n            vertices = np.reshape(vertices, [num_vertices * 3])\n            normal = np.reshape(normal, [num_vertices * 3])\n\n            # add this new data to the mesh object\n            mesh = obj.get_mesh()\n            mesh.vertices.add(num_vertices)\n            mesh.vertices.foreach_set(\"co\", vertices)\n            mesh.vertices.foreach_set(\"normal\", normal)\n\n            # link the faces as vertex indices\n            num_vertex_indicies = len(faces)\n            mesh.loops.add(num_vertex_indicies)\n            mesh.loops.foreach_set(\"vertex_index\", faces)\n\n            # the loops are set based on how the faces are a ranged\n            num_loops = int(num_vertex_indicies / 3)\n            mesh.polygons.add(num_loops)\n            # always 3 vertices form one triangle\n            loop_start = np.arange(0, num_vertex_indicies, 3)\n            # the total size of each triangle is therefore 3\n            loop_total = [3] * num_loops\n            mesh.polygons.foreach_set(\"loop_start\", loop_start)\n            mesh.polygons.foreach_set(\"loop_total\", loop_total)\n\n            # the uv coordinates are reshaped then the face coords are extracted\n            uv_mesh_data = [float(ele) for ele in mesh_data[\"uv\"] if ele is not None]\n            # bb1737bf-dae6-4215-bccf-fab6f584046b.json includes one mesh which only has no UV mapping\n            if uv_mesh_data:\n                uv = np.reshape(np.array(uv_mesh_data), [num_vertices, 2])\n                used_uvs = uv[faces, :]\n                # and again reshaped back to the long list\n                used_uvs = np.reshape(used_uvs, [2 * num_vertex_indicies])\n\n                mesh.uv_layers.new(name=\"new_uv_layer\")\n                mesh.uv_layers[-1].data.foreach_set(\"uv\", used_uvs)\n            else:\n                warnings.warn(f\"This mesh {obj.get_name()} does not have a specified uv map!\")\n\n            # this update converts the upper data into a mesh\n            mesh.update()\n\n            # the generation might fail if the data does not line up\n            # this is not used as even if the data does not line up it is still able to render the objects\n            # We assume that not all meshes in the dataset do conform with the mesh standards set in blender\n            # result = mesh.validate(verbose=False)\n            # if result:\n            #    raise Exception(\"The generation of the mesh: {} failed!\".format(used_obj_name))\n\n        return created_objects\n\n    @staticmethod\n    def load_furniture_objs(data: dict, future_model_path: str, lamp_light_strength: float,\n                            label_mapping: LabelIdMapping) -> List[MeshObject]:\n        \"\"\"\n        Load all furniture objects specified in the json file, these objects are stored as \"raw_model.obj\" in the\n        3D_future_model_path. For lamp the lamp_light_strength value can be changed via the config.\n\n        :param data: json data dir. Should contain \"furniture\"\n        :param future_model_path: Path to the models used in the 3D-Front dataset.\n        :param lamp_light_strength: Strength of the emission shader used in each lamp.\n        :param label_mapping: A dict which maps the names of the objects to ids.\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # collect all loaded furniture objects\n        all_objs = []\n        # for each furniture element\n        for ele in data[\"furniture\"]:\n            # create the paths based on the \"jid\"\n            folder_path = os.path.join(future_model_path, ele[\"jid\"])\n            obj_file = os.path.join(folder_path, \"raw_model.obj\")\n            # if the object exists load it -> a lot of object do not exist\n            # we are unsure why this is -> we assume that not all objects have been made public\n            if os.path.exists(obj_file) and not \"7e101ef3-7722-4af8-90d5-7c562834fabd\" in obj_file:\n                # load all objects from this .obj file\n                objs = load_obj(filepath=obj_file)\n                # extract the name, which serves as category id\n                used_obj_name = \"\"\n                if \"category\" in ele:\n                    used_obj_name = ele[\"category\"]\n                elif \"title\" in ele:\n                    used_obj_name = ele[\"title\"]\n                    if \"/\" in used_obj_name:\n                        used_obj_name = used_obj_name.split(\"/\")[0]\n                if used_obj_name == \"\":\n                    used_obj_name = \"others\"\n                for obj in objs:\n                    obj.set_name(used_obj_name)\n                    # add some custom properties\n                    obj.set_cp(\"uid\", ele[\"uid\"])\n                    # this custom property determines if the object was used before\n                    # is needed to only clone the second appearance of this object\n                    obj.set_cp(\"is_used\", False)\n                    obj.set_cp(\"is_3D_future\", True)\n                    obj.set_cp(\"3D_future_type\", \"Non-Object\")  # is an non object used for the interesting score\n                    # set the category id based on the used obj name\n                    obj.set_cp(\"category_id\", label_mapping.id_from_label(used_obj_name.lower()))\n                    # walk over all materials\n                    for mat in obj.get_materials():\n                        if mat is None:\n                            continue\n                        principled_node = mat.get_nodes_with_type(\"BsdfPrincipled\")\n                        if \"bed\" in used_obj_name.lower() or \"sofa\" in used_obj_name.lower():\n                            if len(principled_node) == 1:\n                                principled_node[0].inputs[\"Roughness\"].default_value = 0.5\n                        is_lamp = \"lamp\" in used_obj_name.lower()\n                        if len(principled_node) == 0 and is_lamp:\n                            # this material has already been transformed\n                            continue\n                        if len(principled_node) == 1:\n                            principled_node = principled_node[0]\n                        else:\n                            raise ValueError(f\"The amount of principle nodes can not be more than 1, \"\n                                             f\"for obj: {obj.get_name()}!\")\n\n                        # Front3d .mtl files contain emission color which make the object mistakenly emissive\n                        # => Reset the emission color\n                        principled_node.inputs[\"Emission\"].default_value[:3] = [0, 0, 0]\n\n                        # For each a texture node\n                        image_node = mat.new_node('ShaderNodeTexImage')\n                        # and load the texture.png\n                        base_image_path = os.path.join(folder_path, \"texture.png\")\n                        image_node.image = bpy.data.images.load(base_image_path, check_existing=True)\n                        mat.link(image_node.outputs['Color'], principled_node.inputs['Base Color'])\n                        # if the object is a lamp, do the same as for the ceiling and add an emission shader\n                        if is_lamp:\n                            mat.make_emissive(lamp_light_strength)\n\n                all_objs.extend(objs)\n            elif \"7e101ef3-7722-4af8-90d5-7c562834fabd\" in obj_file:\n                warnings.warn(f\"This file {obj_file} was skipped as it can not be read by blender.\")\n        return all_objs\n\n    @staticmethod\n    def move_and_duplicate_furniture(data: dict, all_loaded_furniture: list) -> List[MeshObject]:\n        \"\"\"\n        Move and duplicate the furniture depending on the data in the data json dir.\n        After loading each object gets a location based on the data in the json file. Some objects are used more than\n        once these are duplicated and then placed.\n\n        :param data: json data dir. Should contain \"scene\", which should contain \"room\"\n        :param all_loaded_furniture: all objects which have been loaded in load_furniture_objs\n        :return: The list of loaded mesh objects.\n        \"\"\"\n        # this rotation matrix rotates the given quaternion into the blender coordinate system\n        blender_rot_mat = mathutils.Matrix.Rotation(radians(-90), 4, 'X')\n        created_objects = []\n        # for each room\n        for room_id, room in enumerate(data[\"scene\"][\"room\"]):\n            # for each object in that room\n            for child in room[\"children\"]:\n                if \"furniture\" in child[\"instanceid\"]:\n                    # find the object where the uid matches the child ref id\n                    for obj in all_loaded_furniture:\n                        if obj.get_cp(\"uid\") == child[\"ref\"]:\n                            # if the object was used before, duplicate the object and move that duplicated obj\n                            if obj.get_cp(\"is_used\"):\n                                new_obj = obj.duplicate()\n                            else:\n                                # if it is the first time use the object directly\n                                new_obj = obj\n                            created_objects.append(new_obj)\n                            new_obj.set_cp(\"is_used\", True)\n                            new_obj.set_cp(\"room_id\", room_id)\n                            new_obj.set_cp(\"3D_future_type\", \"Object\")  # is an object used for the interesting score\n                            new_obj.set_cp(\"coarse_grained_class\", new_obj.get_cp(\"category_id\"))\n                            # this flips the y and z coordinate to bring it to the blender coordinate system\n                            new_obj.set_location(mathutils.Vector(child[\"pos\"]).xzy)\n                            new_obj.set_scale(child[\"scale\"])\n                            # extract the quaternion and convert it to a rotation matrix\n                            rotation_mat = mathutils.Quaternion(child[\"rot\"]).to_euler().to_matrix().to_4x4()\n                            # transform it into the blender coordinate system and then to an euler\n                            new_obj.set_rotation_euler((blender_rot_mat @ rotation_mat).to_euler())\n        return created_objects\n\n# --- Snippet Separator ---\n\ndef load_blend(path: str, obj_types: Optional[Union[List[str], str]] = None, name_regrex: Optional[str] = None,\n               data_blocks: Union[List[str], str] = \"objects\", link: bool = False) -> List[Entity]:\n    \"\"\"\n    Loads entities (everything that can be stored in a .blend file's folders, see Blender's documentation for\n    bpy.types.ID for more info) that match a name pattern from a specified .blend file's section/data_block.\n\n    :param path: Path to a .blend file.\n    :param obj_types: The type of objects to load. This parameter is only relevant when `data_blocks`\n                      is set to `\"objects\"`. Available options are: ['mesh', 'curve', 'hair', 'armature',\n                      'empty', 'light', 'camera']\n    :param name_regrex: Regular expression representing a name pattern of entities' (everything that can be\n                        stored in a .blend file's folders, see Blender's documentation for bpy.types.ID\n                        for more info) names.\n    :param data_blocks: The data block or a list of data blocks which should be loaded from the given .blend file.\n                        Available options are: ['armatures', 'cameras', 'curves', 'hairs', 'images', 'lights',\n                        'materials', 'meshes', 'objects', 'textures']\n    :param link: whether to link instead of append data blocks from .blend file. Linked objects can not be modified.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    if obj_types is None:\n        obj_types = [\"mesh\", \"empty\"]\n    # get a path to a .blend file\n    path = resolve_path(path)\n    data_blocks = _BlendLoader.validate_and_standardizes_configured_list(data_blocks, _BlendLoader.valid_data_blocks,\n                                                                         \"data block\")\n    obj_types = _BlendLoader.validate_and_standardizes_configured_list(obj_types, _BlendLoader.valid_object_types,\n                                                                       \"object type\")\n\n    # Remember which orphans existed beforehand\n    orphans_before = collect_all_orphan_data_blocks()\n\n    # Start importing blend file. All objects that should be imported need to be copied from \"data_from\" to \"data_to\"\n    with bpy.data.libraries.load(path, link=link) as (data_from, data_to):\n        for data_block in data_blocks:\n            # Verify that the given data block is valid\n            if hasattr(data_from, data_block):\n                # Find all entities of this data block that match the specified pattern\n                data_to_entities = []\n                for entity_name in getattr(data_from, data_block):\n                    if not name_regrex or re.fullmatch(name_regrex, entity_name) is not None:\n                        data_to_entities.append(entity_name)\n                # Import them\n                setattr(data_to, data_block, data_to_entities)\n                print(\"Imported \" + str(len(data_to_entities)) + \" \" + data_block)\n            else:\n                raise Exception(\"No such data block: \" + data_block)\n\n    # Go over all imported objects again\n    loaded_objects: List[Entity] = []\n    for data_block in data_blocks:\n        # Some adjustments that only affect objects\n        if data_block == \"objects\":\n            for obj in getattr(data_to, data_block):\n                # Check that the object type is desired\n                if obj.type.lower() in obj_types:\n                    # Link objects to the scene\n                    bpy.context.collection.objects.link(obj)\n                    loaded_objects.append(convert_to_entity_subclass(obj))\n\n                    # If a camera was imported\n                    if obj.type == 'CAMERA':\n                        # Make it the active camera in the scene\n                        bpy.context.scene.camera = obj\n\n                        # Find the maximum frame number of its key frames\n                        max_keyframe = -1\n                        if obj.animation_data is not None:\n                            fcurves = obj.animation_data.action.fcurves\n                            for curve in fcurves:\n                                keyframe_points = curve.keyframe_points\n                                for keyframe in keyframe_points:\n                                    max_keyframe = max(max_keyframe, keyframe.co[0])\n\n                        # Set frame_end to the next free keyframe\n                        bpy.context.scene.frame_end = max_keyframe + 1\n                else:\n                    # Remove object again if its type is not desired\n                    bpy.data.objects.remove(obj, do_unlink=True)\n            print(\"Selected \" + str(len(loaded_objects)) + \" of the loaded objects by type\")\n        else:\n            loaded_objects.extend(getattr(data_to, data_block))\n\n    # As some loaded objects were deleted again due to their type, we need also to remove the dependent\n    # data blocks that were also loaded and are now orphans\n    _BlendLoader.purge_added_orphans(orphans_before, data_to)\n    return loaded_objects\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that initializes a scene in Blender using the blenderproc library. The code should parse command line arguments for paths to scene, texture, and material files, as well as an output directory. It should load a scene and label its objects based on a mapping from a CSV file. The code should also load materials and randomly assign them to 40% of the objects' materials. \n\nThe code should then load textures for the materials that were assigned to at least one object. It should extract floors and ceilings from wall objects and assign them appropriate category IDs. It should make all lamp objects emit light and make all ceiling objects emit a bit of light to brighten the room. \n\nThe code should then create a BVH tree containing all mesh objects and sample camera locations above the floor, ensuring there are no obstacles in front of the camera and that the scene coverage score is not too low. \n\nFinally, the code should enable normal, depth, and segmentation rendering, render the scene, and write the data to a .hdf5 file in the specified output directory.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 9, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a simulation using the dd4hep library. The code should import necessary modules and set up logging. It should define a function that runs the simulation. This function should import additional modules, set up command line arguments, and load a geometry file. If help is requested, it should print a help message and exit. The function should then load constants, set up Geant4, and print detectors. It should configure the UI, tracking field, and event actions. It should also set up a particle gun and a tracker. Finally, it should build a physics list and execute Geant4. If the script is run as the main program, it should call the function to run the simulation.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def shutdown_server(self) -> str:\n        \"\"\"Shutdown the server.\n\n        The engine should not exit right away.\n        It should set its status to STOPPING, and set up a timer (in a different thread),\n        and return from this call right away (if other restart conditions are met).\n        When the timer fires, it exits.\n        This would give the caller to process the feedback or clean up (e.g. admin cmd response).\n\n        Returns:\n            An error message. An empty string if successful.\n        \"\"\"\n        pass\n\n# --- Snippet Separator ---\n\ndef run_function_modally(function, progress_maximum, *args, **kwargs):\n    \"\"\"Create a temporary ExperimentWithProgressBar and run it modally.\n\n    This convenience function allows a function to be run with a nice progress bar, without the hassle\n    of setting up an Experiment object.  The function must accept a keyword argument, update_progress,\n    which is a function - it should be called periodically, with a numeric argument that starts at zero\n    and increments up to a final value of progress_maximum.  A sensible default for this argument would\n    be ``lambda p: p``, which is a function that does nothing.\n\n    Positional and keyword arguments are passed through, the only other argument needed is\n    progress_maximum, which sets the final value of progress.\n    \"\"\"\n #   function(*args, **kwargs)\n    e = RunFunctionWithProgressBar(function, progress_maximum=progress_maximum)\n    e.run_modally(*args, **kwargs)\n\n# --- Snippet Separator ---\n\nclass DD4hepSimulation(object):\n  \"\"\"Class to hold all the parameters and functions to run simulation\"\"\"\n\n  def __init__(self):\n    self.steeringFile = None\n    self.compactFile = []\n    self.inputFiles = []\n    self.outputFile = defaultOutputFile()\n    self.runType = \"batch\"\n    self.printLevel = 3\n\n    self.numberOfEvents = 0\n    self.skipNEvents = 0\n    self.physicsList = None  # deprecated use physics.list\n    self.crossingAngleBoost = 0.0\n    self.macroFile = ''\n    self.enableGun = False\n    self.enableG4GPS = False\n    self.enableG4Gun = False\n    self._g4gun = None\n    self._g4gps = None\n    self.vertexSigma = [0.0, 0.0, 0.0, 0.0]\n    self.vertexOffset = [0.0, 0.0, 0.0, 0.0]\n    self.enableDetailedShowerMode = False\n\n    self._errorMessages = []\n    self._dumpParameter = False\n    self._dumpSteeringFile = False\n\n    # objects for extended configuration option\n    self.output = Output()\n    self.random = Random()\n    self.gun = Gun()\n    self.part = ParticleHandler()\n    self.field = MagneticField()\n    self.action = Action()\n    self.outputConfig = OutputConfig()\n    self.inputConfig = InputConfig()\n    self.guineapig = GuineaPig()\n    self.lcio = LCIO()\n    self.hepmc3 = HepMC3()\n    self.meta = Meta()\n\n    self.geometry = Geometry()\n    self.filter = Filter()\n    self.physics = Physics()\n    self.ui = UI()\n\n    self._argv = None\n\n  def readSteeringFile(self):\n    \"\"\"Reads a steering file and sets the parameters to that of the\n    DD4hepSimulation object present in the steering file.\n    \"\"\"\n    globs = {}\n    locs = {}\n    if not self.steeringFile:\n      return\n    sFileTemp = self.steeringFile\n    exec(compile(open(self.steeringFile).read(), self.steeringFile, 'exec'), globs, locs)\n    for _name, obj in locs.items():\n      if isinstance(obj, DD4hepSimulation):\n        self.__dict__ = obj.__dict__\n    self.steeringFile = os.path.abspath(sFileTemp)\n\n  def parseOptions(self, argv=None):\n    \"\"\"parse the command line options\"\"\"\n\n    if argv is None:\n      self._argv = list(sys.argv)\n\n    parser = argparse.ArgumentParser(\"Running DD4hep Simulations:\",\n                                     formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument(\"--steeringFile\", \"-S\", action=\"store\", default=self.steeringFile,\n                        help=\"Steering file to change default behaviour\")\n\n    # first we parse just the steering file, but only if we don't want to see the help message\n    if not any(opt in self._argv for opt in ('-h', '--help')):\n      parsed, _unknown = parser.parse_known_args()\n      self.steeringFile = parsed.steeringFile\n      self.readSteeringFile()\n\n    # readSteeringFile will set self._argv to None if there is a steering file\n    if self._argv is None:\n      self._argv = list(argv) if argv else list(sys.argv)\n\n    parser.add_argument(\"--compactFile\", nargs='+', action=\"store\",\n                        default=ConfigHelper.makeList(self.compactFile), type=str,\n                        help=\"The compact XML file, or multiple compact files, if the last one is the closer.\")\n\n    parser.add_argument(\"--runType\", action=\"store\", choices=(\"batch\", \"vis\", \"run\", \"shell\", \"qt\"),\n                        default=self.runType,\n                        help=\"The type of action to do in this invocation\"  # Note: implicit string concatenation\n                        \"\\nbatch: just simulate some events, needs numberOfEvents, and input file or gun\"\n                        \"\\nvis: enable visualisation, run the macroFile if it is set\"\n                        \"\\nqt: enable visualisation in Qt shell, run the macroFile if it is set\"\n                        \"\\nrun: run the macroFile and exit\"\n                        \"\\nshell: enable interactive session\")\n\n    parser.add_argument(\"--inputFiles\", \"-I\", nargs='+', action=\"store\", default=self.inputFiles,\n                        help=\"InputFiles for simulation %s files are supported\" % \", \".join(POSSIBLEINPUTFILES))\n\n    parser.add_argument(\"--outputFile\", \"-O\", action=\"store\", default=self.outputFile,\n                        help=\"Outputfile from the simulation: .slcio, edm4hep.root and .root\"\n                        \" output files are supported\")\n\n    parser.add_argument(\"-v\", \"--printLevel\", action=\"store\", default=self.printLevel, dest=\"printLevel\",\n                        choices=(1, 2, 3, 4, 5, 6, 7, 'VERBOSE', 'DEBUG',\n                                 'INFO', 'WARNING', 'ERROR', 'FATAL', 'ALWAYS'),\n                        type=outputLevelType,\n                        help=\"Verbosity use integers from 1(most) to 7(least) verbose\"\n                        \"\\nor strings: VERBOSE, DEBUG, INFO, WARNING, ERROR, FATAL, ALWAYS\")\n\n    parser.add_argument(\"--numberOfEvents\", \"-N\", action=\"store\", dest=\"numberOfEvents\", default=self.numberOfEvents,\n                        type=int, help=\"number of events to simulate, used in batch mode\")\n\n    parser.add_argument(\"--skipNEvents\", action=\"store\", dest=\"skipNEvents\", default=self.skipNEvents, type=int,\n                        help=\"Skip first N events when reading a file\")\n\n    parser.add_argument(\"--physicsList\", action=\"store\", dest=\"physicsList\", default=self.physicsList,\n                        help=\"Physics list to use in simulation\")\n\n    parser.add_argument(\"--crossingAngleBoost\", action=\"store\", dest=\"crossingAngleBoost\",\n                        default=self.crossingAngleBoost,\n                        type=float, help=\"Lorentz boost for the crossing angle, in radian!\")\n\n    parser.add_argument(\"--vertexSigma\", nargs=4, action=\"store\", dest=\"vertexSigma\",\n                        default=self.vertexSigma, metavar=('X', 'Y', 'Z', 'T'),\n                        type=float, help=\"FourVector of the Sigma for the Smearing of the Vertex position: x y z t\")\n\n    parser.add_argument(\"--vertexOffset\", nargs=4, action=\"store\", dest=\"vertexOffset\",\n                        default=self.vertexOffset, metavar=('X', 'Y', 'Z', 'T'),\n                        type=float, help=\"FourVector of translation for the Smearing of the Vertex position: x y z t\")\n\n    parser.add_argument(\"--macroFile\", \"-M\", action=\"store\", dest=\"macroFile\", default=self.macroFile,\n                        help=\"Macro file to execute for runType 'run' or 'vis'\")\n\n    parser.add_argument(\"--enableGun\", \"-G\", action=\"store_true\", dest=\"enableGun\", default=self.enableGun,\n                        help=\"enable the DDG4 particle gun\")\n\n    parser.add_argument(\"--enableG4GPS\", action=\"store_true\", dest=\"enableG4GPS\", default=self.enableG4GPS,\n                        help=\"enable the Geant4 GeneralParticleSource. Needs a macroFile (runType run)\"\n                        \"or use it with the shell (runType shell)\")\n\n    parser.add_argument(\"--enableG4Gun\", action=\"store_true\", dest=\"enableG4Gun\", default=self.enableG4Gun,\n                        help=\"enable the Geant4 particle gun. Needs a macroFile (runType run)\"\n                        \" or use it with the shell (runType shell)\")\n\n    parser.add_argument(\"--dumpParameter\", \"--dump\", action=\"store_true\", dest=\"dumpParameter\",\n                        default=self._dumpParameter, help=\"Print all configuration Parameters and exit\")\n\n    parser.add_argument(\"--enableDetailedShowerMode\", action=\"store_true\", dest=\"enableDetailedShowerMode\",\n                        default=self.enableDetailedShowerMode,\n                        help=\"use detailed shower mode\")\n\n    parser.add_argument(\"--dumpSteeringFile\", action=\"store_true\", dest=\"dumpSteeringFile\",\n                        default=self._dumpSteeringFile, help=\"print an example steering file to stdout\")\n\n    # output, or do something smarter with fullHelp only for example\n    ConfigHelper.addAllHelper(self, parser)\n    # now parse everything. The default values are now taken from the\n    # steeringFile if they were set so that the steering file parameters can be\n    # overwritten from the command line\n    if ARGCOMPLETEENABLED:\n      argcomplete.autocomplete(parser)\n    parsed = parser.parse_args()\n\n    self._dumpParameter = parsed.dumpParameter\n    self._dumpSteeringFile = parsed.dumpSteeringFile\n\n    self.compactFile = ConfigHelper.makeList(parsed.compactFile)\n    self.inputFiles = parsed.inputFiles\n    self.inputFiles = self.__checkFileFormat(self.inputFiles, POSSIBLEINPUTFILES)\n    self.outputFile = parsed.outputFile\n    self.__checkFileFormat(self.outputFile, ('.root', '.slcio'))\n    self.runType = parsed.runType\n    self.printLevel = self.__checkOutputLevel(parsed.printLevel)\n\n    self.numberOfEvents = parsed.numberOfEvents\n    self.skipNEvents = parsed.skipNEvents\n    self.physicsList = parsed.physicsList\n    self.crossingAngleBoost = parsed.crossingAngleBoost\n    self.macroFile = parsed.macroFile\n    self.enableGun = parsed.enableGun\n    self.enableG4Gun = parsed.enableG4Gun\n    self.enableG4GPS = parsed.enableG4GPS\n    self.enableDetailedShowerMode = parsed.enableDetailedShowerMode\n    self.vertexOffset = parsed.vertexOffset\n    self.vertexSigma = parsed.vertexSigma\n\n    self._consistencyChecks()\n\n    if self.printLevel <= 2:  # VERBOSE or DEBUG\n      logger.setLevel(logging.DEBUG)\n\n    # self.__treatUnknownArgs( parsed, unknown )\n    self.__parseAllHelper(parsed)\n    if self._errorMessages and not (self._dumpParameter or self._dumpSteeringFile):\n      parser.epilog = \"\\n\".join(self._errorMessages)\n      parser.print_help()\n      exit(1)\n\n    if self._dumpParameter:\n      from pprint import pprint\n      logger.info(\"=\" * 80)\n      pprint(vars(self))\n      logger.info(\"=\" * 80)\n      exit(0)\n\n    if self._dumpSteeringFile:\n      self.__printSteeringFile(parser)\n      exit(0)\n\n  def getDetectorLists(self, detectorDescription):\n    ''' get lists of trackers and calorimeters that are defined in detectorDescription (the compact xml file)'''\n    import DDG4\n    trackers, calos, unknown = [], [], []\n    for i in detectorDescription.detectors():\n      det = DDG4.DetElement(i.second.ptr())\n      name = det.name()\n      sd = detectorDescription.sensitiveDetector(name)\n      if sd.isValid():\n        detType = sd.type()\n        logger.info('getDetectorLists - found active detector %s type: %s', name, detType)\n        if any(pat.lower() in detType.lower() for pat in self.action.trackerSDTypes):\n          trackers.append(det.name())\n        elif any(pat.lower() in detType.lower() for pat in self.action.calorimeterSDTypes):\n          calos.append(det.name())\n        else:\n          logger.warning('Unknown sensitive detector type: %s', detType)\n          unknown.append(det.name())\n\n    return trackers, calos, unknown\n\n# ==================================================================================\n\n  def run(self):\n    \"\"\"setup the geometry and dd4hep and geant4 and do what was asked to be done\"\"\"\n    import ROOT\n    ROOT.PyConfig.IgnoreCommandLineOptions = True\n\n    import DDG4\n    import dd4hep\n\n    self.printLevel = getOutputLevel(self.printLevel)\n\n    kernel = DDG4.Kernel()\n    dd4hep.setPrintLevel(self.printLevel)\n\n    for compactFile in self.compactFile:\n      kernel.loadGeometry(str(\"file:\" + os.path.abspath(compactFile)))\n    detectorDescription = kernel.detectorDescription()\n\n    DDG4.importConstants(detectorDescription)\n\n  # ----------------------------------------------------------------------------------\n\n    # simple = DDG4.Geant4( kernel, tracker='Geant4TrackerAction',calo='Geant4CalorimeterAction')\n    # geant4 = DDG4.Geant4( kernel, tracker='Geant4TrackerCombineAction',calo='Geant4ScintillatorCalorimeterAction')\n    geant4 = DDG4.Geant4(kernel, tracker=self.action.tracker, calo=self.action.calo)\n\n    geant4.printDetectors()\n\n    if self.runType == \"vis\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=True, macro=self.macroFile)\n    elif self.runType == \"qt\":\n      uiaction = geant4.setupUI(typ=\"qt\", vis=True, macro=self.macroFile)\n    elif self.runType == \"run\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=self.macroFile, ui=False)\n    elif self.runType == \"shell\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=True)\n    elif self.runType == \"batch\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=False)\n    else:\n      logger.error(\"unknown runType\")\n      exit(1)\n\n    # User Configuration for the Geant4Phases\n    uiaction.ConfigureCommands = self.ui._commandsConfigure\n    uiaction.InitializeCommands = self.ui._commandsInitialize\n    uiaction.PostRunCommands = self.ui._commandsPostRun\n    uiaction.PreRunCommands = self.ui._commandsPreRun\n    uiaction.TerminateCommands = self.ui._commandsTerminate\n\n    kernel.NumEvents = self.numberOfEvents\n\n    # -----------------------------------------------------------------------------------\n    # setup the magnetic field:\n    self.__setMagneticFieldOptions(geant4)\n\n    # configure geometry creation\n    self.geometry.constructGeometry(kernel, geant4, self.output.geometry)\n\n    # ----------------------------------------------------------------------------------\n    # Configure Run actions\n    run1 = DDG4.RunAction(kernel, 'Geant4TestRunAction/RunInit')\n    kernel.registerGlobalAction(run1)\n    kernel.runAction().add(run1)\n\n    # Configure the random seed, do it before the I/O because we might change the seed!\n    self.random.initialize(DDG4, kernel, self.output.random)\n\n    # Configure the output file format and plugin\n    self.outputConfig.initialize(dd4hepsimulation=self, geant4=geant4)\n\n    actionList = []\n\n    if self.enableGun:\n      gun = DDG4.GeneratorAction(kernel, \"Geant4ParticleGun/\" + \"Gun\")\n      self.gun.setOptions(gun)\n      gun.Standalone = False\n      gun.Mask = 1\n      actionList.append(gun)\n      self.__applyBoostOrSmear(kernel, actionList, 1)\n      logger.info(\"++++ Adding DD4hep Particle Gun ++++\")\n\n    if self.enableG4Gun:\n      # GPS Create something\n      self._g4gun = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/Gun\")\n      self._g4gun.Uses = 'G4ParticleGun'\n      self._g4gun.Mask = 2\n      logger.info(\"++++ Adding Geant4 Particle Gun ++++\")\n      actionList.append(self._g4gun)\n\n    if self.enableG4GPS:\n      # GPS Create something\n      self._g4gps = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/GPS\")\n      self._g4gps.Uses = 'G4GeneralParticleSource'\n      self._g4gps.Mask = 3\n      logger.info(\"++++ Adding Geant4 General Particle Source ++++\")\n      actionList.append(self._g4gps)\n\n    start = 4\n    for index, plugin in enumerate(self.inputConfig.userInputPlugin, start=start):\n      gen = plugin(self)\n      gen.Mask = index\n      start = index + 1\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n      logger.info(\"++++ Adding User Plugin %s ++++\", gen.Name)\n\n    for index, inputFile in enumerate(self.inputFiles, start=start):\n      if inputFile.endswith(\".slcio\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/LCIO%d\" % index)\n        gen.Parameters = self.lcio.getParameters()\n        gen.Input = \"LCIOFileReader|\" + inputFile\n      elif inputFile.endswith(\".stdhep\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/STDHEP%d\" % index)\n        gen.Input = \"LCIOStdHepReader|\" + inputFile\n      elif inputFile.endswith(\".HEPEvt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/HEPEvt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtShort|\" + inputFile\n      elif inputFile.endswith(\".hepevt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepevt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtLong|\" + inputFile\n      elif inputFile.endswith(tuple([\".hepmc\"] + HEPMC3_SUPPORTED_EXTENSIONS)):\n        if self.hepmc3.useHepMC3:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Parameters = self.hepmc3.getParameters()\n          gen.Input = \"HEPMC3FileReader|\" + inputFile\n        else:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Input = \"Geant4EventReaderHepMC|\" + inputFile\n      elif inputFile.endswith(\".pairs\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/GuineaPig%d\" % index)\n        gen.Input = \"Geant4EventReaderGuineaPig|\" + inputFile\n        gen.Parameters = self.guineapig.getParameters()\n      else:\n        # this should never happen because we already check at the top, but in case of some LogicError...\n        raise RuntimeError(\"Unknown input file type: %s\" % inputFile)\n      gen.Sync = self.skipNEvents\n      gen.Mask = index\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n\n    if actionList:\n      self._buildInputStage(geant4, actionList, output_level=self.output.inputStage,\n                            have_mctruth=self._enablePrimaryHandler())\n\n    # ================================================================================================\n\n    # And handle the simulation particles.\n    part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n    kernel.generatorAction().adopt(part)\n    # part.SaveProcesses = ['conv','Decay']\n    part.SaveProcesses = self.part.saveProcesses\n    part.MinimalKineticEnergy = self.part.minimalKineticEnergy\n    part.KeepAllParticles = self.part.keepAllParticles\n    part.PrintEndTracking = self.part.printEndTracking\n    part.PrintStartTracking = self.part.printStartTracking\n    part.MinDistToParentVertex = self.part.minDistToParentVertex\n    part.OutputLevel = self.output.part\n    part.enableUI()\n\n    if self.part.enableDetailedHitsAndParticleInfo:\n      self.part.setDumpDetailedParticleInfo(kernel, DDG4)\n\n    self.part.setupUserParticleHandler(part, kernel, DDG4)\n\n    # =================================================================================\n\n    # Setup global filters for use in sensitive detectors\n    try:\n      self.filter.setupFilters(kernel)\n    except RuntimeError as e:\n      logger.error(\"%s\", e)\n      exit(1)\n\n    # =================================================================================\n    # get lists of trackers and calorimeters in detectorDescription\n\n    trk, cal, unk = self.getDetectorLists(detectorDescription)\n\n    for detectors, function, defFilter, abort in [(trk, geant4.setupTracker, self.filter.tracker, False),\n                                                  (cal, geant4.setupCalorimeter, self.filter.calo, False),\n                                                  (unk, geant4.setupDetector, None, True),\n                                                  ]:\n      try:\n        self.__setupSensitiveDetectors(detectors, function, defFilter, abort)\n      except Exception as e:\n        logger.error(\"Failed setting up sensitive detector %s\", e)\n        raise\n\n  # =================================================================================\n    # Now build the physics list:\n    _phys = self.physics.setupPhysics(kernel, name=self.physicsList)\n\n    # add the G4StepLimiterPhysics to activate the max step limits in volumes\n    ph = DDG4.PhysicsList(kernel, 'Geant4PhysicsList/Myphysics')\n    ph.addPhysicsConstructor(str('G4StepLimiterPhysics'))\n    _phys.add(ph)\n\n    dd4hep.setPrintLevel(self.printLevel)\n\n    kernel.configure()\n    kernel.initialize()\n\n    # GPS\n    if self._g4gun is not None:\n      self._g4gun.generator()\n    if self._g4gps is not None:\n      self._g4gps.generator()\n\n    startUpTime, _sysTime, _cuTime, _csTime, _elapsedTime = os.times()\n\n    kernel.run()\n    kernel.terminate()\n\n    totalTimeUser, totalTimeSys, _cuTime, _csTime, _elapsedTime = os.times()\n    if self.printLevel <= 3:\n      logger.info(\"DDSim            INFO  Total Time:   %3.2f s (User), %3.2f s (System)\" %\n                  (totalTimeUser, totalTimeSys))\n      if self.numberOfEvents != 0:\n        eventTime = totalTimeUser - startUpTime\n        perEventTime = eventTime / self.numberOfEvents\n        logger.info(\"DDSim            INFO  StartUp Time: %3.2f s, Event Processing: %3.2f s (%3.2f s/Event) \"\n                    % (startUpTime, eventTime, perEventTime))\n\n  def __setMagneticFieldOptions(self, geant4):\n    \"\"\" create and configure the magnetic tracking setup \"\"\"\n    field = geant4.addConfig('Geant4FieldTrackingSetupAction/MagFieldTrackingSetup')\n    field.stepper = self.field.stepper\n    field.equation = self.field.equation\n    field.eps_min = self.field.eps_min\n    field.eps_max = self.field.eps_max\n    field.min_chord_step = self.field.min_chord_step\n    field.delta_chord = self.field.delta_chord\n    field.delta_intersection = self.field.delta_intersection\n    field.delta_one_step = self.field.delta_one_step\n    field.largest_step = self.field.largest_step\n\n  def __checkFileFormat(self, fileNames, extensions):\n    \"\"\"check if the fileName is allowed, note that the filenames are case\n    sensitive, and in case of hepevt we depend on this to identify short and long versions of the content\n    \"\"\"\n    if isinstance(fileNames, str):\n      fileNames = [fileNames]\n    if not all(fileName.endswith(tuple(extensions)) for fileName in fileNames):\n      self._errorMessages.append(\"ERROR: Unknown fileformat for file: %s\" % fileNames)\n    is_hepmc3_extension = any(fileName.endswith(tuple(HEPMC3_SUPPORTED_EXTENSIONS)) for fileName in fileNames)\n    if not self.hepmc3.useHepMC3 and is_hepmc3_extension:\n      self._errorMessages.append(\"ERROR: HepMC3 files or compressed HepMC2 require the use of HepMC3 library\")\n    return fileNames\n\n  def __applyBoostOrSmear(self, kernel, actionList, mask):\n    \"\"\"apply boost or smearing for given mask index\"\"\"\n    import DDG4\n    if self.crossingAngleBoost:\n      lbo = DDG4.GeneratorAction(kernel, \"Geant4InteractionVertexBoost\")\n      lbo.Angle = self.crossingAngleBoost\n      lbo.Mask = mask\n      actionList.append(lbo)\n\n    if any(self.vertexSigma) or any(self.vertexOffset):\n      vSmear = DDG4.GeneratorAction(kernel, \"Geant4InteractionVertexSmear\")\n      vSmear.Offset = self.vertexOffset\n      vSmear.Sigma = self.vertexSigma\n      vSmear.Mask = mask\n      actionList.append(vSmear)\n\n  def __parseAllHelper(self, parsed):\n    \"\"\" parse all the options for the helper \"\"\"\n    parsedDict = vars(parsed)\n    for name, obj in vars(self).items():\n      if isinstance(obj, ConfigHelper):\n        for var in obj.getOptions():\n          key = \"%s.%s\" % (name, var)\n          if key in parsedDict:\n            try:\n              obj.setOption(var, parsedDict[key])\n            except RuntimeError as e:\n              self._errorMessages.append(\"ERROR: %s \" % e)\n              if logger.level <= logging.DEBUG:\n                self._errorMessages.append(traceback.format_exc())\n        obj._checkProperties()\n\n  def __checkOutputLevel(self, level):\n    \"\"\"return outputlevel as int so we don't have to import anything for faster startup\"\"\"\n    try:\n      return outputLevel(level)\n    except ValueError:\n      self._errorMessages.append(\"ERROR: printLevel is neither integer nor string\")\n      return -1\n    except KeyError:\n      self._errorMessages.append(\"ERROR: printLevel '%s' unknown\" % level)\n      return -1\n\n  def __setupSensitiveDetectors(self, detectors, setupFunction, defaultFilter=None,\n                                abortForMissingAction=False,\n                                ):\n    \"\"\"Attach sensitive detector actions for all subdetectors.\n\n    Can be steered with the `Action` ConfigHelpers\n\n    :param detectors: list of detectors\n    :param setupFunction: function used to register the sensitive detector\n    :param defaultFilter: default filter to apply for given types\n    :param abortForMissingAction: if true end program if there is no action found\n    \"\"\"\n    for det in detectors:\n      logger.info('Setting up SD for %s', det)\n      action = None\n      for pattern in self.action.mapActions:\n        if pattern.lower() in det.lower():\n          action = self.action.mapActions[pattern]\n          logger.info('       replace default action with : %s', action)\n          break\n      if abortForMissingAction and action is None:\n        logger.error('Cannot find Action for detector %s. You have to extend \"action.mapAction\"', det)\n        raise RuntimeError(\"Cannot find Action\")\n      seq, act = setupFunction(det, action)\n      self.filter.applyFilters(seq, det, defaultFilter)\n\n      # set detailed hit creation mode for this\n      if self.enableDetailedShowerMode:\n        if isinstance(act, list):\n          for a in act:\n            a.HitCreationMode = 2\n        else:\n          act.HitCreationMode = 2\n\n  def __printSteeringFile(self, parser):\n    \"\"\"print the parameters formated as a steering file\"\"\"\n\n    steeringFileBase = textwrap.dedent(\"\"\"\\\n        from DDSim.DD4hepSimulation import DD4hepSimulation\n        from g4units import mm, GeV, MeV\n        SIM = DD4hepSimulation()\n        \"\"\")\n    steeringFileBase += \"\\n\"\n    optionDict = parser._option_string_actions\n    parameters = vars(self)\n    for parName, parameter in sorted(list(parameters.items()), key=sortParameters):\n      if parName.startswith(\"_\"):\n        continue\n      if isinstance(parameter, ConfigHelper):\n        steeringFileBase += \"\\n\\n\"\n        steeringFileBase += \"################################################################################\\n\"\n        steeringFileBase += \"## %s \\n\" % \"\\n## \".join(parameter.__doc__.splitlines())\n        steeringFileBase += \"################################################################################\\n\"\n        options = parameter.getOptions()\n        for opt, optionsDict in sorted(options.items(), key=sortParameters):\n          if opt.startswith(\"_\"):\n            continue\n          parValue = optionsDict['default']\n          if isinstance(optionsDict.get('help'), str):\n            steeringFileBase += \"\\n## %s\\n\" % \"\\n## \".join(optionsDict.get('help').splitlines())\n          # add quotes if it is a string\n          if isinstance(parValue, str):\n            steeringFileBase += \"SIM.%s.%s = \\\"%s\\\"\\n\" % (parName, opt, parValue)\n          else:\n            steeringFileBase += \"SIM.%s.%s = %s\\n\" % (parName, opt, parValue)\n      else:\n        # get the docstring from the command line parameter\n        optionObj = optionDict.get(\"--\" + parName, None)\n        if isinstance(optionObj, argparse._StoreAction):\n          steeringFileBase += \"## %s\\n\" % \"\\n## \".join(optionObj.help.splitlines())\n        # add quotes if it is a string\n        if isinstance(parameter, str):\n          steeringFileBase += \"SIM.%s = \\\"%s\\\"\" % (parName, str(parameter))\n        else:\n          steeringFileBase += \"SIM.%s = %s\" % (parName, str(parameter))\n        steeringFileBase += \"\\n\"\n    for line in steeringFileBase.splitlines():\n      print(line)\n\n  def _consistencyChecks(self):\n    \"\"\"Check if the requested setup makes sense, or if there is something preventing it from working correctly\n\n    Appends error messages to self._errorMessages\n\n    :returns: None\n    \"\"\"\n\n    if not self.compactFile:\n      self._errorMessages.append(\"ERROR: No geometry compact file provided\")\n\n    if self.runType == \"batch\":\n      if not self.numberOfEvents:\n        self._errorMessages.append(\"ERROR: Batch mode requested, but did not set number of events\")\n      if not (self.inputFiles or self.enableGun or self.inputConfig.userInputPlugin):\n        self._errorMessages.append(\"ERROR: Batch mode requested, but did not set inputFile(s), gun, or userInputPlugin\")\n\n    if self.inputFiles and (self.enableG4Gun or self.enableG4GPS):\n      self._errorMessages.append(\"ERROR: Cannot use both inputFiles and Geant4Gun or GeneralParticleSource\")\n\n    if self.enableGun and (self.enableG4Gun or self.enableG4GPS):\n      self._errorMessages.append(\"ERROR: Cannot use both DD4hepGun and Geant4 Gun or GeneralParticleSource\")\n\n    if self.inputConfig.userInputPlugin and (self.enableG4Gun or self.enableG4GPS):\n      self._errorMessages.append(\"ERROR: Cannot use both userInputPlugin and Geant4 Gun or GeneralParticleSource\")\n\n    if self.numberOfEvents < 0 and not self.inputFiles:\n      self._errorMessages.append(\"ERROR: Negative number of events only sensible for inputFiles\")\n\n  def _enablePrimaryHandler(self):\n    \"\"\" the geant4 Gun or GeneralParticleSource cannot be used together with the PrimaryHandler.\n        Particles would be simulated multiple times\n\n    :returns: True or False\n    \"\"\"\n    enablePrimaryHandler = not (self.enableG4Gun or self.enableG4GPS)\n    if enablePrimaryHandler:\n      logger.info(\"Enabling the PrimaryHandler\")\n    else:\n      logger.info(\"Disabling the PrimaryHandler\")\n    return enablePrimaryHandler\n\n  def _buildInputStage(self, geant4, generator_input_modules, output_level=None, have_mctruth=True):\n    \"\"\"\n    Generic build of the input stage with multiple input modules.\n    Actions executed are:\n    1) Register Generation initialization action\n    2) Append all modules to build the complete input record\n    These modules are readers/particle sources, boosters and/or smearing actions.\n    3) Merge all existing interaction records\n    4) Add the MC truth handler\n    \"\"\"\n    from DDG4 import GeneratorAction\n    ga = geant4.kernel().generatorAction()\n\n    # Register Generation initialization action\n    gen = GeneratorAction(geant4.kernel(), \"Geant4GeneratorActionInit/GenerationInit\")\n    if output_level is not None:\n      gen.OutputLevel = output_level\n    ga.adopt(gen)\n\n    # Now append all modules to build the complete input record\n    # These modules are readers/particle sources, boosters and/or smearing actions\n    for gen in generator_input_modules:\n      gen.enableUI()\n      if output_level is not None:\n        gen.OutputLevel = output_level\n      ga.adopt(gen)\n\n    # Merge all existing interaction records\n    gen = GeneratorAction(geant4.kernel(), \"Geant4InteractionMerger/InteractionMerger\")\n    gen.enableUI()\n    if output_level is not None:\n      gen.OutputLevel = output_level\n    ga.adopt(gen)\n\n    # Finally generate Geant4 primaries\n    if have_mctruth:\n      gen = GeneratorAction(geant4.kernel(), \"Geant4PrimaryHandler/PrimaryHandler\")\n      gen.RejectPDGs = ConfigHelper.makeString(self.physics.rejectPDGs)\n      gen.ZeroTimePDGs = ConfigHelper.makeString(self.physics.zeroTimePDGs)\n      gen.enableUI()\n      if output_level is not None:\n        gen.OutputLevel = output_level\n      ga.adopt(gen)\n    # Puuuhh! All done.\n    return None\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a simulation using the dd4hep library. The code should import necessary modules and set up logging. It should define a function that runs the simulation. This function should import additional modules, set up command line arguments, and load a geometry file. If help is requested, it should print a help message and exit. The function should then load constants, set up Geant4, and print detectors. It should configure the UI, tracking field, and event actions. It should also set up a particle gun and a tracker. Finally, it should build a physics list and execute Geant4. If the script is run as the main program, it should call the function to run the simulation.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 10, "repo_full_name": "simpeg__simpeg", "instruction": "Generate code that performs a 1D inversion of Magnetic Susceptibility from Frequency-Domain Electromagnetic (FDEM) data, assuming a fixed electrical conductivity. The code should set up a cylindrically symmetric mesh, define geologic parameters and electrical conductivity, and set up the relative magnetic permeability. It should also define mappings and set up the FDEM problem and survey. The code should then perform the FDEM inversion, set up inversion directives, and run the inversion. If a flag is set to true, the code should plot the conductivity model, the permeability model, and the data misfits. The code should be able to handle the absence of the PardisoSolver by falling back to the SolverLU.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class EMPropMap(Maps.PropMap):\n    \"\"\" \n        Property Map for EM Problems. The electrical conductivity (\\\\(\\\\sigma\\\\)) is the default inversion property, and the default value of the magnetic permeability is that of free space (\\\\(\\\\mu = 4\\\\pi\\\\times 10^{-7} \\\\) H/m)\n    \"\"\"\n\n    sigma = Maps.Property(\"Electrical Conductivity\", defaultInvProp = True, propertyLink=('rho',Maps.ReciprocalMap))\n    mu = Maps.Property(\"Inverse Magnetic Permeability\", defaultVal = mu_0, propertyLink=('mui',Maps.ReciprocalMap))\n\n    rho = Maps.Property(\"Electrical Resistivity\", propertyLink=('sigma', Maps.ReciprocalMap)) \n    mui = Maps.Property(\"Inverse Magnetic Permeability\", defaultVal = 1./mu_0, propertyLink=('mu', Maps.ReciprocalMap))\n\n# --- Snippet Separator ---\n\ndef setup_cls(self) -> Type[CodesTask]:\n        \"\"\"\n        Class defining the run modes for the setup stage of the solver.\n\n        Typically, this class performs parameter mappings for some\n        external code, or derives dependent parameters. But it can also\n        define any required non-computational set up.\n        \"\"\"\n        pass\n\n# --- Snippet Separator ---\n\nclass CodesSolver(abc.ABC):\n    \"\"\"\n    Base class for solvers running an external code.\n    \"\"\"\n\n    params: MappedParameterFrame\n\n    def __init__(self, params: MappedParameterFrame):\n        self.params = params\n        self._setup = self.setup_cls(self.params, self.name)\n        self._run = self.run_cls(self.params, self.name)\n        self._teardown = self.teardown_cls(self.params, self.name)\n\n    @abc.abstractproperty\n    def name(self):\n        \"\"\"\n        The name of the solver.\n\n        In the base class, this is used to find mappings and specialise\n        error messages for the concrete solver.\n        \"\"\"\n        pass\n\n    @abc.abstractproperty\n    def setup_cls(self) -> Type[CodesTask]:\n        \"\"\"\n        Class defining the run modes for the setup stage of the solver.\n\n        Typically, this class performs parameter mappings for some\n        external code, or derives dependent parameters. But it can also\n        define any required non-computational set up.\n        \"\"\"\n        pass\n\n    @abc.abstractproperty\n    def run_cls(self) -> Type[CodesTask]:\n        \"\"\"\n        Class defining the run modes for the computational stage of the\n        solver.\n\n        This class is where computations should be defined. This may be\n        something like calling a Bluemira problem, or executing some\n        external code or process.\n        \"\"\"\n        pass\n\n    @abc.abstractproperty\n    def teardown_cls(self) -> Type[CodesTask]:\n        \"\"\"\n        Class defining the run modes for the teardown stage of the\n        solver.\n\n        This class should perform any clean-up operations required by\n        the solver. This may be deleting temporary files, or could\n        involve mapping parameters from some external code to Bluemira\n        parameters.\n        \"\"\"\n        pass\n\n    @abc.abstractproperty\n    def run_mode_cls(self) -> Type[BaseRunMode]:\n        \"\"\"\n        Class enumerating the run modes for this solver.\n\n        Common run modes are RUN, MOCK, READ, etc,.\n        \"\"\"\n        pass\n\n    def execute(self, run_mode: Union[str, BaseRunMode]) -> Any:\n        \"\"\"Execute the setup, run, and teardown tasks, in order.\"\"\"\n        if isinstance(run_mode, str):\n            run_mode = self.run_mode_cls.from_string(run_mode)\n        result = None\n        if setup := self._get_execution_method(self._setup, run_mode):\n            result = setup()\n        if run := self._get_execution_method(self._run, run_mode):\n            result = run(result)\n        if teardown := self._get_execution_method(self._teardown, run_mode):\n            result = teardown(result)\n        return result\n\n    def modify_mappings(self, send_recv: Dict[str, Dict[str, bool]]):\n        \"\"\"\n        Modify the send/receive truth values of a parameter.\n\n        If a parameter's 'send' is set to False, its value will not be\n        passed to the external code (a default will be used). Likewise,\n        if a parameter's 'recv' is False, its value will not be updated\n        from the external code's outputs.\n\n        Parameters\n        ----------\n        mappings:\n            A dictionary where keys are variables to change the mappings\n            of, and values specify 'send', and or, 'recv' booleans.\n\n            E.g.,\n\n            .. code-block:: python\n\n                {\n                    \"var1\": {\"send\": False, \"recv\": True},\n                    \"var2\": {\"recv\": False}\n                }\n        \"\"\"\n        param_mappings = self.params.mappings\n        for key, val in send_recv.items():\n            try:\n                p_map = param_mappings[key]\n            except KeyError:\n                bluemira_warn(f\"No mapping known for '{key}' in '{self.name}'.\")\n            else:\n                for sr_key, sr_val in val.items():\n                    setattr(p_map, sr_key, sr_val)\n\n    def _get_execution_method(\n        self, task: CodesTask, run_mode: BaseRunMode\n    ) -> Optional[Callable]:\n        \"\"\"\n        Return the method on the task corresponding to this solver's run\n        mode (e.g., :code:`task.run`).\n\n        If the method on the task does not exist, return :code:`None`.\n        \"\"\"\n        return getattr(task, run_mode.to_string(), None)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a 1D inversion of Magnetic Susceptibility from Frequency-Domain Electromagnetic (FDEM) data, assuming a fixed electrical conductivity. The code should set up a cylindrically symmetric mesh, define geologic parameters and electrical conductivity, and set up the relative magnetic permeability. It should also define mappings and set up the FDEM problem and survey. The code should then perform the FDEM inversion, set up inversion directives, and run the inversion. If a flag is set to true, the code should plot the conductivity model, the permeability model, and the data misfits. The code should be able to handle the absence of the PardisoSolver by falling back to the SolverLU.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 11, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seed-emulator library. The emulation should include three layers: base, routing, and eBGP. The base layer should create three autonomous systems with specific routers and networks. The first autonomous system should create five hosts and a router, all of which join a network. The second autonomous system should create three routers, each joining a different network. The third autonomous system should create two routers, both joining the same network. The eBGP layer should add private peering between different autonomous systems. Finally, the code should add all layers to the emulator and dump the emulator state to a binary file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def makeEmulatorBaseWith5StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef makeEmulatorBaseWith10StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 160, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 161, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 162, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 163, 104, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 164, 104, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(103, [3],  [160, 161, 162], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(104, [3, 4], [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [4],  [163], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [12], [164], PeerRelationship.Provider)\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef makeTransitAs(base: Base, asn: int, exchanges: List[int],\n    intra_ix_links: List[Tuple[int, int]]) -> AutonomousSystem:\n    \"\"\"!\n    @brief create a transit AS.\n\n    @param base reference to the base layer.\n    @param asn ASN of the newly created AS.\n    @param exchanges list of IXP IDs to join.\n    @param intra_ix_links list of tuple of IXP IDs, to create intra-IX links at.\n\n    @returns transit AS object.\n    \"\"\"\n\n    transit_as = base.createAutonomousSystem(asn)\n\n    routers: Dict[int, Router] = {}\n\n    # Create a BGP router for each internet exchange (for peering purpose)\n    for ix in exchanges:\n        routers[ix] = transit_as.createRouter('r{}'.format(ix))\n        routers[ix].joinNetwork('ix{}'.format(ix))\n\n    # For each pair, create an internal network to connect the BGP routers\n    # from two internet exchanges. There is no need to create a full-mesh\n    # network among the BGP routers. As long as they can reach each other\n    # over a single or multiple hops, it is OK.\n    for (a, b) in intra_ix_links:\n        name = 'net_{}_{}'.format(a, b)\n\n        transit_as.createNetwork(name)\n        routers[a].joinNetwork(name)\n        routers[b].joinNetwork(name)\n\n    return transit_as\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulation using the seed-emulator library. The emulation should include three layers: base, routing, and eBGP. The base layer should create three autonomous systems with specific routers and networks. The first autonomous system should create five hosts and a router, all of which join a network. The second autonomous system should create three routers, each joining a different network. The third autonomous system should create two routers, both joining the same network. The eBGP layer should add private peering between different autonomous systems. Finally, the code should add all layers to the emulator and dump the emulator state to a binary file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 12, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that imports necessary libraries and creates a TetrahedronMesh using the fealpy library. The code should define nodes and cells for the mesh, calculate the number of nodes, edges, faces, and cells, and store these entities. It should also calculate and store the barycenter coordinates for each entity, the measure of each entity, and the relationships between each entity (cell to cell, cell to face, etc.). The code should also identify boundary flags for each entity and the indices of boundary nodes, edges, faces, and cells. Finally, the code should plot the mesh using matplotlib, showing the indices of nodes, edges, and cells.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def duplicate_nodes(g, nodes, offset):\n    \"\"\"\n    Duplicate nodes on a fracture. The number of duplication will depend on\n    the cell topology around the node. If the node is not on a fracture 1\n    duplicate will be added. If the node is on a single fracture 2 duplicates\n    will be added. If the node is on a T-intersection 3 duplicates will be\n    added. If the node is on a X-intersection 4 duplicates will be added.\n    Equivalently for other types of intersections.\n\n    Parameters:\n    ----------\n    g         - The grid for which the nodes are duplicated\n    nodes     - The nodes to be duplicated\n    offset    - How far from the original node the duplications should be\n                placed.\n    \"\"\"\n    node_count = 0\n\n    # We wish to convert the sparse csc matrix to a sparse\n    # csr matrix to easily add rows. However, the convertion sorts the\n    # indices, which will change the node order when we convert back. We\n    # therefore find the inverse sorting of the nodes of each face.\n    # After we have performed the row operations we will map the nodes\n    # back to their original position.\n\n    _, iv = sort_sub_list(g.face_nodes.indices, g.face_nodes.indptr)\n    g.face_nodes = g.face_nodes.tocsr()\n    # Iterate over each internal node and split it according to the graph.\n    # For each cell attached to the node, we check wich color the cell has.\n    # All cells with the same color is then attached to a new copy of the\n    # node.\n    cell_nodes = g.cell_nodes().tocsr()\n    for node in nodes:\n        # t_node takes into account the added nodes.\n        t_node = node + node_count\n        # Find cells connected to node\n\n        cells = sparse_mat.slice_indices(cell_nodes, node)\n#        cell_nodes = g.cell_nodes().tocsr()\n#        ind_ptr = cell_nodes.indptr\n#        cells = cell_nodes.indices[\n#            mcolon(ind_ptr[t_node], ind_ptr[t_node + 1])]\n        cells = np.unique(cells)\n        # Find the color of each cell. A group of cells is given the same color\n        # if they are connected by faces. This means that all cells on one side\n        # of a fracture will have the same color, but a different color than\n        # the cells on the other side of the fracture. Equivalently, the cells\n        # at a X-intersection will be given four different colors\n        colors = find_cell_color(g, cells)\n        # Find which cells share the same color\n        colors, ix = np.unique(colors, return_inverse=True)\n        # copy coordinate of old node\n        new_nodes = np.repeat(g.nodes[:, t_node, None], colors.size, axis=1)\n        faces = np.array([], dtype=int)\n        face_pos = np.array([g.face_nodes.indptr[t_node]])\n        for j in range(colors.size):\n            # For each color we wish to add one node. First we find all faces that\n            # are connected to the fracture node, and have the correct cell\n            # color\n            local_faces = (g.cell_faces[:, cells[ix == j]]).nonzero()[0]\n            local_faces = np.unique(local_faces)\n            con_to_node = np.ravel(g.face_nodes[t_node, local_faces].todense())\n            faces = np.append(faces, local_faces[con_to_node])\n            # These faces is then attached to new node number j.\n            face_pos = np.append(face_pos, face_pos[-1] + np.sum(con_to_node))\n            # If an offset is given, we will change the position of the nodes.\n            # We move the nodes a length of offset away from the fracture(s).\n            if offset > 0 and colors.size > 1:\n                new_nodes[:, j] -= avg_normal(g,\n                                              local_faces[con_to_node]) * offset\n        # The total number of faces should not have changed, only their\n        # connection to nodes. We can therefore just update the indices and\n        # indptr map.\n        g.face_nodes.indices[face_pos[0]:face_pos[-1]] = faces\n        node_count += colors.size - 1\n        g.face_nodes.indptr = np.insert(g.face_nodes.indptr,\n                                        t_node + 1, face_pos[1:-1])\n        g.face_nodes._shape = (g.face_nodes.shape[0] + colors.size - 1,\n                               g.face_nodes._shape[1])\n        # We delete the old node because of the offset. If we do not\n        # have an offset we could keep it and add one less node.\n        g.nodes = np.delete(g.nodes, t_node, axis=1)\n        g.nodes = np.insert(g.nodes, [t_node] * new_nodes.shape[1],\n                            new_nodes, axis=1)\n\n    # Transform back to csc format and fix node ordering.\n    g.face_nodes = g.face_nodes.tocsc()\n    g.face_nodes.indices = g.face_nodes.indices[iv]  # For fast row operation\n\n    return node_count\n\n# --- Snippet Separator ---\n\ndef __compute_geometry_3d(self):\n        \"\"\"\n        Helper function to compute geometry for 3D grids\n\n        The implementation is motivated by the similar MRST function.\n\n        NOTE: The function is very long, and could have been broken up into\n        parts (face and cell computations are an obvious solution).\n\n        \"\"\"\n        num_face_nodes = self.face_nodes.nnz\n        face_node_ptr = self.face_nodes.indptr\n\n        num_nodes_per_face = face_node_ptr[1:] - face_node_ptr[:-1]\n\n        # Face-node relationships. Note that the elements here will also\n        # serve as a representation of an edge along the face (face_nodes[i]\n        #  represents the edge running from face_nodes[i] to face_nodes[i+1])\n        face_nodes = self.face_nodes.indices\n        # For each node, index of its parent face\n        face_node_ind = matrix_compression.rldecode(np.arange(\n            self.num_faces), num_nodes_per_face)\n\n        # Index of next node on the edge list. Note that this assumes the\n        # elements in face_nodes is stored in an ordered fasion\n        next_node = np.arange(num_face_nodes) + 1\n        # Close loops, for face i, the next node is the first of face i\n        next_node[face_node_ptr[1:] - 1] = face_node_ptr[:-1]\n\n        # Mapping from cells to faces\n        edge_2_face = sps.coo_matrix((np.ones(num_face_nodes),\n                                      (np.arange(num_face_nodes),\n                                       face_node_ind))).tocsc()\n\n        # Define temporary face center as the mean of the face nodes\n        tmp_face_center = self.nodes[:, face_nodes] * \\\n            edge_2_face / num_nodes_per_face\n        # Associate this value with all the edge of this face\n        tmp_face_center = edge_2_face * tmp_face_center.transpose()\n\n        # Vector along each edge\n        along_edge = self.nodes[:, face_nodes[next_node]] - \\\n            self.nodes[:, face_nodes]\n        # Vector from face center to start node of each edge\n        face_2_node = tmp_face_center.transpose() - self.nodes[:, face_nodes]\n\n        # Assign a normal vector with this edge, by taking the cross product\n        # between along_edge and face_2_node\n        # Divide by two to ensure that the normal vector has length equal to\n        # the area of the face triangle (by properties of cross product)\n        sub_normals = np.vstack((along_edge[1] * face_2_node[2] -\n                                 along_edge[2] * face_2_node[1],\n                                 along_edge[2] * face_2_node[0] -\n                                 along_edge[0] * face_2_node[2],\n                                 along_edge[0] * face_2_node[1] -\n                                 along_edge[1] * face_2_node[0])) / 2\n\n        def nrm(v):\n            return np.sqrt(np.sum(v * v, axis=0))\n\n        # Calculate area of sub-face associated with each edge - note that\n        # the sub-normals are area weighted\n        sub_areas = nrm(sub_normals)\n\n        # Centers of sub-faces are given by the centroid coordinates,\n        # e.g. the mean coordinate of the edge endpoints and the temporary\n        # face center\n        sub_centroids = (self.nodes[:, face_nodes] +\n                         self.nodes[:, face_nodes[next_node]]\n                         + tmp_face_center.transpose()) / 3\n\n        # Face normals are given as the sum of the sub-components\n        face_normals = sub_normals * edge_2_face\n        # Similar with face areas\n        face_areas = edge_2_face.transpose() * sub_areas\n\n        # Test whether the sub-normals are pointing in the same direction as\n        # the main normal: Distribute the main normal onto the edges,\n        # and take scalar product by element-wise multiplication with\n        # sub-normals, and sum over the components (axis=0).\n        # NOTE: There should be a built-in function for this in numpy?\n        sub_normals_sign = np.sign(np.sum(sub_normals * (edge_2_face *\n                                                         face_normals.transpose()).transpose(),\n                                          axis=0))\n\n        # Finally, face centers are the area weighted means of centroids of\n        # the sub-faces\n        face_centers = sub_areas * sub_centroids * edge_2_face / face_areas\n\n        # .. and we're done with the faces. Store information\n        self.face_centers = face_centers\n        self.face_normals = face_normals\n        self.face_areas = face_areas\n\n        # Cells\n\n        # Temporary cell center coordinates as the mean of the face center\n        # coordinates. The cells are divided into sub-tetrahedra (\n        # corresponding to triangular sub-faces above), with the temporary\n        # cell center as the final node\n\n        # Mapping from edges to cells. Take absolute value of cell_faces,\n        # since the elements are signed (contains the divergence).\n        # Note that edge_2_cell will contain more elements than edge_2_face,\n        # since the former will count internal faces twice (one for each\n        # adjacent cell)\n        edge_2_cell = edge_2_face * np.abs(self.cell_faces)\n        # Sort indices to avoid messing up the mappings later\n        edge_2_cell.sort_indices()\n\n        # Obtain relations between edges, faces and cells, in the form of\n        # index lists. Each element in the list corresponds to an edge seen\n        # from a cell (e.g. edges on internal faces are seen twice).\n\n        # Cell numbers are obtained from the columns in edge_2_cell.\n        cell_numbers = matrix_compression.rldecode(np.arange(self.num_cells),\n                                                   np.diff(edge_2_cell.indptr))\n        # Edge numbers from the rows. Here it is crucial that the indices\n        # are sorted\n        edge_numbers = edge_2_cell.indices\n        # Face numbers are obtained from the face-node relations (with the\n        # nodes doubling as representation of edges)\n        face_numbers = face_node_ind[edge_numbers]\n\n        # Number of edges per cell\n        num_cell_edges = edge_2_cell.indptr[1:] - edge_2_cell.indptr[:-1]\n\n        def bincount_nd(arr, weights):\n            \"\"\" Utility function to sum vector quantities by np.bincount. We\n            could probably have used np.apply_along_axis, but I could not\n            make it work.\n\n            Intended use: Map sub-cell centroids to a quantity for the cell.\n            \"\"\"\n            dim = weights.shape[0]\n            sz = arr.max() + 1\n\n            count = np.zeros((dim, sz))\n            for iter1 in range(dim):\n                count[iter1] = np.bincount(arr, weights=weights[iter1],\n                                           minlength=sz)\n            return count\n\n        # First estimate of cell centers as the mean of its faces' centers\n        # Divide by num_cell_edges here since all edges bring in their faces\n        tmp_cell_centers = bincount_nd(cell_numbers,\n                                       face_centers[:, face_numbers]\n                                       / num_cell_edges[cell_numbers])\n\n        # Distance from the temporary cell center to the sub-centroids (of\n        # the tetrahedra associated with each edge)\n        dist_cellcenter_subface = sub_centroids[:, edge_numbers] \\\n            - tmp_cell_centers[:, cell_numbers]\n\n        # Get sign of normal vectors, seen from all faces.\n        # Make sure we get a numpy ndarray, and not a matrix (.A), and that\n        # the array is 1D (squeeze)\n        orientation = np.squeeze(self.cell_faces[face_numbers, cell_numbers].A)\n\n        # Get outwards pointing sub-normals for all sub-faces: We need to\n        # account for both the orientation of the face, and the orientation\n        # of sub-faces relative to faces.\n        outer_normals = sub_normals[:, edge_numbers] \\\n            * orientation * sub_normals_sign[edge_numbers]\n\n        # Volumes of tetrahedra are now given by the dot product between the\n        #  outer normal (which is area weighted, and thus represent the base\n        #  of the tet), with the distancance from temporary cell center (the\n        # dot product gives the hight).\n        tet_volumes = np.sum(dist_cellcenter_subface * outer_normals,\n                             axis=0) / 3\n\n        # Sometimes the sub-tet volumes can have a volume of numerical zero.\n        # Why this is so is not clear, but for the moment, we allow for a\n        # slightly negative value.\n        assert np.all(tet_volumes > -1e-12)  # On the fly test\n\n        # The cell volumes are now found by summing sub-tetrahedra\n        cell_volumes = np.bincount(cell_numbers, weights=tet_volumes)\n        tri_centroids = 3 / 4 * dist_cellcenter_subface\n\n        # Compute a correction to the temporary cell center, by a volume\n        # weighted sum of the sub-tetrahedra\n        rel_centroid = bincount_nd(cell_numbers, tet_volumes * tri_centroids) \\\n            / cell_volumes\n        cell_centers = tmp_cell_centers + rel_centroid\n\n        # ... and we're done\n        self.cell_centers = cell_centers\n        self.cell_volumes = cell_volumes\n\n# --- Snippet Separator ---\n\ndef cell_ind_for_partial_update(g, cells=None, faces=None, nodes=None):\n    \"\"\" Obtain indices of cells and faces needed for a partial update of the\n    discretization stencil.\n\n    Implementation note: This function should really be split into three parts,\n    one for each of the modes (cell, face, node).\n\n    The subgrid can be specified in terms of cells, faces and nodes to be\n    updated. The method will then define a sufficiently large subgrid to\n    account for changes in the flux discretization. The idea is that cells are\n    used to account for updates in material parameters (or geometry), faces\n    when faces are split (fracture growth), while the parameter nodes is mainly\n    aimed at a gradual build of the discretization of the entire grid (for\n    memory conservation, see comments in mpfa.mpfa()). For more details, see\n    the implementations and comments below.\n\n    Cautionary note: The option to combine cells, faces and nodes in one go has\n    not been tested. Problems may arise for grid configurations where separate\n    regions are close to touching. This is however speculation at the time of\n    writing.\n\n    Parameters:\n        g (core.grids.grid): grid to be discretized\n        cells (np.array, int, optional): Index of cells on which to base the\n            subgrid computation. Defaults to None.\n        faces (np.array, int, optional): Index of faces on which to base the\n            subgrid computation. Defaults to None.\n        nodes (np.array, int, optional): Index of faces on which to base the\n            subgrid computation. Defaults to None.\n\n    Returns:\n        np.array, int: Cell indexes of the subgrid. No guarantee that they form\n            a connected grid.\n        np.array, int: Indexes of faces to have their discretization updated.\n\n    \"\"\"\n\n    # Faces that are active, and should have their discretization stencil\n    # updated / returned.\n    active_faces = np.zeros(g.num_faces, dtype=np.bool)\n\n    # Index of cells to include in the subgrid.\n    cell_ind = np.empty(0)\n\n    if cells is not None:\n        # To understand the update stencil for a cell-based update, consider\n        # the Cartesian 2d configuration below.\n        #\n        #    _ s s s _\n        #    s o o o s\n        #    s o x o s\n        #    s o o o s\n        #    - s s s -\n        #\n        # The central cell (x) is to be updated. The support of MPFA basis\n        # functions dictates that the stencil between the central cell and its\n        # primary neighbors (o) must be updated, as must the stencil for the\n        # sub-faces between o-cells that shares a vertex with x. Since the\n        # flux information is stored face-wise (not sub-face), the whole o-o\n        # faces must be recomputed, and this involves the secondary neighbors\n        # of x (denoted s). This is most easily realized by defining an overlap\n        # of 2. This will also involve some cells and nodes not needed;\n        # specifically those marked by -. This requires quite a song and dance,\n        # see below; but most of this is necessary to get hold of the active\n        # faces anyhow.\n        #\n        # Note that a simpler option, with a somewhat higher computational cost,\n        # would be to define\n        #   cell_overlap = partition.overlap(g, cells, num_layers=2)\n        # This would however include more cells (all marked - in the\n        # illustration, and potentially significantly many more in 3d, in\n        # particular for unstructured grids).\n\n        cn = g.cell_nodes()\n\n        # The active faces (to be updated; (o-x and o-o above) are those that\n        # share at least one vertex with cells in ind.\n        prim_cells = np.zeros(g.num_cells, dtype=np.bool)\n        prim_cells[cells] = 1\n        # Vertexes of the cells\n        active_vertexes = np.zeros(g.num_nodes, dtype=np.bool)\n        active_vertexes[np.squeeze(np.where(cn * prim_cells > 0))] = 1\n\n        # Faces of the vertexes, these will be the active faces.\n        active_face_ind = np.squeeze(np.where(g.face_nodes.transpose()\n                                              * active_vertexes > 0))\n        active_faces[active_face_ind] = 1\n\n        # Secondary vertexes, involved in at least one of the active faces,\n        # that is, the faces to be updated. Corresponds to vertexes between o-o\n        # above.\n        active_vertexes[np.squeeze(np.where(g.face_nodes\n                                            * active_faces > 0))] = 1\n\n        # Finally, get hold of all cells that shares one of the secondary\n        # vertexes.\n        cells_overlap = np.squeeze(np.where((cn.transpose()\n                                             * active_vertexes) > 0))\n        # And we have our overlap!\n        cell_ind = np.hstack((cell_ind, cells_overlap))\n\n    if faces is not None:\n        # The faces argument is intended used when the configuration of the\n        # specified faces has changed, e.g. due to the introduction of an\n        # external boundary. This requires the recomputation of all faces that\n        # share nodes with the specified faces. Since data is not stored on\n        # sub-faces. This further requires the inclusion of all cells that\n        # share a node with a secondary face.\n        #\n        #      o o o\n        #    o o x o o\n        #    o o x o o\n        #      o o o\n        #\n        # To illustrate for the Cartesian configuration above: The face\n        # between the two x-cells are specified, and this requires the\n        # inclusion of all o-cells.\n        #\n\n        cf = g.cell_faces\n        # This avoids overwriting data in cell_faces.\n        data = np.ones_like(cf.data)\n        cf = sps.csc_matrix((data, cf.indices, cf.indptr))\n\n        primary_faces = np.zeros(g.num_faces, dtype=np.bool)\n        primary_faces[faces] = 1\n\n        # The active faces are those sharing a vertex with the primary faces\n        primary_vertex = np.zeros(g.num_nodes, dtype=np.bool)\n        primary_vertex[np.squeeze(np.where((g.face_nodes\n                                            * primary_faces) > 0))] = 1\n        active_face_ind = np.squeeze(np.where((g.face_nodes.transpose()\n                                               * primary_vertex) > 0))\n        active_faces[active_face_ind] = 1\n\n        # Find vertexes of the active faces\n        active_nodes = np.zeros(g.num_nodes, dtype=np.bool)\n        active_nodes[np.squeeze(np.where((g.face_nodes\n                                          * active_faces) > 0))] = 1\n\n        active_cells = np.zeros(g.num_cells, dtype=np.bool)\n        # Primary cells, those that have the faces as a boundary\n        cells_overlap = np.squeeze(np.where((g.cell_nodes().transpose()\n                                             * active_nodes) > 0))\n        cell_ind = np.hstack((cell_ind, cells_overlap))\n\n    if nodes is not None:\n        # Pick out all cells that have the specified nodes as a vertex.\n        # The active faces will be those that have all their vertexes included\n        # in nodes.\n        cn = g.cell_nodes()\n        # Introduce active nodes, and make the input nodes active\n        # The data type of active_vertex is int (in contrast to similar cases\n        # in other parts of this function), since we will use it to count the\n        # number of active face_nodes below.\n        active_vertexes = np.zeros(g.num_nodes, dtype=np.int)\n        active_vertexes[nodes] = 1\n\n        # Find cells that share these nodes\n        active_cells = np.squeeze(np.where((cn.transpose()\n                                            * active_vertexes) > 0))\n        # Append the newly found active cells\n        cell_ind = np.hstack((cell_ind, active_cells))\n\n        # Multiply face_nodes.transpose() (e.g. node-faces) with the active\n        # vertexes to get the number of active nodes perm face\n        num_active_face_nodes = np.array(g.face_nodes.transpose()\n                                         * active_vertexes)\n        # Total number of nodes per face\n        num_face_nodes = np.array(g.face_nodes.sum(axis=0))\n        # Active faces are those where all nodes are active.\n        active_face_ind = np.squeeze(np.argwhere((num_active_face_nodes ==\n                                                  num_face_nodes).ravel('F')))\n        active_faces[active_face_ind] = 1\n\n    face_ind = np.squeeze(np.where(active_faces))\n\n    # Do a sort of the indexes to be returned.\n    cell_ind.sort()\n    face_ind.sort()\n    # Return, with data type int\n    return cell_ind.astype('int'), face_ind.astype('int')\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary libraries and creates a TetrahedronMesh using the fealpy library. The code should define nodes and cells for the mesh, calculate the number of nodes, edges, faces, and cells, and store these entities. It should also calculate and store the barycenter coordinates for each entity, the measure of each entity, and the relationships between each entity (cell to cell, cell to face, etc.). The code should also identify boundary flags for each entity and the indices of boundary nodes, edges, faces, and cells. Finally, the code should plot the mesh using matplotlib, showing the indices of nodes, edges, and cells.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 13, "repo_full_name": "microsoft__qlib", "instruction": "Generate code that creates a class for simulating the OnlineManager based on rolling tasks using the qlib library. The class should initialize with parameters such as provider_uri, region, experiment name, task URL, task database name, task pool name, rolling step, start time, end time, tasks, and trainer. The class should have methods to reset the experiment, run the entire workflow automatically, and train tasks by other processes or machines for multiprocessing. The main method should include steps to reset, simulate, collect results, get signals, perform backtesting, and risk analysis. The class should be executable from the command line with user-defined parameters.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class TaskGen(metaclass=abc.ABCMeta):\n    @abc.abstractmethod\n    def __call__(self, *args, **kwargs) -> typing.List[dict]:\n        \"\"\"\n        generate\n\n        Parameters\n        ----------\n        args, kwargs:\n            The info for generating tasks\n            Example 1):\n                input: a specific task template\n                output: rolling version of the tasks\n            Example 2):\n                input: a specific task template\n                output: a set of tasks with different losses\n\n        Returns\n        -------\n        typing.List[dict]:\n            A list of tasks\n        \"\"\"\n        pass\n\n# --- Snippet Separator ---\n\ndef __call__(self, *args, **kwargs) -> typing.List[dict]:\n        \"\"\"\n        generate\n\n        Parameters\n        ----------\n        args, kwargs:\n            The info for generating tasks\n            Example 1):\n                input: a specific task template\n                output: rolling version of the tasks\n            Example 2):\n                input: a specific task template\n                output: a set of tasks with different losses\n\n        Returns\n        -------\n        typing.List[dict]:\n            A list of tasks\n        \"\"\"\n        pass\n\n# --- Snippet Separator ---\n\nclass Tasks(ResourcePatchMixin, Resource):\n    \"\"\"\n    Tasks API client.\n\n    \"\"\"\n    URI = '/rest/tasks'\n\n    def __init__(self, connection, data=None):\n        super(Tasks, self).__init__(connection, data)\n\n    def get_all(self, start=0, count=-1, fields='', filter='', query='', sort='', view='', topCount=0, childLimit=0):\n        \"\"\"\n        Gets all the tasks based upon filters provided.\n\n        Note:\n            Filters are optional.\n\n        Args:\n            start:\n                 The first item to return, using 0-based indexing. If not specified, the default is 0 - start with the\n                 first available item.\n            count:\n                The number of resources to return. A count of -1 requests all items. The actual number of items in\n                the response may differ from the requested count if the sum of start and count exceed the total number\n                of items.\n            fields:\n                 Specifies which fields should be returned in the result set.\n            filter (list or str):\n                 A general filter/query string to narrow the list of items returned. The default is no filter; all\n                 resources are returned.\n            query:\n                 A general query string to narrow the list of resources returned. The default is no query (all\n                 resources are returned).\n            sort:\n                The sort order of the returned data set. By default, the sort order is based on create time, with the\n                oldest entry first.\n            view:\n                 Returns a specific subset of the attributes of the resource or collection, by specifying the name of a\n                 predefined view. The default view is expand (show all attributes of the resource and all elements of\n                 collections of resources).\n            childLimit:\n                 Total number of associated resources in an aggregated manner. Default value is 10.\n            topCount:\n                 Total number of immediate children the task should send back. Otherwise, the task sends back the\n                 aggregated view of the tree. Default value is 3.\n\n        Returns:\n            list: A list of tasks.\n        \"\"\"\n        return self._helper.get_all(start=start, count=count, filter=filter, query=query, sort=sort, view=view,\n                                    fields=fields, childLimit=childLimit, topCount=topCount)\n\n    def patch(self, uri, timeout=-1):\n        \"\"\"\n        Sets the state of task to cancelling only if IsCancellable is set to true for the task and its children or\n        children are in terminal state.\n\n        Args:\n            uri: URI of task resource.\n            timeout: Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation\n                in OneView; it just stops waiting for its completion.\n\n        Returns:\n            dict: Updated resource.\n        \"\"\"\n        resp, body = self._connection.do_http('PATCH', path=uri, body=None)\n\n        if resp.status >= 400:\n            raise HPEOneViewException(body)\n        elif resp.status == 304:\n            if body and not isinstance(body, dict):\n                try:\n                    body = json.loads(body)\n                except Exception:\n                    pass\n        elif resp.status == 202:\n            task = self._connection.__get_task_from_response(resp, body)\n            return self._task_monitor.wait_for_task(task, timeout)\n\n        return body\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a class for simulating the OnlineManager based on rolling tasks using the qlib library. The class should initialize with parameters such as provider_uri, region, experiment name, task URL, task database name, task pool name, rolling step, start time, end time, tasks, and trainer. The class should have methods to reset the experiment, run the entire workflow automatically, and train tasks by other processes or machines for multiprocessing. The main method should include steps to reset, simulate, collect results, get signals, perform backtesting, and risk analysis. The class should be executable from the command line with user-defined parameters.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 14, "repo_full_name": "smdogroup__tacs", "instruction": "Generate code that performs the following tasks using the TACS library:\n\n1. Import necessary libraries including numpy, os, MPI from mpi4py, and several modules from tacs.\n2. Load a structural mesh from a BDF file.\n3. Set constitutive properties such as density, elastic modulus, poisson's ratio, shear correction factor, yield stress, and thickness.\n4. Loop over components of the mesh, creating stiffness and element object for each.\n5. Create a TACS assembler object from the mesh loader.\n6. Create a KS function and get the design variable values.\n7. Get the node locations and create the forces.\n8. Set up and solve the analysis problem by creating vectors, assembling the Jacobian, factoring, and solving the linear system.\n9. Evaluate the function and solve for the adjoint variables.\n10. Compute the total derivative with respect to material design variables and nodal locations.\n11. Create a random direction along which to perturb the nodes and compute the total derivative with respect to nodal locations.\n12. Set the complex step and compute the perturbed solution.\n13. Evaluate the function for the perturbed solution and compute the projected derivative.\n14. Output the results for visualization.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class TACSSystem(BaseUI):\n    \"\"\"\n    Base class for TACS problem/constraint types. Contains methods common to all TACS systems dealing with design variables.\n    \"\"\"\n\n    def __init__(\n        self, assembler, comm=None, options=None, outputViewer=None, meshLoader=None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        assembler : tacs.TACS.Assembler\n            Cython object responsible for creating and setting tacs objects used to solve problem\n\n        comm : mpi4py.MPI.Intracomm\n            The comm object on which to create the pyTACS object.\n\n        options : dict\n            Dictionary holding problem-specific option parameters (case-insensitive).\n\n        outputViewer : tacs.TACS.TACSToFH5\n            Cython object used to write out f5 files that can be converted and used for postprocessing.\n\n        meshLoader : tacs.pymeshloader.pyMeshLoader\n            pyMeshLoader object used to create the assembler.\n        \"\"\"\n        # TACS assembler object\n        self.assembler = assembler\n        # TACS F5 output writer\n        self.outputViewer = outputViewer\n        # TACS pyMeshLoader object\n        self.meshLoader = meshLoader\n        # pyNastran BDF object\n        if self.meshLoader:\n            self.bdfInfo = self.meshLoader.getBDFInfo()\n\n        # Create Design variable vector\n        self.x = self.assembler.createDesignVec()\n        self.assembler.getDesignVars(self.x)\n        self.varName = \"struct\"\n        # Create Nodal coordinate vector\n        self.Xpts = self.assembler.createNodeVec()\n        self.assembler.getNodes(self.Xpts)\n        self.coordName = \"Xpts\"\n\n        # Setup comm and options\n        BaseUI.__init__(self, options=options, comm=comm)\n\n        return\n\n    ####### Design variable methods ########\n\n    def setVarName(self, varName):\n        \"\"\"\n        Set a name for the structural variables in pyOpt. Only needs\n        to be changed if more than 1 pytacs object is used in an\n        optimization\n\n        Parameters\n        ----------\n        varName : str\n            Name of the structural variable used in addVarGroup().\n        \"\"\"\n        self.varName = varName\n\n    def getDesignVars(self):\n        \"\"\"\n        Get the current set of  design variables for this problem.\n\n        Returns\n        ----------\n        x : numpy.ndarray\n            The current design variable vector set in tacs.\n\n        \"\"\"\n        return self.x.getArray().copy()\n\n    def setDesignVars(self, x):\n        \"\"\"\n        Update the design variables used by tacs.\n\n        Parameters\n        ----------\n        x : numpy.ndarray or dict or tacs.TACS.Vec\n            The variables (typically from the optimizer) to set. It\n            looks for variable in the ``self.varName`` attribute if in dict.\n\n        \"\"\"\n        # Check if the design variables are being handed in a dict\n        if isinstance(x, dict):\n            if self.varName in x:\n                self.x.getArray()[:] = x[self.varName]\n        # or array\n        elif isinstance(x, np.ndarray):\n            self.x.getArray()[:] = x\n        # Or TACS BVec\n        elif isinstance(x, tacs.TACS.Vec):\n            self.x.copyValues(x)\n        else:\n            raise ValueError(\n                \"setDesignVars must be called with either a numpy array, dict, or TACS Vec as input.\"\n            )\n\n        # Set the variables in tacs\n        self.assembler.setDesignVars(self.x)\n\n    def getDesignVarRange(self):\n        \"\"\"\n        get the lower/upper bounds for the design variables.\n\n        Returns\n        ----------\n        xlb : numpy.ndarray\n            The design variable lower bound.\n        xub : numpy.ndarray\n            The design variable upper bound.\n\n        \"\"\"\n        xlb = self.assembler.createDesignVec()\n        xub = self.assembler.createDesignVec()\n        self.assembler.getDesignVarRange(xlb, xub)\n        return xlb.getArray(), xub.getArray()\n\n    def _arrayToDesignVec(self, dvArray):\n        \"\"\"\n        Converts a distributed numpy array into a TACS design variable BVec.\n\n        Parameters\n        ----------\n        dvArray : numpy.ndarray\n                  Numpy array for which to convert to TACS designVec.\n\n        Returns\n        -------\n        xVec : tacs.TACS.Vec\n               Converted TACS designVec.\n\n        Notes\n        -----\n        dvArray must have correct size on each processor.\n        \"\"\"\n        xVec = self.assembler.createDesignVec()\n\n        # Set values\n        xVec.getArray()[:] = dvArray\n\n        # Return as tacs bvec object\n        return xVec\n\n    def getNumDesignVars(self):\n        \"\"\"\n        Return the number of design variables on this processor.\n\n        Returns\n        -------\n        ndvs : int\n            Number of design variables on this processor.\n        \"\"\"\n        return self.x.getSize()\n\n    def getNodes(self):\n        \"\"\"\n        Return the mesh coordinates of this problem.\n\n        Returns\n        -------\n        coords : numpy.ndarray\n            Structural coordinate in array of size (N * 3) where N is\n            the number of structural nodes on this processor.\n        \"\"\"\n        return self.Xpts.getArray().copy()\n\n    def setNodes(self, Xpts):\n        \"\"\"\n        Set the mesh coordinates of the structure.\n\n        Parameters\n        ----------\n        coords : numpy.ndarray\n            Structural coordinate in array of size (N * 3) where N is\n            the number of structural nodes on this processor.\n        \"\"\"\n        # Check if the design variables are being handed in a dict\n        if isinstance(Xpts, dict):\n            if self.coordName in Xpts:\n                self.Xpts.getArray()[:] = Xpts[self.coordName]\n        # or array\n        elif isinstance(Xpts, np.ndarray):\n            self.Xpts.getArray()[:] = Xpts\n        # Or TACS BVec\n        elif isinstance(Xpts, tacs.TACS.Vec):\n            self.Xpts.copyValues(Xpts)\n        else:\n            raise ValueError(\n                \"setNodes must be called with either a numpy array, dict, or TACS Vec as input.\"\n            )\n        self.assembler.setNodes(self.Xpts)\n\n    def _arrayToNodeVec(self, xptsArray):\n        \"\"\"\n        Converts a distributed numpy array into a TACS node BVec.\n\n        Parameters\n        ----------\n        xptsArray : numpy.ndarray\n                    Numpy array for which to convert to TACS nodeVec.\n\n        Returns\n        -------\n        Xptsvec : tacs.TACS.Vec\n                  Converted TACS nodeVec.\n\n        Notes\n        -----\n        xptsArray must have correct size on each processor.\n        \"\"\"\n        Xptsvec = self.assembler.createNodeVec()\n\n        # Set values\n        Xptsvec.getArray()[:] = xptsArray\n\n        # Return as tacs bvec object\n        return Xptsvec\n\n    def getNumCoordinates(self):\n        \"\"\"\n        Return the number of mesh coordinates on this processor.\n\n        Returns\n        -------\n        ncoords : int\n            Number of mesh coordinates on this processor.\n        \"\"\"\n        return self.Xpts.getSize()\n\n    ####### Variable methods ########\n\n    def getVarsPerNode(self):\n        \"\"\"\n        Get the number of variables per node for the model.\n\n        Returns\n        -------\n        vpn : int\n            Number of variables per node.\n        \"\"\"\n        return self.assembler.getVarsPerNode()\n\n    def getNumOwnedNodes(self):\n        \"\"\"\n        Get the number of nodes owned by this processor.\n\n        Returns\n        -------\n        nnodes : int\n            Number of nodes on this processor.\n        \"\"\"\n        return self.assembler.getNumOwnedNodes()\n\n    def _arrayToVec(self, varArray):\n        \"\"\"\n        Converts a distributed numpy array into a TACS state variable BVec.\n\n        Parameters\n        ----------\n        varArray : numpy.ndarray\n                   Numpy array for which to convert to TACS Vec.\n\n        Returns\n        -------\n        varVec : tacs.TACS.Vec\n                 Converted TACS Vec.\n\n        Notes\n        -----\n        varArray must have correct size on each processor.\n        \"\"\"\n        varVec = self.assembler.createVec()\n\n        # Set values\n        varVec.getArray()[:] = varArray\n\n        # Return as tacs bvec object\n        return varVec\n\n    def getNumVariables(self):\n        \"\"\"\n        Return the number of degrees of freedom (states) that are\n        on this processor\n\n        Returns\n        -------\n        nstate : int\n            number of states.\n        \"\"\"\n        vpn = self.getVarsPerNode()\n        nnodes = self.getNumOwnedNodes()\n        return vpn * nnodes\n\n# --- Snippet Separator ---\n\ndef __init__(\n        self, assembler, comm=None, options=None, outputViewer=None, meshLoader=None\n    ):\n        \"\"\"\n        Parameters\n        ----------\n        assembler : tacs.TACS.Assembler\n            Cython object responsible for creating and setting tacs objects used to solve problem\n\n        comm : mpi4py.MPI.Intracomm\n            The comm object on which to create the pyTACS object.\n\n        options : dict\n            Dictionary holding problem-specific option parameters (case-insensitive).\n\n        outputViewer : tacs.TACS.TACSToFH5\n            Cython object used to write out f5 files that can be converted and used for postprocessing.\n\n        meshLoader : tacs.pymeshloader.pyMeshLoader\n            pyMeshLoader object used to create the assembler.\n        \"\"\"\n        # TACS assembler object\n        self.assembler = assembler\n        # TACS F5 output writer\n        self.outputViewer = outputViewer\n        # TACS pyMeshLoader object\n        self.meshLoader = meshLoader\n        # pyNastran BDF object\n        if self.meshLoader:\n            self.bdfInfo = self.meshLoader.getBDFInfo()\n\n        # Create Design variable vector\n        self.x = self.assembler.createDesignVec()\n        self.assembler.getDesignVars(self.x)\n        self.varName = \"struct\"\n        # Create Nodal coordinate vector\n        self.Xpts = self.assembler.createNodeVec()\n        self.assembler.getNodes(self.Xpts)\n        self.coordName = \"Xpts\"\n\n        # Setup comm and options\n        BaseUI.__init__(self, options=options, comm=comm)\n\n        return\n\n# --- Snippet Separator ---\n\ndef __init__(\n        self,\n        name,\n        assembler,\n        comm,\n        outputViewer=None,\n        meshLoader=None,\n        options=None,\n    ):\n        \"\"\"\n        NOTE: This class should not be initialized directly by the user.\n        Use pyTACS.createStaticProblem instead.\n\n        Parameters\n        ----------\n        name : str\n            Name of this tacs problem\n\n        assembler : tacs.TACS.Assembler\n            Cython object responsible for creating and setting tacs objects used to solve problem\n\n        comm : mpi4py.MPI.Intracomm\n            The comm object on which to create the pyTACS object.\n\n        outputViewer : tacs.TACS.TACSToFH5\n            Cython object used to write out f5 files that can be converted and used for postprocessing.\n\n        meshLoader : tacs.pymeshloader.pyMeshLoader\n            pyMeshLoader object used to create the assembler.\n\n        options : dict\n            Dictionary holding problem-specific option parameters (case-insensitive).\n\n        \"\"\"\n\n        # Problem name\n        self.name = name\n\n        # Set linear solver to None, until we set it up later\n        self.KSM = None\n\n        # Default setup for common problem class objects, sets up comm and options\n        TACSProblem.__init__(self, assembler, comm, options, outputViewer, meshLoader)\n\n        # Create problem-specific variables\n        self._createVariables()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks using the TACS library:\n\n1. Import necessary libraries including numpy, os, MPI from mpi4py, and several modules from tacs.\n2. Load a structural mesh from a BDF file.\n3. Set constitutive properties such as density, elastic modulus, poisson's ratio, shear correction factor, yield stress, and thickness.\n4. Loop over components of the mesh, creating stiffness and element object for each.\n5. Create a TACS assembler object from the mesh loader.\n6. Create a KS function and get the design variable values.\n7. Get the node locations and create the forces.\n8. Set up and solve the analysis problem by creating vectors, assembling the Jacobian, factoring, and solving the linear system.\n9. Evaluate the function and solve for the adjoint variables.\n10. Compute the total derivative with respect to material design variables and nodal locations.\n11. Create a random direction along which to perturb the nodes and compute the total derivative with respect to nodal locations.\n12. Set the complex step and compute the perturbed solution.\n13. Evaluate the function for the perturbed solution and compute the projected derivative.\n14. Output the results for visualization.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 15, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that solves the time-harmonic equation using adaptive methods. The code should include the following functionalities:\n\n1. Define functions to recover the curl of a given solution and to calculate the least squares matrix for each node of a mesh.\n2. Parse command-line arguments to set the degree of the first kind Nedelec element, the initial mesh size, the maximum number of adaptive iterations, and the theta parameter for adaptive iteration.\n3. Initialize the problem using the CosSinData function from the fealpy library.\n4. Create a 2D box mesh using the MeshFactory class from the fealpy library and remove the fourth quadrant of the mesh.\n5. Iterate over the maximum number of adaptive iterations, during each iteration:\n   - Define the function space using the FirstKindNedelecFiniteElementSpace2d class from the fealpy library.\n   - Apply Dirichlet boundary conditions using the DirichletBC class from the fealpy library.\n   - Solve the system of equations using the scipy library's spsolve function.\n   - Calculate the L2 error between the solution and the exact solution, the curl of the solution and the exact curl, and the curl of the solution and the recovered curl.\n   - If not the last iteration, mark the cells for refinement based on the recovery error and refine the mesh.\n6. Plot the error rates using the showmultirate function from the fealpy library.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class FemMagnetostatic2d:\n    \"\"\"\n    A 2D magnetostatic solver. The solver is thought as support for the fem fixed\n    boundary module and it is limited to axisymmetric magnetostatic problem\n    with toroidal current sources. The Maxwell equations, as function of the poloidal\n    magnetic flux (:math:`\\\\Psi`), are then reduced to the form ([Zohm]_, page 25):\n\n    .. math::\n        r^2 \\\\nabla\\\\cdot\\\\left(\\\\frac{\\\\nabla\\\\Psi}{r^2}\\\\right) = 2\n        \\\\pi r \\\\mu_0 J_{\\\\Phi}\n\n    whose weak formulation is defined as ([Villone]_):\n\n    .. math::\n        \\\\int_{D_p} {\\\\frac{1}{r}}{\\\\nabla}{\\\\Psi}{\\\\cdot}{\\\\nabla} v \\\\,dr\\\\,dz = 2\n        \\\\pi \\\\mu_0 \\\\int_{D_p} J_{\\\\Phi} v \\\\,dr\\\\,dz\n\n    where :math:`v` is the basis element function of the defined functional subspace\n    :math:`V`.\n\n    .. [Zohm] H. Zohm, Magnetohydrodynamic Stability of Tokamaks, Wiley-VCH, Germany,\n       2015\n    .. [Villone] VILLONE, F. et al. Plasma Phys. Control. Fusion 55 (2013) 095008,\n       https://doi.org/10.1088/0741-3335/55/9/095008\n\n    Parameters\n    ----------\n    p_order:\n        Order of the approximating polynomial basis functions\n    \"\"\"\n\n    def __init__(self, p_order: int = 2):\n        self.p_order = p_order\n        self.mesh = None\n        self.a = None\n        self.u = None\n        self.v = None\n        self.V = None\n        self.g = None\n        self.L = None\n        self.boundaries = None\n        self.bcs = None\n\n        self.psi = None\n        self.B = None\n\n    def set_mesh(\n        self,\n        mesh: Union[dolfin.Mesh, str],\n        boundaries: Optional[Union[dolfin.Mesh, str]] = None,\n    ):\n        \"\"\"\n        Set the mesh for the solver\n\n        Parameters\n        ----------\n        mesh:\n            Filename of the xml file with the mesh definition or a dolfin mesh\n        boundaries:\n            Filename of the xml file with the boundaries definition or a MeshFunction\n            that defines the boundaries\n        \"\"\"\n        # check whether mesh is a filename or a mesh, then load it or use it\n        self.mesh = dolfin.Mesh(mesh) if isinstance(mesh, str) else mesh\n\n        # define boundaries\n        if boundaries is None:\n            # initialize the MeshFunction\n            self.boundaries = dolfin.MeshFunction(\n                \"size_t\", mesh, mesh.topology().dim() - 1\n            )\n        elif isinstance(boundaries, str):\n            # check weather boundaries is a filename or a MeshFunction,\n            # then load it or use it\n            self.boundaries = dolfin.MeshFunction(\n                \"size_t\", self.mesh, boundaries\n            )  # define the boundaries\n        else:\n            self.boundaries = boundaries\n\n        # define the function space and bilinear forms\n        # the Continuos Galerkin function space has been chosen as suitable for the\n        # solution of the magnetostatic weak formulation in a Soblev Space H1(D)\n        self.V = dolfin.FunctionSpace(self.mesh, \"CG\", self.p_order)\n\n        # define trial and test functions\n        self.u = dolfin.TrialFunction(self.V)\n        self.v = dolfin.TestFunction(self.V)\n\n        # Define r\n        r = dolfin.Expression(\"x[0]\", degree=self.p_order)\n\n        self.a = (\n            1\n            / (2.0 * dolfin.pi * MU_0)\n            * (1 / r * dolfin.dot(dolfin.grad(self.u), dolfin.grad(self.v)))\n            * dolfin.dx\n        )\n\n        # initialize solution\n        self.psi = dolfin.Function(self.V)\n        self.psi.set_allow_extrapolation(True)\n\n        # initialize g to zero\n        self.g = dolfin.Function(self.V)\n\n    def define_g(self, g: Union[dolfin.Expression, dolfin.Function]):\n        \"\"\"\n        Define g, the right hand side function of the Poisson problem\n\n        Parameters\n        ----------\n        g:\n            Right hand side function of the Poisson problem\n        \"\"\"\n        self.g = g\n\n    def solve(\n        self,\n        dirichlet_bc_function: Optional[\n            Union[dolfin.Expression, dolfin.Function]\n        ] = None,\n        dirichlet_marker: Optional[int] = None,\n        neumann_bc_function: Optional[Union[dolfin.Expression, dolfin.Function]] = None,\n    ) -> dolfin.Function:\n        \"\"\"\n        Solve the weak formulation maxwell equation given a right hand side g,\n        Dirichlet and Neumann boundary conditions.\n\n        Parameters\n        ----------\n        dirichlet_bc_function:\n            Dirichlet boundary condition function\n        dirichlet_marker:\n            Identification number for the dirichlet boundary\n        neumann_bc_function:\n            Neumann boundary condition function\n\n        Returns\n        -------\n        Poloidal magnetic flux function as solution of the magnetostatic problem\n        \"\"\"\n        if neumann_bc_function is None:\n            neumann_bc_function = dolfin.Expression(\"0.0\", degree=self.p_order)\n\n        # define the right hand side\n        self.L = self.g * self.v * dolfin.dx - neumann_bc_function * self.v * dolfin.ds\n\n        # define the Dirichlet boundary conditions\n        if dirichlet_bc_function is None:\n            dirichlet_bc_function = dolfin.Expression(\"0.0\", degree=self.p_order)\n            dirichlet_bc = dolfin.DirichletBC(\n                self.V, dirichlet_bc_function, \"on_boundary\"\n            )\n        else:\n            dirichlet_bc = dolfin.DirichletBC(\n                self.V, dirichlet_bc_function, self.boundaries, dirichlet_marker\n            )\n        self.bcs = [dirichlet_bc]\n\n        # solve the system taking into account the boundary conditions\n        dolfin.solve(\n            self.a == self.L,\n            self.psi,\n            self.bcs,\n            solver_parameters={\"linear_solver\": \"default\"},\n        )\n\n        return self.psi\n\n    def calculate_b(self) -> dolfin.Function:\n        \"\"\"\n        Calculates the magnetic field intensity from psi\n\n        Note: code from Fenics_tutorial (\n        https://link.springer.com/book/10.1007/978-3-319-52462-7), pag. 104\n        \"\"\"\n        # new function space for mapping B as vector\n        w = dolfin.VectorFunctionSpace(self.mesh, \"CG\", 1)\n\n        r = dolfin.Expression(\"x[0]\", degree=1)\n\n        # calculate derivatives\n        Bx = -self.psi.dx(1) / (2 * dolfin.pi * r)\n        Bz = self.psi.dx(0) / (2 * dolfin.pi * r)\n\n        # project B as vector to new function space\n        self.B = dolfin.project(dolfin.as_vector((Bx, Bz)), w)\n\n        return self.B\n\n# --- Snippet Separator ---\n\nclass LShapeRSinData:\n    def __int__(self):\n        pass\n\n    def __init__(self):\n        pass\n\n    def init_mesh(self, n=4, meshtype='tri'):\n        node = np.array([\n            (-1, -1),\n            (0, -1),\n            (-1, 0),\n            (0, 0),\n            (1, 0),\n            (-1, 1),\n            (0, 1),\n            (1, 1)], dtype=np.float64)\n        if meshtype == 'tri':\n            cell = np.array([\n                (1, 3, 0),\n                (2, 0, 3),\n                (3, 6, 2),\n                (5, 2, 6),\n                (4, 7, 3),\n                (6, 3, 7)], dtype=np.int_)\n            mesh = TriangleMesh(node, cell)\n            mesh.uniform_refine(n)\n            return mesh\n        elif meshtype == 'quadtree':\n            cell = np.array([\n                (0, 1, 3, 2),\n                (2, 3, 6, 5),\n                (3, 4, 7, 6)], dtype=np.int_)\n            mesh = Quadtree(node, cell)\n            mesh.uniform_refine(n)\n            return mesh\n        elif meshtype == 'tritree':\n            cell = np.array([\n                (1, 3, 0),\n                (2, 0, 3),\n                (3, 6, 2),\n                (5, 2, 6),\n                (4, 7, 3),\n                (6, 3, 7)], dtype=np.int_)\n            mesh = Tritree(node, cell)\n            mesh.uniform_refine(n)\n            return mesh\n        else:\n            raise ValueError(\"I don't know the meshtype %s\".format(meshtype))\n\n    def domain(self):\n        points = [[0, 0], [1, 0], [1, 1], [-1, 1], [-1, -1], [0, -1]]\n        facets = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)]\n        return points, facets\n\n    @cartesian\n    def solution(self, p):\n        x = p[..., 0]\n        y = p[..., 1]\n        theta = np.arctan2(y, x)\n        beta=2/3\n        val = np.zeros_like(p)\n        # the true solution we need \n        val[..., 0] = beta*(x*x+y*y)**((beta-1)/2)*np.sin((beta-1)*theta)\n        val[..., 1] = beta*(x*x+y*y)**((beta-1)/2)*np.cos((beta-1)*theta)\n\n        # 1-test solution\n        '''\n        val[..., 0] = (x*x+y*y)**(1/2)*np.sin(theta)\n        val[..., 1] = (x*x+y*y)**(1/2)*np.cos(theta)\n        '''\n\n        # 2-test solution,continuous\n        #val[..., 0] = x*x+y*y\n        #val[..., 1] = 2*x*y**2\n\n        # 3-test solution,continuous\n        #val[..., 0] = x*x*y*y\n        #val[..., 1] = x*y\n\n        # 4-test solution, continous\n        #val[..., 0] = x*x+y*y\n        #val[..., 1] = 2*(x*x+y*y)**(1/2)\n\n\n        # 5-test solution, continous\n        #val[..., 0] = x\n        #val[..., 1] = y\n        return val\n\n    @cartesian\n    def curl(self, p):\n        x = p[..., 0]\n        y = p[..., 1]\n        theta = np.arctan2(y, x)\n        beta=2/3\n        # the curl we need \n        curlu = 0\n\n        # 1-test curl\n        #curlu = 0\n\n        # 2-test curl\n        #curlu = 2*y**2-2*y\n\n        # 3-test curl\n        #curlu = y-2*x**2*y\n\n        # 4-test curl, sigularity at (0,0)\n        #curlu = 2*np.cos(theta) - 2*(x*x+y*y)**(1/2)*np.sin(theta)\n\n\n        # the function we use to test edge integral\n        # space.integralalg.edge_integral(pde.curl)\n        #curlu = x\n\n        return curlu\n\n    @cartesian\n    def curlcurl(self, p):\n        x = p[..., 0]\n        y = p[..., 1]\n        theta = np.arctan2(y, x)\n        beta=2/3\n\n        curlcurlu = np.zeros_like(p)\n        curlcurlu[...,0] = 0\n\n        return curlcurlu\n\n\n    @cartesian\n    def source(self, p):\n        \"\"\" The right hand side of time harmonic_2d\n        INPUT:\n            p: array object,  \n        \"\"\"\n        x = p[..., 0]\n        y = p[..., 1]\n        val = np.zeros_like(p)  \n        curlcurlE = self.curlcurl(p)\n        E = self.solution(p)\n        val= curlcurlE - E\n        #val[...,0] = x*x + y*y\n        #val[...,1] = x + y \n        #print(\"OOOKKKK\")\n        return val\n\n# --- Snippet Separator ---\n\nclass LShapeRSinData:\n    def __init__(self):\n        pass\n\n    def init_mesh(self, n=4, meshtype='tri'):\n        node = np.array([\n            (-1, -1),\n            (0, -1),\n            (-1, 0),\n            (0, 0),\n            (1, 0),\n            (-1, 1),\n            (0, 1),\n            (1, 1)], dtype=np.float64)\n        if meshtype == 'tri':\n            cell = np.array([\n                (1, 3, 0),\n                (2, 0, 3),\n                (3, 6, 2),\n                (5, 2, 6),\n                (4, 7, 3),\n                (6, 3, 7)], dtype=np.int_)\n            mesh = TriangleMesh(node, cell)\n            mesh.uniform_refine(n)\n            return mesh\n        elif meshtype == 'quadtree':\n            cell = np.array([\n                (0, 1, 3, 2),\n                (2, 3, 6, 5),\n                (3, 4, 7, 6)], dtype=np.int_)\n            mesh = Quadtree(node, cell)\n            mesh.uniform_refine(n)\n            return mesh\n        elif meshtype == 'tritree':\n            cell = np.array([\n                (1, 3, 0),\n                (2, 0, 3),\n                (3, 6, 2),\n                (5, 2, 6),\n                (4, 7, 3),\n                (6, 3, 7)], dtype=np.int_)\n            mesh = Tritree(node, cell)\n            mesh.uniform_refine(n)\n            return mesh\n        else:\n            raise ValueError(\"I don't know the meshtype %s\".format(meshtype))\n\n    def domain(self):\n        points = [[0, 0], [1, 0], [1, 1], [-1, 1], [-1, -1], [0, -1]]\n        facets = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)]\n        return points, facets\n\n    @cartesian\n    def solution(self, p):\n        \"\"\" The exact solution \n        Parameters\n        ---------\n        p : \n\n\n        Examples\n        -------\n        p = np.array([0, 1], dtype=np.float)\n        p = np.array([[0, 1], [0.5, 0.5]], dtype=np.float)\n        \"\"\"\n        x = p[..., 0]\n        y = p[..., 1]\n\n        pi = np.pi\n        theta = np.arctan2(y, x)\n        theta = (theta >= 0)*theta + (theta < 0)*(theta+2*pi)\n        beta=2/3\n        val = np.zeros_like(p)\n\n        # the true solution we need \n        val[..., 0] = beta*(x*x+y*y)**((beta-1)/2)*np.sin((beta-1)*theta)\n        val[..., 1] = beta*(x*x+y*y)**((beta-1)/2)*np.cos((beta-1)*theta)\n\n        return val \n\n    @cartesian\n    def source(self, p):\n        \"\"\" The right hand side of Possion equation\n        INPUT:\n            p: array object,  \n        \"\"\"\n        x = p[..., 0]\n        y = p[..., 1]\n\n\n        pi = np.pi\n        theta = np.arctan2(y, x)\n        theta = (theta >= 0)*theta + (theta < 0)*(theta+2*pi)\n        beta=2/3\n        val = np.zeros_like(p) \n\n        # the source we need\n        val[..., 0] = -beta*(x*x+y*y)**((beta-1)/2)*np.sin((beta-1)*theta)\n        val[..., 1] = -beta*(x*x+y*y)**((beta-1)/2)*np.cos((beta-1)*theta)\n\n        return val\n\n    @cartesian\n    def curl(self, p):\n        \"\"\" The curl of the exact solution \n        \"\"\"\n        x = p[..., 0]\n        y = p[..., 1]\n        pi = np.pi\n        # the curl we need \n        val = 0\n        return val \n\n    @cartesian\n    def curlcurl(self, p):\n        x = p[..., 0]\n        y = p[..., 1]\n        theta = np.arctan2(y, x)\n        beta=2/3\n\n        val = np.zeros_like(p)\n        # the curl curl we need\n        val[...,0] = 0\n        val[...,1] = 0\n        return val\n\n    @cartesian\n    def dirichlet(self, p, t):\n        \"\"\"\n        Notes\n        -----\n        p : (NQ, NE, GD)\n        t:  (NE, GD)\n        \"\"\"\n        val = np.sum(self.solution(p)*t, axis=-1)\n        return val \n\n    @cartesian\n    def is_dirichlet_boundary(self, p):\n        x = p[..., 0]\n        return np.abs(x) < 1e-12 \n\n    @cartesian\n    def neumann(self, p, n):\n        \"\"\" \n        Neuman  boundary condition\n\n        Parameters\n        ----------\n\n        p: (NQ, NE, 2)\n        n: (NE, 2)\n\n        grad*n : (NQ, NE, 2)\n        \"\"\"\n        grad = self.gradient(p) # (NQ, NE, 2)\n        val = np.sum(grad*n, axis=-1) # (NQ, NE)\n        return val\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that solves the time-harmonic equation using adaptive methods. The code should include the following functionalities:\n\n1. Define functions to recover the curl of a given solution and to calculate the least squares matrix for each node of a mesh.\n2. Parse command-line arguments to set the degree of the first kind Nedelec element, the initial mesh size, the maximum number of adaptive iterations, and the theta parameter for adaptive iteration.\n3. Initialize the problem using the CosSinData function from the fealpy library.\n4. Create a 2D box mesh using the MeshFactory class from the fealpy library and remove the fourth quadrant of the mesh.\n5. Iterate over the maximum number of adaptive iterations, during each iteration:\n   - Define the function space using the FirstKindNedelecFiniteElementSpace2d class from the fealpy library.\n   - Apply Dirichlet boundary conditions using the DirichletBC class from the fealpy library.\n   - Solve the system of equations using the scipy library's spsolve function.\n   - Calculate the L2 error between the solution and the exact solution, the curl of the solution and the exact curl, and the curl of the solution and the recovered curl.\n   - If not the last iteration, mark the cells for refinement based on the recovery error and refine the mesh.\n6. Plot the error rates using the showmultirate function from the fealpy library.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 16, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that uses the SIRF library to create multiplicative sinograms from normalisation and/or attenuation data. The code should accept command-line options for the path to data files, template sinogram, attenuation image file, ECAT8 bin normalisation file, output file, transform for attenuation image, transform type, and an option for non-interactive mode. The code should check if the provided files exist, and if not, use default files. It should also handle different types of transformations for the attenuation image. The main function of the code should create an acquisition model, check if norm and attenuation are present, and based on that, create an acquisition sensitivity model. It should then project the data if normalisation is added, and finally write the multiplicative sinogram to the specified output file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def __init__(self, src, other_src=None):\n        \"\"\"\n        Create new AcquisitionSensitivityModel object.\n\n        Sources:\n        - from a manufacturer normalisation file (supported by STIR) or\n        - from ImageData object containing attenuation image (units: 1/cm) or\n        - from AcquisitionData object containing bin efficiencies or\n        - by chaining two existing AcquisitionSensitivityModel objects\n        src: file name or ImageData object or AcquisitionData object\n        other_src: AcquisitionSensitivityModel object (optional)\n        \"\"\"\n        self.handle = None\n        self.name = 'AcquisitionSensitivityModel'\n        if src is None:\n            return\n        if isinstance(src, str):\n            # create from ECAT8/GE norm file\n            print('Reading manufacturer PET normalisation file from ' + src)\n            handle = pyiutil.charDataHandle(src)\n            self.handle = pystir.cSTIR_createPETAcquisitionSensitivityModel(\n                handle, 'n')\n            pyiutil.deleteDataHandle(handle)\n        elif isinstance(src, ImageData):\n            # create from attenuation image\n            if src.handle is None:\n                raise AssertionError()\n            if other_src is None:\n                raise AssertionError('AcquisitionSensitivityModel constructor' +\n                ' with attenuation image needs an AcquisitionModel' +\n                ' as second argument (for ray tracing)')\n            assert_validity(other_src, AcquisitionModel)\n            self.handle = pystir.cSTIR_createPETAttenuationModel(\n                src.handle, other_src.handle)\n        elif isinstance(src, AcquisitionData):\n            # create from bin efficiencies (cf. AcquisitionModel)\n            if src.handle is None:\n                raise AssertionError()\n            self.handle = pystir.cSTIR_createPETAcquisitionSensitivityModel(\n                src.handle, 's')\n        elif isinstance(src, AcquisitionSensitivityModel) and \\\n                isinstance(other_src, AcquisitionSensitivityModel):\n            # chain two acquisition sensitivity models\n            if src.handle is None:\n                raise AssertionError()\n            if other_src.handle is None:\n                raise AssertionError()\n            self.handle = pystir.cSTIR_chainPETAcquisitionSensitivityModels(\n                src.handle, other_src.handle)\n        else:\n            raise error(\n                'Wrong source in AcquisitionSensitivityModel constructor')\n        check_status(self.handle)\n\n# --- Snippet Separator ---\n\nclass AcquisitionSensitivityModel(object):\n    \"\"\"\n    Class that handles PET scanner detector efficiencies and attenuation.\n\n    Is used by AcquisitionModel (see below).\n    \"\"\"\n\n    def __init__(self, src, other_src=None):\n        \"\"\"\n        Create new AcquisitionSensitivityModel object.\n\n        Sources:\n        - from a manufacturer normalisation file (supported by STIR) or\n        - from ImageData object containing attenuation image (units: 1/cm) or\n        - from AcquisitionData object containing bin efficiencies or\n        - by chaining two existing AcquisitionSensitivityModel objects\n        src: file name or ImageData object or AcquisitionData object\n        other_src: AcquisitionSensitivityModel object (optional)\n        \"\"\"\n        self.handle = None\n        self.name = 'AcquisitionSensitivityModel'\n        if src is None:\n            return\n        if isinstance(src, str):\n            # create from ECAT8/GE norm file\n            print('Reading manufacturer PET normalisation file from ' + src)\n            handle = pyiutil.charDataHandle(src)\n            self.handle = pystir.cSTIR_createPETAcquisitionSensitivityModel(\n                handle, 'n')\n            pyiutil.deleteDataHandle(handle)\n        elif isinstance(src, ImageData):\n            # create from attenuation image\n            if src.handle is None:\n                raise AssertionError()\n            if other_src is None:\n                raise AssertionError('AcquisitionSensitivityModel constructor' +\n                ' with attenuation image needs an AcquisitionModel' +\n                ' as second argument (for ray tracing)')\n            assert_validity(other_src, AcquisitionModel)\n            self.handle = pystir.cSTIR_createPETAttenuationModel(\n                src.handle, other_src.handle)\n        elif isinstance(src, AcquisitionData):\n            # create from bin efficiencies (cf. AcquisitionModel)\n            if src.handle is None:\n                raise AssertionError()\n            self.handle = pystir.cSTIR_createPETAcquisitionSensitivityModel(\n                src.handle, 's')\n        elif isinstance(src, AcquisitionSensitivityModel) and \\\n                isinstance(other_src, AcquisitionSensitivityModel):\n            # chain two acquisition sensitivity models\n            if src.handle is None:\n                raise AssertionError()\n            if other_src.handle is None:\n                raise AssertionError()\n            self.handle = pystir.cSTIR_chainPETAcquisitionSensitivityModels(\n                src.handle, other_src.handle)\n        else:\n            raise error(\n                'Wrong source in AcquisitionSensitivityModel constructor')\n        check_status(self.handle)\n\n    def set_up(self, ad):\n        \"\"\"Sets up the object.\"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        assert_validity(ad, AcquisitionData)\n        try_calling(pystir.cSTIR_setupAcquisitionSensitivityModel(\n            self.handle, ad.handle))\n\n    def normalise(self, ad):\n        \"\"\"Multiplies ad by the inverse n of S from AcquisitionModel (F).\n\n        If self is a chain of two AcquisitionSensitivityModels, then n is\n        a product of two normalisations.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        assert_validity(ad, AcquisitionData)\n        try_calling(pystir.cSTIR_applyAcquisitionSensitivityModel(\n            self.handle, ad.handle, 'normalise'))\n\n    def unnormalise(self, ad):\n        \"\"\"Multiply the argument by S from AcquisitionModel (F).\n\n        If self is a chain of two AcquisitionSensitivityModels, then S is\n        a product of two un-normalisations.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        assert_validity(ad, AcquisitionData)\n        try_calling(pystir.cSTIR_applyAcquisitionSensitivityModel(\n            self.handle, ad.handle, 'unnormalise'))\n\n    def forward(self, ad):\n        \"\"\"Alias of unnormalise except the argument remains unchanged\n\n        and a new AcquisitionData equal to the argument multiplied\n        by S is returned.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        assert_validity(ad, AcquisitionData)\n        fd = AcquisitionData()\n        fd.handle = pystir.cSTIR_applyAcquisitionSensitivityModel(\n            self.handle, ad.handle, 'fwd')\n        check_status(fd.handle)\n        return fd\n\n    def invert(self, ad):\n        \"\"\"Alias of normalise except that the argument remains unchanged\n\n        and a new AcquisitionData equal to the argument multiplied\n        by the inverse of S is returned.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        assert_validity(ad, AcquisitionData)\n        fd = AcquisitionData()\n        fd.handle = pystir.cSTIR_applyAcquisitionSensitivityModel(\n            self.handle, ad.handle, 'inv')\n        check_status(fd.handle)\n        return fd\n\n    def __del__(self):\n        \"\"\"del.\"\"\"\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n# --- Snippet Separator ---\n\ndef write_driver(self, nodes, read_write_info, prefix, postfix,\n                     region_name, writer=FortranWriter()):\n        # pylint: disable=too-many-arguments\n        '''This function uses the ``get_driver_as_string()`` function to get a\n        a stand-alone driver, and then writes this source code to a file. The\n        file name is derived from the region name:\n        \"driver-\"+module_name+\"_\"+region_name+\".F90\"\n\n        :param nodes: a list of nodes containing the body of the driver\n            routine.\n        :type nodes: List[:py:class:`psyclone.psyir.nodes.Node`]\n        :param read_write_info: information about all input and output \\\n            parameters.\n        :type read_write_info: :py:class:`psyclone.psyir.tools.ReadWriteInfo`\n        :param str prefix: the prefix to use for each PSyData symbol, \\\n            e.g. 'extract' as prefix will create symbols `extract_psydata`.\n        :param str postfix: a postfix that is appended to an output variable \\\n            to create the corresponding variable that stores the output \\\n            value from the kernel data file. The caller must guarantee that \\\n            no name clashes are created when adding the postfix to a variable \\\n            and that the postfix is consistent between extract code and \\\n            driver code (see 'ExtractTrans.determine_postfix()').\n        :param Tuple[str,str] region_name: an optional name to \\\n            use for this PSyData area, provided as a 2-tuple containing a \\\n            location name followed by a local name. The pair of strings \\\n            should uniquely identify a region.\n        :param writer: a backend visitor to convert PSyIR \\\n            representation to the selected language. It defaults to \\\n            the FortranWriter.\n        :type writer: \\\n            :py:class:`psyclone.psyir.backend.language_writer.LanguageWriter`\n\n        '''\n        code = self.get_driver_as_string(nodes, read_write_info, prefix,\n                                         postfix, region_name, writer=writer)\n        fll = FortLineLength()\n        code = fll.process(code)\n        if not code:\n            # This indicates an error that was already printed,\n            # so ignore it here.\n            return\n        module_name, local_name = region_name\n        with open(f\"driver-{module_name}-{local_name}.F90\", \"w\",\n                  encoding='utf-8') as out:\n            out.write(code)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the SIRF library to create multiplicative sinograms from normalisation and/or attenuation data. The code should accept command-line options for the path to data files, template sinogram, attenuation image file, ECAT8 bin normalisation file, output file, transform for attenuation image, transform type, and an option for non-interactive mode. The code should check if the provided files exist, and if not, use default files. It should also handle different types of transformations for the attenuation image. The main function of the code should create an acquisition model, check if norm and attenuation are present, and based on that, create an acquisition sensitivity model. It should then project the data if normalisation is added, and finally write the multiplicative sinogram to the specified output file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 17, "repo_full_name": "fusion-power-plant-framework__bluemira", "instruction": "Generate code that uses the bluemira library to solve a 2D magnetostatic problem for a single coil. The code should first import necessary modules and define parameters for the coil and enclosure. Then, it should create the coil and enclosure using the bluemira library's geometry tools and set the mesh options for each. The code should also create components for the universe, enclosure, and coil. \n\nNext, the code should create a mesh and convert it for use in the FEniCS library. After setting up the mesh, the code should instantiate a magnetostatic solver and define the source term for the problem. The source term should be plotted for visualization. \n\nThe code should then solve the magnetostatic problem and calculate the magnetic field. Finally, the code should compare the calculated magnetic field with the theoretical value along the z-axis and along a radial path at a certain z-offset. The differences between the calculated and theoretical values should be plotted for each comparison.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def Bz_coil_axis(\n    r: float,\n    z: Optional[float] = 0,\n    pz: Optional[float] = 0,\n    current: Optional[float] = 1,\n) -> float:\n    \"\"\"\n    Calculate the theoretical vertical magnetic field of a filament coil\n    (of radius r and centred in (0, z)) on a point on the coil axis at\n    a distance pz from the axis origin.\n\n    Parameters\n    ----------\n    r:\n        Coil radius [m]\n    z:\n        Vertical position of the coil centroid [m]\n    pz:\n        Vertical position of the point on the axis on which the magnetic field\n        shall be calculated [m]\n    current:\n        Current of the coil [A]\n\n    Returns\n    -------\n    Vertical magnetic field on the axis [T]\n\n    Notes\n    -----\n    \\t:math:`\\\\dfrac{1}{2}\\\\dfrac{\\\\mu_{0}Ir^2}{(r^{2}+(pz-z)^{2})^{3/2}}`\n    \"\"\"\n    return 0.5 * MU_0 * current * r**2 / (r**2 + (pz - z) ** 2) ** 1.5\n\n# --- Snippet Separator ---\n\ndef add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))\n\n# --- Snippet Separator ---\n\ndef plot_parameter(self, location, parameter, formatter='.0f', **kwargs):\n        \"\"\"At the specified location in the station model plot a set of values.\n\n        This specifies that at the offset `location`, the data in `parameter` should be\n        plotted. The conversion of the data values to a string is controlled by ``formatter``.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        If something has already been plotted at this location, it will be replaced.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this parameter. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions; increments\n            are multiplied by `spacing` to give offsets in x and y relative to the center.\n        parameter : array-like\n            The numeric values that should be plotted\n        formatter : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        plot_units: `pint.unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n\n        See Also\n        --------\n        plot_barb, plot_symbol, plot_text\n\n        \"\"\"\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        parameter = self._scalar_plotting_units(parameter, plotting_units)\n        if hasattr(parameter, 'units'):\n            parameter = parameter.magnitude\n        text = self._to_string_list(parameter, formatter)\n        return self.plot_text(location, text, **kwargs)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the bluemira library to solve a 2D magnetostatic problem for a single coil. The code should first import necessary modules and define parameters for the coil and enclosure. Then, it should create the coil and enclosure using the bluemira library's geometry tools and set the mesh options for each. The code should also create components for the universe, enclosure, and coil. \n\nNext, the code should create a mesh and convert it for use in the FEniCS library. After setting up the mesh, the code should instantiate a magnetostatic solver and define the source term for the problem. The source term should be plotted for visualization. \n\nThe code should then solve the magnetostatic problem and calculate the magnetic field. Finally, the code should compare the calculated magnetic field with the theoretical value along the z-axis and along a radial path at a certain z-offset. The differences between the calculated and theoretical values should be plotted for each comparison.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 18, "repo_full_name": "silx-kit__silx", "instruction": "Generate code that creates a simple GUI application using the silx library in Python. The application should have a main window that displays a variety of widgets provided by the silx library. These widgets should include a WaitingPushButton, ThreadPoolPushButton, RangeSlider, LegendIconWidget, and ElidedLabel. Each widget should be labeled and have specific functionalities. For instance, the WaitingPushButton should swap its waiting state when clicked, the ThreadPoolPushButton should compute a power operation, and the RangeSlider should print events when its value or position changes. The LegendIconWidget should display different styles of lines, symbols, and colormaps, and the ElidedLabel should display long texts with different elide modes. The application should handle exceptions using the silx library's exception handler and should clean up after execution.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class CanvasBackend(BaseCanvasBackend):\n    \"\"\"Template backend\n\n    Events to emit are shown below. Most backends will probably\n    have one method for each event:\n\n        self._vispy_canvas.events.initialize()\n        self._vispy_canvas.events.resize(size=(w, h))\n        self._vispy_canvas.events.draw(region=None)\n        self._vispy_canvas.close()\n        self._vispy_canvas.events.mouse_press(pos=(x, y), button=1,\n                                              modifiers=())\n        self._vispy_canvas.events.mouse_release(pos=(x, y), button=1,\n                                                modifiers=())\n        self._vispy_canvas.events.mouse_double_click(pos=(x, y), button=1,\n                                                     modifiers=())\n        self._vispy_canvas.events.mouse_move(pos=(x, y), modifiers=())\n        self._vispy_canvas.events.mouse_wheel(pos=(x, y), delta=(0, 0),\n                                              modifiers=())\n        self._vispy_canvas.events.key_press(key=key, text=text, modifiers=())\n        self._vispy_canvas.events.key_release(key=key, text=text, modifiers=())\n\n    In most cases, if the window-cross is clicked, a native close-event is\n    generated, which should then call canvas.close(). The Canvas class is\n    responsible for firing the close event and calling\n    backend_canvas._vispy_close, which closes the native widget.\n    If this happens to result in a second close event, canvas.close() gets\n    called again, but Canvas knows it is closing so it stops there.\n\n    If canvas.close() is called (by the user), it calls\n    backend_canvas._vispy_close, which closes the native widget,\n    and we get the same stream of actions as above. This deviation from\n    having events come from the CanvasBackend is necessitated by how\n    different backends handle close events, and the various ways such\n    events can be triggered.\n    \"\"\"\n\n    def __init__(self, vispy_canvas, **kwargs):\n        BaseCanvasBackend.__init__(self, vispy_canvas)\n        # We use _process_backend_kwargs() to \"serialize\" the kwargs\n        # and to check whether they match this backend's capability\n        p = self._process_backend_kwargs(kwargs)\n\n        # Deal with config\n        # ... use context.config\n        # Deal with context\n        p.context.shared.add_ref('backend-name', self)\n        if p.context.shared.ref is self:\n            self._native_context = None  # ...\n        else:\n            self._native_context = p.context.shared.ref._native_context\n\n        # NativeWidgetClass.__init__(self, foo, bar)\n\n    def _vispy_set_current(self):\n        # Make this the current context\n        raise NotImplementedError()\n\n    def _vispy_swap_buffers(self):\n        # Swap front and back buffer\n        raise NotImplementedError()\n\n    def _vispy_set_title(self, title):\n        # Set the window title. Has no effect for widgets\n        raise NotImplementedError()\n\n    def _vispy_set_size(self, w, h):\n        # Set size of the widget or window\n        raise NotImplementedError()\n\n    def _vispy_set_position(self, x, y):\n        # Set location of the widget or window. May have no effect for widgets\n        raise NotImplementedError()\n\n    def _vispy_set_visible(self, visible):\n        # Show or hide the window or widget\n        raise NotImplementedError()\n\n    def _vispy_set_fullscreen(self, fullscreen):\n        # Set the current fullscreen state\n        raise NotImplementedError()\n\n    def _vispy_update(self):\n        # Invoke a redraw\n        raise NotImplementedError()\n\n    def _vispy_close(self):\n        # Force the window or widget to shut down\n        raise NotImplementedError()\n\n    def _vispy_get_size(self):\n        # Should return widget size\n        raise NotImplementedError()\n\n    def _vispy_get_position(self):\n        # Should return widget position\n        raise NotImplementedError()\n\n    def _vispy_get_fullscreen(self):\n        # Should return the current fullscreen state\n        raise NotImplementedError()\n\n    def _vispy_get_native_canvas(self):\n        # Should return the native widget object.\n        # If this is self, this method can be omitted.\n        return self\n\n# --- Snippet Separator ---\n\ndef shutdown_server(self) -> str:\n        \"\"\"Shutdown the server.\n\n        The engine should not exit right away.\n        It should set its status to STOPPING, and set up a timer (in a different thread),\n        and return from this call right away (if other restart conditions are met).\n        When the timer fires, it exits.\n        This would give the caller to process the feedback or clean up (e.g. admin cmd response).\n\n        Returns:\n            An error message. An empty string if successful.\n        \"\"\"\n        pass\n\n# --- Snippet Separator ---\n\ndef __init__(self, w: Widget):\n        \"\"\"\n        w -- widget to wrap, stored as self._w\n\n        This object will pass the functions defined in Widget interface\n        definition to self._w.\n\n        The purpose of this widget is to provide a base class for\n        widgets that compose other widgets for their display and\n        behaviour.  The details of that composition should not affect\n        users of the subclass.  The subclass may decide to expose some\n        of the wrapped widgets by behaving like a ContainerWidget or\n        WidgetDecoration, or it may hide them from outside access.\n        \"\"\"\n        self._wrapped_widget = w\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a simple GUI application using the silx library in Python. The application should have a main window that displays a variety of widgets provided by the silx library. These widgets should include a WaitingPushButton, ThreadPoolPushButton, RangeSlider, LegendIconWidget, and ElidedLabel. Each widget should be labeled and have specific functionalities. For instance, the WaitingPushButton should swap its waiting state when clicked, the ThreadPoolPushButton should compute a power operation, and the RangeSlider should print events when its value or position changes. The LegendIconWidget should display different styles of lines, symbols, and colormaps, and the ElidedLabel should display long texts with different elide modes. The application should handle exceptions using the silx library's exception handler and should clean up after execution.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 19, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that sets up a simple runaway scenario simulation using the DREAM library. The simulation should use constant temperature, density, and electric field to generate a runaway current. The physical parameters should be set to an electric field strength of 6 V/m, an electron density of 5e19 m^-3, and a temperature of 100 eV. The grid parameters should be set to a maximum momentum of 1 m_e*c, 300 momentum grid points, 20 pitch grid points, a simulation time of 1e-3 seconds, and 20 time steps. The code should also set up the radial grid, solver type, and time stepper. The output should be saved to an HDF5 file named 'output.h5'.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def setPrescribedData(self, efield, radius=0, times=0):\n        \"\"\"\n        When ``TYPE_PRESCRIBED``, sets the spatiotemporal evolution of the\n        electric field during the simulation. The parameter ``efield`` may be\n        either a scalar (in which case the electric field is taken to be\n        constant and uniform in time and radius) or a 2D array of shape\n        (nt, nr). The associated time grid ``times`` must be of size ``nt`` and\n        the radial grid must be of size ``nr``.\n\n        :param efield: Prescribed electric field.\n        :param radius: Radial grid on which the electric field is prescribed.\n        :param times:  Time grid on which the electric field is prescribed.\n        \"\"\"\n        _data, _rad, _tim = self._setPrescribedData(efield, radius, times)\n        self.efield = _data\n        self.radius = _rad\n        self.times  = _tim\n\n        self._verifySettingsPrescribedData()\n\n# --- Snippet Separator ---\n\nclass ElectricField(FluidQuantity):\n\n\n    def __init__(self, name, data, grid, output, attr=list()):\n        \"\"\"\n        Constructor.\n        \"\"\"\n        super().__init__(name=name, data=data, attr=attr, grid=grid, output=output)\n\n\n    def getNormEfield(self, field, r=None, t=None):\n        \"\"\"\n        Returns an electric field from the other quantities by name.\n        This routine is intended as a uniform interface for fetching\n        quantities such as Ec, Eceff, ED etc.\n        \"\"\"\n        # List of supported quantities (to avoid user error)\n        nrm = ['Eceff', 'Ecfree', 'Ectot', 'Ec', 'ED', 'EDreic']\n        if field == 'Ec': field = 'Ectot'\n        elif field == 'ED': field = 'EDreic'\n\n        if 'fluid' not in self.output.other:\n            raise OutputException('No \"other\" fluid quantities saved in output. Normalizing electric fields are thus not available.')\n        if field not in nrm:\n            raise OutputException(\"Cannot normalize to '{}': This seems to not make sense.\".format(field))\n        if field not in self.output.other.fluid:\n            raise OutputException(\"Cannot normalize to '{}': quantity not saved to output after simulation.\".format(field))\n\n        return self.output.other.fluid[field].get(r=r, t=t)\n\n\n    def maxEnergy(self, t=-1):\n        r\"\"\"\n        Evaluates the maximum attainable runaway kinetic energy (in normalized\n        units) at time ``t``. This energy is obtained by integrating the\n        equation of motion:\n\n        .. math::\n\n            \\frac{\\mathrm{d}p}{\\mathrm{d}t} = eE \\quad\\implies\\quad\n            p = \\int_0^t eE(t)\\,\\mathrm{d}t',\\\\\n            W = mc^2(\\sqrt{p^2+1}-1),\n\n        where :math:`e` is the elementary charge and :math:`p` is the electron\n        momentum.\n\n        :param int t: Index of time to calculate transferred momentum until.\n        \"\"\"\n        p = self.maxMomentum(t=t)\n        return np.sqrt(p**2 + 1)-1\n\n\n    def maxMomentum(self, t=-1):\n        r\"\"\"\n        Evaluates the maximum attainable runaway momentum (in normalized units)\n        at time ``t``. This momentum is obtained by integrating the equation of\n        motion:\n\n        .. math::\n\n            \\frac{\\mathrm{d}p}{\\mathrm{d}t} = eE \\quad\\implies\\quad\n            p = \\int_0^t eE(t)\\,\\mathrm{d}t',\n\n        where :math:`e` is the elementary charge and :math:`p` is the electron\n        momentum.\n\n        :param int t: Index of time to calculate transferred momentum until.\n        \"\"\"\n        if np.isscalar(t):\n            p = np.trapz(self[:t], self.grid.t[:t], axis=0)\n        else:\n            p = []\n            t = np.asarray(t)\n\n            if t.ndim != 1:\n                raise OutputException(\"Unrecognized dimensions of time index: {}.\".format(t.ndim))\n\n            for time in t:\n                p.append(np.trapz(self[:time], self.grid.t[:time], axis=0))\n\n            p = np.array(p)\n\n        p *= scipy.constants.e / (scipy.constants.m_e * scipy.constants.c)\n        return p\n\n\n    def norm(self, to='Ec'):\n        \"\"\"\n        Return the value of this quantity normalized to the\n        quantity specified by 'to'.\n\n        to: Name of quantity to normalize electric field to.\n            Possible values:\n               Eceff   Effective critical electric field (as defined by Hesslow et al)\n               Ecfree  Connor-Hastie threshold field (calculated with n=n_free)\n               Ectot   Connor-Hastie threshold field (calculated with n=n_tot)\n               Ec      (alias for 'Ectot')\n               ED      Dreicer field\n            Note that the quantity with which to normalize to\n            must be saved as an 'OtherQuantity'.\n        \"\"\"\n        return self.normalize(to=to)\n\n\n    def normalize(self, to='Ec'):\n        \"\"\"\n        Return the value of this quantity normalized to the\n        quantity specified by 'to'.\n\n        to: Name of quantity to normalize electric field to.\n            Possible values:\n               Eceff   Effective critical electric field (as defined by Hesslow et al)\n               Ecfree  Connor-Hastie threshold field (calculated with n=n_free)\n               Ectot   Connor-Hastie threshold field (calculated with n=n_tot)\n               Ec      (alias for 'Ectot')\n               EDreic  Dreicer field\n               ED      (alias for 'EDreic')\n            Note that the quantity with which to normalize to\n            must be saved as an 'OtherQuantity'.\n        \"\"\"\n        # OtherQuantity's are not defined at t=0, so we extend them\n        # arbitrarily here (in order for the resulting FluidQuantity to\n        # be plottable on the regular time grid)\n        Enorm = np.zeros(self.data.shape)\n        Enorm[1:,:] = self.getNormEfield(field=to)\n        Enorm[0,:]  = Enorm[1,:]\n\n        data = self.data[:] / Enorm\n        return FluidQuantity(name='E / {}'.format(to), data=data, grid=self.grid, output=self.output)\n\n\n    def plot(self, norm=None, **kwargs):\n        \"\"\"\n        Wrapper for FluidQuantity.plot(), adding the 'norm' argument\n        (for directly plotting a normalized electric field)\n        \"\"\"\n        if norm is None:\n            return super().plot(**kwargs)\n        else:\n            _E = self.normalize(to=norm)\n            return _E.plot(**kwargs)\n\n\n    def plotRadialProfile(self, norm=None, **kwargs):\n        \"\"\"\n        Wrapper for FluidQuantity.plotRadialProfile(), adding the 'norm'\n        argument (for directly plotting a normalized electric field)\n        \"\"\"\n        if norm is None:\n            return super().plotRadialProfile(**kwargs)\n        else:\n            _E = self.normalize(to=norm)\n            return _E.plotRadialProfile(**kwargs)\n\n\n    def plotTimeProfile(self, norm=None, **kwargs):\n        \"\"\"\n        Wrapper for FluidQuantity.plotTimeProfile(), adding the 'norm'\n        argument (for directly plotting a normalized electric field)\n        \"\"\"\n        if norm is None:\n            return super().plotTimeProfile(**kwargs)\n        else:\n            _E = self.normalize(to=norm)\n            return _E.plotTimeProfile(**kwargs)\n\n\n    def plotIntegral(self, norm=None, **kwargs):\n        \"\"\"\n        Wrapper for FluidQuantity.plotTimeProfile(), adding the 'norm'\n        argument (for directly plotting a normalized electric field)\n        \"\"\"\n        if norm is None:\n            return super().plotIntegral(**kwargs)\n        else:\n            _E = self.normalize(to=norm)\n            return _E.plotIntegral(**kwargs)\n\n# --- Snippet Separator ---\n\ndef __init__(self, name, enabled=True, ttype=TYPE_PXI, np=0, nxi=0, pmax=None):\n        \"\"\"\n        Constructor.\n\n        name:    Name, indicating what type of grid this is (hot-tail or runaway).\n        enabled: If 'True', enables the hot-tail grid in the simulation.\n        ttype:   Type of momentum grid (p/xi or ppar/pperp).\n        np:      Number of momentum grid points.\n        nxi:     Number of pitch grid points.\n        pmax:    Maximum momentum on grid.\n\n        \"\"\"\n        self.name = name\n\n        self.set(enabled=enabled, ttype=ttype, np=np, nxi=nxi, pmax=pmax)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a simple runaway scenario simulation using the DREAM library. The simulation should use constant temperature, density, and electric field to generate a runaway current. The physical parameters should be set to an electric field strength of 6 V/m, an electron density of 5e19 m^-3, and a temperature of 100 eV. The grid parameters should be set to a maximum momentum of 1 m_e*c, 300 momentum grid points, 20 pitch grid points, a simulation time of 1e-3 seconds, and 20 time steps. The code should also set up the radial grid, solver type, and time stepper. The output should be saved to an HDF5 file named 'output.h5'.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 20, "repo_full_name": "avslab__basilisk", "instruction": "Generate code that sets up a Monte Carlo simulation using the Basilisk library, specifically using the Python Spice setup. The simulation should create a simple spacecraft with specific initial conditions. The code should also include the loading of Spice kernels within Python to pull the Hubble states from Spice. This Python Spice call should be performed within each Monte Carlo thread. The Hubble states should then be printed to the terminal. The Monte Carlo scenario should be set up to run 12 times. The code should also include a Controller class with Spice kernel loading code that is commented out. The simulation should be set up within a class called \"MySimulation\". The code should also include a function to access the Spice Kernel and print out the state. Finally, the code should include a main function that sets up and executes the Monte Carlo simulation, and cleans up the data after the test.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def Pdshis(self, rlab=\"\", name=\"\", type=\"\", conf=\"\", **kwargs):\n        \"\"\"\n        APDL Command: PDSHIS\n\n        Plots the sample history values.\n\n        Parameters\n        ----------\n        rlab\n            Result set label. Identifies the result set to be used for\n            postprocessing. A result set label can be the solution set label\n            you defined in a PDEXE command, or the response surface set label\n            defined in an RSFIT command.\n\n        name\n            Parameter name. The parameter must have been previously defined as\n            a random input variable or a random output parameter using the\n            PDVAR command.\n\n        type\n            Keyword to identify which type of data is to be plotted:\n\n            SAMP - Sampled values (default).\n\n            MEAN - Mean values for results based on Monte Carlo simulation methods only.\n\n            STDV - Standard deviations for results based on Monte Carlo simulation methods only.\n\n            MIN - Minimum values for results based on Monte Carlo simulation methods only.\n\n            MAX - Maximum values for results based on Monte Carlo simulation methods only.\n\n        conf\n            Confidence level. The confidence level is used to plot confidence\n            bounds for the history value. The value for the confidence level\n            must be between 0.0 and 1.0 and it defaults to 0.95 (95%).\n            Confidence bound(s) plotting is suppressed for CONF  0.5. This\n            option is ignored for Type = SAMP (no confidence bounds are\n            plotted).\n\n        Notes\n        -----\n        Plots the sample history values as a function of the number of\n        simulation loops.\n\n        If Rlab is left blank, then the result set label is inherited from the\n        last PDEXE command (Slab), RSFIT command (RSlab), or the most recently\n        used PDS postprocessing command where a result set label was explicitly\n        specified.\n\n        The confidence level is a probability expressing the confidence that\n        the value for the requested probability is in fact between the\n        confidence bounds. The larger the confidence level, the wider the\n        confidence bounds. For Type = MEAN and Type = STDV, lower and upper\n        confidence curves are plotted. For Type = MEAN, the mean value curve\n        starts at the first simulation and the confidence bounds start with\n        simulation number 2. For Type = MIN only the upper confidence bound is\n        shown (the interpretation is that by a certain probability the true\n        minimum value is below this curve). This probability (or confidence) is\n        set using CONF. Likewise, for Type = MAX, only the lower confidence\n        bound is shown. For all Type options, confidence curves are plotted\n        starting with the simulation at which enough data is available to\n        calculate the bounds. However, for scaling reasons, no confidence\n        bounds are plotted for simulation numbers 1 to 5 even if they might be\n        available.\n\n        \"\"\"\n        command = \"PDSHIS,%s,%s,%s,%s\" % (str(rlab), str(name), str(type), str(conf))\n        return self.Run(command, **kwargs)\n\n# --- Snippet Separator ---\n\ndef test_gravityGradientModule(show_plots, cmOffset, planetCase):\n    r\"\"\"\n    **Validation Test Description**\n\n    This test creates a spacecraft in orbit about either Earth or Venus to check if the correct gravity gradient\n    torque is evaluated.  Multiple test scenario combinations are possible where either a single or multiple\n    gravity bodies are included, using either zero planet ephemeris for the single planet case, or using SPICE\n    for the multi-planet scenario.\n\n    **Test Parameters**\n\n    The following list discusses in detail the various test parameters used. These are test tested in\n    all possible permutations (except show_plots of course) which is turned off for ``pytest`` usage.\n\n    :param show_plots:  flag to show some simulation plots\n    :param cmOffset:    center of mass offset vector in meters\n    :param planetCase: integer flag with values (0,1,2,3).  The cases consider the following simulation scenarios:\n\n                        - Case 0 indicates a simulation with only Earth present at (0,0,0).\n                        - Case 1 is a simulation with both Earth and Venus present using Spice, but the gravity\n                          gradient torque is only evaluated using Earth.\n                        - Case 2 is same as 1 but Venus is also included in the torque evaluation.\n                        - Case 3 is like 2 but here the spacecraft is orbiting venus.\n    :return: None\n\n    **Description of Variables Being Tested**\n\n    The gravity effector torque output message is compared against a python evaluated vector.\n\n    \"\"\"\n    # each test method requires a single assert method to be called\n    [testResults, testMessage] = run(\n            show_plots, cmOffset, planetCase, 2.0)\n    assert testResults < 1, testMessage\n\n# --- Snippet Separator ---\n\ndef mcts(\n    observation: types.Observation,\n    model: models.Model,\n    search_policy: SearchPolicy,\n    evaluation: types.EvaluationFn,\n    num_simulations: int,\n    num_actions: int,\n    discount: float = 1.,\n    dirichlet_alpha: float = 1,\n    exploration_fraction: float = 0.,\n) -> Node:\n  \"\"\"Does Monte Carlo tree search (MCTS), AlphaZero style.\"\"\"\n\n  # Evaluate the prior policy for this state.\n  prior, value = evaluation(observation)\n  assert prior.shape == (num_actions,)\n\n  # Add exploration noise to the prior.\n  noise = np.random.dirichlet(alpha=[dirichlet_alpha] * num_actions)\n  prior = prior * (1 - exploration_fraction) + noise * exploration_fraction\n\n  # Create a fresh tree search.\n  root = Node()\n  root.expand(prior)\n\n  # Save the model state so that we can reset it for each simulation.\n  model.save_checkpoint()\n  for _ in range(num_simulations):\n    # Start a new simulation from the top.\n    trajectory = [root]\n    node = root\n\n    # Generate a trajectory.\n    timestep = None\n    while node.children:\n      # Select an action according to the search policy.\n      action = search_policy(node)\n\n      # Point the node at the corresponding child.\n      node = node.children[action]\n\n      # Step the simulator and add this timestep to the node.\n      timestep = model.step(action)\n      node.reward = timestep.reward or 0.\n      node.terminal = timestep.last()\n      trajectory.append(node)\n\n    if timestep is None:\n      raise ValueError('Generated an empty rollout; this should not happen.')\n\n    # Calculate the bootstrap for leaf nodes.\n    if node.terminal:\n      # If terminal, there is no bootstrap value.\n      value = 0.\n    else:\n      # Otherwise, bootstrap from this node with our value function.\n      prior, value = evaluation(timestep.observation)\n\n      # We also want to expand this node for next time.\n      node.expand(prior)\n\n    # Load the saved model state.\n    model.load_checkpoint()\n\n    # Monte Carlo back-up with bootstrap from value function.\n    ret = value\n    while trajectory:\n      # Pop off the latest node in the trajectory.\n      node = trajectory.pop()\n\n      # Accumulate the discounted return\n      ret *= discount\n      ret += node.reward\n\n      # Update the node.\n      node.total_value += ret\n      node.visit_count += 1\n\n  return root\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a Monte Carlo simulation using the Basilisk library, specifically using the Python Spice setup. The simulation should create a simple spacecraft with specific initial conditions. The code should also include the loading of Spice kernels within Python to pull the Hubble states from Spice. This Python Spice call should be performed within each Monte Carlo thread. The Hubble states should then be printed to the terminal. The Monte Carlo scenario should be set up to run 12 times. The code should also include a Controller class with Spice kernel loading code that is commented out. The simulation should be set up within a class called \"MySimulation\". The code should also include a function to access the Spice Kernel and print out the state. Finally, the code should include a main function that sets up and executes the Monte Carlo simulation, and cleans up the data after the test.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 21, "repo_full_name": "rlberry-py__rlberry", "instruction": "Generate code that compares several bandit agents using the rlberry library. The code should define the parameters of the problem, construct the experiment, define several classes of agents (UCB, UCBV, ETC, MOSS, IMED, NPTS, EXP3), and train these agents. After training, the code should compute and plot the cumulative pseudo-regret and cumulative regret for each agent. Finally, the code should compute and plot the number of times each arm was selected. The agents should be wrapped with a WriterWrapper to track the action and reward. The experiment should be managed using the ExperimentManager class. The plots should be created using matplotlib and seaborn, with varying line styles for each agent.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def plot_episode_rewards(agent_stats,\n                         cumulative=False,\n                         fignum=None,\n                         show=True,\n                         max_value=None,\n                         plot_regret=False):\n    \"\"\"\n    Given a list of AgentStats, plot the rewards obtained in each episode.\n    The dictionary returned by agents' .fit() method must contain a key 'episode_rewards'.\n\n    Parameters\n    ----------\n    agent_stats : AgentStats, or list of AgentStats\n    cumulative : bool, default: False\n        If true, plot cumulative rewards.\n    fignum: string or int\n        Identifier of plot figure.\n    show: bool\n        If true, calls plt.show().\n    max_value: double, default: None\n        Maximum reward achievable in one episode.\n    plot_regret: bool, default: False\n        If true, plots the regret. Requires max_val to be given.\n    \"\"\"\n    agent_stats_list = agent_stats\n    if not isinstance(agent_stats_list, list):\n        agent_stats_list = [agent_stats_list]\n\n    if plot_regret and max_value is None:\n        raise ValueError(\"max_value must be provided for regret plot\")\n\n    # line style\n    lines = [\"-\", \"--\", \"-.\", \":\"]\n    linecycler = cycle(lines)\n\n    plt.figure(fignum)\n    for stats in agent_stats_list:\n        # train agents if they are not already trained\n        if stats.fitted_agents is None:\n            stats.fit()\n\n        if 'episode_rewards' not in stats.fit_info:\n            logger.warning(\"episode_rewards not available for %s.\" % stats.agent_name)\n            continue\n\n        # get reward statistics and plot them\n        rewards = np.array(stats.fit_statistics['episode_rewards'])\n        if cumulative and (not plot_regret):\n            data = np.cumsum(rewards, axis=1)\n            label = \"total reward\"\n        elif plot_regret:\n            data = np.cumsum(max_value-rewards, axis=1)\n            label = \"regret\"\n        else:\n            data = rewards\n            label = \"reward in one episode\"\n\n        mean_r = data.mean(axis=0)\n        std_r = data.std(axis=0)\n        episodes = np.arange(1, data.shape[1]+1)\n\n        plt.plot(episodes, mean_r, next(linecycler), label=stats.agent_name)\n        plt.fill_between(episodes, mean_r-std_r, mean_r+std_r, alpha=0.4)\n        plt.legend()\n        plt.xlabel(\"episodes\")\n        plt.ylabel(label)\n        plt.grid(True, alpha=0.75)\n\n    if show:\n        plt.show()\n\n# --- Snippet Separator ---\n\ndef mc_policy_evaluation(agent,\n                         eval_env,\n                         eval_horizon=10**5,\n                         n_sim=10,\n                         gamma=1.0,\n                         policy_kwargs=None,\n                         stationary_policy=True):\n    \"\"\"\n    Monte-Carlo Policy evaluation [1]_ of an agent to estimate the value at the initial state.\n\n    If a list of agents is provided as input, for each evaluation, one of the agents is sampled\n    uniformly at random.\n\n    Parameters\n    ----------\n    agent : Agent or list of agents.\n        Trained agent(s).\n    eval_env : Env\n        Evaluation environment.\n    eval_horizon : int, default: 10**5\n        Horizon, maximum episode length.\n    n_sim : int, default: 10\n        Number of Monte Carlo simulations.\n    gamma : double, default: 1.0\n        Discount factor.\n    policy_kwargs : dict or None\n        Optional kwargs for agent.policy() method.\n    stationary_policy : bool, default: True\n        If False, the time step h (0<= h <= eval_horizon) is sent as argument\n        to agent.policy() for policy evaluation.\n\n    Return\n    ------\n    Numpy array of shape (n_sim, ) containing the sum of rewards in each simulation.\n\n    References\n    ----------\n    .. [1] http://incompleteideas.net/book/first/ebook/node50.html\n    \"\"\"\n    rng = seeding.get_rng()\n    if not isinstance(agent, list):\n        agents = [agent]\n    else:\n        agents = agent\n\n    policy_kwargs = policy_kwargs or {}\n\n    episode_rewards = np.zeros(n_sim)\n    for sim in range(n_sim):\n        idx = rng.integers(len(agents))\n\n        observation = eval_env.reset()\n        for hh in range(eval_horizon):\n            if stationary_policy:\n                action = agents[idx].policy(observation, **policy_kwargs)\n            else:\n                action = agents[idx].policy(observation, hh, **policy_kwargs)\n            observation, reward, done, _ = eval_env.step(action)\n            episode_rewards[sim] += reward * np.power(gamma, hh)\n            if done:\n                break\n\n    return episode_rewards\n\n# --- Snippet Separator ---\n\ndef compare_policies(agent_stats_list,\n                     eval_env=None,\n                     eval_horizon=None,\n                     stationary_policy=True,\n                     n_sim=10,\n                     fignum=None,\n                     show=True,\n                     plot=True,\n                     **kwargs):\n    \"\"\"\n    Compare the policies of each of the agents in agent_stats_list.\n    Each element of the agent_stats_list contains a list of fitted agents.\n    To evaluate the policy, we repeat n_sim times:\n        * choose one of the fitted agents uniformly at random\n        * run its policy in eval_env for eval_horizon time steps\n\n\n    Parameters\n    ----------\n    agent_stats_list : list of AgentStats objects.\n    eval_env : Model\n        Environment where to evaluate the policies.\n        If None, it is taken from AgentStats.\n    eval_horizon : int\n        Number of time steps for policy evaluation.\n        If None, it is taken from AgentStats.\n    stationary_policy : bool\n        If False, the time step h (0<= h <= eval_horizon) is sent as argument\n        to agent.policy() for policy evaluation.\n    n_sim : int\n        Number of simulations to evaluate each policy.\n    fignum: string or int\n        Identifier of plot figure.\n    show: bool\n        If true, calls plt.show().\n    plot: bool\n        If false, do not plot.\n    kwargs:\n        Extra parameters for sns.boxplot\n    \"\"\"\n    #\n    # evaluation\n    #\n    use_eval_from_agent_stats = (eval_env is None)\n    use_horizon_from_agent_stats = (eval_horizon is None)\n\n    rng = seeding.get_rng()\n    agents_rewards = []\n    for agent_stats in agent_stats_list:\n        # train agents if they are not already trained\n        if agent_stats.fitted_agents is None:\n            agent_stats.fit()\n\n        # eval env and horizon\n        if use_eval_from_agent_stats:\n            eval_env = agent_stats.eval_env\n            assert eval_env is not None, \\\n                \"eval_env not in AgentStats %s\" % agent_stats.agent_name\n        if use_horizon_from_agent_stats:\n            eval_horizon = agent_stats.eval_horizon\n            assert eval_horizon is not None, \\\n                \"eval_horizon not in AgentStats %s\" % agent_stats.agent_name\n\n        # evaluate agent\n        episode_rewards = np.zeros(n_sim)\n        for sim in range(n_sim):\n            # choose one of the fitted agents randomly\n            agent_idx = rng.integers(len(agent_stats.fitted_agents))\n            agent = agent_stats.fitted_agents[agent_idx]\n            # evaluate agent\n            observation = eval_env.reset()\n            for hh in range(eval_horizon):\n                if stationary_policy:\n                    action = agent.policy(observation,\n                                          **agent_stats.policy_kwargs)\n                else:\n                    action = agent.policy(observation, hh,\n                                          **agent_stats.policy_kwargs)\n                observation, reward, done, _ = eval_env.step(action)\n                episode_rewards[sim] += reward\n                if done:\n                    break\n        # store rewards\n        agents_rewards.append(episode_rewards)\n\n    #\n    # plot\n    #\n\n    # build unique agent IDs (in case there are two agents with the same ID)\n    unique_ids = []\n    id_count = {}\n    for agent_stats in agent_stats_list:\n        name = agent_stats.agent_name\n        if name not in id_count:\n            id_count[name] = 1\n        else:\n            id_count[name] += 1\n\n        unique_ids.append(name + \"*\"*(id_count[name]-1))\n\n    # convert output to DataFrame\n    data = {}\n    for agent_id, agent_rewards in zip(unique_ids, agents_rewards):\n        data[agent_id] = agent_rewards\n    output = pd.DataFrame(data)\n\n    # plot\n    if plot:\n        plt.figure(fignum)\n\n        with sns.axes_style(\"whitegrid\"):\n            ax = sns.boxplot(data=output, **kwargs)\n            ax.set_xlabel(\"agent\")\n            ax.set_ylabel(\"rewards in one episode\")\n            plt.title(\"Environment = %s\" %\n                      getattr(eval_env.unwrapped, \"name\",\n                              eval_env.unwrapped.__class__.__name__))\n            if show:\n                plt.show()\n\n    return output\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that compares several bandit agents using the rlberry library. The code should define the parameters of the problem, construct the experiment, define several classes of agents (UCB, UCBV, ETC, MOSS, IMED, NPTS, EXP3), and train these agents. After training, the code should compute and plot the cumulative pseudo-regret and cumulative regret for each agent. Finally, the code should compute and plot the number of times each arm was selected. The agents should be wrapped with a WriterWrapper to track the action and reward. The experiment should be managed using the ExperimentManager class. The plots should be created using matplotlib and seaborn, with varying line styles for each agent.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 22, "repo_full_name": "burnysc2__python-sc2", "instruction": "Generate code that creates a StarCraft II bot using the python-sc2 library. The bot should be a Zerg race bot that performs a rush strategy. The bot should have methods to handle the start of the game, each step of the game, and the end of the game. \n\nOn start, the bot should set the game step to 2. On each step, the bot should perform a series of actions such as sending a chat message, attacking enemy structures, injecting hatcheries with larva, managing vespene gas and mineral resources, researching upgrades, training units, and building structures. \n\nThe bot should also have a method to draw a creep pixelmap for debugging purposes. At the end of the game, the bot should log that the game has ended. \n\nFinally, the bot should be run on a specific map against a computer opponent of the Terran race with medium difficulty. The game should not be run in real time and a replay of the game should be saved.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def start_location(self) -> Point2:\n        \"\"\"\n        Returns the spawn location of the bot, using the position of the first created townhall.\n        This will be None if the bot is run on an arcade or custom map that does not feature townhalls at game start.\n        \"\"\"\n        return self._game_info.player_start_location\n\n# --- Snippet Separator ---\n\ndef start_location(self) -> Point2:\n        \"\"\"\n        Returns the spawn location of the bot, using the position of the first created townhall.\n        This will be None if the bot is run on an arcade or custom map that does not feature townhalls at game start.\n        \"\"\"\n        return self._game_info.player_start_location\n\n# --- Snippet Separator ---\n\nclass CompetitiveBot(sc2.BotAI):\n\n    async def on_start(self):\n        print(\"Game started\")\n        # Do things here before the game starts\n\n    async def on_step(self, iteration):\n        # Populate this function with whatever your bot should do!\n        pass\n\n    def on_end(self, result):\n        print(\"Game ended.\")\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a StarCraft II bot using the python-sc2 library. The bot should be a Zerg race bot that performs a rush strategy. The bot should have methods to handle the start of the game, each step of the game, and the end of the game. \n\nOn start, the bot should set the game step to 2. On each step, the bot should perform a series of actions such as sending a chat message, attacking enemy structures, injecting hatcheries with larva, managing vespene gas and mineral resources, researching upgrades, training units, and building structures. \n\nThe bot should also have a method to draw a creep pixelmap for debugging purposes. At the end of the game, the bot should log that the game has ended. \n\nFinally, the bot should be run on a specific map against a computer opponent of the Terran race with medium difficulty. The game should not be run in real time and a replay of the game should be saved.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 23, "repo_full_name": "kivy__kivy", "instruction": "Generate code that creates a custom class named \"SelectableGrid\" that inherits from \"FocusBehavior\", \"CompoundSelectionBehavior\", and \"GridLayout\" from the Kivy library. This class should have methods to handle key down and key up events, navigate to a node by typing its number, select and deselect nodes, and handle touch events. The selection of a node should change its background color. The class should also print the selected nodes when the selection changes. After defining the class, create an instance of it with specific parameters and add 40 buttons to it, each with a unique number as its text. Bind the touch down event of each button to the touch handling method of the grid. Finally, run the application with the grid as the root widget.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class CompoundSelectionBehavior(object):\n    '''The Selection behavior `mixin <https://en.wikipedia.org/wiki/Mixin>`_\n    implements the logic behind keyboard and touch\n    selection of selectable widgets managed by the derived widget. Please see\n    the :mod:`compound selection behaviors module\n    <kivy.uix.behaviors.compoundselection>` documentation\n    for more information.\n\n    .. versionadded:: 1.9.0\n    '''\n\n    selected_nodes = ListProperty([])\n    '''The list of selected nodes.\n\n    .. note::\n\n        Multiple nodes can be selected right after one another e.g. using the\n        keyboard. When listening to :attr:`selected_nodes`, one should be\n        aware of this.\n\n    :attr:`selected_nodes` is a :class:`~kivy.properties.ListProperty` and\n    defaults to the empty list, []. It is read-only and should not be modified.\n    '''\n\n    touch_multiselect = BooleanProperty(False)\n    '''A special touch mode which determines whether touch events, as\n    processed by :meth:`select_with_touch`, will add the currently touched\n    node to the selection, or if it will clear the selection before adding the\n    node. This allows the selection of multiple nodes by simply touching them.\n\n    This is different from :attr:`multiselect` because when it is True,\n    simply touching an unselected node will select it, even if ctrl is not\n    pressed. If it is False, however, ctrl must be pressed in order to\n    add to the selection when :attr:`multiselect` is True.\n\n    .. note::\n\n        :attr:`multiselect`, when False, will disable\n        :attr:`touch_multiselect`.\n\n    :attr:`touch_multiselect` is a :class:`~kivy.properties.BooleanProperty`\n    and defaults to False.\n    '''\n\n    multiselect = BooleanProperty(False)\n    '''Determines whether multiple nodes can be selected. If enabled, keyboard\n    shift and ctrl selection, optionally combined with touch, for example, will\n    be able to select multiple widgets in the normally expected manner.\n    This dominates :attr:`touch_multiselect` when False.\n\n    :attr:`multiselect` is a :class:`~kivy.properties.BooleanProperty` and\n    defaults to False.\n    '''\n\n    touch_deselect_last = BooleanProperty(not _is_desktop)\n    '''Determines whether the last selected node can be deselected when\n    :attr:`multiselect` or :attr:`touch_multiselect` is False.\n\n    .. versionadded:: 1.10.0\n\n    :attr:`touch_deselect_last` is a :class:`~kivy.properties.BooleanProperty`\n    and defaults to True on mobile, False on desktop platforms.\n    '''\n\n    keyboard_select = BooleanProperty(True)\n    '''Determines whether the keyboard can be used for selection. If False,\n    keyboard inputs will be ignored.\n\n    :attr:`keyboard_select` is a :class:`~kivy.properties.BooleanProperty`\n    and defaults to True.\n    '''\n\n    page_count = NumericProperty(10)\n    '''Determines by how much the selected node is moved up or down, relative\n    to the position of the last selected node, when pageup (or pagedown) is\n    pressed.\n\n    :attr:`page_count` is a :class:`~kivy.properties.NumericProperty` and\n    defaults to 10.\n    '''\n\n    up_count = NumericProperty(1)\n    '''Determines by how much the selected node is moved up or down, relative\n    to the position of the last selected node, when the up (or down) arrow on\n    the keyboard is pressed.\n\n    :attr:`up_count` is a :class:`~kivy.properties.NumericProperty` and\n    defaults to 1.\n    '''\n\n    right_count = NumericProperty(1)\n    '''Determines by how much the selected node is moved up or down, relative\n    to the position of the last selected node, when the right (or left) arrow\n    on the keyboard is pressed.\n\n    :attr:`right_count` is a :class:`~kivy.properties.NumericProperty` and\n    defaults to 1.\n    '''\n\n    scroll_count = NumericProperty(0)\n    '''Determines by how much the selected node is moved up or down, relative\n    to the position of the last selected node, when the mouse scroll wheel is\n    scrolled.\n\n    :attr:`right_count` is a :class:`~kivy.properties.NumericProperty` and\n    defaults to 0.\n    '''\n\n    nodes_order_reversed = BooleanProperty(True)\n    ''' (Internal) Indicates whether the order of the nodes as displayed top-\n    down is reversed compared to their order in :meth:`get_selectable_nodes`\n    (e.g. how the children property is reversed compared to how\n    it's displayed).\n    '''\n\n    text_entry_timeout = NumericProperty(1.)\n    '''When typing characters in rapid succession (i.e. the time difference\n    since the last character is less than :attr:`text_entry_timeout`), the\n    keys get concatenated and the combined text is passed as the key argument\n    of :meth:`goto_node`.\n\n    .. versionadded:: 1.10.0\n    '''\n\n    _anchor = None  # the last anchor node selected (e.g. shift relative node)\n    # the idx may be out of sync\n    _anchor_idx = 0  # cache indexes in case list hasn't changed\n    _last_selected_node = None  # the absolute last node selected\n    _last_node_idx = 0\n    _ctrl_down = False  # if it's pressed - for e.g. shift selection\n    _shift_down = False\n    # holds str used to find node, e.g. if word is typed. passed to goto_node\n    _word_filter = ''\n    _last_key_time = 0  # time since last press, for finding whole strs in node\n    _key_list = []  # keys that are already pressed, to not press continuously\n    _offset_counts = {}  # cache of counts for faster access\n\n    def __init__(self, **kwargs):\n        super(CompoundSelectionBehavior, self).__init__(**kwargs)\n        self._key_list = []\n\n        def ensure_single_select(*l):\n            if (not self.multiselect) and len(self.selected_nodes) > 1:\n                self.clear_selection()\n        update_counts = self._update_counts\n        update_counts()\n        fbind = self.fbind\n        fbind('multiselect', ensure_single_select)\n        fbind('page_count', update_counts)\n        fbind('up_count', update_counts)\n        fbind('right_count', update_counts)\n        fbind('scroll_count', update_counts)\n\n    def select_with_touch(self, node, touch=None):\n        '''(internal) Processes a touch on the node. This should be called by\n        the derived widget when a node is touched and is to be used for\n        selection. Depending on the keyboard keys pressed and the\n        configuration, it could select or deslect this and other nodes in the\n        selectable nodes list, :meth:`get_selectable_nodes`.\n\n        :Parameters:\n            `node`\n                The node that received the touch. Can be None for a scroll\n                type touch.\n            `touch`\n                Optionally, the touch. Defaults to None.\n\n        :Returns:\n            bool, True if the touch was used, False otherwise.\n        '''\n        multi = self.multiselect\n        multiselect = multi and (self._ctrl_down or self.touch_multiselect)\n        range_select = multi and self._shift_down\n\n        if touch and 'button' in touch.profile and touch.button in\\\n            ('scrollup', 'scrolldown', 'scrollleft', 'scrollright'):\n            node_src, idx_src = self._resolve_last_node()\n            node, idx = self.goto_node(touch.button, node_src, idx_src)\n            if node == node_src:\n                return False\n            if range_select:\n                self._select_range(multiselect, True, node, idx)\n            else:\n                if not multiselect:\n                    self.clear_selection()\n                self.select_node(node)\n            return True\n        if node is None:\n            return False\n\n        if (node in self.selected_nodes and (not range_select)):  # selected\n            if multiselect:\n                self.deselect_node(node)\n            else:\n                selected_node_count = len(self.selected_nodes)\n                self.clear_selection()\n                if not self.touch_deselect_last or selected_node_count > 1:\n                    self.select_node(node)\n        elif range_select:\n            # keep anchor only if not multiselect (ctrl-type selection)\n            self._select_range(multiselect, not multiselect, node, 0)\n        else:   # it's not selected at this point\n            if not multiselect:\n                self.clear_selection()\n            self.select_node(node)\n        return True\n\n    def select_with_key_down(self, keyboard, scancode, codepoint, modifiers,\n                             **kwargs):\n        '''Processes a key press. This is called when a key press is to be used\n        for selection. Depending on the keyboard keys pressed and the\n        configuration, it could select or deselect nodes or node ranges\n        from the selectable nodes list, :meth:`get_selectable_nodes`.\n\n        The parameters are such that it could be bound directly to the\n        on_key_down event of a keyboard. Therefore, it is safe to be called\n        repeatedly when the key is held down as is done by the keyboard.\n\n        :Returns:\n            bool, True if the keypress was used, False otherwise.\n        '''\n        if not self.keyboard_select:\n            return False\n        keys = self._key_list\n        multi = self.multiselect\n        node_src, idx_src = self._resolve_last_node()\n        text = scancode[1]\n\n        if text == 'shift':\n            self._shift_down = True\n        elif text in ('ctrl', 'lctrl', 'rctrl'):\n            self._ctrl_down = True\n        elif (multi and 'ctrl' in modifiers and text in ('a', 'A') and\n              text not in keys):\n            sister_nodes = self.get_selectable_nodes()\n            select = self.select_node\n            for node in sister_nodes:\n                select(node)\n            keys.append(text)\n        else:\n            s = text\n            if len(text) > 1:\n                d = {'divide': '/', 'mul': '*', 'substract': '-', 'add': '+',\n                     'decimal': '.'}\n                if text.startswith('numpad'):\n                    s = text[6:]\n                    if len(s) > 1:\n                        if s in d:\n                            s = d[s]\n                        else:\n                            s = None\n                else:\n                    s = None\n\n            if s is not None:\n                if s not in keys:  # don't keep adding while holding down\n                    if time() - self._last_key_time <= self.text_entry_timeout:\n                        self._word_filter += s\n                    else:\n                        self._word_filter = s\n                    keys.append(s)\n\n                self._last_key_time = time()\n                node, idx = self.goto_node(self._word_filter, node_src,\n                                           idx_src)\n            else:\n                self._word_filter = ''\n                node, idx = self.goto_node(text, node_src, idx_src)\n\n            if node == node_src:\n                return False\n\n            multiselect = multi and 'ctrl' in modifiers\n            if multi and 'shift' in modifiers:\n                self._select_range(multiselect, True, node, idx)\n            else:\n                if not multiselect:\n                    self.clear_selection()\n                self.select_node(node)\n            return True\n        self._word_filter = ''\n        return False\n\n    def select_with_key_up(self, keyboard, scancode, **kwargs):\n        '''(internal) Processes a key release. This must be called by the\n        derived widget when a key that :meth:`select_with_key_down` returned\n        True is released.\n\n        The parameters are such that it could be bound directly to the\n        on_key_up event of a keyboard.\n\n        :Returns:\n            bool, True if the key release was used, False otherwise.\n        '''\n        if scancode[1] == 'shift':\n            self._shift_down = False\n        elif scancode[1] in ('ctrl', 'lctrl', 'rctrl'):\n            self._ctrl_down = False\n        else:\n            try:\n                self._key_list.remove(scancode[1])\n                return True\n            except ValueError:\n                return False\n        return True\n\n    def _update_counts(self, *largs):\n        # doesn't invert indices here\n        pc = self.page_count\n        uc = self.up_count\n        rc = self.right_count\n        sc = self.scroll_count\n        self._offset_counts = {'pageup': -pc, 'pagedown': pc, 'up': -uc,\n        'down': uc, 'right': rc, 'left': -rc, 'scrollup': sc,\n        'scrolldown': -sc, 'scrollright': -sc, 'scrollleft': sc}\n\n    def _resolve_last_node(self):\n        # for offset selection, we have a anchor, and we select everything\n        # between anchor and added offset relative to last node\n        sister_nodes = self.get_selectable_nodes()\n        if not len(sister_nodes):\n            return None, 0\n        last_node = self._last_selected_node\n        last_idx = self._last_node_idx\n        end = len(sister_nodes) - 1\n\n        if last_node is None:\n            last_node = self._anchor\n            last_idx = self._anchor_idx\n        if last_node is None:\n            return sister_nodes[end], end\n        if last_idx > end or sister_nodes[last_idx] != last_node:\n            try:\n                return last_node, self.get_index_of_node(last_node,\n                                                         sister_nodes)\n            except ValueError:\n                return sister_nodes[end], end\n        return last_node, last_idx\n\n    def _select_range(self, multiselect, keep_anchor, node, idx):\n        '''Selects a range between self._anchor and node or idx.\n        If multiselect is True, it will be added to the selection, otherwise\n        it will unselect everything before selecting the range. This is only\n        called if self.multiselect is True.\n        If keep anchor is False, the anchor is moved to node. This should\n        always be True for keyboard selection.\n        '''\n        select = self.select_node\n        sister_nodes = self.get_selectable_nodes()\n        end = len(sister_nodes) - 1\n        last_node = self._anchor\n        last_idx = self._anchor_idx\n\n        if last_node is None:\n            last_idx = end\n            last_node = sister_nodes[end]\n        else:\n            if last_idx > end or sister_nodes[last_idx] != last_node:\n                try:\n                    last_idx = self.get_index_of_node(last_node, sister_nodes)\n                except ValueError:\n                    # list changed - cannot do select across them\n                    return\n        if idx > end or sister_nodes[idx] != node:\n            try:    # just in case\n                idx = self.get_index_of_node(node, sister_nodes)\n            except ValueError:\n                return\n\n        if last_idx > idx:\n            last_idx, idx = idx, last_idx\n        if not multiselect:\n            self.clear_selection()\n        for item in sister_nodes[last_idx:idx + 1]:\n            select(item)\n\n        if keep_anchor:\n            self._anchor = last_node\n            self._anchor_idx = last_idx\n        else:\n            self._anchor = node  # in case idx was reversed, reset\n            self._anchor_idx = idx\n        self._last_selected_node = node\n        self._last_node_idx = idx\n\n    def clear_selection(self):\n        ''' Deselects all the currently selected nodes.\n        '''\n        # keep the anchor and last selected node\n        deselect = self.deselect_node\n        nodes = self.selected_nodes\n        # empty beforehand so lookup in deselect will be fast\n        for node in nodes[:]:\n            deselect(node)\n\n    def get_selectable_nodes(self):\n        '''(internal) Returns a list of the nodes that can be selected. It can\n        be overwritten by the derived widget to return the correct list.\n\n        This list is used to determine which nodes to select with group\n        selection. E.g. the last element in the list will be selected when\n        home is pressed, pagedown will move (or add to, if shift is held) the\n        selection from the current position by negative :attr:`page_count`\n        nodes starting from the position of the currently selected node in\n        this list and so on. Still, nodes can be selected even if they are not\n        in this list.\n\n        .. note::\n\n            It is safe to dynamically change this list including removing,\n            adding, or re-arranging its elements. Nodes can be selected even\n            if they are not on this list. And selected nodes removed from the\n            list will remain selected until :meth:`deselect_node` is called.\n\n        .. warning::\n\n            Layouts display their children in the reverse order. That is, the\n            contents of :attr:`~kivy.uix.widget.Widget.children` is displayed\n            form right to left, bottom to top. Therefore, internally, the\n            indices of the elements returned by this function are reversed to\n            make it work by default for most layouts so that the final result\n            is consistent e.g. home, although it will select the last element\n            in this list visually, will select the first element when\n            counting from top to bottom and left to right. If this behavior is\n            not desired, a reversed list should be returned instead.\n\n        Defaults to returning :attr:`~kivy.uix.widget.Widget.children`.\n        '''\n        return self.children\n\n    def get_index_of_node(self, node, selectable_nodes):\n        '''(internal) Returns the index of the `node` within the\n        `selectable_nodes` returned by :meth:`get_selectable_nodes`.\n        '''\n        return selectable_nodes.index(node)\n\n    def goto_node(self, key, last_node, last_node_idx):\n        '''(internal) Used by the controller to get the node at the position\n        indicated by key. The key can be keyboard inputs, e.g. pageup,\n        or scroll inputs from the mouse scroll wheel, e.g. scrollup.\n        'last_node' is the last node selected and is used to find the resulting\n        node. For example, if the key is up, the returned node is one node\n        up from the last node.\n\n        It can be overwritten by the derived widget.\n\n        :Parameters:\n            `key`\n                str, the string used to find the desired node. It can be any\n                of the keyboard keys, as well as the mouse scrollup,\n                scrolldown, scrollright, and scrollleft strings. If letters\n                are typed in quick succession, the letters will be combined\n                before it's passed in as key and can be used to find nodes that\n                have an associated string that starts with those letters.\n            `last_node`\n                The last node that was selected.\n            `last_node_idx`\n                The cached index of the last node selected in the\n                :meth:`get_selectable_nodes` list. If the list hasn't changed\n                it saves having to look up the index of `last_node` in that\n                list.\n\n        :Returns:\n            tuple, the node targeted by key and its index in the\n            :meth:`get_selectable_nodes` list. Returning\n            `(last_node, last_node_idx)` indicates a node wasn't found.\n        '''\n        sister_nodes = self.get_selectable_nodes()\n        end = len(sister_nodes) - 1\n        counts = self._offset_counts\n        if end == -1:\n            return last_node, last_node_idx\n        if last_node_idx > end or sister_nodes[last_node_idx] != last_node:\n            try:    # just in case\n                last_node_idx = self.get_index_of_node(last_node, sister_nodes)\n            except ValueError:\n                return last_node, last_node_idx\n\n        is_reversed = self.nodes_order_reversed\n        if key in counts:\n            count = -counts[key] if is_reversed else counts[key]\n            idx = max(min(count + last_node_idx, end), 0)\n            return sister_nodes[idx], idx\n        elif key == 'home':\n            if is_reversed:\n                return sister_nodes[end], end\n            return sister_nodes[0], 0\n        elif key == 'end':\n            if is_reversed:\n                return sister_nodes[0], 0\n            return sister_nodes[end], end\n        else:\n            return last_node, last_node_idx\n\n    def select_node(self, node):\n        ''' Selects a node.\n\n        It is called by the controller when it selects a node and can be\n        called from the outside to select a node directly. The derived widget\n        should overwrite this method and change the node state to selected\n        when called.\n\n        :Parameters:\n            `node`\n                The node to be selected.\n\n        :Returns:\n            bool, True if the node was selected, False otherwise.\n\n        .. warning::\n\n            This method must be called by the derived widget using super if it\n            is overwritten.\n        '''\n        nodes = self.selected_nodes\n        if node in nodes:\n            return False\n\n        if (not self.multiselect) and len(nodes):\n            self.clear_selection()\n        if node not in nodes:\n            nodes.append(node)\n        self._anchor = node\n        self._last_selected_node = node\n        return True\n\n    def deselect_node(self, node):\n        ''' Deselects a possibly selected node.\n\n        It is called by the controller when it deselects a node and can also\n        be called from the outside to deselect a node directly. The derived\n        widget should overwrite this method and change the node to its\n        unselected state when this is called\n\n        :Parameters:\n            `node`\n                The node to be deselected.\n\n        .. warning::\n\n            This method must be called by the derived widget using super if it\n            is overwritten.\n        '''\n        try:\n            self.selected_nodes.remove(node)\n            return True\n        except ValueError:\n            return False\n\n# --- Snippet Separator ---\n\nclass LayoutSelectionBehavior(CompoundSelectionBehavior):\n    '''The :class:`LayoutSelectionBehavior` can be combined with\n    :class:`RecycleLayoutManagerBehavior` to allow its derived classes\n    selection behaviors similarly to how\n    :class:`~kivy.uix.behaviors.compoundselection.CompoundSelectionBehavior`\n    can be used to add selection behaviors to normal layout.\n\n    :class:`RecycleLayoutManagerBehavior` manages its children\n    differently than normal layouts or widgets so this class adapts\n    :class:`~kivy.uix.behaviors.compoundselection.CompoundSelectionBehavior`\n    based selection to work with :class:`RecycleLayoutManagerBehavior` as well.\n\n    Similarly to\n    :class:`~kivy.uix.behaviors.compoundselection.CompoundSelectionBehavior`,\n    one can select using the keyboard or touch, which calls :meth:`select_node`\n    or :meth:`deselect_node`, or one can call these methods directly. When a\n    item is selected or deselected :meth:`apply_selection` is called. See\n    :meth:`apply_selection`.\n\n\n    '''\n\n    key_selection = StringProperty(None, allownone=True)\n    '''The key used to check whether a view of a data item can be selected\n    with touch or the keyboard.\n\n    :attr:`key_selection` is the key in data, which if present and ``True``\n    will enable selection for this item from the keyboard or with a touch.\n    When None, the default, not item will be selectable.\n\n    :attr:`key_selection` is a :class:`StringProperty` and defaults to None.\n\n    .. note::\n        All data items can be selected directly using :meth:`select_node` or\n        :meth:`deselect_node`, even if :attr:`key_selection` is False.\n    '''\n\n    _selectable_nodes = []\n    _nodes_map = {}\n\n    def __init__(self, **kwargs):\n        self.nodes_order_reversed = False\n        super(LayoutSelectionBehavior, self).__init__(**kwargs)\n\n    def compute_sizes_from_data(self, data, flags):\n        # overwrite this method so that when data changes we update\n        # selectable nodes.\n        key = self.key_selection\n        if key is None:\n            nodes = self._selectable_nodes = []\n        else:\n            nodes = self._selectable_nodes = [\n                i for i, d in enumerate(data) if d.get(key)]\n\n        self._nodes_map = {v: k for k, v in enumerate(nodes)}\n        return super(LayoutSelectionBehavior, self).compute_sizes_from_data(\n            data, flags)\n\n    def get_selectable_nodes(self):\n        # the indices of the data is used as the nodes\n        return self._selectable_nodes\n\n    def get_index_of_node(self, node, selectable_nodes):\n        # the indices of the data is used as the nodes, so node\n        return self._nodes_map[node]\n\n    def goto_node(self, key, last_node, last_node_idx):\n        node, idx = super(LayoutSelectionBehavior, self).goto_node(\n            key, last_node, last_node_idx)\n        if node is not last_node:\n            self.goto_view(node)\n        return node, idx\n\n    def select_node(self, node):\n        if super(LayoutSelectionBehavior, self).select_node(node):\n            view = self.recycleview.view_adapter.get_visible_view(node)\n            if view is not None:\n                self.apply_selection(node, view, True)\n\n    def deselect_node(self, node):\n        if super(LayoutSelectionBehavior, self).deselect_node(node):\n            view = self.recycleview.view_adapter.get_visible_view(node)\n            if view is not None:\n                self.apply_selection(node, view, False)\n\n    def apply_selection(self, index, view, is_selected):\n        '''Applies the selection to the view. This is called internally when\n        a view is displayed and it needs to be shown as selected or as not\n        selected.\n\n        It is called when :meth:`select_node` or :meth:`deselect_node` is\n        called or when a view needs to be refreshed. Its function is purely to\n        update the view to reflect the selection state. So the function may be\n        called multiple times even if the selection state may not have changed.\n\n        If the view is a instance of\n        :class:`~kivy.uix.recycleview.views.RecycleDataViewBehavior`, its\n        :meth:`~kivy.uix.recycleview.views.RecycleDataViewBehavior.\\\napply_selection` method will be called every time the view needs to refresh\n        the selection state. Otherwise, the this method is responsible\n        for applying the selection.\n\n        :Parameters:\n\n            `index`: int\n                The index of the data item that is associated with the view.\n            `view`: widget\n                The widget that is the view of this data item.\n            `is_selected`: bool\n                Whether the item is selected.\n        '''\n        viewclass = view.__class__\n        if viewclass not in _view_base_cache:\n            _view_base_cache[viewclass] = isinstance(view,\n                                                     RecycleDataViewBehavior)\n\n        if _view_base_cache[viewclass]:\n            view.apply_selection(self.recycleview, index, is_selected)\n\n    def refresh_view_layout(self, index, layout, view, viewport):\n        super(LayoutSelectionBehavior, self).refresh_view_layout(\n            index, layout, view, viewport)\n        self.apply_selection(index, view, index in self.selected_nodes)\n\n# --- Snippet Separator ---\n\nclass EventManagerBase(object):\n    '''Abstract class with methods :meth:`start`, :meth:`stop` and\n    :meth:`dispatch` for specific class to implement.\n\n    Example of the manager receiving touch and hover events::\n\n        class TouchHoverManager(EventManagerBase):\n\n            type_ids = ('touch', 'hover')\n\n            def start(self):\n                # Create additional resources, bind callbacks to self.window\n\n            def dispatch(self, etype, me):\n                if me.type_id == 'touch':\n                    # Handle touch event\n                elif me.type_id == 'hover'\n                    # Handle hover event\n\n            def stop(self):\n                # Release resources\n\n    '''\n\n    type_ids = None\n    '''Override this attribute to declare the type ids of the events which\n    manager wants to receive. This attribute will be used by\n    :class:`~kivy.core.window.WindowBase` to know which events to pass to the\n    :meth:`dispatch` method.\n\n    .. versionadded:: 2.1.0\n    '''\n\n    window = None\n    '''Holds the instance of the :class:`~kivy.core.window.WindowBase`.\n\n    .. versionadded:: 2.1.0\n    '''\n\n    def start(self):\n        '''Start the manager, bind callbacks to the objects and create\n        additional resources. Attribute :attr:`window` is assigned when this\n        method is called.\n\n        .. versionadded:: 2.1.0\n        '''\n\n    def dispatch(self, etype, me):\n        '''Dispatch event `me` to the widgets in the :attr:`window`.\n\n        :Parameters:\n            `etype`: `str`\n                One of \"begin\", \"update\" or \"end\"\n            `me`: :class:`~kivy.input.motionevent.MotionEvent`\n                The Motion Event currently dispatched.\n        :Returns: `bool`\n            `True` to stop event dispatching\n\n        .. versionadded:: 2.1.0\n        '''\n\n    def stop(self):\n        '''Stop the manager, unbind from any objects and release any allocated\n        resources.\n\n        .. versionadded:: 2.1.0\n        '''\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a custom class named \"SelectableGrid\" that inherits from \"FocusBehavior\", \"CompoundSelectionBehavior\", and \"GridLayout\" from the Kivy library. This class should have methods to handle key down and key up events, navigate to a node by typing its number, select and deselect nodes, and handle touch events. The selection of a node should change its background color. The class should also print the selected nodes when the selection changes. After defining the class, create an instance of it with specific parameters and add 40 buttons to it, each with a unique number as its text. Bind the touch down event of each button to the touch handling method of the grid. Finally, run the application with the grid as the root widget.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 24, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that imports necessary libraries and modules such as argparse, sys, numpy, matplotlib, scipy, mumps, and fealpy. The code should define a Poisuille PDE from the navier_stokes_mold_2d module and set up a command-line argument parser to accept parameters like the degree of motion finite element space, degree of pressure finite element space, number of time divisions, evolution end time, output directory, steps, and non-linearization method. \n\nThe code should then parse these arguments and use them to define variables. It should also create a TriangleMesh from a unit square and a UniformTimeLine for time evolution. The code should define LagrangeFiniteElementSpace for motion and pressure, and calculate the number of global degrees of freedom for both. \n\nThe code should then set up a BilinearForm and MixedBilinearForm, and add domain integrators to them. It should assemble these forms and get their matrices. The code should also calculate the mass matrix of the motion space and initialize an error matrix. \n\nIn a loop, the code should advance to the next time level, add a ScalarConvectionIntegrator to a new BilinearForm, assemble it, and get its matrix. It should calculate a divergence matrix and a new matrix M. It should also calculate a source vector and set up boundary conditions. \n\nThe code should then solve the system of equations, update the motion and pressure functions, calculate the L2 error and maximum error, and advance to the next time level. Finally, the code should print the sum of absolute values of the motion function.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def run(self, *args, **kwargs):\n        \"\"\"This method should be the meat of the experiment (needs overriden).\n\n        This is where your experiment code goes.  Note that you should use\n        `self.wait_or_stop()` to pause your experiment between readings, to\n        allow the background thread to be stopped if necessary.\n\n        If you set `self.latest_data`, this may be used to display your\n        results in real time.  You can also use `self.log()` to output text\n        describing the experiment's progress; this may be picked up and \n        displayed graphically or in the console.\n\n        The arguments are passed through from start() to here, so you should\n        either use or ignore them as appropriate.  These are the same args\n        as are passed to run(), so if one of the two functions requires an\n        argument you should make sure the other won't fail if the same\n        argument is passed to it (simple rule: accept *args, **kwargs in\n        both, in addition to any arguments you might have).\n        \"\"\"\n        NotImplementedError(\"The run() method of an Experiment must be overridden!\")\n\n# --- Snippet Separator ---\n\ndef _create_bound_rhs(bnd, bound_exclusion,\n                      subcell_topology, sgn, g, num_flux, num_pr):\n    \"\"\"\n    Define rhs matrix to get basis functions for incorporates boundary\n    conditions\n\n    Parameters\n    ----------\n    bnd\n    exclude_dirichlet\n    exclude_neumann\n    fno\n    sgn : +-1, defining here and there of the faces\n    g : grid\n    num_flux : number of equations for flux continuity\n    num_pr: number of equations for pressure continuity\n\n    Returns\n    -------\n    rhs_bound: Matrix that can be multiplied with inverse block matrix to get\n               basis functions for boundary values\n    \"\"\"\n    # For primal-like discretizations like the MPFA, internal boundaries\n    # are handled by assigning Neumann conditions.\n    is_dir = np.logical_and(bnd.is_dir, np.logical_not(bnd.is_internal))\n    is_neu = np.logical_or(bnd.is_neu, bnd.is_internal)\n\n    fno = subcell_topology.fno_unique\n    num_neu = np.sum(is_neu[fno])\n    num_dir = np.sum(is_dir[fno])\n    num_bound = num_neu + num_dir\n\n    # Neumann boundary conditions\n    # Find Neumann faces, exclude Dirichlet faces (since these are excluded\n    # from the right hand side linear system), and do necessary formating.\n    neu_ind = np.argwhere(bound_exclusion.exclude_dirichlet(\n        is_neu[fno].astype('int64'))).ravel('F')\n    # We also need to map the respective Neumann and Dirichlet half-faces to\n    # the global half-face numbering (also interior faces). The latter should\n    # not have Dirichlet and Neumann excluded (respectively), and thus we need\n    # new fields\n    neu_ind_all = np.argwhere(is_neu[fno].astype('int')).ravel('F')\n    dir_ind_all = np.argwhere(is_dir[fno].astype('int')).ravel('F')\n    num_face_nodes = g.face_nodes.sum(axis=0).A.ravel(order='F')\n\n    # For the Neumann boundary conditions, we define the value as seen from\n    # the innside of the domain. E.g. outflow is defined to be positive. We\n    # therefore set the matrix indices to -1. We also have to scale it with\n    # the number of nodes per face because the flux of face is the sum of its\n    # half-faces.\n    scaled_sgn = - 1 / num_face_nodes[fno[neu_ind_all]]\n    if neu_ind.size > 0:\n        neu_cell = sps.coo_matrix((scaled_sgn,\n                                   (neu_ind, np.arange(neu_ind.size))),\n                                  shape=(num_flux, num_bound))\n    else:\n        # Special handling when no elements are found. Not sure if this is\n        # necessary, or if it is me being stupid\n        neu_cell = sps.coo_matrix((num_flux, num_bound))\n\n    # Dirichlet boundary conditions\n    dir_ind = np.argwhere(bound_exclusion.exclude_neumann(\n        is_dir[fno].astype('int64'))).ravel('F')\n    if dir_ind.size > 0:\n        dir_cell = sps.coo_matrix((sgn[dir_ind_all], (dir_ind, num_neu +\n                                                      np.arange(dir_ind.size))),\n                                  shape=(num_pr, num_bound))\n    else:\n        # Special handling when no elements are found. Not sure if this is\n        # necessary, or if it is me being stupid\n        dir_cell = sps.coo_matrix((num_pr, num_bound))\n\n    # Number of elements in neu_ind and neu_ind_all are equal, we can test with\n    # any of them. Same with dir.\n    if neu_ind.size > 0 and dir_ind.size > 0:\n        neu_dir_ind = np.hstack([neu_ind_all, dir_ind_all]).ravel('F')\n    elif neu_ind.size > 0:\n        neu_dir_ind = neu_ind_all\n    elif dir_ind.size > 0:\n        neu_dir_ind = dir_ind_all\n    else:\n        raise ValueError(\"Boundary values should be either Dirichlet or \"\n                         \"Neumann\")\n\n    num_subfno = subcell_topology.num_subfno_unique\n\n    # The columns in neu_cell, dir_cell are ordered from 0 to num_bound-1.\n    # Map these to all half-face indices\n    bnd_2_all_hf = sps.coo_matrix((np.ones(num_bound),\n                                   (np.arange(num_bound), neu_dir_ind)),\n                                  shape=(num_bound, num_subfno))\n    # The user of the discretization should now nothing about half faces,\n    # thus map from half face to face indices.\n    hf_2_f = sps.coo_matrix((np.ones(subcell_topology.subfno_unique.size),\n                             (subcell_topology.subfno_unique,\n                              subcell_topology.fno_unique)),\n                            shape=(num_subfno, g.num_faces))\n    rhs_bound = sps.vstack([neu_cell, dir_cell]) * bnd_2_all_hf * hf_2_f\n    return rhs_bound\n\n# --- Snippet Separator ---\n\nclass ExcludeBoundaries(object):\n    \"\"\" Wrapper class to store mapping for exclusion of equations that are\n    redundant due to the presence of boundary conditions.\n\n    The systems being set up in mpfa (and mpsa) describe continuity of flux and\n    potential (respectively stress and displacement) on all sub-faces. For\n    boundary faces, one of the two should be excluded (e.g. for a Dirichlet\n    boundary condition, there is no concept of continuity of flux/stresses).\n    The class contains mappings to eliminate the necessary fields.\n\n    \"\"\"\n\n    def __init__(self, subcell_topology, bound, nd):\n        \"\"\"\n        Define mappings to exclude boundary faces with dirichlet and neumann\n        conditions\n\n        Parameters\n        ----------\n        subcell_topology\n        bound\n\n        Returns\n        -------\n        exclude_neumann: Matrix, mapping from all faces to those having flux\n                         continuity\n        exclude_dirichlet: Matrix, mapping from all faces to those having pressure\n                           continuity\n        \"\"\"\n        self.nd = nd\n\n        # Short hand notation\n        fno = subcell_topology.fno_unique\n        num_subfno = subcell_topology.num_subfno_unique\n\n        # Define mappings to exclude boundary values\n        col_neu = np.argwhere([not it for it in bound.is_neu[fno]])\n        row_neu = np.arange(col_neu.size)\n        self.exclude_neu = sps.coo_matrix((np.ones(row_neu.size),\n                                           (row_neu, col_neu.ravel('C'))),\n                                          shape=(row_neu.size,\n                                                 num_subfno)).tocsr()\n        col_dir = np.argwhere([not it for it in bound.is_dir[fno]])\n        row_dir = np.arange(col_dir.size)\n        self.exclude_dir = sps.coo_matrix((np.ones(row_dir.size),\n                                           (row_dir, col_dir.ravel('C'))),\n                                          shape=(row_dir.size,\n                                                 num_subfno)).tocsr()\n\n    def exclude_dirichlet(self, other):\n        \"\"\" Mapping to exclude faces with Dirichlet boundary conditions from\n        local systems.\n\n        Parameters:\n            other (scipy.sparse matrix): Matrix of local equations for\n                continuity of flux and pressure.\n\n        Returns:\n            scipy.sparse matrix, with rows corresponding to faces with\n                Dirichlet conditions eliminated.\n\n        \"\"\"\n        return self.exclude_dir * other\n\n    def exclude_neumann(self, other):\n        \"\"\" Mapping to exclude faces with Neumann boundary conditions from\n        local systems.\n\n        Parameters:\n            other (scipy.sparse matrix): Matrix of local equations for\n                continuity of flux and pressure.\n\n        Returns:\n            scipy.sparse matrix, with rows corresponding to faces with\n                Neumann conditions eliminated.\n\n        \"\"\"\n        return self.exclude_neu * other\n\n    def exclude_dirichlet_nd(self, other):\n        \"\"\" Exclusion of Dirichlet conditions for vector equations (elasticity).\n        See above method without _nd suffix for description.\n\n        \"\"\"\n        exclude_dirichlet_nd = sps.kron(sps.eye(self.nd),\n                                        self.exclude_dir)\n        return exclude_dirichlet_nd * other\n\n    def exclude_neumann_nd(self, other):\n        \"\"\" Exclusion of Neumann conditions for vector equations (elasticity).\n        See above method without _nd suffix for description.\n\n        \"\"\"\n        exclude_neumann_nd = sps.kron(sps.eye(self.nd), self.exclude_neu)\n        return exclude_neumann_nd * other\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary libraries and modules such as argparse, sys, numpy, matplotlib, scipy, mumps, and fealpy. The code should define a Poisuille PDE from the navier_stokes_mold_2d module and set up a command-line argument parser to accept parameters like the degree of motion finite element space, degree of pressure finite element space, number of time divisions, evolution end time, output directory, steps, and non-linearization method. \n\nThe code should then parse these arguments and use them to define variables. It should also create a TriangleMesh from a unit square and a UniformTimeLine for time evolution. The code should define LagrangeFiniteElementSpace for motion and pressure, and calculate the number of global degrees of freedom for both. \n\nThe code should then set up a BilinearForm and MixedBilinearForm, and add domain integrators to them. It should assemble these forms and get their matrices. The code should also calculate the mass matrix of the motion space and initialize an error matrix. \n\nIn a loop, the code should advance to the next time level, add a ScalarConvectionIntegrator to a new BilinearForm, assemble it, and get its matrix. It should calculate a divergence matrix and a new matrix M. It should also calculate a source vector and set up boundary conditions. \n\nThe code should then solve the system of equations, update the motion and pressure functions, calculate the L2 error and maximum error, and advance to the next time level. Finally, the code should print the sum of absolute values of the motion function.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 25, "repo_full_name": "hiddensymmetries__simsopt", "instruction": "Generate code that solves a coil optimization problem using the simsopt library. The goal is to find coils that generate a specific target normal field on a given surface. The objective function includes terms for the magnetic field, coil length, coil-to-coil distance, coil-to-surface distance, curvature, and mean squared curvature. The code should initialize the boundary magnetic surface, create initial coils, define the individual terms of the objective function, and form the total objective function. It should then perform a Taylor test and run the optimization. After the optimization, the code should use the result as the initial guess for a subsequent optimization with reduced penalty for the coil length. Finally, the code should save the optimized coil shapes and currents.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def coil_optimization(s, bs, base_curves, curves, out_dir=''):\n    \"\"\"\n    Optimize the coils for the QA, QH, or other configurations.\n\n    Args:\n        s: plasma boundary.\n        bs: Biot Savart class object, presumably representing the\n          magnetic fields generated by the coils.\n        base_curves: List of CurveXYZFourier class objects.\n        curves: List of Curve class objects.\n        out_dir: Path or string for the output directory for saved files.\n\n    Returns:\n        bs: Biot Savart class object, presumably representing the\n          OPTIMIZED magnetic fields generated by the coils.\n    \"\"\"\n\n    from simsopt.geo import CurveLength, CurveCurveDistance, \\\n        MeanSquaredCurvature, LpCurveCurvature, CurveSurfaceDistance\n    from simsopt.objectives import QuadraticPenalty\n    from simsopt.geo import curves_to_vtk\n    from simsopt.objectives import SquaredFlux\n\n    out_dir = Path(out_dir)\n    nphi = len(s.quadpoints_phi)\n    ntheta = len(s.quadpoints_theta)\n    ncoils = len(base_curves)\n\n    # Weight on the curve lengths in the objective function:\n    LENGTH_WEIGHT = 1e-4\n\n    # Threshold and weight for the coil-to-coil distance penalty in the objective function:\n    CC_THRESHOLD = 0.1\n    CC_WEIGHT = 1e-1\n\n    # Threshold and weight for the coil-to-surface distance penalty in the objective function:\n    CS_THRESHOLD = 0.1\n    CS_WEIGHT = 1e-2\n\n    # Threshold and weight for the curvature penalty in the objective function:\n    CURVATURE_THRESHOLD = 0.1\n    CURVATURE_WEIGHT = 1e-9\n\n    # Threshold and weight for the mean squared curvature penalty in the objective function:\n    MSC_THRESHOLD = 0.1\n    MSC_WEIGHT = 1e-9\n\n    MAXITER = 500  # number of iterations for minimize\n\n    # Define the objective function:\n    Jf = SquaredFlux(s, bs)\n    Jls = [CurveLength(c) for c in base_curves]\n    Jccdist = CurveCurveDistance(curves, CC_THRESHOLD, num_basecurves=ncoils)\n    Jcsdist = CurveSurfaceDistance(curves, s, CS_THRESHOLD)\n    Jcs = [LpCurveCurvature(c, 2, CURVATURE_THRESHOLD) for c in base_curves]\n    Jmscs = [MeanSquaredCurvature(c) for c in base_curves]\n\n    # Form the total objective function.\n    JF = Jf \\\n        + LENGTH_WEIGHT * sum(Jls) \\\n        + CC_WEIGHT * Jccdist \\\n        + CS_WEIGHT * Jcsdist \\\n        + CURVATURE_WEIGHT * sum(Jcs) \\\n        + MSC_WEIGHT * sum(QuadraticPenalty(J, MSC_THRESHOLD) for J in Jmscs)\n\n    def fun(dofs):\n        \"\"\" Function for coil optimization grabbed from stage_two_optimization.py \"\"\"\n        JF.x = dofs\n        J = JF.J()\n        grad = JF.dJ()\n        jf = Jf.J()\n        BdotN = np.mean(np.abs(np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)))\n        outstr = f\"J={J:.1e}, Jf={jf:.1e}, ⟨B·n⟩={BdotN:.1e}\"\n        cl_string = \", \".join([f\"{J.J():.1f}\" for J in Jls])\n        kap_string = \", \".join(f\"{np.max(c.kappa()):.1f}\" for c in base_curves)\n        msc_string = \", \".join(f\"{J.J():.1f}\" for J in Jmscs)\n        outstr += f\", Len=sum([{cl_string}])={sum(J.J() for J in Jls):.1f}, ϰ=[{kap_string}], ∫ϰ²/L=[{msc_string}]\"\n        outstr += f\", C-C-Sep={Jccdist.shortest_distance():.2f}, C-S-Sep={Jcsdist.shortest_distance():.2f}\"\n        outstr += f\", ║∇J║={np.linalg.norm(grad):.1e}\"\n        print(outstr)\n        return J, grad\n\n    print(\"\"\"\n    ################################################################################\n    ### Perform a Taylor test ######################################################\n    ################################################################################\n    \"\"\")\n    f = fun\n    dofs = JF.x\n    np.random.seed(1)\n    h = np.random.uniform(size=dofs.shape)\n\n    J0, dJ0 = f(dofs)\n    dJh = sum(dJ0 * h)\n    for eps in [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]:\n        J1, _ = f(dofs + eps*h)\n        J2, _ = f(dofs - eps*h)\n        print(\"err\", (J1-J2)/(2*eps) - dJh)\n\n    print(\"\"\"\n    ################################################################################\n    ### Run the optimisation #######################################################\n    ################################################################################\n    \"\"\")\n    res = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 300}, tol=1e-15)\n    curves_to_vtk(curves, out_dir / \"curves_opt\")\n    bs.set_points(s.gamma().reshape((-1, 3)))\n    return bs\n\n# --- Snippet Separator ---\n\ndef initialize_coils(config_flag, TEST_DIR, s, out_dir=''):\n    \"\"\"\n    Initializes coils for each of the target configurations that are\n    used for permanent magnet optimization.\n\n    Args:\n        config_flag: String denoting the stellarator configuration \n          being initialized.\n        TEST_DIR: String denoting where to find the input files.\n        out_dir: Path or string for the output directory for saved files.\n        s: plasma boundary surface.\n    Returns:\n        base_curves: List of CurveXYZ class objects.\n        curves: List of Curve class objects.\n        coils: List of Coil class objects.\n    \"\"\"\n    from simsopt.geo import create_equally_spaced_curves\n    from simsopt.field import Current, Coil, coils_via_symmetries\n    from simsopt.geo import curves_to_vtk\n\n    out_dir = Path(out_dir)\n    if 'muse' in config_flag:\n        # Load in pre-optimized coils\n        coils_filename = TEST_DIR / 'muse_tf_coils.focus'\n        base_curves, base_currents, ncoils = read_focus_coils(coils_filename)\n        coils = []\n        for i in range(ncoils):\n            coils.append(Coil(base_curves[i], base_currents[i]))\n        base_currents[0].fix_all()\n\n        # fix all the coil shapes\n        for i in range(ncoils):\n            base_curves[i].fix_all()\n    elif config_flag == 'qh':\n        # generate planar TF coils\n        ncoils = 4\n        R0 = s.get_rc(0, 0)\n        R1 = s.get_rc(1, 0) * 2\n        order = 5\n\n        # qh needs to be scaled to 0.1 T on-axis magnetic field strength\n        from simsopt.mhd.vmec import Vmec\n        vmec_file = 'wout_LandremanPaul2021_QH_reactorScale_lowres_reference.nc'\n        total_current = Vmec(TEST_DIR / vmec_file).external_current() / (2 * s.nfp) / 8.75 / 5.69674966667\n        base_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order, numquadpoints=128)\n        base_currents = [(Current(total_current / ncoils * 1e-5) * 1e5) for _ in range(ncoils-1)]\n        total_current = Current(total_current)\n        total_current.fix_all()\n        base_currents += [total_current - sum(base_currents)]\n        coils = coils_via_symmetries(base_curves, base_currents, s.nfp, True)\n\n        # fix all the coil shapes so only the currents are optimized\n        for i in range(ncoils):\n            base_curves[i].fix_all()\n    elif config_flag == 'qa':\n        # generate planar TF coils\n        ncoils = 8\n        R0 = 1.0\n        R1 = 0.65\n        order = 5\n\n        # qa needs to be scaled to 0.1 T on-axis magnetic field strength\n        from simsopt.mhd.vmec import Vmec\n        vmec_file = 'wout_LandremanPaul2021_QA_lowres.nc'\n        total_current = Vmec(TEST_DIR / vmec_file).external_current() / (2 * s.nfp) / 7.131\n        base_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order, numquadpoints=128)\n        base_currents = [(Current(total_current / ncoils * 1e-5) * 1e5) for _ in range(ncoils-1)]\n        total_current = Current(total_current)\n        total_current.fix_all()\n        base_currents += [total_current - sum(base_currents)]\n        coils = coils_via_symmetries(base_curves, base_currents, s.nfp, True)\n        # fix all the coil shapes so only the currents are optimized\n        for i in range(ncoils):\n            base_curves[i].fix_all()\n\n    # Initialize the coil curves and save the data to vtk\n    curves = [c.curve for c in coils]\n    curves_to_vtk(curves, out_dir / \"curves_init\")\n    return base_curves, curves, coils\n\n# --- Snippet Separator ---\n\nclass CoilsetPositionCOP(CoilsetOptimisationProblem):\n    \"\"\"\n    Coilset OptimisationProblem for coil currents and positions\n    subject to maximum current bounds and positions bounded within\n    a provided region.\n\n    Coil currents and positions are optimised simultaneously.\n\n    Parameters\n    ----------\n    coilset:\n        Coilset to optimise.\n    eq: Equilibrium\n        Equilibrium object used to update magnetic field targets.\n    targets:\n        Set of magnetic field targets to use in objective function.\n    pfregions:\n        Dictionary of Coordinates that specify convex hull regions inside which\n        each PF control coil position is to be optimised.\n        The Coordinates must be 2d in x,z in units of [m].\n    max_currents:\n        Maximum allowed current for each independent coil current in coilset [A].\n        If specified as a float, the float will set the maximum allowed current\n        for all coils.\n    gamma:\n        Tikhonov regularisation parameter in units of [A⁻¹].\n    optimiser:\n        Optimiser object to use for constrained optimisation.\n    constraints:\n        Optional list of OptimisationConstraint objects storing\n        information about constraints that must be satisfied\n        during the coilset optimisation, to be provided to the\n        optimiser.\n\n    Notes\n    -----\n    Setting stopval and maxeval is the most reliable way to stop optimisation\n    at the desired figure of merit and number of iterations respectively.\n    Some NLOpt optimisers display unexpected behaviour when setting xtol and\n    ftol, and may not terminate as expected when those criteria are reached.\n    \"\"\"\n\n    def __init__(\n        self,\n        coilset: CoilSet,\n        eq: Equilibrium,\n        targets: MagneticConstraintSet,\n        pfregions: dict,\n        max_currents=None,\n        gamma=1e-8,\n        optimiser=Optimiser(\n            algorithm_name=\"SBPLX\",\n            opt_conditions={\n                \"stop_val\": 1.0,\n                \"max_eval\": 100,\n            },\n        ),\n        constraints=None,\n    ):\n        # noqa :N803\n\n        # Create region map\n        self.region_mapper = RegionMapper(pfregions)\n\n        # Store inputs (optional, but useful for constraints)\n        self.eq = eq\n        self.targets = targets\n\n        # Set objective function for this OptimisationProblem,\n        # and initialise\n        objective = OptimisationObjective(\n            objectives.ad_objective,\n            {\"objective\": self.get_state_figure_of_merit, \"objective_args\": {}},\n        )\n        super().__init__(coilset, optimiser, objective, constraints)\n\n        # Set up bounds\n        bounds = self.get_mapped_state_bounds(self.region_mapper, max_currents)\n        # Add bounds information to help automatic differentiation of objective\n        self._objective._args[\"ad_args\"] = {\"bounds\": bounds}\n        self._objective._args[\"objective_args\"] = {\n            \"coilset\": coilset,\n            \"eq\": eq,\n            \"targets\": targets,\n            \"region_mapper\": self.region_mapper,\n            \"current_scale\": self.scale,\n            \"gamma\": gamma,\n        }\n\n        # Set up optimiser\n        dimension = len(bounds[0])\n        self.set_up_optimiser(dimension, bounds)\n\n    def get_mapped_state_bounds(self, region_mapper: RegionMapper, max_currents):\n        \"\"\"\n        Get mapped bounds on the coilset state vector from the coil regions and\n        maximum coil currents.\n\n        Parameters\n        ----------\n        region_mapper: RegionMapper\n            RegionMapper mapping coil positions within the allowed optimisation\n            regions.\n        max_currents Union[float, np.ndarray] (default = None)\n            Maximum allowed current for each independent coil current in coilset [A].\n            If specified as a float, the float will set the maximum allowed current\n            for all coils.\n\n        Returns\n        -------\n        bounds: np.array\n            Array containing state vectors representing lower and upper bounds\n            for coilset state degrees of freedom.\n        \"\"\"\n        # Get mapped position bounds from RegionMapper\n        _, lower_lmap_bounds, upper_lmap_bounds = region_mapper.get_Lmap(self.coilset)\n        current_bounds = self.get_current_bounds(self.coilset, max_currents, self.scale)\n\n        lower_bounds = np.concatenate((lower_lmap_bounds, current_bounds[0]))\n        upper_bounds = np.concatenate((upper_lmap_bounds, current_bounds[1]))\n        bounds = (lower_bounds, upper_bounds)\n        return bounds\n\n    def optimise(self):\n        \"\"\"\n        Optimiser handle. Used in __call__\n\n        Returns\n        -------\n        self.coilset: CoilSet\n            Optimised CoilSet object.\n        \"\"\"\n        # Get initial state and apply region mapping to coil positions.\n        initial_state, _ = self.read_coilset_state(self.coilset, self.scale)\n        _, _, initial_currents = np.array_split(initial_state, self.substates)\n        initial_mapped_positions, _, _ = self.region_mapper.get_Lmap(self.coilset)\n        initial_mapped_state = np.concatenate(\n            (initial_mapped_positions, initial_currents)\n        )\n\n        # Optimise\n        state = self.opt.optimise(initial_mapped_state)\n\n        # Call objective function final time on optimised state\n        # to set coilset.\n        # Necessary as optimised state may not always be the final\n        # one evaluated by optimiser.\n        self._objective(state, np.empty(shape=(0, 0)))\n        return self.coilset\n\n    @staticmethod\n    def get_state_figure_of_merit(\n        vector,\n        grad,\n        coilset: CoilSet,\n        eq: Equilibrium,\n        targets: MagneticConstraintSet,\n        region_mapper: RegionMapper,\n        current_scale: float,\n        gamma: float,\n    ):\n        \"\"\"\n        Calculates figure of merit from objective function,\n        consisting of a least-squares objective with Tikhonov\n        regularisation term, which updates the gradient in-place.\n\n        Parameters\n        ----------\n        vector:\n            State vector. Numpy array formed by concatenation of coil radial\n            coordinates, coil vertical coordinates, and (scaled) coil currents.\n        grad:\n            Dummy variable for NLOpt calls. Not updated.\n        coilset:\n            CoilSet to update using state vector.\n        eq:\n            Equilibrium object used to update magnetic field targets.\n        targets:\n            Set of magnetic field targets to optimise Equilibrium towards,\n            using least-squares objective with Tikhonov regularisation.\n        region_mapper\n            RegionMapper mapping coil positions within the allowed optimisation\n            regions.\n        current_scale:\n            Scale factor to scale currents in state vector up by to\n            give currents in [A].\n        gamma:\n            Tikhonov regularisation parameter in units of [A⁻¹].\n\n        Returns\n        -------\n        fom: float\n            Value of objective function (figure of merit).\n        \"\"\"\n        mapped_x, mapped_z, currents = np.array_split(vector, 3)\n        mapped_positions = np.concatenate((mapped_x, mapped_z))\n        region_mapper.set_Lmap(mapped_positions)\n        x_vals, z_vals = region_mapper.get_xz_arrays()\n        coilset_state = np.concatenate((x_vals, z_vals, currents))\n\n        CoilsetOptimisationProblem.set_coilset_state(\n            coilset, coilset_state, current_scale\n        )\n\n        # Update target\n        eq._remap_greens()\n\n        # Set up data needed in FoM evaluation.\n        # Scale the control matrix and constraint vector by weights.\n        targets(eq, I_not_dI=True, fixed_coils=False)\n        _, a_mat, b_vec = targets.get_weighted_arrays()\n\n        # Calculate objective function\n        fom, err = regularised_lsq_fom(currents * current_scale, a_mat, b_vec, gamma)\n        return fom\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that solves a coil optimization problem using the simsopt library. The goal is to find coils that generate a specific target normal field on a given surface. The objective function includes terms for the magnetic field, coil length, coil-to-coil distance, coil-to-surface distance, curvature, and mean squared curvature. The code should initialize the boundary magnetic surface, create initial coils, define the individual terms of the objective function, and form the total objective function. It should then perform a Taylor test and run the optimization. After the optimization, the code should use the result as the initial guess for a subsequent optimization with reduced penalty for the coil length. Finally, the code should save the optimized coil shapes and currents.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 26, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation environment using the seedemu library. The environment should include multiple layers such as Base, Routing, Ebgp, Ibgp, Ospf, and WebService. It should create several Internet Exchanges with custom display names. It should also create Transit Autonomous Systems and single-homed stub Autonomous Systems with various services. The code should also add a host with a customized IP address to one of the Autonomous Systems and create a real-world Autonomous System. It should enable remote access to one of the Autonomous System's network. The code should also set up peering via a route server and private peering with different peer relationships. Finally, the code should add all the layers to the emulator, save the emulator to a component file, and render and compile the emulator.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def makeEmulatorBaseWith5StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef makeEmulatorBaseWith10StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 160, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 161, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 162, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 163, 104, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 164, 104, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(103, [3],  [160, 161, 162], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(104, [3, 4], [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [4],  [163], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [12], [164], PeerRelationship.Provider)\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef configure(self, emulator: Emulator):\n        self._log('registering nodes...')\n        for asobj in self.__ases.values():\n            if len(asobj.getNameServers()) == 0:\n                asobj.setNameServers(self.__name_servers)\n\n            asobj.registerNodes(emulator)\n\n        self._log('setting up internet exchanges...')\n        for ix in self.__ixes.values(): ix.configure(emulator)\n\n        self._log('setting up autonomous systems...')\n        for asobj in self.__ases.values(): asobj.configure(emulator)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulation environment using the seedemu library. The environment should include multiple layers such as Base, Routing, Ebgp, Ibgp, Ospf, and WebService. It should create several Internet Exchanges with custom display names. It should also create Transit Autonomous Systems and single-homed stub Autonomous Systems with various services. The code should also add a host with a customized IP address to one of the Autonomous Systems and create a real-world Autonomous System. It should enable remote access to one of the Autonomous System's network. The code should also set up peering via a route server and private peering with different peer relationships. Finally, the code should add all the layers to the emulator, save the emulator to a component file, and render and compile the emulator.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 27, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that simulates the energy balance in a plasma by plotting ohmic heating and radiative losses as a function of temperature at equilibrium ionization. The simulation should be performed using the DREAM library. The simulation should include setting up a radial grid, setting time steps, adding ions, setting up temperature and electric field, and disabling runaway and hot-tail grid. The simulation should be run in four stages: initialization, ionization, equilibration, and radiation. After each stage, the settings should be saved and the simulation should be run. Finally, the results should be plotted.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def setPrescribedData(self, efield, radius=0, times=0):\n        \"\"\"\n        When ``TYPE_PRESCRIBED``, sets the spatiotemporal evolution of the\n        electric field during the simulation. The parameter ``efield`` may be\n        either a scalar (in which case the electric field is taken to be\n        constant and uniform in time and radius) or a 2D array of shape\n        (nt, nr). The associated time grid ``times`` must be of size ``nt`` and\n        the radial grid must be of size ``nr``.\n\n        :param efield: Prescribed electric field.\n        :param radius: Radial grid on which the electric field is prescribed.\n        :param times:  Time grid on which the electric field is prescribed.\n        \"\"\"\n        _data, _rad, _tim = self._setPrescribedData(efield, radius, times)\n        self.efield = _data\n        self.radius = _rad\n        self.times  = _tim\n\n        self._verifySettingsPrescribedData()\n\n# --- Snippet Separator ---\n\nclass SimulationParameters():\n    \"\"\"\n    This class represents the run parameters for a simulation, with information including\n\n     - a function that creates the simulation\n     - a function that executes the simulation\n     - the dispersions to use on that simulation\n     - parameters describing the data to be retained for a simulation\n     - whether randomized seeds should be applied to the simulation\n     - whether data should be archived\n    \"\"\"\n\n    def __init__(self, creationFunction, executionFunction, configureFunction,\n                 retentionPolicies, dispersions, shouldDisperseSeeds,\n                 shouldArchiveParameters, filename, icfilename, index=None, verbose=False, modifications={},\n                 showProgressBar=False):\n        self.index = index\n        self.creationFunction = creationFunction\n        self.executionFunction = executionFunction\n        self.configureFunction = configureFunction\n        self.retentionPolicies = retentionPolicies\n        self.dispersions = dispersions\n        self.shouldDisperseSeeds = shouldDisperseSeeds\n        self.shouldArchiveParameters = shouldArchiveParameters\n        self.filename = filename\n        self.icfilename = icfilename\n        self.verbose = verbose\n        self.modifications = modifications\n        self.dispersionMag = {}\n        self.saveDispMag = False\n        self.showProgressBar = showProgressBar\n\n# --- Snippet Separator ---\n\ndef test_hingedBodyLinearProfiler(show_plots, startTime, endTime, startTheta, endTheta):\n    r\"\"\"\n    **Validation Test Description**\n\n    For a given deployment, checks that the theta before, during, and after deployment are correct.\n\n    **Test Parameters**\n\n    Discuss the test parameters used.\n\n    Args:\n        startTime (uint64_t): starting time in nanoseconds\n        endTime (uint64_t): ending time in nanoseconds\n        startTheta (double): starting angle of deployment in radians\n        endTheta (double): ending angle of deployment in radians\n\n    **Description of Variables Being Tested**\n\n    For a deployment from 0 to 1 degree, starting at 1 second and ending at 2 seconds into the simulation, checks that the angle and angle rates are as expected before, during, and after deployment.\n\n    Before deployment, theta should be 0 and ``thetaDot`` 0. During deployment, ``thetaDot`` should be 1 degree per second, with theta varying linearly. After deployment, theta should be 1 degree and ``thetaDot`` 0.\n\n    \"\"\"\n    [testResults, testMessage] = hingedBodyLinearProfilerTestFunction(show_plots, startTime, endTime, startTheta, endTheta)\n    assert testResults < 1, testMessage\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that simulates the energy balance in a plasma by plotting ohmic heating and radiative losses as a function of temperature at equilibrium ionization. The simulation should be performed using the DREAM library. The simulation should include setting up a radial grid, setting time steps, adding ions, setting up temperature and electric field, and disabling runaway and hot-tail grid. The simulation should be run in four stages: initialization, ionization, equilibration, and radiation. After each stage, the settings should be saved and the simulation should be run. Finally, the results should be plotted.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 28, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that implements a user-defined Ordered Subset Maximum A Posteriori One Step Late (OSMAPOSL) reconstruction algorithm using the SIRF library. The code should accept command-line options for the raw data file, path to data files, number of subsets, number of sub-iterations, reconstruction engine, and an option to disable plots. The reconstruction algorithm should be implemented in a function that takes an image, objective function, prior, filter, number of subsets, and number of sub-iterations as parameters. The main function should handle the creation of the acquisition model, acquisition data, filter, initial image estimate, prior, and objective function. It should also call the reconstruction function and display the reconstructed image if the non-interactive option is not set. The code should handle errors and print an error message if an exception is thrown.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class ObjectiveFunction(object):\n    \"\"\"Class for the objective function\n\n    maximised by the iterative reconstruction algorithms.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"init.\"\"\"\n        self.handle = None\n\n    def __del__(self):\n        \"\"\"del.\"\"\"\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n    def set_prior(self, prior):\n        \"\"\"Sets the prior,\n\n        a penalty term to be added to the objective function.\"\"\"\n        assert_validity(prior, Prior)\n        parms.set_parameter(self.handle, 'GeneralisedObjectiveFunction',\n                            'prior', prior.handle)\n        self.prior = prior\n\n    def get_prior(self):\n        \"\"\"Returns the prior currently used by this objective function.\"\"\"\n        prior = Prior()\n        prior.handle = pystir.cSTIR_parameter(\n            self.handle, 'GeneralisedObjectiveFunction', 'prior')\n        check_status(prior.handle)\n        return prior\n\n    def set_num_subsets(self, n):\n        \"\"\"Sets the number of subsets.\n\n        Sets the number of subsets of ray projections to be used for computing\n        additive components of the gradient used by Ordered Subset algorithms\n        for maximizing this objective function.\n        If the ray tracing projector G is a matrix, the subsets in question are\n        subsets of its rows.\n        n: number of subsets, Python integer scalar\n        \"\"\"\n        parms.set_int_par(\n            self.handle, 'GeneralisedObjectiveFunction', 'num_subsets', n)\n\n    def get_num_subsets(self):\n        \"\"\"Returns the number of subsets.\"\"\"\n        return parms.int_par(\n            self.handle, 'GeneralisedObjectiveFunction', 'num_subsets')\n\n    def set_up(self, image):\n        \"\"\"Prepares this object for use.\n\n        image: ImageData object\n        \"\"\"\n        assert_validity(image, ImageData)\n        try_calling(pystir.cSTIR_setupObjectiveFunction(\n            self.handle, image.handle))\n\n    def value(self, image):\n        \"\"\"Returns the value of this objective function on the specified image.\n\n        image: ImageData object\n        \"\"\"\n        assert_validity(image, ImageData)\n        handle = pystir.cSTIR_objectiveFunctionValue(self.handle, image.handle)\n        check_status(handle)\n        v = pyiutil.floatDataFromHandle(handle)\n        pyiutil.deleteDataHandle(handle)\n        return v\n\n    def __call__(self, image):\n        '''Alias of value: Returns the value of this objective function on the specified image.\n\n        image: ImageData object'''\n        return self.value(image)\n\n    def get_value(self, image):\n        \"\"\"Returns the value of this objective function on the specified image.\n\n        image: ImageData object\n        \"\"\"\n        return self.value(image)\n\n    def gradient(self, image, subset=-1):\n        \"\"\"Returns the value of the additive component of the gradient\n\n        of this objective function on the specified image corresponding to the\n        specified subset (see set_num_subsets() method).\n        If no subset is specified, returns the full gradient, i.e. the sum of\n        the subset components.\n        image: ImageData object\n        subset: Python integer scalar\n        \"\"\"\n        assert_validity(image, ImageData)\n        grad = ImageData()\n        grad.handle = pystir.cSTIR_objectiveFunctionGradient(\n            self.handle, image.handle, subset)\n        check_status(grad.handle)\n        return grad\n\n    def get_gradient(self, image):\n        \"\"\"Returns the gradient of the objective function on specified image.\n\n        image: ImageData object\n        \"\"\"\n        return self.gradient(image)\n\n    def get_subset_gradient(self, image, subset):\n        \"\"\"Returns the value of the additive component of the gradient\n\n        of this objective function on <image> corresponding to the specified\n        subset (see set_num_subsets() method).\n        image: ImageData object\n        subset: Python integer scalar\n        \"\"\"\n        return self.gradient(image, subset)\n\n    @abc.abstractmethod\n    def get_subset_sensitivity(self, subset):\n        #print('in base class ObjectiveFunction')\n        pass\n\n# --- Snippet Separator ---\n\nclass FBP2DReconstructor(object):\n    \"\"\"Class for 2D Filtered Back Projection reconstructor.\n\n    This is an implementation of the 2D FBP algorithm.\n    Oblique angles in data will be ignored. The exception is the span=1 case,\n    where the ring differences +1 and -1 are first combined to give indirect\n    sinograms.\n    By default, the algorithm uses the ramp filter. An apodizing filter can be\n    added by using set_alpha_cosine_window and/or set_frequency_cut_off.\n    The apodizing filter in frequency space has the form\n\n        (alpha + (1 - alpha) * cos(pi * f / fc))\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"init.\"\"\"\n        self.handle = None\n        self.handle = pystir.cSTIR_newObject('FBP2D')\n        check_status(self.handle)\n\n    def __del__(self):\n        \"\"\"del.\"\"\"\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n    def set_input(self, input_data):\n        \"\"\"Sets the acquisition data to use for reconstruction.\"\"\"\n        assert_validity(input_data, AcquisitionData)\n        parms.set_parameter(self.handle, 'FBP2D', 'input', input_data.handle)\n\n    def set_zoom(self, v):\n        \"\"\"Sets zoom.\"\"\"\n        parms.set_float_par(self.handle, 'FBP2D', 'zoom', v)\n\n    def set_alpha_cosine_window(self, v):\n        \"\"\"Sets alpha in the apodizing filter.\n\n        See the class documentation for the filter. The value of alpha should\n        be between 0.5 and 1. alpha=0.5 corresponds to the Hann filter, while\n        0.54 corresponds to the Hamming filter.\n        \"\"\"\n        parms.set_float_par(self.handle, 'FBP2D', 'alpha', v)\n\n    def set_frequency_cut_off(self, v):\n        \"\"\"Sets the cut-off frequency for the apodizing filter.\n\n        See the class documentation for the filter. The value of fc should be\n        between 0 and 0.5.\n        \"\"\"\n        parms.set_float_par(self.handle, 'FBP2D', 'fc', v)\n\n    def set_output_image_size_xy(self, xy):\n        \"\"\"Sets output image size (xy).\"\"\"\n        parms.set_int_par(self.handle, 'FBP2D', 'xy', xy)\n\n    def set_up(self, image):\n        \"\"\"Sets up the reconstructor.\"\"\"\n        try_calling(pystir.cSTIR_setupFBP2DReconstruction(\n            self.handle, image.handle))\n\n    def process(self):\n        \"\"\"Performs reconstruction.\"\"\"\n        try_calling(pystir.cSTIR_runFBP2DReconstruction(self.handle))\n\n    def get_output(self):\n        \"\"\"Returns the reconstructed image.\"\"\"\n        image = ImageData()\n        image.handle = parms.parameter_handle(self.handle, 'FBP2D', 'output')\n        check_status(image.handle)\n        return image\n\n# --- Snippet Separator ---\n\nclass KOSMAPOSLReconstructor(IterativeReconstructor):\n    \"\"\"KOSMAPOSL recontstructor class.\n\n    Class for reconstructor objects using Kernel Ordered Subsets Maximum\n    A Posteriori One Step Late reconstruction algorithm.\n\n    This class implements the iterative algorithm obtained using the Kernel\n    method (KEM) and Hybrid kernel method (HKEM). This implementation\n    corresponds to the one presented by Deidda D et al, \"Hybrid PET-MR\n    list-mode kernelized expectation maximization  reconstruction\", Inverse\n    Problems, 2019, DOI: https://doi.org/10.1088/1361-6420/ab013f.\n    However, this allows also sinogram-based reconstruction. Each voxel value\n    of the image X can be represented as a linear combination using the kernel\n    method.  If we have an image with prior information, we can construct for\n    each voxel j of the emission image a feature vector, v, using the prior\n    information. The image X can then be described using the kernel matrix\n\n    X = A*K\n\n    where K is the kernel matrix. The resulting algorithm with OSEM,\n    for example, is the following:\n\n    A^(n+1) =  A^n/(K^n * S) * K^n * P * Y/(P * K^n *A^n + S)\n\n    where kernel can be written as:\n\n    K^n = K_m * K_p;\n\n    with\n\n    K_m = exp(-(v_j - v_l)^2/(2*sigma_m^2)) *\n          exp(-(x_j - x_l)^2 /(2*sigma_dm^2))\n\n    being the MR component of the kernel and\n\n    K_p = exp(-(z_j - z_l)^2/(2*sigma_p^2)) *\n          exp(-(x_j - x_l)^2 /(2*sigma_dp^2))\n\n    is the part coming from the emission iterative update. Here, the Gaussian\n    kernel functions have been modulated by the distance between voxels in the\n    image space.\n    \"\"\"\n\n    def __init__(self, filename=''):\n        \"\"\"init.\"\"\"\n        IterativeReconstructor.__init__(self)\n        self.handle = None\n        self.image = None\n        self.name = 'KOSMAPOSL'\n        self.handle = pystir.cSTIR_objectFromFile(\n            'KOSMAPOSLReconstruction', filename)\n        check_status(self.handle)\n        self.disable_output()\n\n    def __del__(self):\n        \"\"\"del.\"\"\"\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n    def set_anatomical_prior(self, ap):\n        \"\"\"Sets anatomical prior.\"\"\"\n        assert_validity(ap, ImageData)\n        parms.set_parameter(\n            self.handle, 'KOSMAPOSL', 'anatomical_prior', ap.handle)\n\n    def set_num_neighbours(self, n):\n        \"\"\"Sets number of neighbours.\"\"\"\n        parms.set_int_par(\n            self.handle, 'KOSMAPOSL', 'num_neighbours', n)\n\n    def set_num_non_zero_features(self, n):\n        \"\"\"Sets number of non-zero features.\"\"\"\n        parms.set_int_par(\n            self.handle, 'KOSMAPOSL', 'num_non_zero_features', n)\n\n    def set_sigma_m(self, v):\n        \"\"\"Sets sigma m.\"\"\"\n        parms.set_float_par(self.handle, 'KOSMAPOSL', 'sigma_m', v)\n\n    def set_sigma_p(self, v):\n        \"\"\"Sets sigma p.\"\"\"\n        parms.set_float_par(self.handle, 'KOSMAPOSL', 'sigma_p', v)\n\n    def set_sigma_dm(self, v):\n        \"\"\"Sets sigma dm.\"\"\"\n        parms.set_float_par(self.handle, 'KOSMAPOSL', 'sigma_dm', v)\n\n    def set_sigma_dp(self, v):\n        \"\"\"Sets sigma dp.\"\"\"\n        parms.set_float_par(self.handle, 'KOSMAPOSL', 'sigma_dp', v)\n\n    def set_only_2D(self, tf):\n        \"\"\"Sets only_2D flag.\"\"\"\n        v = 1 if tf else 0\n        parms.set_int_par(self.handle, 'KOSMAPOSL', 'only_2D', v)\n\n    def set_hybrid(self, tf):\n        \"\"\"Sets use hybrid mode flag.\"\"\"\n        v = 1 if tf else 0\n        parms.set_int_par(self.handle, 'KOSMAPOSL', 'hybrid', v)\n\n    def compute_kernelised_image(self, image, alpha):\n        assert_validity(image, ImageData)\n        assert_validity(alpha, ImageData)\n        ki = ImageData()\n        ki.handle = pystir.cSTIR_computeKernelisedImage \\\n            (self.handle, image.handle, alpha.handle)\n        check_status(ki.handle)\n        return ki\n\n    def get_objective_function(self):\n        obj_fun = PoissonLogLikelihoodWithLinearModelForMean()\n        obj_fun.handle = pystir.cSTIR_parameter\\\n            (self.handle, self.name, 'objective_function')\n        check_status(obj_fun.handle)\n        return obj_fun\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that implements a user-defined Ordered Subset Maximum A Posteriori One Step Late (OSMAPOSL) reconstruction algorithm using the SIRF library. The code should accept command-line options for the raw data file, path to data files, number of subsets, number of sub-iterations, reconstruction engine, and an option to disable plots. The reconstruction algorithm should be implemented in a function that takes an image, objective function, prior, filter, number of subsets, and number of sub-iterations as parameters. The main function should handle the creation of the acquisition model, acquisition data, filter, initial image estimate, prior, and objective function. It should also call the reconstruction function and display the reconstructed image if the non-interactive option is not set. The code should handle errors and print an error message if an exception is thrown.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 29, "repo_full_name": "pyvista__pyvista", "instruction": "Generate code that uses the PyVista library to create a 3D visualization of the solar system. The code should load models of the planets, apply textures to them, and position them in a 3D space. It should also create a light source to simulate the sun. The code should then add these models to a plotter and display them. Additionally, the code should create subplots for individual planets, showing their textures. Finally, the code should create a visualization of Venus with and without its atmosphere.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def write_driver(self, nodes, read_write_info, prefix, postfix,\n                     region_name, writer=FortranWriter()):\n        # pylint: disable=too-many-arguments\n        '''This function uses the ``get_driver_as_string()`` function to get a\n        a stand-alone driver, and then writes this source code to a file. The\n        file name is derived from the region name:\n        \"driver-\"+module_name+\"_\"+region_name+\".F90\"\n\n        :param nodes: a list of nodes containing the body of the driver\n            routine.\n        :type nodes: List[:py:class:`psyclone.psyir.nodes.Node`]\n        :param read_write_info: information about all input and output \\\n            parameters.\n        :type read_write_info: :py:class:`psyclone.psyir.tools.ReadWriteInfo`\n        :param str prefix: the prefix to use for each PSyData symbol, \\\n            e.g. 'extract' as prefix will create symbols `extract_psydata`.\n        :param str postfix: a postfix that is appended to an output variable \\\n            to create the corresponding variable that stores the output \\\n            value from the kernel data file. The caller must guarantee that \\\n            no name clashes are created when adding the postfix to a variable \\\n            and that the postfix is consistent between extract code and \\\n            driver code (see 'ExtractTrans.determine_postfix()').\n        :param Tuple[str,str] region_name: an optional name to \\\n            use for this PSyData area, provided as a 2-tuple containing a \\\n            location name followed by a local name. The pair of strings \\\n            should uniquely identify a region.\n        :param writer: a backend visitor to convert PSyIR \\\n            representation to the selected language. It defaults to \\\n            the FortranWriter.\n        :type writer: \\\n            :py:class:`psyclone.psyir.backend.language_writer.LanguageWriter`\n\n        '''\n        code = self.get_driver_as_string(nodes, read_write_info, prefix,\n                                         postfix, region_name, writer=writer)\n        fll = FortLineLength()\n        code = fll.process(code)\n        if not code:\n            # This indicates an error that was already printed,\n            # so ignore it here.\n            return\n        module_name, local_name = region_name\n        with open(f\"driver-{module_name}-{local_name}.F90\", \"w\",\n                  encoding='utf-8') as out:\n            out.write(code)\n\n# --- Snippet Separator ---\n\nclass DotProduct2CodeTrans(Operator2CodeTrans):\n    '''Provides a transformation from a PSyIR DOT_PRODUCT Operator node to\n    equivalent code in a PSyIR tree. Validity checks are also\n    performed.\n\n    If ``R`` is a scalar and ``A``, and ``B`` have dimension ``N``,\n    The transformation replaces:\n\n    .. code-block:: fortran\n\n        R = ... DOT_PRODUCT(A,B) ...\n\n    with the following code:\n\n    .. code-block:: fortran\n\n        TMP = 0.0\n        do I=1,N\n            TMP = TMP + A(i)*B(i)\n        R = ... TMP ...\n\n    For example:\n\n    >>> from psyclone.psyir.backend.fortran import FortranWriter\n    >>> from psyclone.psyir.frontend.fortran import FortranReader\n    >>> from psyclone.psyir.nodes import BinaryOperation\n    >>> from psyclone.psyir.transformations import DotProduct2CodeTrans\n    >>> code = (\"subroutine dot_product_test(v1,v2)\\\\n\"\n    ...         \"real,intent(in) :: v1(10), v2(10)\\\\n\"\n    ...         \"real :: result\\\\n\"\n    ...         \"result = dot_product(v1,v2)\\\\n\"\n    ...         \"end subroutine\\\\n\")\n    >>> psyir = FortranReader().psyir_from_source(code)\n    >>> trans = DotProduct2CodeTrans()\n    >>> trans.apply(psyir.walk(BinaryOperation)[0])\n    >>> print(FortranWriter()(psyir))\n    subroutine dot_product_test(v1, v2)\n      real, dimension(10), intent(in) :: v1\n      real, dimension(10), intent(in) :: v2\n      real :: result\n      integer :: i\n      real :: res_dot_product\n    <BLANKLINE>\n      res_dot_product = 0.0\n      do i = 1, 10, 1\n        res_dot_product = res_dot_product + v1(i) * v2(i)\n      enddo\n      result = res_dot_product\n    <BLANKLINE>\n    end subroutine dot_product_test\n    <BLANKLINE>\n\n    '''\n    def __init__(self):\n        super().__init__()\n        self._operator_name = \"DOTPRODUCT\"\n        self._classes = (BinaryOperation,)\n        self._operators = (BinaryOperation.Operator.DOT_PRODUCT,)\n\n    def validate(self, node, options=None):\n        '''Perform checks to ensure that it is valid to apply the\n        DotProduct2CodeTran transformation to the supplied node.\n\n        Note, this validation does not check for invalid argument\n        combinations to dot_product (e.g. different precision or\n        different datatypes) as that should have already been picked\n        up when creating the PSyIR.\n\n        :param node: the node that is being checked.\n        :type node: :py:class:`psyclone.psyir.nodes.BinaryOperation`\n        :param options: a dictionary with options for transformations.\n        :type options: dict of str:str or None\n\n        :raises TransformationError: if one of the arguments is not a \\\n            Reference node.\n        :raises TransformationError: if an argument does not use array \\\n            slice notation and is not a 1d array.\n        :raises TransformationError: if an argument uses array slice \\\n            notation but the array slice is not for the first dimension of \\\n            the array.\n        :raises TransformationError: if an argument uses array slice \\\n            notation but it is not for the full range of the dimension.\n\n        '''\n        super().validate(node, options)\n\n        # Both arguments should be references (or array references)\n        for arg in node.children:\n            if arg.__class__ not in [Reference, ArrayReference]:\n                raise TransformationError(\n                    f\"The DotProduct2CodeTrans transformation only supports \"\n                    f\"the transformation of a dotproduct intrinsic if its \"\n                    f\"arguments are plain arrays, but found \"\n                    f\"{arg.debug_string()} in {node.debug_string()}.\")\n\n        for arg in node.children:\n            # The argument should be a 1D array if the argument does\n            # not provide any array slice information (i.e. it is a\n            # Reference)\n            if arg.__class__ is Reference:\n                symbol = arg.symbol\n                # This symbol should be a 1D array\n                if not (isinstance(symbol, DataSymbol) and symbol.is_array and\n                        len(symbol.shape) == 1):\n                    raise TransformationError(\n                        f\"The DotProduct2CodeTrans transformation only \"\n                        f\"supports the transformation of a dotproduct \"\n                        f\"intrinsic with an argument not containing an array \"\n                        f\"slice if the argument is a 1D array, but found \"\n                        f\"{arg.debug_string()} with {len(symbol.shape)} \"\n                        f\"dimensions in {node.debug_string()}.\")\n\n        for arg in node.children:\n            # If the argument does provide array slice information\n            # then check the array slice is in the first dimension of\n            # the array and that the slice is for the full range of that\n            # dimension i.e. uses a ':'.\n            if isinstance(arg, ArrayReference):\n                if not isinstance(arg.indices[0], Range):\n                    raise TransformationError(\n                        f\"The DotProduct2CodeTrans transformation only \"\n                        f\"supports the transformation of a dotproduct \"\n                        f\"intrinsic with an argument containing an array \"\n                        f\"slice if the array slice is for the 1st dimension \"\n                        f\"of the array, but found {arg.debug_string()} in \"\n                        f\"{node.debug_string()}.\")\n\n                if not arg.is_full_range(0):\n                    raise TransformationError(\n                        f\"The DotProduct2CodeTrans transformation only \"\n                        f\"supports the transformation of a dotproduct \"\n                        f\"intrinsic with an argument containing an array \"\n                        f\"slice if the argument is for the 1st dimension \"\n                        f\"of the array and is for the full range of that \"\n                        f\"dimension, but found {arg.debug_string()} in \"\n                        f\"{node.debug_string()}.\")\n\n        # Both arguments should be real (as other intrinsic datatypes\n        # are not supported).\n        for arg in node.children:\n            if arg.symbol.datatype.intrinsic != ScalarType.Intrinsic.REAL:\n                raise TransformationError(\n                    f\"The DotProduct2CodeTrans transformation only supports \"\n                    f\"arrays of real data, but found {arg.debug_string()} of \"\n                    f\"type {arg.symbol.datatype.intrinsic.name} in \"\n                    f\"{node.debug_string()}.\")\n\n    def apply(self, node, options=None):\n        '''Apply the DOT_PRODUCT intrinsic conversion transformation to the\n        specified node. This node must be a DOT_PRODUCT\n        BinaryOperation.  If the transformation is successful then an\n        assignment which includes a DOT_PRODUCT BinaryOperation node\n        is converted to equivalent inline code.\n\n        :param node: a DOT_PRODUCT Binary-Operation node.\n        :type node: :py:class:`psyclone.psyir.nodes.BinaryOperation`\n        :param options: a dictionary with options for transformations.\n        :type options: dict of str:str or None\n\n        '''\n        self.validate(node)\n\n        assignment = node.ancestor(Assignment)\n        vector1 = node.children[0]\n        vector2 = node.children[1]\n        symbol_table = node.ancestor(Routine).symbol_table\n\n        # Create new i loop iterator.\n        i_loop_symbol = symbol_table.new_symbol(\n            \"i\", symbol_type=DataSymbol, datatype=INTEGER_TYPE)\n\n        # Create temporary result variable. Use the datatype of one of\n        # the arguments. We can do this as the validate method only\n        # allows plain real arrays.\n        vector1_datatype = vector1.symbol.datatype\n        datatype = ScalarType(\n            vector1_datatype.intrinsic, vector1_datatype.precision)\n        symbol_res_var = symbol_table.new_symbol(\n            \"res_dot_product\", symbol_type=DataSymbol,\n            datatype=datatype)\n\n        # Replace operation with the temporary result variable.\n        result_ref = Reference(symbol_res_var)\n        node.replace_with(result_ref)\n\n        # Create \"vector1(i)\"\n        vector1_dims = [Reference(i_loop_symbol)]\n        if len(vector1.children) > 1:\n            # Add any additional dimensions (in case of an array slice)\n            for child in vector1.children[1:]:\n                vector1_dims.append(child.copy())\n        vector1_array_reference = ArrayReference.create(\n            vector1.symbol, vector1_dims)\n        # Create \"vector2(i)\"\n        vector2_dims = [Reference(i_loop_symbol)]\n        if len(vector2.children) > 1:\n            # Add any additional dimensions (in case of an array slice)\n            for child in vector2.children[1:]:\n                vector2_dims.append(child.copy())\n        vector2_array_reference = ArrayReference.create(\n            vector2.symbol, vector2_dims)\n        # Create \"vector1(i) * vector2(i)\"\n        multiply = BinaryOperation.create(\n            BinaryOperation.Operator.MUL, vector1_array_reference,\n            vector2_array_reference)\n        # Create \"result + vector1(i) * vector2(i)\"\n        rhs = BinaryOperation.create(\n            BinaryOperation.Operator.ADD, result_ref.copy(), multiply)\n        # Create \"result = result + vector1(i) * vector2(i)\"\n        assign = Assignment.create(result_ref.copy(), rhs)\n        # Work out the loop bounds\n        lower_bound, upper_bound, step = _get_array_bound(vector1, vector2)\n        # Create i loop and add the above code as a child\n        iloop = Loop.create(i_loop_symbol, lower_bound.copy(),\n                            upper_bound.copy(), step.copy(), [assign])\n        # Create \"result = 0.0\"\n        assign = Assignment.create(result_ref.copy(),\n                                   Literal(\"0.0\", REAL_TYPE))\n        # Add the initialisation and loop nodes into the PSyIR tree\n        assignment.parent.children.insert(assignment.position, assign)\n        assignment.parent.children.insert(assignment.position, iloop)\n\n# --- Snippet Separator ---\n\ndef write_driver(self, nodes, read_write_info, prefix, postfix,\n                     region_name, writer=FortranWriter()):\n        # pylint: disable=too-many-arguments\n        '''This function uses the `get_driver_as_string()` function to get a\n        a stand-alone driver, and then writes this source code to a file. The\n        file name is derived from the region name:\n        \"driver-\"+module_name+\"_\"+region_name+\".f90\"\n\n        :param nodes: a list of nodes.\n        :type nodes: List[:py:class:`psyclone.psyir.nodes.Node`]\n        :param read_write_info: information about all input and output \\\n            parameters.\n        :type read_write_info: :py:class:`psyclone.psyir.tools.ReadWriteInfo`\n        :param str prefix: the prefix to use for each PSyData symbol, \\\n            e.g. 'extract' as prefix will create symbols `extract_psydata`.\n        :param str postfix: a postfix that is appended to an output variable \\\n            to create the corresponding variable that stores the output \\\n            value from the kernel data file. The caller must guarantee that \\\n            no name clashes are created when adding the postfix to a variable \\\n            and that the postfix is consistent between extract code and \\\n            driver code (see 'ExtractTrans.determine_postfix()').\n        :param (str,str) region_name: an optional name to \\\n            use for this PSyData area, provided as a 2-tuple containing a \\\n            location name followed by a local name. The pair of strings \\\n            should uniquely identify a region.\n        :param language_writer: a backend visitor to convert PSyIR \\\n            representation to the selected language. It defaults to \\\n            the FortranWriter.\n        :type language_writer: \\\n            :py:class:`psyclone.psyir.backend.language_writer.LanguageWriter`\n\n        '''\n        code = self.get_driver_as_string(nodes, read_write_info, prefix,\n                                         postfix, region_name, writer=writer)\n        module_name, local_name = region_name\n        with open(f\"driver-{module_name}-{local_name}.f90\", \"w\",\n                  encoding='utf-8') as out:\n            out.write(code)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the PyVista library to create a 3D visualization of the solar system. The code should load models of the planets, apply textures to them, and position them in a 3D space. It should also create a light source to simulate the sun. The code should then add these models to a plotter and display them. Additionally, the code should create subplots for individual planets, showing their textures. Finally, the code should create a visualization of Venus with and without its atmosphere.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 30, "repo_full_name": "explosion__thinc", "instruction": "Generate code that trains a transformer tagging model using Huggingface's Transformers and Thinc libraries. The code should include a configuration for the model, optimizer, learning rate, and training parameters. It should define a main function that checks for GPU usage, loads the configuration, resolves the configuration to construct objects, loads a dataset, initializes the model, and trains the model over a specified number of epochs. The code should also define a dataclass to hold the output of the Huggingface 'batch_encode_plus' method, and functions to create a transformer tagger model, a transformer tokenizer, and a transformer model. It should also include functions to convert transformer inputs and outputs, evaluate sequences, and group pairs of sequences into minibatches. Finally, the code should run the main function if the script is run as the main program.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def _fit_transformer(self, y):\n        \"\"\"Check transformer and fit transformer.\n\n        Create the default transformer, fit it and make additional inverse\n        check on a subset (optional).\n\n        \"\"\"\n        if self.transformer is not None and (\n            self.func is not None or self.inverse_func is not None\n        ):\n            raise ValueError(\n                \"'transformer' and functions 'func'/'inverse_func' cannot both be set.\"\n            )\n        elif self.transformer is not None:\n            self.transformer_ = clone(self.transformer)\n        else:\n            if self.func is not None and self.inverse_func is None:\n                raise ValueError(\n                    \"When 'func' is provided, 'inverse_func' must also be provided\"\n                )\n            self.transformer_ = FunctionTransformer(\n                func=self.func,\n                inverse_func=self.inverse_func,\n                validate=True,\n                check_inverse=self.check_inverse,\n            )\n        # XXX: sample_weight is not currently passed to the\n        # transformer. However, if transformer starts using sample_weight, the\n        # code should be modified accordingly. At the time to consider the\n        # sample_prop feature, it is also a good use case to be considered.\n        self.transformer_.fit(y)\n        if self.check_inverse:\n            idx_selected = slice(None, None, max(1, y.shape[0] // 10))\n            y_sel = _safe_indexing(y, idx_selected)\n            y_sel_t = self.transformer_.transform(y_sel)\n            if not np.allclose(y_sel, self.transformer_.inverse_transform(y_sel_t)):\n                warnings.warn(\n                    (\n                        \"The provided functions or transformer are\"\n                        \" not strictly inverse of each other. If\"\n                        \" you are sure you want to proceed regardless\"\n                        \", set 'check_inverse=False'\"\n                    ),\n                    UserWarning,\n                )\n\n# --- Snippet Separator ---\n\nclass OutputRenderer(Generic[OT], ABC):\n    \"\"\"\n    Output Renderer\n\n    Transforms the output (of type `IT`) of an app-supplied output value function\n    (`value_fn`) into type (`OT`). This transformed value is then sent to be an\n    :class:`~shiny.Outputs` output value.\n\n    When the `.__call__` method is invoked, the transform function (`transform_fn`)\n    (typically defined by package authors) is invoked. The wrapping classes\n    (:class:`~shiny.render.transformer.OutputRendererSync` and\n    :class:`~shiny.render.transformer.OutputRendererAsync`) will enforce whether the\n    transform function is synchronous or asynchronous independent of the awaitable\n    syntax.\n\n    The transform function (`transform_fn`) is given `meta` information\n    (:class:`~shiny.render.transformer.TranformerMetadata`), the (app-supplied) value\n    function (`ValueFn[IT]`), and any keyword arguments supplied to the render decorator\n    (`P`). For consistency, the first two parameters have been (arbitrarily) implemented\n    as `_meta` and `_fn`.\n\n    Typing\n    ------\n    * `IT`\n        * The type returned by the app-supplied output value function (`value_fn`). This\n          value should contain a `None` value to conform to the convention of app authors\n          being able to return `None` to display nothing in the rendered output. Note that\n          in many cases but not all, `IT` and `OT` will be the same.\n    * `OT`\n        * The type of the object returned by the transform function (`transform_fn`). This\n          value should contain a `None` value to conform to display nothing in the\n          rendered output.\n    * `P`\n        * The parameter specification defined by the transform function (`transform_fn`).\n          It should **not** contain any `*args`. All keyword arguments should have a type\n          and default value.\n\n\n    See Also\n    --------\n    * :class:`~shiny.render.transformer.OutputRendererSync`\n    * :class:`~shiny.render.transformer.OutputRendererAsync`\n    \"\"\"\n\n    @abstractmethod\n    def __call__(self) -> OT:\n        \"\"\"\n        Executes the output renderer as a function. Must be implemented by subclasses.\n        \"\"\"\n        ...\n\n    def __init__(\n        self,\n        *,\n        value_fn: ValueFn[IT],\n        transform_fn: TransformFn[IT, P, OT],\n        params: TransformerParams[P],\n    ) -> None:\n        \"\"\"\n        Parameters\n        ----------\n        value_fn\n            App-provided output value function. It should return an object of type `IT`.\n        transform_fn\n            Package author function that transforms an object of type `IT` into type\n            `OT`. The `params` will used as variadic keyword arguments. This method\n            should only use `await` syntax when the value function (`ValueFn[IT]`) is\n            awaitable. If the value function is not awaitable (a _synchronous_\n            function), then the function should execute synchronously.\n        params\n            App-provided parameters for the transform function (`transform_fn`).\n\n        \"\"\"\n\n        # Copy over function name as it is consistent with how Session and Output\n        # retrieve function names\n        self.__name__ = value_fn.__name__\n\n        if not is_async_callable(transform_fn):\n            raise TypeError(\n                self.__class__.__name__\n                + \" requires an async tranformer function (`transform_fn`)\"\n            )\n\n        self._value_fn = value_fn\n        self._transformer = transform_fn\n        self._params = params\n\n    def _set_metadata(self, session: Session, name: str) -> None:\n        \"\"\"\n        When `Renderer`s are assigned to Output object slots, this method is used to\n        pass along Session and name information.\n        \"\"\"\n        self._session: Session = session\n        self._name: str = name\n\n    def _meta(self) -> TransformerMetadata:\n        \"\"\"\n        Returns a named tuple of values: `session` (the :class:`~shiny.Session` object),\n        and `name` (the name of the output being rendered)\n        \"\"\"\n        return TransformerMetadata(\n            session=self._session,\n            name=self._name,\n        )\n\n    async def _run(self) -> OT:\n        \"\"\"\n        Executes the (async) tranform function\n\n        The transform function will receive the following arguments: meta information of\n        type :class:`~shiny.render.transformer.TransformerMetadata`, an app-defined\n        render function of type :class:`~shiny.render.RenderFnAsync`, and `*args` and\n        `**kwargs` of type `P`.\n\n        Note: `*args` will always be empty as it is an expansion of\n        :class:`~shiny.render.transformer.TransformerParams` which does not allow positional arguments.\n        `*args` is required to use with `**kwargs` when using\n        `typing.ParamSpec`.\n        \"\"\"\n        ret = await self._transformer(\n            # TransformerMetadata\n            self._meta(),\n            # Callable[[], Awaitable[IT]] | Callable[[], IT]\n            self._value_fn,\n            # P\n            *self._params.args,\n            **self._params.kwargs,\n        )\n        return ret\n\n# --- Snippet Separator ---\n\nclass TransformedTargetRegressor(RegressorMixin, BaseEstimator):\n    \"\"\"Meta-estimator to regress on a transformed target.\n\n    Useful for applying a non-linear transformation to the target `y` in\n    regression problems. This transformation can be given as a Transformer\n    such as the :class:`~sklearn.preprocessing.QuantileTransformer` or as a\n    function and its inverse such as `np.log` and `np.exp`.\n\n    The computation during :meth:`fit` is::\n\n        regressor.fit(X, func(y))\n\n    or::\n\n        regressor.fit(X, transformer.transform(y))\n\n    The computation during :meth:`predict` is::\n\n        inverse_func(regressor.predict(X))\n\n    or::\n\n        transformer.inverse_transform(regressor.predict(X))\n\n    Read more in the :ref:`User Guide <transformed_target_regressor>`.\n\n    .. versionadded:: 0.20\n\n    Parameters\n    ----------\n    regressor : object, default=None\n        Regressor object such as derived from\n        :class:`~sklearn.base.RegressorMixin`. This regressor will\n        automatically be cloned each time prior to fitting. If `regressor is\n        None`, :class:`~sklearn.linear_model.LinearRegression` is created and used.\n\n    transformer : object, default=None\n        Estimator object such as derived from\n        :class:`~sklearn.base.TransformerMixin`. Cannot be set at the same time\n        as `func` and `inverse_func`. If `transformer is None` as well as\n        `func` and `inverse_func`, the transformer will be an identity\n        transformer. Note that the transformer will be cloned during fitting.\n        Also, the transformer is restricting `y` to be a numpy array.\n\n    func : function, default=None\n        Function to apply to `y` before passing to :meth:`fit`. Cannot be set\n        at the same time as `transformer`. The function needs to return a\n        2-dimensional array. If `func is None`, the function used will be the\n        identity function.\n\n    inverse_func : function, default=None\n        Function to apply to the prediction of the regressor. Cannot be set at\n        the same time as `transformer`. The function needs to return a\n        2-dimensional array. The inverse function is used to return\n        predictions to the same space of the original training labels.\n\n    check_inverse : bool, default=True\n        Whether to check that `transform` followed by `inverse_transform`\n        or `func` followed by `inverse_func` leads to the original targets.\n\n    Attributes\n    ----------\n    regressor_ : object\n        Fitted regressor.\n\n    transformer_ : object\n        Transformer used in :meth:`fit` and :meth:`predict`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`. Only defined if the\n        underlying regressor exposes such an attribute when fit.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    sklearn.preprocessing.FunctionTransformer : Construct a transformer from an\n        arbitrary callable.\n\n    Notes\n    -----\n    Internally, the target `y` is always converted into a 2-dimensional array\n    to be used by scikit-learn transformers. At the time of prediction, the\n    output will be reshaped to a have the same number of dimensions as `y`.\n\n    Examples\n    --------\n    >>> import numpy as np\n    >>> from sklearn.linear_model import LinearRegression\n    >>> from sklearn.compose import TransformedTargetRegressor\n    >>> tt = TransformedTargetRegressor(regressor=LinearRegression(),\n    ...                                 func=np.log, inverse_func=np.exp)\n    >>> X = np.arange(4).reshape(-1, 1)\n    >>> y = np.exp(2 * X).ravel()\n    >>> tt.fit(X, y)\n    TransformedTargetRegressor(...)\n    >>> tt.score(X, y)\n    1.0\n    >>> tt.regressor_.coef_\n    array([2.])\n\n    For a more detailed example use case refer to\n    :ref:`sphx_glr_auto_examples_compose_plot_transformed_target.py`.\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"regressor\": [HasMethods([\"fit\", \"predict\"]), None],\n        \"transformer\": [HasMethods(\"transform\"), None],\n        \"func\": [callable, None],\n        \"inverse_func\": [callable, None],\n        \"check_inverse\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        regressor=None,\n        *,\n        transformer=None,\n        func=None,\n        inverse_func=None,\n        check_inverse=True,\n    ):\n        self.regressor = regressor\n        self.transformer = transformer\n        self.func = func\n        self.inverse_func = inverse_func\n        self.check_inverse = check_inverse\n\n    def _fit_transformer(self, y):\n        \"\"\"Check transformer and fit transformer.\n\n        Create the default transformer, fit it and make additional inverse\n        check on a subset (optional).\n\n        \"\"\"\n        if self.transformer is not None and (\n            self.func is not None or self.inverse_func is not None\n        ):\n            raise ValueError(\n                \"'transformer' and functions 'func'/'inverse_func' cannot both be set.\"\n            )\n        elif self.transformer is not None:\n            self.transformer_ = clone(self.transformer)\n        else:\n            if self.func is not None and self.inverse_func is None:\n                raise ValueError(\n                    \"When 'func' is provided, 'inverse_func' must also be provided\"\n                )\n            self.transformer_ = FunctionTransformer(\n                func=self.func,\n                inverse_func=self.inverse_func,\n                validate=True,\n                check_inverse=self.check_inverse,\n            )\n        # XXX: sample_weight is not currently passed to the\n        # transformer. However, if transformer starts using sample_weight, the\n        # code should be modified accordingly. At the time to consider the\n        # sample_prop feature, it is also a good use case to be considered.\n        self.transformer_.fit(y)\n        if self.check_inverse:\n            idx_selected = slice(None, None, max(1, y.shape[0] // 10))\n            y_sel = _safe_indexing(y, idx_selected)\n            y_sel_t = self.transformer_.transform(y_sel)\n            if not np.allclose(y_sel, self.transformer_.inverse_transform(y_sel_t)):\n                warnings.warn(\n                    (\n                        \"The provided functions or transformer are\"\n                        \" not strictly inverse of each other. If\"\n                        \" you are sure you want to proceed regardless\"\n                        \", set 'check_inverse=False'\"\n                    ),\n                    UserWarning,\n                )\n\n    @_fit_context(\n        # TransformedTargetRegressor.regressor/transformer are not validated yet.\n        prefer_skip_nested_validation=False\n    )\n    def fit(self, X, y, **fit_params):\n        \"\"\"Fit the model according to the given training data.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Training vector, where `n_samples` is the number of samples and\n            `n_features` is the number of features.\n\n        y : array-like of shape (n_samples,)\n            Target values.\n\n        **fit_params : dict\n            Parameters passed to the `fit` method of the underlying\n            regressor.\n\n        Returns\n        -------\n        self : object\n            Fitted estimator.\n        \"\"\"\n        if y is None:\n            raise ValueError(\n                f\"This {self.__class__.__name__} estimator \"\n                \"requires y to be passed, but the target y is None.\"\n            )\n        y = check_array(\n            y,\n            input_name=\"y\",\n            accept_sparse=False,\n            force_all_finite=True,\n            ensure_2d=False,\n            dtype=\"numeric\",\n            allow_nd=True,\n        )\n\n        # store the number of dimension of the target to predict an array of\n        # similar shape at predict\n        self._training_dim = y.ndim\n\n        # transformers are designed to modify X which is 2d dimensional, we\n        # need to modify y accordingly.\n        if y.ndim == 1:\n            y_2d = y.reshape(-1, 1)\n        else:\n            y_2d = y\n        self._fit_transformer(y_2d)\n\n        # transform y and convert back to 1d array if needed\n        y_trans = self.transformer_.transform(y_2d)\n        # FIXME: a FunctionTransformer can return a 1D array even when validate\n        # is set to True. Therefore, we need to check the number of dimension\n        # first.\n        if y_trans.ndim == 2 and y_trans.shape[1] == 1:\n            y_trans = y_trans.squeeze(axis=1)\n\n        if self.regressor is None:\n            from ..linear_model import LinearRegression\n\n            self.regressor_ = LinearRegression()\n        else:\n            self.regressor_ = clone(self.regressor)\n\n        self.regressor_.fit(X, y_trans, **fit_params)\n\n        if hasattr(self.regressor_, \"feature_names_in_\"):\n            self.feature_names_in_ = self.regressor_.feature_names_in_\n\n        return self\n\n    def predict(self, X, **predict_params):\n        \"\"\"Predict using the base regressor, applying inverse.\n\n        The regressor is used to predict and the `inverse_func` or\n        `inverse_transform` is applied before returning the prediction.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Samples.\n\n        **predict_params : dict of str -> object\n            Parameters passed to the `predict` method of the underlying\n            regressor.\n\n        Returns\n        -------\n        y_hat : ndarray of shape (n_samples,)\n            Predicted values.\n        \"\"\"\n        check_is_fitted(self)\n        pred = self.regressor_.predict(X, **predict_params)\n        if pred.ndim == 1:\n            pred_trans = self.transformer_.inverse_transform(pred.reshape(-1, 1))\n        else:\n            pred_trans = self.transformer_.inverse_transform(pred)\n        if (\n            self._training_dim == 1\n            and pred_trans.ndim == 2\n            and pred_trans.shape[1] == 1\n        ):\n            pred_trans = pred_trans.squeeze(axis=1)\n\n        return pred_trans\n\n    def _more_tags(self):\n        regressor = self.regressor\n        if regressor is None:\n            from ..linear_model import LinearRegression\n\n            regressor = LinearRegression()\n\n        return {\n            \"poor_score\": True,\n            \"multioutput\": _safe_tags(regressor, key=\"multioutput\"),\n        }\n\n    @property\n    def n_features_in_(self):\n        \"\"\"Number of features seen during :term:`fit`.\"\"\"\n        # For consistency with other estimators we raise a AttributeError so\n        # that hasattr() returns False the estimator isn't fitted.\n        try:\n            check_is_fitted(self)\n        except NotFittedError as nfe:\n            raise AttributeError(\n                \"{} object has no n_features_in_ attribute.\".format(\n                    self.__class__.__name__\n                )\n            ) from nfe\n\n        return self.regressor_.n_features_in_\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that trains a transformer tagging model using Huggingface's Transformers and Thinc libraries. The code should include a configuration for the model, optimizer, learning rate, and training parameters. It should define a main function that checks for GPU usage, loads the configuration, resolves the configuration to construct objects, loads a dataset, initializes the model, and trains the model over a specified number of epochs. The code should also define a dataclass to hold the output of the Huggingface 'batch_encode_plus' method, and functions to create a transformer tagger model, a transformer tokenizer, and a transformer model. It should also include functions to convert transformer inputs and outputs, evaluate sequences, and group pairs of sequences into minibatches. Finally, the code should run the main function if the script is run as the main program.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 31, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that uses the pyaedt library to create a flex cable CPWG (coplanar waveguide with ground) in HFSS. The code should first import necessary libraries and set the non-graphical mode. Then, it should launch AEDT in a specified version and solution type, and set some properties such as material override, automatically use causal materials, open region, model units, and initial mesh. \n\nNext, the code should define variables for the flex cable CPWG, including total length, theta, radius, width, height, spacing, ground width, and ground thickness. It should also define a function to create a bending based on the curvature radius and extension.\n\nThe code should then draw a signal line and a ground line to create a bent signal wire and two bent ground wires respectively. It should also draw a dielectric to create a dielectric cable, and create bottom metals. \n\nAfterwards, the code should create port interfaces (PEC enclosures) and a Perfect E boundary condition. It should also create ports and a setup and sweep with specified properties. \n\nFinally, the code should plot the model and release AEDT.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def create_read_in_code(program, psy_data, read_write_info, postfix):\n        '''This function creates the code that reads in the NetCDF file\n        produced during extraction. For each:\n\n        - variable that is read-only, it will declare the symbol and add code\n          that reads in the variable using the PSyData library.\n        - variable that is read and written, it will create code to read in the\n          variable that is read, and create a new variable with the same name\n          and \"_post\" added which is read in to store the values from the\n          NetCDF file after the instrumented region was executed. In the end,\n          the variable that was read and written should have the same value\n          as the corresponding \"_post\" variable.\n        - variable that is written only, it will create a variable with \"_post\"\n          as postfix that reads in the output data from the NetCDF file. It\n          then also declares a variable without postfix (which will be the\n          parameter to the function), allocates it based on the shape of\n          the corresponding \"_post\" variable, and initialises it with 0.\n\n        :param program: the PSyIR Routine to which any code must \\\n            be added. It also contains the symbol table to be used.\n        :type program: :py:class:`psyclone.psyir.nodes.Routine`\n        :param psy_data: the PSyData symbol to be used.\n        :type psy_data: :py:class:`psyclone.psyir.symbols.DataSymbol`\n        :param read_write_info: information about all input and output \\\n            parameters.\n        :type read_write_info: :py:class:`psyclone.psyir.tools.ReadWriteInfo`\n        :param str postfix: a postfix that is added to a variable to \\\n            create the corresponding variable that stores the output \\\n            value from the kernel data file.\n\n        :returns: a list with all output parameters, i.e. variables that \\\n            need to be verified after executing the kernel. Each entry is \\\n            a 2-tuple containing the symbol of the computed variable, and \\\n            the symbol of the variable that contains the value read from \\\n            the file.\n        :rtype: list of 2-tuples of \\\n            :py:class:`psyclone.psyir.symbols.Symbol`\n\n        '''\n        symbol_table = program.scope.symbol_table\n        read_var = f\"{psy_data.name}%ReadVariable\"\n\n        # Collect all output symbols to later create the tests for\n        # correctness. This list stores 2-tuples: first one the\n        # variable that stores the output from the kernel, the second\n        # one the variable that stores the output values read from the\n        # file. The content of these two variables should be identical\n        # at the end.\n        output_symbols = []\n\n        # First handle variables that are read:\n        # -------------------------------------\n        for signature in read_write_info.signatures_read:\n            # Find the right symbol for the variable. Note that all variables\n            # in the input and output list have been detected as being used\n            # when the variable accesses were analysed. Therefore, these\n            # variables have References, and will already have been declared\n            # in the symbol table (in add_all_kernel_symbols).\n            sig_str = str(signature)\n            sym = symbol_table.lookup_with_tag(sig_str)\n            name_lit = Literal(sig_str, CHARACTER_TYPE)\n            ExtractDriverCreator.add_call(program, read_var,\n                                          [name_lit, Reference(sym)])\n\n        # Then handle all variables that are written (note that some\n        # variables might be read and written)\n        for signature in read_write_info.signatures_written:\n            # Find the right symbol for the variable. Note that all variables\n            # in the input and output list have been detected as being used\n            # when the variable accesses were analysed. Therefore, these\n            # variables have References, and will already have been declared\n            # in the symbol table (in add_all_kernel_symbols).\n            sig_str = str(signature)\n            sym = symbol_table.lookup_with_tag(sig_str)\n\n            # The variable is written (and maybe read as well)\n            # ------------------------------------------------\n            # Declare a 'post' variable of the same type and\n            # read in its value.\n            post_name = sig_str+postfix\n            post_sym = symbol_table.new_symbol(post_name,\n                                               symbol_type=DataSymbol,\n                                               datatype=sym.datatype)\n            ExtractDriverCreator.add_call(program, read_var,\n                                          [Literal(post_name, CHARACTER_TYPE),\n                                           Reference(post_sym)])\n\n            # Now if a variable is written to, but not read, the variable\n            # is not allocated. So we need to allocate it and set it to 0.\n            if not read_write_info.is_read(signature):\n                if isinstance(post_sym.datatype, ArrayType):\n                    alloc = IntrinsicCall.create(\n                        IntrinsicCall.Intrinsic.ALLOCATE,\n                        [Reference(sym), (\"mold\", Reference(post_sym))])\n                    program.addchild(alloc)\n                set_zero = Assignment.create(Reference(sym),\n                                             Literal(\"0\", INTEGER_TYPE))\n                program.addchild(set_zero)\n            output_symbols.append((sym, post_sym))\n        return output_symbols\n\n# --- Snippet Separator ---\n\ndef plot_parameter(self, location, parameter, formatter='.0f', **kwargs):\n        \"\"\"At the specified location in the station model plot a set of values.\n\n        This specifies that at the offset `location`, the data in `parameter` should be\n        plotted. The conversion of the data values to a string is controlled by ``formatter``.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        If something has already been plotted at this location, it will be replaced.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this parameter. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions; increments\n            are multiplied by `spacing` to give offsets in x and y relative to the center.\n        parameter : array-like\n            The numeric values that should be plotted\n        formatter : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        plot_units: `pint.unit`\n            Units to plot in (performing conversion if necessary). Defaults to given units.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n\n        See Also\n        --------\n        plot_barb, plot_symbol, plot_text\n\n        \"\"\"\n        # If plot_units specified, convert the data to those units\n        plotting_units = kwargs.pop('plot_units', None)\n        parameter = self._scalar_plotting_units(parameter, plotting_units)\n        if hasattr(parameter, 'units'):\n            parameter = parameter.magnitude\n        text = self._to_string_list(parameter, formatter)\n        return self.plot_text(location, text, **kwargs)\n\n# --- Snippet Separator ---\n\ndef create_perfecte_from_objects(\n        self, startobj, endobject, axisdir=0, sourcename=None, is_infinite_gnd=False, bound_on_plane=True\n    ):\n        \"\"\"Create a Perfect E taking the closest edges of two objects.\n\n        Parameters\n        ----------\n        startobj :\n            Starting object for the integration line.\n        endobject :\n           Ending object for the integration line.\n        axisdir : int or :class:`pyaedt.application.Analysis.Analysis.AxisDir`, optional\n            Position of the port. It should be one of the values for\n            ``Application.AxisDir``, which are: ``XNeg``, ``YNeg``,\n            ``ZNeg``, ``XPos``, ``YPos``, and ``ZPos``.  The default\n            is ``Application.AxisDir.XNeg``.\n        sourcename : str, optional\n            Perfect E name. The default is ``None``.\n        is_infinite_gnd : bool, optional\n            Whether the Perfect E is an infinite ground. The default is ``False``.\n        bound_on_plane : bool, optional\n            Whether to create the Perfect E on the plane orthogonal to\n            ``AxisDir``. The default is ``True``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject` or bool\n            Boundary object if successful, ``False`` otherwise.\n\n        References\n        ----------\n\n        >>> oModule.AssignPerfectE\n\n        Examples\n        --------\n\n        Create two boxes that will be used to create a Perfect E named ``'PerfectE'``.\n\n        >>> box1 = hfss.modeler.create_box([0,0,0], [10,10,5],\n        ...                                \"perfect1\", \"Copper\")\n        >>> box2 = hfss.modeler.create_box([0, 0, 10], [10, 10, 5],\n        ...                                \"perfect2\", \"copper\")\n        >>> perfect_e = hfss.create_perfecte_from_objects(\"perfect1\", \"perfect2\",\n        ...                                               hfss.AxisDir.ZNeg, \"PerfectE\")\n        pyaedt info: Connection Correctly created\n        >>> type(perfect_e)\n        <class 'pyaedt.modules.Boundary.BoundaryObject'>\n\n        \"\"\"\n\n        if not self.modeler.does_object_exists(startobj) or not self.modeler.does_object_exists(endobject):\n            self.logger.error(\"One or both objects do not exist. Check and retry.\")\n            return False\n        if self.solution_type in [\"Modal\", \"Terminal\", \"Transient Network\"]:\n            sheet_name, point0, point1 = self.modeler._create_sheet_from_object_closest_edge(\n                startobj, endobject, axisdir, bound_on_plane\n            )\n\n            if not sourcename:\n                sourcename = generate_unique_name(\"PerfE\")\n            elif sourcename in self.modeler.get_boundaries_name():\n                sourcename = generate_unique_name(sourcename)\n            return self.create_boundary(self.BoundaryType.PerfectE, sheet_name, sourcename, is_infinite_gnd)\n        return False\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the pyaedt library to create a flex cable CPWG (coplanar waveguide with ground) in HFSS. The code should first import necessary libraries and set the non-graphical mode. Then, it should launch AEDT in a specified version and solution type, and set some properties such as material override, automatically use causal materials, open region, model units, and initial mesh. \n\nNext, the code should define variables for the flex cable CPWG, including total length, theta, radius, width, height, spacing, ground width, and ground thickness. It should also define a function to create a bending based on the curvature radius and extension.\n\nThe code should then draw a signal line and a ground line to create a bent signal wire and two bent ground wires respectively. It should also draw a dielectric to create a dielectric cable, and create bottom metals. \n\nAfterwards, the code should create port interfaces (PEC enclosures) and a Perfect E boundary condition. It should also create ports and a setup and sweep with specified properties. \n\nFinally, the code should plot the model and release AEDT.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 32, "repo_full_name": "scikit-learn__scikit-learn", "instruction": "Generate code that compares different clustering algorithms on various toy datasets. The code should generate several datasets, including noisy circles, noisy moons, blobs, anisotropicly distributed data, blobs with varied variances, and a dataset with no structure. It should then set up parameters for clustering and apply a variety of clustering algorithms to each dataset, including MeanShift, MiniBatchKMeans, AgglomerativeClustering, SpectralClustering, DBSCAN, HDBSCAN, OPTICS, AffinityPropagation, Birch, and GaussianMixture. The code should also handle warnings related to kneighbors_graph and measure the time taken for each algorithm to fit the data. Finally, it should visualize the results of each clustering algorithm on each dataset, displaying the time taken for each algorithm in the plot.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class HDBSCAN(ClusterMixin, BaseEstimator):\n    \"\"\"Cluster data using hierarchical density-based clustering.\n\n    HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\n    with Noise. Performs :class:`~sklearn.cluster.DBSCAN` over varying epsilon\n    values and integrates the result to find a clustering that gives the best\n    stability over epsilon.\n    This allows HDBSCAN to find clusters of varying densities (unlike\n    :class:`~sklearn.cluster.DBSCAN`), and be more robust to parameter selection.\n    Read more in the :ref:`User Guide <hdbscan>`.\n\n    For an example of how to use HDBSCAN, as well as a comparison to\n    :class:`~sklearn.cluster.DBSCAN`, please see the :ref:`plotting demo\n    <sphx_glr_auto_examples_cluster_plot_hdbscan.py>`.\n\n    .. versionadded:: 1.3\n\n    Parameters\n    ----------\n    min_cluster_size : int, default=5\n        The minimum number of samples in a group for that group to be\n        considered a cluster; groupings smaller than this size will be left\n        as noise.\n\n    min_samples : int, default=None\n        The number of samples in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n        When `None`, defaults to `min_cluster_size`.\n\n    cluster_selection_epsilon : float, default=0.0\n        A distance threshold. Clusters below this value will be merged.\n        See [5]_ for more information.\n\n    max_cluster_size : int, default=None\n        A limit to the size of clusters returned by the `\"eom\"` cluster\n        selection algorithm. There is no limit when `max_cluster_size=None`.\n        Has no effect if `cluster_selection_method=\"leaf\"`.\n\n    metric : str or callable, default='euclidean'\n        The metric to use when calculating distance between instances in a\n        feature array.\n\n        - If metric is a string or callable, it must be one of\n          the options allowed by :func:`~sklearn.metrics.pairwise.pairwise_distances`\n          for its metric parameter.\n\n        - If metric is \"precomputed\", X is assumed to be a distance matrix and\n          must be square.\n\n    metric_params : dict, default=None\n        Arguments passed to the distance metric.\n\n    alpha : float, default=1.0\n        A distance scaling parameter as used in robust single linkage.\n        See [3]_ for more information.\n\n    algorithm : {\"auto\", \"brute\", \"kd_tree\", \"ball_tree\"}, default=\"auto\"\n        Exactly which algorithm to use for computing core distances; By default\n        this is set to `\"auto\"` which attempts to use a\n        :class:`~sklearn.neighbors.KDTree` tree if possible, otherwise it uses\n        a :class:`~sklearn.neighbors.BallTree` tree. Both `\"kd_tree\"` and\n        `\"ball_tree\"` algorithms use the\n        :class:`~sklearn.neighbors.NearestNeighbors` estimator.\n\n        If the `X` passed during `fit` is sparse or `metric` is invalid for\n        both :class:`~sklearn.neighbors.KDTree` and\n        :class:`~sklearn.neighbors.BallTree`, then it resolves to use the\n        `\"brute\"` algorithm.\n\n        .. deprecated:: 1.4\n           The `'kdtree'` option was deprecated in version 1.4,\n           and will be renamed to `'kd_tree'` in 1.6.\n\n        .. deprecated:: 1.4\n           The `'balltree'` option was deprecated in version 1.4,\n           and will be renamed to `'ball_tree'` in 1.6.\n\n    leaf_size : int, default=40\n        Leaf size for trees responsible for fast nearest neighbour queries when\n        a KDTree or a BallTree are used as core-distance algorithms. A large\n        dataset size and small `leaf_size` may induce excessive memory usage.\n        If you are running out of memory consider increasing the `leaf_size`\n        parameter. Ignored for `algorithm=\"brute\"`.\n\n    n_jobs : int, default=None\n        Number of jobs to run in parallel to calculate distances.\n        `None` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        `-1` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    cluster_selection_method : {\"eom\", \"leaf\"}, default=\"eom\"\n        The method used to select clusters from the condensed tree. The\n        standard approach for HDBSCAN* is to use an Excess of Mass (`\"eom\"`)\n        algorithm to find the most persistent clusters. Alternatively you can\n        instead select the clusters at the leaves of the tree -- this provides\n        the most fine grained and homogeneous clusters.\n\n    allow_single_cluster : bool, default=False\n        By default HDBSCAN* will not produce a single cluster, setting this\n        to True will override this and allow single cluster results in\n        the case that you feel this is a valid result for your dataset.\n\n    store_centers : str, default=None\n        Which, if any, cluster centers to compute and store. The options are:\n\n        - `None` which does not compute nor store any centers.\n        - `\"centroid\"` which calculates the center by taking the weighted\n          average of their positions. Note that the algorithm uses the\n          euclidean metric and does not guarantee that the output will be\n          an observed data point.\n        - `\"medoid\"` which calculates the center by taking the point in the\n          fitted data which minimizes the distance to all other points in\n          the cluster. This is slower than \"centroid\" since it requires\n          computing additional pairwise distances between points of the\n          same cluster but guarantees the output is an observed data point.\n          The medoid is also well-defined for arbitrary metrics, and does not\n          depend on a euclidean metric.\n        - `\"both\"` which computes and stores both forms of centers.\n\n    copy : bool, default=False\n        If `copy=True` then any time an in-place modifications would be made\n        that would overwrite data passed to :term:`fit`, a copy will first be\n        made, guaranteeing that the original data will be unchanged.\n        Currently, it only applies when `metric=\"precomputed\"`, when passing\n        a dense array or a CSR sparse matrix and when `algorithm=\"brute\"`.\n\n    Attributes\n    ----------\n    labels_ : ndarray of shape (n_samples,)\n        Cluster labels for each point in the dataset given to :term:`fit`.\n        Outliers are labeled as follows:\n\n        - Noisy samples are given the label -1.\n        - Samples with infinite elements (+/- np.inf) are given the label -2.\n        - Samples with missing data are given the label -3, even if they\n          also have infinite elements.\n\n    probabilities_ : ndarray of shape (n_samples,)\n        The strength with which each sample is a member of its assigned\n        cluster.\n\n        - Clustered samples have probabilities proportional to the degree that\n          they persist as part of the cluster.\n        - Noisy samples have probability zero.\n        - Samples with infinite elements (+/- np.inf) have probability 0.\n        - Samples with missing data have probability `np.nan`.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n    centroids_ : ndarray of shape (n_clusters, n_features)\n        A collection containing the centroid of each cluster calculated under\n        the standard euclidean metric. The centroids may fall \"outside\" their\n        respective clusters if the clusters themselves are non-convex.\n\n        Note that `n_clusters` only counts non-outlier clusters. That is to\n        say, the `-1, -2, -3` labels for the outlier clusters are excluded.\n\n    medoids_ : ndarray of shape (n_clusters, n_features)\n        A collection containing the medoid of each cluster calculated under\n        the whichever metric was passed to the `metric` parameter. The\n        medoids are points in the original cluster which minimize the average\n        distance to all other points in that cluster under the chosen metric.\n        These can be thought of as the result of projecting the `metric`-based\n        centroid back onto the cluster.\n\n        Note that `n_clusters` only counts non-outlier clusters. That is to\n        say, the `-1, -2, -3` labels for the outlier clusters are excluded.\n\n    See Also\n    --------\n    DBSCAN : Density-Based Spatial Clustering of Applications\n        with Noise.\n    OPTICS : Ordering Points To Identify the Clustering Structure.\n    Birch : Memory-efficient, online-learning algorithm.\n\n    References\n    ----------\n\n    .. [1] :doi:`Campello, R. J., Moulavi, D., & Sander, J. Density-based clustering\n      based on hierarchical density estimates.\n      <10.1007/978-3-642-37456-2_14>`\n    .. [2] :doi:`Campello, R. J., Moulavi, D., Zimek, A., & Sander, J.\n       Hierarchical density estimates for data clustering, visualization,\n       and outlier detection.<10.1145/2733381>`\n\n    .. [3] `Chaudhuri, K., & Dasgupta, S. Rates of convergence for the\n       cluster tree.\n       <https://papers.nips.cc/paper/2010/hash/\n       b534ba68236ba543ae44b22bd110a1d6-Abstract.html>`_\n\n    .. [4] `Moulavi, D., Jaskowiak, P.A., Campello, R.J., Zimek, A. and\n       Sander, J. Density-Based Clustering Validation.\n       <https://www.dbs.ifi.lmu.de/~zimek/publications/SDM2014/DBCV.pdf>`_\n\n    .. [5] :arxiv:`Malzer, C., & Baum, M. \"A Hybrid Approach To Hierarchical\n       Density-based Cluster Selection.\"<1911.02282>`.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import HDBSCAN\n    >>> from sklearn.datasets import load_digits\n    >>> X, _ = load_digits(return_X_y=True)\n    >>> hdb = HDBSCAN(min_cluster_size=20)\n    >>> hdb.fit(X)\n    HDBSCAN(min_cluster_size=20)\n    >>> hdb.labels_\n    array([ 2,  6, -1, ..., -1, -1, -1])\n    \"\"\"\n\n    _parameter_constraints = {\n        \"min_cluster_size\": [Interval(Integral, left=2, right=None, closed=\"left\")],\n        \"min_samples\": [Interval(Integral, left=1, right=None, closed=\"left\"), None],\n        \"cluster_selection_epsilon\": [\n            Interval(Real, left=0, right=None, closed=\"left\")\n        ],\n        \"max_cluster_size\": [\n            None,\n            Interval(Integral, left=1, right=None, closed=\"left\"),\n        ],\n        \"metric\": [StrOptions(FAST_METRICS | {\"precomputed\"}), callable],\n        \"metric_params\": [dict, None],\n        \"alpha\": [Interval(Real, left=0, right=None, closed=\"neither\")],\n        # TODO(1.6): Remove \"kdtree\" and \"balltree\"  option\n        \"algorithm\": [\n            StrOptions(\n                {\"auto\", \"brute\", \"kd_tree\", \"ball_tree\", \"kdtree\", \"balltree\"},\n                deprecated={\"kdtree\", \"balltree\"},\n            ),\n        ],\n        \"leaf_size\": [Interval(Integral, left=1, right=None, closed=\"left\")],\n        \"n_jobs\": [Integral, None],\n        \"cluster_selection_method\": [StrOptions({\"eom\", \"leaf\"})],\n        \"allow_single_cluster\": [\"boolean\"],\n        \"store_centers\": [None, StrOptions({\"centroid\", \"medoid\", \"both\"})],\n        \"copy\": [\"boolean\"],\n    }\n\n    def __init__(\n        self,\n        min_cluster_size=5,\n        min_samples=None,\n        cluster_selection_epsilon=0.0,\n        max_cluster_size=None,\n        metric=\"euclidean\",\n        metric_params=None,\n        alpha=1.0,\n        algorithm=\"auto\",\n        leaf_size=40,\n        n_jobs=None,\n        cluster_selection_method=\"eom\",\n        allow_single_cluster=False,\n        store_centers=None,\n        copy=False,\n    ):\n        self.min_cluster_size = min_cluster_size\n        self.min_samples = min_samples\n        self.alpha = alpha\n        self.max_cluster_size = max_cluster_size\n        self.cluster_selection_epsilon = cluster_selection_epsilon\n        self.metric = metric\n        self.metric_params = metric_params\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.n_jobs = n_jobs\n        self.cluster_selection_method = cluster_selection_method\n        self.allow_single_cluster = allow_single_cluster\n        self.store_centers = store_centers\n        self.copy = copy\n\n    def fit(self, X, y=None):\n        \"\"\"Find clusters based on hierarchical density-based clustering.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                ndarray of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            `metric='precomputed'`.\n\n        y : None\n            Ignored.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        self._validate_params()\n        self._metric_params = self.metric_params or {}\n        if self.metric != \"precomputed\":\n            # Non-precomputed matrices may contain non-finite values.\n            X = self._validate_data(\n                X,\n                accept_sparse=[\"csr\", \"lil\"],\n                force_all_finite=False,\n                dtype=np.float64,\n            )\n            self._raw_data = X\n            all_finite = True\n            try:\n                _assert_all_finite(X.data if issparse(X) else X)\n            except ValueError:\n                all_finite = False\n\n            if not all_finite:\n                # Pass only the purely finite indices into hdbscan\n                # We will later assign all non-finite points their\n                # corresponding labels, as specified in `_OUTLIER_ENCODING`\n\n                # Reduce X to make the checks for missing/outlier samples more\n                # convenient.\n                reduced_X = X.sum(axis=1)\n\n                # Samples with missing data are denoted by the presence of\n                # `np.nan`\n                missing_index = np.isnan(reduced_X).nonzero()[0]\n\n                # Outlier samples are denoted by the presence of `np.inf`\n                infinite_index = np.isinf(reduced_X).nonzero()[0]\n\n                # Continue with only finite samples\n                finite_index = _get_finite_row_indices(X)\n                internal_to_raw = {x: y for x, y in enumerate(finite_index)}\n                X = X[finite_index]\n        elif issparse(X):\n            # Handle sparse precomputed distance matrices separately\n            X = self._validate_data(\n                X,\n                accept_sparse=[\"csr\", \"lil\"],\n                dtype=np.float64,\n            )\n        else:\n            # Only non-sparse, precomputed distance matrices are handled here\n            # and thereby allowed to contain numpy.inf for missing distances\n\n            # Perform data validation after removing infinite values (numpy.inf)\n            # from the given distance matrix.\n            X = self._validate_data(X, force_all_finite=False, dtype=np.float64)\n            if np.isnan(X).any():\n                # TODO: Support np.nan in Cython implementation for precomputed\n                # dense HDBSCAN\n                raise ValueError(\"np.nan values found in precomputed-dense\")\n        if X.shape[0] == 1:\n            raise ValueError(\"n_samples=1 while HDBSCAN requires more than one sample\")\n        self._min_samples = (\n            self.min_cluster_size if self.min_samples is None else self.min_samples\n        )\n\n        if self._min_samples > X.shape[0]:\n            raise ValueError(\n                f\"min_samples ({self._min_samples}) must be at most the number of\"\n                f\" samples in X ({X.shape[0]})\"\n            )\n\n        # TODO(1.6): Remove\n        if self.algorithm == \"kdtree\":\n            warn(\n                (\n                    \"`algorithm='kdtree'`has been deprecated in 1.4 and will be renamed\"\n                    \" to'kd_tree'`in 1.6. To keep the past behaviour, set\"\n                    \" `algorithm='kd_tree'`.\"\n                ),\n                FutureWarning,\n            )\n            self.algorithm = \"kd_tree\"\n\n        # TODO(1.6): Remove\n        if self.algorithm == \"balltree\":\n            warn(\n                (\n                    \"`algorithm='balltree'`has been deprecated in 1.4 and will be\"\n                    \" renamed to'ball_tree'`in 1.6. To keep the past behaviour, set\"\n                    \" `algorithm='ball_tree'`.\"\n                ),\n                FutureWarning,\n            )\n            self.algorithm = \"ball_tree\"\n\n        mst_func = None\n        kwargs = dict(\n            X=X,\n            min_samples=self._min_samples,\n            alpha=self.alpha,\n            metric=self.metric,\n            n_jobs=self.n_jobs,\n            **self._metric_params,\n        )\n        if self.algorithm == \"kd_tree\" and self.metric not in KDTree.valid_metrics:\n            raise ValueError(\n                f\"{self.metric} is not a valid metric for a KDTree-based algorithm.\"\n                \" Please select a different metric.\"\n            )\n        elif (\n            self.algorithm == \"ball_tree\" and self.metric not in BallTree.valid_metrics\n        ):\n            raise ValueError(\n                f\"{self.metric} is not a valid metric for a BallTree-based algorithm.\"\n                \" Please select a different metric.\"\n            )\n\n        if self.algorithm != \"auto\":\n            if (\n                self.metric != \"precomputed\"\n                and issparse(X)\n                and self.algorithm != \"brute\"\n            ):\n                raise ValueError(\"Sparse data matrices only support algorithm `brute`.\")\n\n            if self.algorithm == \"brute\":\n                mst_func = _hdbscan_brute\n                kwargs[\"copy\"] = self.copy\n            elif self.algorithm == \"kd_tree\":\n                mst_func = _hdbscan_prims\n                kwargs[\"algo\"] = \"kd_tree\"\n                kwargs[\"leaf_size\"] = self.leaf_size\n            else:\n                mst_func = _hdbscan_prims\n                kwargs[\"algo\"] = \"ball_tree\"\n                kwargs[\"leaf_size\"] = self.leaf_size\n        else:\n            if issparse(X) or self.metric not in FAST_METRICS:\n                # We can't do much with sparse matrices ...\n                mst_func = _hdbscan_brute\n                kwargs[\"copy\"] = self.copy\n            elif self.metric in KDTree.valid_metrics:\n                # TODO: Benchmark KD vs Ball Tree efficiency\n                mst_func = _hdbscan_prims\n                kwargs[\"algo\"] = \"kd_tree\"\n                kwargs[\"leaf_size\"] = self.leaf_size\n            else:\n                # Metric is a valid BallTree metric\n                mst_func = _hdbscan_prims\n                kwargs[\"algo\"] = \"ball_tree\"\n                kwargs[\"leaf_size\"] = self.leaf_size\n\n        self._single_linkage_tree_ = mst_func(**kwargs)\n\n        self.labels_, self.probabilities_ = tree_to_labels(\n            self._single_linkage_tree_,\n            self.min_cluster_size,\n            self.cluster_selection_method,\n            self.allow_single_cluster,\n            self.cluster_selection_epsilon,\n            self.max_cluster_size,\n        )\n        if self.metric != \"precomputed\" and not all_finite:\n            # Remap indices to align with original data in the case of\n            # non-finite entries. Samples with np.inf are mapped to -1 and\n            # those with np.nan are mapped to -2.\n            self._single_linkage_tree_ = remap_single_linkage_tree(\n                self._single_linkage_tree_,\n                internal_to_raw,\n                # There may be overlap for points w/ both `np.inf` and `np.nan`\n                non_finite=set(np.hstack([infinite_index, missing_index])),\n            )\n            new_labels = np.empty(self._raw_data.shape[0], dtype=np.int32)\n            new_labels[finite_index] = self.labels_\n            new_labels[infinite_index] = _OUTLIER_ENCODING[\"infinite\"][\"label\"]\n            new_labels[missing_index] = _OUTLIER_ENCODING[\"missing\"][\"label\"]\n            self.labels_ = new_labels\n\n            new_probabilities = np.zeros(self._raw_data.shape[0], dtype=np.float64)\n            new_probabilities[finite_index] = self.probabilities_\n            # Infinite outliers have probability 0 by convention, though this\n            # is arbitrary.\n            new_probabilities[infinite_index] = _OUTLIER_ENCODING[\"infinite\"][\"prob\"]\n            new_probabilities[missing_index] = _OUTLIER_ENCODING[\"missing\"][\"prob\"]\n            self.probabilities_ = new_probabilities\n\n        if self.store_centers:\n            self._weighted_cluster_center(X)\n        return self\n\n    def fit_predict(self, X, y=None):\n        \"\"\"Cluster X and return the associated cluster labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n                ndarray of shape (n_samples, n_samples)\n            A feature array, or array of distances between samples if\n            `metric='precomputed'`.\n\n        y : None\n            Ignored.\n\n        Returns\n        -------\n        y : ndarray of shape (n_samples,)\n            Cluster labels.\n        \"\"\"\n        self.fit(X)\n        return self.labels_\n\n    def _weighted_cluster_center(self, X):\n        \"\"\"Calculate and store the centroids/medoids of each cluster.\n\n        This requires `X` to be a raw feature array, not precomputed\n        distances. Rather than return outputs directly, this helper method\n        instead stores them in the `self.{centroids, medoids}_` attributes.\n        The choice for which attributes are calculated and stored is mediated\n        by the value of `self.store_centers`.\n\n        Parameters\n        ----------\n        X : ndarray of shape (n_samples, n_features)\n            The feature array that the estimator was fit with.\n\n        \"\"\"\n        # Number of non-noise clusters\n        n_clusters = len(set(self.labels_) - {-1, -2})\n        mask = np.empty((X.shape[0],), dtype=np.bool_)\n        make_centroids = self.store_centers in (\"centroid\", \"both\")\n        make_medoids = self.store_centers in (\"medoid\", \"both\")\n\n        if make_centroids:\n            self.centroids_ = np.empty((n_clusters, X.shape[1]), dtype=np.float64)\n        if make_medoids:\n            self.medoids_ = np.empty((n_clusters, X.shape[1]), dtype=np.float64)\n\n        # Need to handle iteratively seen each cluster may have a different\n        # number of samples, hence we can't create a homogeneous 3D array.\n        for idx in range(n_clusters):\n            mask = self.labels_ == idx\n            data = X[mask]\n            strength = self.probabilities_[mask]\n            if make_centroids:\n                self.centroids_[idx] = np.average(data, weights=strength, axis=0)\n            if make_medoids:\n                # TODO: Implement weighted argmin PWD backend\n                dist_mat = pairwise_distances(\n                    data, metric=self.metric, **self._metric_params\n                )\n                dist_mat = dist_mat * strength\n                medoid_index = np.argmin(dist_mat.sum(axis=1))\n                self.medoids_[idx] = data[medoid_index]\n        return\n\n    def dbscan_clustering(self, cut_distance, min_cluster_size=5):\n        \"\"\"Return clustering given by DBSCAN without border points.\n\n        Return clustering that would be equivalent to running DBSCAN* for a\n        particular cut_distance (or epsilon) DBSCAN* can be thought of as\n        DBSCAN without the border points.  As such these results may differ\n        slightly from `cluster.DBSCAN` due to the difference in implementation\n        over the non-core points.\n\n        This can also be thought of as a flat clustering derived from constant\n        height cut through the single linkage tree.\n\n        This represents the result of selecting a cut value for robust single linkage\n        clustering. The `min_cluster_size` allows the flat clustering to declare noise\n        points (and cluster smaller than `min_cluster_size`).\n\n        Parameters\n        ----------\n        cut_distance : float\n            The mutual reachability distance cut value to use to generate a\n            flat clustering.\n\n        min_cluster_size : int, default=5\n            Clusters smaller than this value with be called 'noise' and remain\n            unclustered in the resulting flat clustering.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            An array of cluster labels, one per datapoint.\n            Outliers are labeled as follows:\n\n            - Noisy samples are given the label -1.\n            - Samples with infinite elements (+/- np.inf) are given the label -2.\n            - Samples with missing data are given the label -3, even if they\n              also have infinite elements.\n        \"\"\"\n        labels = labelling_at_cut(\n            self._single_linkage_tree_, cut_distance, min_cluster_size\n        )\n        # Infer indices from labels generated during `fit`\n        infinite_index = self.labels_ == _OUTLIER_ENCODING[\"infinite\"][\"label\"]\n        missing_index = self.labels_ == _OUTLIER_ENCODING[\"missing\"][\"label\"]\n\n        # Overwrite infinite/missing outlier samples (otherwise simple noise)\n        labels[infinite_index] = _OUTLIER_ENCODING[\"infinite\"][\"label\"]\n        labels[missing_index] = _OUTLIER_ENCODING[\"missing\"][\"label\"]\n        return labels\n\n    def _more_tags(self):\n        return {\"allow_nan\": self.metric != \"precomputed\"}\n\n# --- Snippet Separator ---\n\ndef dbscan(\n    X,\n    eps=0.5,\n    *,\n    min_samples=5,\n    metric=\"minkowski\",\n    metric_params=None,\n    algorithm=\"auto\",\n    leaf_size=30,\n    p=2,\n    sample_weight=None,\n    n_jobs=None,\n):\n    \"\"\"Perform DBSCAN clustering from vector array or distance matrix.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    X : {array-like, sparse (CSR) matrix} of shape (n_samples, n_features) or \\\n            (n_samples, n_samples)\n        A feature array, or array of distances between samples if\n        ``metric='precomputed'``.\n\n    eps : float, default=0.5\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, default=5\n        The number of samples (or total weight) in a neighborhood for a point\n        to be considered as a core point. This includes the point itself.\n\n    metric : str or callable, default='minkowski'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square during fit.\n        X may be a :term:`sparse graph <sparse graph>`,\n        in which case only \"nonzero\" elements may be considered neighbors.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, default=2\n        The power of the Minkowski metric to be used to calculate distance\n        between points.\n\n    sample_weight : array-like of shape (n_samples,), default=None\n        Weight of each sample, such that a sample with a weight of at least\n        ``min_samples`` is by itself a core sample; a sample with negative\n        weight may inhibit its eps-neighbor from being core.\n        Note that weights are absolute, and default to 1.\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run for neighbors search. ``None`` means\n        1 unless in a :obj:`joblib.parallel_backend` context. ``-1`` means\n        using all processors. See :term:`Glossary <n_jobs>` for more details.\n        If precomputed distance are used, parallel execution is not available\n        and thus n_jobs will have no effect.\n\n    Returns\n    -------\n    core_samples : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    labels : ndarray of shape (n_samples,)\n        Cluster labels for each point.  Noisy samples are given the label -1.\n\n    See Also\n    --------\n    DBSCAN : An estimator interface for this clustering algorithm.\n    OPTICS : A similar estimator interface clustering at multiple values of\n        eps. Our implementation is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_dbscan.py\n    <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower\n    memory usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    :doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n    <10.1145/3068335>`\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n    \"\"\"\n\n    est = DBSCAN(\n        eps=eps,\n        min_samples=min_samples,\n        metric=metric,\n        metric_params=metric_params,\n        algorithm=algorithm,\n        leaf_size=leaf_size,\n        p=p,\n        n_jobs=n_jobs,\n    )\n    est.fit(X, sample_weight=sample_weight)\n    return est.core_sample_indices_, est.labels_\n\n# --- Snippet Separator ---\n\nclass DBSCAN(ClusterMixin, BaseEstimator):\n    \"\"\"Perform DBSCAN clustering from vector array or distance matrix.\n\n    DBSCAN - Density-Based Spatial Clustering of Applications with Noise.\n    Finds core samples of high density and expands clusters from them.\n    Good for data which contains clusters of similar density.\n\n    The worst case memory complexity of DBSCAN is :math:`O({n}^2)`, which can\n    occur when the `eps` param is large and `min_samples` is low.\n\n    Read more in the :ref:`User Guide <dbscan>`.\n\n    Parameters\n    ----------\n    eps : float, default=0.5\n        The maximum distance between two samples for one to be considered\n        as in the neighborhood of the other. This is not a maximum bound\n        on the distances of points within a cluster. This is the most\n        important DBSCAN parameter to choose appropriately for your data set\n        and distance function.\n\n    min_samples : int, default=5\n        The number of samples (or total weight) in a neighborhood for a point to\n        be considered as a core point. This includes the point itself. If\n        `min_samples` is set to a higher value, DBSCAN will find denser clusters,\n        whereas if it is set to a lower value, the found clusters will be more\n        sparse.\n\n    metric : str, or callable, default='euclidean'\n        The metric to use when calculating distance between instances in a\n        feature array. If metric is a string or callable, it must be one of\n        the options allowed by :func:`sklearn.metrics.pairwise_distances` for\n        its metric parameter.\n        If metric is \"precomputed\", X is assumed to be a distance matrix and\n        must be square. X may be a :term:`sparse graph`, in which\n        case only \"nonzero\" elements may be considered neighbors for DBSCAN.\n\n        .. versionadded:: 0.17\n           metric *precomputed* to accept precomputed sparse matrix.\n\n    metric_params : dict, default=None\n        Additional keyword arguments for the metric function.\n\n        .. versionadded:: 0.19\n\n    algorithm : {'auto', 'ball_tree', 'kd_tree', 'brute'}, default='auto'\n        The algorithm to be used by the NearestNeighbors module\n        to compute pointwise distances and find nearest neighbors.\n        See NearestNeighbors module documentation for details.\n\n    leaf_size : int, default=30\n        Leaf size passed to BallTree or cKDTree. This can affect the speed\n        of the construction and query, as well as the memory required\n        to store the tree. The optimal value depends\n        on the nature of the problem.\n\n    p : float, default=None\n        The power of the Minkowski metric to be used to calculate distance\n        between points. If None, then ``p=2`` (equivalent to the Euclidean\n        distance).\n\n    n_jobs : int, default=None\n        The number of parallel jobs to run.\n        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n        for more details.\n\n    Attributes\n    ----------\n    core_sample_indices_ : ndarray of shape (n_core_samples,)\n        Indices of core samples.\n\n    components_ : ndarray of shape (n_core_samples, n_features)\n        Copy of each core sample found by training.\n\n    labels_ : ndarray of shape (n_samples)\n        Cluster labels for each point in the dataset given to fit().\n        Noisy samples are given the label -1.\n\n    n_features_in_ : int\n        Number of features seen during :term:`fit`.\n\n        .. versionadded:: 0.24\n\n    feature_names_in_ : ndarray of shape (`n_features_in_`,)\n        Names of features seen during :term:`fit`. Defined only when `X`\n        has feature names that are all strings.\n\n        .. versionadded:: 1.0\n\n    See Also\n    --------\n    OPTICS : A similar clustering at multiple values of eps. Our implementation\n        is optimized for memory usage.\n\n    Notes\n    -----\n    For an example, see :ref:`examples/cluster/plot_dbscan.py\n    <sphx_glr_auto_examples_cluster_plot_dbscan.py>`.\n\n    This implementation bulk-computes all neighborhood queries, which increases\n    the memory complexity to O(n.d) where d is the average number of neighbors,\n    while original DBSCAN had memory complexity O(n). It may attract a higher\n    memory complexity when querying these nearest neighborhoods, depending\n    on the ``algorithm``.\n\n    One way to avoid the query complexity is to pre-compute sparse\n    neighborhoods in chunks using\n    :func:`NearestNeighbors.radius_neighbors_graph\n    <sklearn.neighbors.NearestNeighbors.radius_neighbors_graph>` with\n    ``mode='distance'``, then using ``metric='precomputed'`` here.\n\n    Another way to reduce memory and computation time is to remove\n    (near-)duplicate points and use ``sample_weight`` instead.\n\n    :class:`~sklearn.cluster.OPTICS` provides a similar clustering with lower memory\n    usage.\n\n    References\n    ----------\n    Ester, M., H. P. Kriegel, J. Sander, and X. Xu, `\"A Density-Based\n    Algorithm for Discovering Clusters in Large Spatial Databases with Noise\"\n    <https://www.dbs.ifi.lmu.de/Publikationen/Papers/KDD-96.final.frame.pdf>`_.\n    In: Proceedings of the 2nd International Conference on Knowledge Discovery\n    and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996\n\n    Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017).\n    :doi:`\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN.\"\n    <10.1145/3068335>`\n    ACM Transactions on Database Systems (TODS), 42(3), 19.\n\n    Examples\n    --------\n    >>> from sklearn.cluster import DBSCAN\n    >>> import numpy as np\n    >>> X = np.array([[1, 2], [2, 2], [2, 3],\n    ...               [8, 7], [8, 8], [25, 80]])\n    >>> clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n    >>> clustering.labels_\n    array([ 0,  0,  0,  1,  1, -1])\n    >>> clustering\n    DBSCAN(eps=3, min_samples=2)\n    \"\"\"\n\n    _parameter_constraints: dict = {\n        \"eps\": [Interval(Real, 0.0, None, closed=\"neither\")],\n        \"min_samples\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"metric\": [\n            StrOptions(set(_VALID_METRICS) | {\"precomputed\"}),\n            callable,\n        ],\n        \"metric_params\": [dict, None],\n        \"algorithm\": [StrOptions({\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"})],\n        \"leaf_size\": [Interval(Integral, 1, None, closed=\"left\")],\n        \"p\": [Interval(Real, 0.0, None, closed=\"left\"), None],\n        \"n_jobs\": [Integral, None],\n    }\n\n    def __init__(\n        self,\n        eps=0.5,\n        *,\n        min_samples=5,\n        metric=\"euclidean\",\n        metric_params=None,\n        algorithm=\"auto\",\n        leaf_size=30,\n        p=None,\n        n_jobs=None,\n    ):\n        self.eps = eps\n        self.min_samples = min_samples\n        self.metric = metric\n        self.metric_params = metric_params\n        self.algorithm = algorithm\n        self.leaf_size = leaf_size\n        self.p = p\n        self.n_jobs = n_jobs\n\n    @_fit_context(\n        # DBSCAN.metric is not validated yet\n        prefer_skip_nested_validation=False\n    )\n    def fit(self, X, y=None, sample_weight=None):\n        \"\"\"Perform DBSCAN clustering from features, or distance matrix.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        Returns\n        -------\n        self : object\n            Returns a fitted instance of self.\n        \"\"\"\n        X = self._validate_data(X, accept_sparse=\"csr\")\n\n        if sample_weight is not None:\n            sample_weight = _check_sample_weight(sample_weight, X)\n\n        # Calculate neighborhood for all samples. This leaves the original\n        # point in, which needs to be considered later (i.e. point i is in the\n        # neighborhood of point i. While True, its useless information)\n        if self.metric == \"precomputed\" and sparse.issparse(X):\n            # set the diagonal to explicit values, as a point is its own\n            # neighbor\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\", sparse.SparseEfficiencyWarning)\n                X.setdiag(X.diagonal())  # XXX: modifies X's internals in-place\n\n        neighbors_model = NearestNeighbors(\n            radius=self.eps,\n            algorithm=self.algorithm,\n            leaf_size=self.leaf_size,\n            metric=self.metric,\n            metric_params=self.metric_params,\n            p=self.p,\n            n_jobs=self.n_jobs,\n        )\n        neighbors_model.fit(X)\n        # This has worst case O(n^2) memory complexity\n        neighborhoods = neighbors_model.radius_neighbors(X, return_distance=False)\n\n        if sample_weight is None:\n            n_neighbors = np.array([len(neighbors) for neighbors in neighborhoods])\n        else:\n            n_neighbors = np.array(\n                [np.sum(sample_weight[neighbors]) for neighbors in neighborhoods]\n            )\n\n        # Initially, all samples are noise.\n        labels = np.full(X.shape[0], -1, dtype=np.intp)\n\n        # A list of all core samples found.\n        core_samples = np.asarray(n_neighbors >= self.min_samples, dtype=np.uint8)\n        dbscan_inner(core_samples, neighborhoods, labels)\n\n        self.core_sample_indices_ = np.where(core_samples)[0]\n        self.labels_ = labels\n\n        if len(self.core_sample_indices_):\n            # fix for scipy sparse indexing issue\n            self.components_ = X[self.core_sample_indices_].copy()\n        else:\n            # no core samples\n            self.components_ = np.empty((0, X.shape[1]))\n        return self\n\n    def fit_predict(self, X, y=None, sample_weight=None):\n        \"\"\"Compute clusters from a data or distance matrix and predict labels.\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features), or \\\n            (n_samples, n_samples)\n            Training instances to cluster, or distances between instances if\n            ``metric='precomputed'``. If a sparse matrix is provided, it will\n            be converted into a sparse ``csr_matrix``.\n\n        y : Ignored\n            Not used, present here for API consistency by convention.\n\n        sample_weight : array-like of shape (n_samples,), default=None\n            Weight of each sample, such that a sample with a weight of at least\n            ``min_samples`` is by itself a core sample; a sample with a\n            negative weight may inhibit its eps-neighbor from being core.\n            Note that weights are absolute, and default to 1.\n\n        Returns\n        -------\n        labels : ndarray of shape (n_samples,)\n            Cluster labels. Noisy samples are given the label -1.\n        \"\"\"\n        self.fit(X, sample_weight=sample_weight)\n        return self.labels_\n\n    def _more_tags(self):\n        return {\"pairwise\": self.metric == \"precomputed\"}\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that compares different clustering algorithms on various toy datasets. The code should generate several datasets, including noisy circles, noisy moons, blobs, anisotropicly distributed data, blobs with varied variances, and a dataset with no structure. It should then set up parameters for clustering and apply a variety of clustering algorithms to each dataset, including MeanShift, MiniBatchKMeans, AgglomerativeClustering, SpectralClustering, DBSCAN, HDBSCAN, OPTICS, AffinityPropagation, Birch, and GaussianMixture. The code should also handle warnings related to kneighbors_graph and measure the time taken for each algorithm to fit the data. Finally, it should visualize the results of each clustering algorithm on each dataset, displaying the time taken for each algorithm in the plot.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 33, "repo_full_name": "pyqtgraph__pyqtgraph", "instruction": "Generate code that creates a PyQtGraph application with a main window that displays three plots. The first plot should be non-interactive and display an image with an integrated vertical color bar. The second plot should be interactive and display a noisy image with an integrated horizontal color bar. The third and fourth plots should display noisy images and share a separate color bar. The color bars should be created using the ColorBarItem class from the PyQtGraph library. The images should be created using the ImageItem class from the PyQtGraph library. The plots should be created using the addPlot method of a GraphicsLayoutWidget instance. The application should start the Qt event loop if it is the main module.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class MainWindow(pg.GraphicsLayoutWidget):\n    \"\"\" example application main window \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.resize(420,400)\n        self.show()\n\n        plot = self.addPlot() # title=\"non-interactive\")\n\n        # prepare demonstration data:\n        data = np.fromfunction(lambda i, j: (1+0.3*np.sin(i)) * (i)**2 + (j)**2, (100, 100))\n        noisy_data = data * (1 + 0.2 * np.random.random(data.shape) )\n\n        # Example: False color image with interactive level adjustment\n        img = pg.ImageItem(image=noisy_data) # create monochrome image from demonstration data\n        plot.addItem( img )            # add to PlotItem 'plot'\n        cm = pg.colormap.get('CET-L9') # prepare a linear color map\n        bar = pg.ColorBarItem( values= (0, 20_000), cmap=cm ) # prepare interactive color bar\n        # Have ColorBarItem control colors of img and appear in 'plot':\n        bar.setImageItem( img, insert_in=plot ) \n\n        self.timer = pg.QtCore.QTimer( singleShot=True )\n        self.timer.timeout.connect(self.export)\n        self.timer.start(100)\n\n    def export(self):\n        print('exporting')\n        exporter = exp.ImageExporter(self.scene())\n        exporter.parameters()['width'] = 420\n        exporter.export('example_false_color_image.png')\n\n# --- Snippet Separator ---\n\ndef __init__(self):\n        super().__init__()\n        self.resize(420,400)\n        self.show()\n\n        plot = self.addPlot() # title=\"non-interactive\")\n\n        # prepare demonstration data:\n        data = np.fromfunction(lambda i, j: (1+0.3*np.sin(i)) * (i)**2 + (j)**2, (100, 100))\n        noisy_data = data * (1 + 0.2 * np.random.random(data.shape) )\n\n        # Example: False color image with interactive level adjustment\n        img = pg.ImageItem(image=noisy_data) # create monochrome image from demonstration data\n        plot.addItem( img )            # add to PlotItem 'plot'\n        cm = pg.colormap.get('CET-L9') # prepare a linear color map\n        bar = pg.ColorBarItem( values= (0, 20_000), cmap=cm ) # prepare interactive color bar\n        # Have ColorBarItem control colors of img and appear in 'plot':\n        bar.setImageItem( img, insert_in=plot ) \n\n        self.timer = pg.QtCore.QTimer( singleShot=True )\n        self.timer.timeout.connect(self.export)\n        self.timer.start(100)\n\n# --- Snippet Separator ---\n\ndef bar(self, x, y, color=None, orientation=\"V\", label=None):\n        \"\"\"Add a bar plot to this chart.\n\n        Parameters\n        ----------\n        x : array_like\n            Positions (along the x-axis for a vertical orientation,\n            along the y-axis for a horizontal orientation) of the bars\n            to draw.\n\n        y : array_like\n            Size of the bars to draw. Multiple bars can be stacked by\n            passing a sequence of sequences.\n\n        color : ColorLike, default: \"b\"\n            Color of the bars drawn in this plot. Any color parsable\n            by :class:`pyvista.Color` is allowed.\n\n        orientation : str, default: \"V\"\n            Orientation of the bars drawn in this plot. Either ``\"H\"``\n            for an horizontal orientation or ``\"V\"`` for a vertical\n            orientation.\n\n        label : str, default: \"\"\n            Label of this plot, as shown in the chart's legend.\n\n        Returns\n        -------\n        plotting.charts.BarPlot\n            The created bar plot.\n\n        Examples\n        --------\n        Generate a bar plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> plot = chart.bar([0, 1, 2], [2, 1, 3])\n        >>> chart.show()\n\n        \"\"\"\n        return self._add_plot(\"bar\", x, y, color=color, orientation=orientation, label=label)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a PyQtGraph application with a main window that displays three plots. The first plot should be non-interactive and display an image with an integrated vertical color bar. The second plot should be interactive and display a noisy image with an integrated horizontal color bar. The third and fourth plots should display noisy images and share a separate color bar. The color bars should be created using the ColorBarItem class from the PyQtGraph library. The images should be created using the ImageItem class from the PyQtGraph library. The plots should be created using the addPlot method of a GraphicsLayoutWidget instance. The application should start the Qt event loop if it is the main module.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 34, "repo_full_name": "federatedai__fate", "instruction": "Generate code that creates a pipeline for a machine learning task using the FATE library. The pipeline should include the following steps: reading data, data transformation, intersection, feature scaling, feature binning, data statistics, Pearson correlation, one-hot encoding, feature selection, logistic regression, and evaluation. The pipeline should be set up with roles for guest, host, and arbiter. The data reading, data transformation, intersection, and feature scaling steps should be performed for both the guest and host. The feature binning parameters, feature selection parameters, and logistic regression parameters should be defined as dictionaries. The pipeline should be compiled and fitted. The script should accept a configuration file as an argument from the command line. If no configuration file is provided, a default one should be used.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def maxabs_scale(X, *, axis=0, copy=True):\n    \"\"\"Scale each feature to the [-1, 1] range without breaking the sparsity.\n\n    This estimator scales each feature individually such\n    that the maximal absolute value of each feature in the\n    training set will be 1.0.\n\n    This scaler can also be applied to sparse CSR or CSC matrices.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data.\n\n    axis : {0, 1}, default=0\n        Axis used to scale along. If 0, independently scale each feature,\n        otherwise (if 1) scale each sample.\n\n    copy : bool, default=True\n        Set to False to perform inplace scaling and avoid a copy (if the input\n        is already a numpy array).\n\n    Returns\n    -------\n    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The transformed data.\n\n    .. warning:: Risk of data leak\n\n        Do not use :func:`~sklearn.preprocessing.maxabs_scale` unless you know\n        what you are doing. A common mistake is to apply it to the entire data\n        *before* splitting into training and test sets. This will bias the\n        model evaluation because information would have leaked from the test\n        set to the training set.\n        In general, we recommend using\n        :class:`~sklearn.preprocessing.MaxAbsScaler` within a\n        :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n        leaking: `pipe = make_pipeline(MaxAbsScaler(), LogisticRegression())`.\n\n    See Also\n    --------\n    MaxAbsScaler : Performs scaling to the [-1, 1] range using\n        the Transformer API (e.g. as part of a preprocessing\n        :class:`~sklearn.pipeline.Pipeline`).\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded to compute the statistics,\n    and maintained during the data transformation.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n    \"\"\"\n    # Unlike the scaler object, this function allows 1d input.\n\n    # If copy is required, it will be done inside the scaler object.\n    X = check_array(\n        X,\n        accept_sparse=(\"csr\", \"csc\"),\n        copy=False,\n        ensure_2d=False,\n        dtype=FLOAT_DTYPES,\n        force_all_finite=\"allow-nan\",\n    )\n    original_ndim = X.ndim\n\n    if original_ndim == 1:\n        X = X.reshape(X.shape[0], 1)\n\n    s = MaxAbsScaler(copy=copy)\n    if axis == 0:\n        X = s.fit_transform(X)\n    else:\n        X = s.fit_transform(X.T).T\n\n    if original_ndim == 1:\n        X = X.ravel()\n\n    return X\n\n# --- Snippet Separator ---\n\ndef minmax_scale(X, feature_range=(0, 1), *, axis=0, copy=True):\n    \"\"\"Transform features by scaling each feature to a given range.\n\n    This estimator scales and translates each feature individually such\n    that it is in the given range on the training set, i.e. between\n    zero and one.\n\n    The transformation is given by (when ``axis=0``)::\n\n        X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n        X_scaled = X_std * (max - min) + min\n\n    where min, max = feature_range.\n\n    The transformation is calculated as (when ``axis=0``)::\n\n       X_scaled = scale * X + min - X.min(axis=0) * scale\n       where scale = (max - min) / (X.max(axis=0) - X.min(axis=0))\n\n    This transformation is often used as an alternative to zero mean,\n    unit variance scaling.\n\n    Read more in the :ref:`User Guide <preprocessing_scaler>`.\n\n    .. versionadded:: 0.17\n       *minmax_scale* function interface\n       to :class:`~sklearn.preprocessing.MinMaxScaler`.\n\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The data.\n\n    feature_range : tuple (min, max), default=(0, 1)\n        Desired range of transformed data.\n\n    axis : {0, 1}, default=0\n        Axis used to scale along. If 0, independently scale each feature,\n        otherwise (if 1) scale each sample.\n\n    copy : bool, default=True\n        Set to False to perform inplace scaling and avoid a copy (if the input\n        is already a numpy array).\n\n    Returns\n    -------\n    X_tr : ndarray of shape (n_samples, n_features)\n        The transformed data.\n\n    .. warning:: Risk of data leak\n\n        Do not use :func:`~sklearn.preprocessing.minmax_scale` unless you know\n        what you are doing. A common mistake is to apply it to the entire data\n        *before* splitting into training and test sets. This will bias the\n        model evaluation because information would have leaked from the test\n        set to the training set.\n        In general, we recommend using\n        :class:`~sklearn.preprocessing.MinMaxScaler` within a\n        :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n        leaking: `pipe = make_pipeline(MinMaxScaler(), LogisticRegression())`.\n\n    See Also\n    --------\n    MinMaxScaler : Performs scaling to a given range using the Transformer\n        API (e.g. as part of a preprocessing\n        :class:`~sklearn.pipeline.Pipeline`).\n\n    Notes\n    -----\n    For a comparison of the different scalers, transformers, and normalizers,\n    see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n    \"\"\"\n    # Unlike the scaler object, this function allows 1d input.\n    # If copy is required, it will be done inside the scaler object.\n    X = check_array(\n        X, copy=False, ensure_2d=False, dtype=FLOAT_DTYPES, force_all_finite=\"allow-nan\"\n    )\n    original_ndim = X.ndim\n\n    if original_ndim == 1:\n        X = X.reshape(X.shape[0], 1)\n\n    s = MinMaxScaler(feature_range=feature_range, copy=copy)\n    if axis == 0:\n        X = s.fit_transform(X)\n    else:\n        X = s.fit_transform(X.T).T\n\n    if original_ndim == 1:\n        X = X.ravel()\n\n    return X\n\n# --- Snippet Separator ---\n\ndef main():\n    # parties config\n    guest = 9999\n    host = 10000\n    arbiter = 10000\n\n    # specify input data name & namespace in database\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    guest_eval_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_eval_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    # initialize pipeline\n    pipeline = PipeLine()\n    # set job initiator\n    pipeline.set_initiator(role=\"guest\", party_id=guest)\n    # set participants information\n    pipeline.set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    # define Reader components to read in data\n    reader_0 = Reader(name=\"reader_0\")\n    # configure Reader for guest\n    reader_0.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_train_data)\n    # configure Reader for host\n    reader_0.get_party_instance(role=\"host\", party_id=host).component_param(table=host_train_data)\n\n    reader_1 = Reader(name=\"reader_1\")\n    reader_1.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_1.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n\n    # define DataIO components\n    dataio_0 = DataIO(name=\"dataio_0\")\n    dataio_1 = DataIO(name=\"dataio_1\")\n\n    # get DataIO party instance of guest\n    dataio_0_guest_party_instance = dataio_0.get_party_instance(role=\"guest\", party_id=guest)\n    # configure DataIO for guest\n    dataio_0_guest_party_instance.component_param(with_label=True, output_format=\"dense\")\n    # get and configure DataIO party instance of host\n    dataio_0.get_party_instance(role=\"host\", party_id=host).component_param(with_label=False)\n\n    # define Intersection components\n    intersection_0 = Intersection(name=\"intersection_0\")\n    intersection_1 = Intersection(name=\"intersection_1\")\n\n    # define HeteroLR component\n    hetero_lr_0 = HeteroLR(name=\"hetero_lr_0\", early_stop=\"weight_diff\", learning_rate=0.15, optimizer=\"rmsprop\",\n                           max_iter=10, early_stopping_rounds=2, validation_freqs=1)\n\n    # add components to pipeline, in order of task execution\n    pipeline.add_component(reader_0)\n    pipeline.add_component(reader_1)\n    pipeline.add_component(dataio_0, data=Data(data=reader_0.output.data))\n    # set dataio_1 to replicate model from dataio_0\n    pipeline.add_component(dataio_1, data=Data(data=reader_1.output.data), model=Model(dataio_0.output.model))\n    # set data input sources of intersection components\n    pipeline.add_component(intersection_0, data=Data(data=dataio_0.output.data))\n    pipeline.add_component(intersection_1, data=Data(data=dataio_1.output.data))\n    # set train & validate data of hetero_lr_0 component\n    pipeline.add_component(\n        hetero_lr_0,\n        data=Data(\n            train_data=intersection_0.output.data,\n            validate_data=intersection_1.output.data))\n\n    # compile pipeline once finished adding modules, this step will form conf and dsl files for running job\n    pipeline.compile()\n\n    # fit model\n    pipeline.fit()\n    # query component summary\n    import json\n    print(json.dumps(pipeline.get_component(\"hetero_lr_0\").get_summary(), indent=4))\n\n    # predict\n    # deploy required components\n    pipeline.deploy_component([dataio_0, intersection_0, hetero_lr_0])\n\n    # initiate predict pipeline\n    predict_pipeline = PipeLine()\n\n    reader_2 = Reader(name=\"reader_2\")\n    reader_2.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_2.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n    # add data reader onto predict pipeline\n    predict_pipeline.add_component(reader_2)\n    # add selected components from train pipeline onto predict pipeline\n    # specify data source\n    predict_pipeline.add_component(pipeline,\n                                   data=Data(predict_input={pipeline.dataio_0.input.data: reader_2.output.data}))\n    # run predict model\n    predict_pipeline.predict()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a pipeline for a machine learning task using the FATE library. The pipeline should include the following steps: reading data, data transformation, intersection, feature scaling, feature binning, data statistics, Pearson correlation, one-hot encoding, feature selection, logistic regression, and evaluation. The pipeline should be set up with roles for guest, host, and arbiter. The data reading, data transformation, intersection, and feature scaling steps should be performed for both the guest and host. The feature binning parameters, feature selection parameters, and logistic regression parameters should be defined as dictionaries. The pipeline should be compiled and fitted. The script should accept a configuration file as an argument from the command line. If no configuration file is provided, a default one should be used.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 35, "repo_full_name": "burnysc2__python-sc2", "instruction": "Generate code that creates a StarCraft II bot using the python-sc2 library. The bot should be able to manage resources, build structures, train units, and engage in combat. It should be able to distribute workers, build pylons when on low supply, train probes, build gateways, build gas, research warp gate, morph to warp gate when research is complete, warp new units, make stalkers attack either closest enemy unit or enemy spawn location, build proxy pylon, and chrono boost nexus or cybercore. The bot should also be able to handle situations when there are no nexuses left. The bot should be run on the \"(2)CatalystLE\" map against a Protoss computer opponent with easy difficulty.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def train(\n        self,\n        unit_type: UnitTypeId,\n        amount: int = 1,\n        closest_to: Point2 = None,\n        train_only_idle_buildings: bool = True\n    ) -> int:\n        \"\"\"Trains a specified number of units. Trains only one if amount is not specified.\n        Warning: currently has issues with warp gate warp ins\n\n        New function. Please report any bugs!\n\n        Example Zerg::\n\n            self.train(UnitTypeId.QUEEN, 5)\n            # This should queue 5 queens in 5 different townhalls if you have enough townhalls, enough minerals and enough free supply left\n\n        Example Terran::\n\n            # Assuming you have 2 idle barracks with reactors, one barracks without addon and one with techlab\n            # It should only queue 4 marines in the 2 idle barracks with reactors\n            self.train(UnitTypeId.MARINE, 4)\n\n        Example distance to::\n\n            # If you want to train based on distance to a certain point, you can use \"closest_to\"\n            self.train(UnitTypeId.MARINE, 4, closest_to = self.game_info.map_center)\n\n\n        :param unit_type:\n        :param amount:\n        :param closest_to:\n        :param train_only_idle_buildings:\"\"\"\n        # Tech requirement not met\n        if self.tech_requirement_progress(unit_type) < 1:\n            race_dict = {\n                Race.Protoss: PROTOSS_TECH_REQUIREMENT,\n                Race.Terran: TERRAN_TECH_REQUIREMENT,\n                Race.Zerg: ZERG_TECH_REQUIREMENT,\n            }\n            unit_info_id = race_dict[self.race][unit_type]\n            logger.warning(\n                \"{} Trying to produce unit {} in self.train() but tech requirement is not met: {}\".format(\n                    self.time_formatted, unit_type, unit_info_id\n                )\n            )\n            return 0\n\n        # Not affordable\n        if not self.can_afford(unit_type):\n            return 0\n\n        trained_amount = 0\n        # All train structure types: queen can made from hatchery, lair, hive\n        train_structure_type: Set[UnitTypeId] = UNIT_TRAINED_FROM[unit_type]\n        train_structures = self.structures if self.race != Race.Zerg else self.structures | self.larva\n        requires_techlab = any(\n            TRAIN_INFO[structure_type][unit_type].get(\"requires_techlab\", False)\n            for structure_type in train_structure_type\n        )\n        is_protoss = self.race == Race.Protoss\n        is_terran = self.race == Race.Terran\n        can_have_addons = any(\n            u in train_structure_type for u in {UnitTypeId.BARRACKS, UnitTypeId.FACTORY, UnitTypeId.STARPORT}\n        )\n        # Sort structures closest to a point\n        if closest_to is not None:\n            train_structures = train_structures.sorted_by_distance_to(closest_to)\n        elif can_have_addons:\n            # This should sort the structures in ascending order: first structures with reactor, then naked, then with techlab\n            train_structures = train_structures.sorted(\n                key=lambda structure: -1 * (structure.add_on_tag in self.reactor_tags) + 1 *\n                (structure.add_on_tag in self.techlab_tags)\n            )\n\n        structure: Unit\n        for structure in train_structures:\n            # Exit early if we can't afford\n            if not self.can_afford(unit_type):\n                return trained_amount\n            if (\n                # If structure hasn't received an action/order this frame\n                structure.tag not in self.unit_tags_received_action\n                # If structure can train this unit at all\n                and structure.type_id in train_structure_type\n                # Structure has to be completed to be able to train\n                and structure.build_progress == 1\n                # If structure is protoss, it needs to be powered to train\n                and (not is_protoss or structure.is_powered or structure.type_id == UnitTypeId.NEXUS)\n                # Either parameter \"train_only_idle_buildings\" is False or structure is idle or structure has less than 2 orders and has reactor\n                and (\n                    not train_only_idle_buildings\n                    or len(structure.orders) < 1 + int(structure.add_on_tag in self.reactor_tags)\n                )\n                # If structure type_id does not accept addons, it cant require a techlab\n                # Else we have to check if building has techlab as addon\n                and (not requires_techlab or structure.add_on_tag in self.techlab_tags)\n            ):\n                # Warp in at location\n                # TODO: find fast warp in locations either random location or closest to the given parameter \"closest_to\"\n                # TODO: find out which pylons have fast warp in by checking distance to nexus and warpgates.ready\n                if structure.type_id == UnitTypeId.WARPGATE:\n                    pylons = self.structures(UnitTypeId.PYLON)\n                    location = pylons.random.position.random_on_distance(4)\n                    successfully_trained = structure.warp_in(unit_type, location)\n                else:\n                    # Normal train a unit from larva or inside a structure\n                    successfully_trained = self.do(\n                        structure.train(unit_type), subtract_cost=True, subtract_supply=True, ignore_warning=True\n                    )\n                    # Check if structure has reactor: queue same unit again\n                    if (\n                        # Only terran can have reactors\n                        is_terran\n                        # Check if we have enough cost or supply for this unit type\n                        and self.can_afford(unit_type)\n                        # Structure needs to be idle in the current frame\n                        and not structure.orders\n                        # We are at least 2 away from goal\n                        and trained_amount + 1 < amount\n                        # Unit type does not require techlab\n                        and not requires_techlab\n                        # Train structure has reactor\n                        and structure.add_on_tag in self.reactor_tags\n                    ):\n                        trained_amount += 1\n                        # With one command queue=False and one queue=True, you can queue 2 marines in a reactored barracks in one frame\n                        successfully_trained = self.do(\n                            structure.train(unit_type, queue=True),\n                            subtract_cost=True,\n                            subtract_supply=True,\n                            ignore_warning=True,\n                        )\n\n                if successfully_trained:\n                    trained_amount += 1\n                    if trained_amount == amount:\n                        # Target unit train amount reached\n                        return trained_amount\n                else:\n                    # Some error occured and we couldn't train the unit\n                    return trained_amount\n        return trained_amount\n\n# --- Snippet Separator ---\n\nasync def on_step(self, iteration):\n        if iteration == 0:\n            await self.chat_send(\"(glhf)\")\n\n        # Draw creep pixelmap for debugging\n        # self.draw_creep_pixelmap()\n\n        # If townhall no longer exists: attack move with all units to enemy start location\n        if not self.townhalls:\n            for unit in self.units.exclude_type({UnitTypeId.EGG, UnitTypeId.LARVA}):\n                unit.attack(self.enemy_start_locations[0])\n            return\n\n        hatch: Unit = self.townhalls[0]\n\n        # Pick a target location\n        target: Point2 = self.enemy_structures.not_flying.random_or(self.enemy_start_locations[0]).position\n\n        # Give all zerglings an attack command\n        for zergling in self.units(UnitTypeId.ZERGLING):\n            zergling.attack(target)\n\n        # Inject hatchery if queen has more than 25 energy\n        for queen in self.units(UnitTypeId.QUEEN):\n            if queen.energy >= 25 and not hatch.has_buff(BuffId.QUEENSPAWNLARVATIMER):\n                queen(AbilityId.EFFECT_INJECTLARVA, hatch)\n\n        # Pull workers out of gas if we have almost enough gas mined, this will stop mining when we reached 100 gas mined\n        if self.vespene >= 88 or self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) > 0:\n            gas_drones: Units = self.workers.filter(lambda w: w.is_carrying_vespene and len(w.orders) < 2)\n            drone: Unit\n            for drone in gas_drones:\n                minerals: Units = self.mineral_field.closer_than(10, hatch)\n                if minerals:\n                    mineral: Unit = minerals.closest_to(drone)\n                    drone.gather(mineral, queue=True)\n\n        # If we have 100 vespene, this will try to research zergling speed once the spawning pool is at 100% completion\n        if self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED\n                                        ) == 0 and self.can_afford(UpgradeId.ZERGLINGMOVEMENTSPEED):\n            spawning_pools_ready: Units = self.structures(UnitTypeId.SPAWNINGPOOL).ready\n            if spawning_pools_ready:\n                self.research(UpgradeId.ZERGLINGMOVEMENTSPEED)\n\n        # If we have less than 2 supply left and no overlord is in the queue: train an overlord\n        if self.supply_left < 2 and self.already_pending(UnitTypeId.OVERLORD) < 1:\n            self.train(UnitTypeId.OVERLORD, 1)\n\n        # While we have less than 88 vespene mined: send drones into extractor one frame at a time\n        if (\n            self.gas_buildings.ready and self.vespene < 88\n            and self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) == 0\n        ):\n            extractor: Unit = self.gas_buildings.first\n            if extractor.surplus_harvesters < 0:\n                self.workers.random.gather(extractor)\n\n        # If we have lost of minerals, make a macro hatchery\n        if self.minerals > 500:\n            for d in range(4, 15):\n                pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                if await self.can_place_single(UnitTypeId.HATCHERY, pos):\n                    self.workers.random.build(UnitTypeId.HATCHERY, pos)\n                    break\n\n        # While we have less than 16 drones, make more drones\n        if self.can_afford(UnitTypeId.DRONE) and self.supply_workers < 16:\n            self.train(UnitTypeId.DRONE)\n\n        # If our spawningpool is completed, start making zerglings\n        if self.structures(UnitTypeId.SPAWNINGPOOL).ready and self.larva and self.can_afford(UnitTypeId.ZERGLING):\n            amount_trained: int = self.train(UnitTypeId.ZERGLING, self.larva.amount)\n\n        # If we have no extractor, build extractor\n        if (\n            self.gas_buildings.amount + self.already_pending(UnitTypeId.EXTRACTOR) == 0\n            and self.can_afford(UnitTypeId.EXTRACTOR) and self.workers\n        ):\n            drone: Unit = self.workers.random\n            target: Unit = self.vespene_geyser.closest_to(drone)\n            drone.build_gas(target)\n\n        # If we have no spawning pool, try to build spawning pool\n        elif self.structures(UnitTypeId.SPAWNINGPOOL).amount + self.already_pending(UnitTypeId.SPAWNINGPOOL) == 0:\n            if self.can_afford(UnitTypeId.SPAWNINGPOOL):\n                for d in range(4, 15):\n                    pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                    if await self.can_place_single(UnitTypeId.SPAWNINGPOOL, pos):\n                        drone: Unit = self.workers.closest_to(pos)\n                        drone.build(UnitTypeId.SPAWNINGPOOL, pos)\n\n        # If we have no queen, try to build a queen if we have a spawning pool compelted\n        elif (\n            self.units(UnitTypeId.QUEEN).amount + self.already_pending(UnitTypeId.QUEEN) < self.townhalls.amount\n            and self.structures(UnitTypeId.SPAWNINGPOOL).ready\n        ):\n            if self.can_afford(UnitTypeId.QUEEN):\n                self.train(UnitTypeId.QUEEN)\n\n# --- Snippet Separator ---\n\nclass CompetitiveBot(BotAI):\n\n    async def on_start(self):\n        self.client.game_step = 2\n\n    async def on_step(self, iteration):\n        if iteration == 0:\n            await self.chat_send(\"(glhf)\")\n\n        # Draw creep pixelmap for debugging\n        # self.draw_creep_pixelmap()\n\n        # If townhall no longer exists: attack move with all units to enemy start location\n        if not self.townhalls:\n            for unit in self.units.exclude_type({UnitTypeId.EGG, UnitTypeId.LARVA}):\n                unit.attack(self.enemy_start_locations[0])\n            return\n\n        hatch: Unit = self.townhalls[0]\n\n        # Pick a target location\n        target: Point2 = self.enemy_structures.not_flying.random_or(self.enemy_start_locations[0]).position\n\n        # Give all zerglings an attack command\n        for zergling in self.units(UnitTypeId.ZERGLING):\n            zergling.attack(target)\n\n        # Inject hatchery if queen has more than 25 energy\n        for queen in self.units(UnitTypeId.QUEEN):\n            if queen.energy >= 25 and not hatch.has_buff(BuffId.QUEENSPAWNLARVATIMER):\n                queen(AbilityId.EFFECT_INJECTLARVA, hatch)\n\n        # Pull workers out of gas if we have almost enough gas mined, this will stop mining when we reached 100 gas mined\n        if self.vespene >= 88 or self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) > 0:\n            gas_drones: Units = self.workers.filter(lambda w: w.is_carrying_vespene and len(w.orders) < 2)\n            drone: Unit\n            for drone in gas_drones:\n                minerals: Units = self.mineral_field.closer_than(10, hatch)\n                if minerals:\n                    mineral: Unit = minerals.closest_to(drone)\n                    drone.gather(mineral, queue=True)\n\n        # If we have 100 vespene, this will try to research zergling speed once the spawning pool is at 100% completion\n        if self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED\n                                        ) == 0 and self.can_afford(UpgradeId.ZERGLINGMOVEMENTSPEED):\n            spawning_pools_ready: Units = self.structures(UnitTypeId.SPAWNINGPOOL).ready\n            if spawning_pools_ready:\n                self.research(UpgradeId.ZERGLINGMOVEMENTSPEED)\n\n        # If we have less than 2 supply left and no overlord is in the queue: train an overlord\n        if self.supply_left < 2 and self.already_pending(UnitTypeId.OVERLORD) < 1:\n            self.train(UnitTypeId.OVERLORD, 1)\n\n        # While we have less than 88 vespene mined: send drones into extractor one frame at a time\n        if (\n            self.gas_buildings.ready and self.vespene < 88\n            and self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) == 0\n        ):\n            extractor: Unit = self.gas_buildings.first\n            if extractor.surplus_harvesters < 0:\n                self.workers.random.gather(extractor)\n\n        # If we have lost of minerals, make a macro hatchery\n        if self.minerals > 500:\n            for d in range(4, 15):\n                pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                if await self.can_place_single(UnitTypeId.HATCHERY, pos):\n                    self.workers.random.build(UnitTypeId.HATCHERY, pos)\n                    break\n\n        # While we have less than 16 drones, make more drones\n        if self.can_afford(UnitTypeId.DRONE) and self.supply_workers < 16:\n            self.train(UnitTypeId.DRONE)\n\n        # If our spawningpool is completed, start making zerglings\n        if self.structures(UnitTypeId.SPAWNINGPOOL).ready and self.larva and self.can_afford(UnitTypeId.ZERGLING):\n            amount_trained: int = self.train(UnitTypeId.ZERGLING, self.larva.amount)\n\n        # If we have no extractor, build extractor\n        if (\n            self.gas_buildings.amount + self.already_pending(UnitTypeId.EXTRACTOR) == 0\n            and self.can_afford(UnitTypeId.EXTRACTOR) and self.workers\n        ):\n            drone: Unit = self.workers.random\n            target: Unit = self.vespene_geyser.closest_to(drone)\n            drone.build_gas(target)\n\n        # If we have no spawning pool, try to build spawning pool\n        elif self.structures(UnitTypeId.SPAWNINGPOOL).amount + self.already_pending(UnitTypeId.SPAWNINGPOOL) == 0:\n            if self.can_afford(UnitTypeId.SPAWNINGPOOL):\n                for d in range(4, 15):\n                    pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                    if await self.can_place_single(UnitTypeId.SPAWNINGPOOL, pos):\n                        drone: Unit = self.workers.closest_to(pos)\n                        drone.build(UnitTypeId.SPAWNINGPOOL, pos)\n\n        # If we have no queen, try to build a queen if we have a spawning pool compelted\n        elif (\n            self.units(UnitTypeId.QUEEN).amount + self.already_pending(UnitTypeId.QUEEN) < self.townhalls.amount\n            and self.structures(UnitTypeId.SPAWNINGPOOL).ready\n        ):\n            if self.can_afford(UnitTypeId.QUEEN):\n                self.train(UnitTypeId.QUEEN)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a StarCraft II bot using the python-sc2 library. The bot should be able to manage resources, build structures, train units, and engage in combat. It should be able to distribute workers, build pylons when on low supply, train probes, build gateways, build gas, research warp gate, morph to warp gate when research is complete, warp new units, make stalkers attack either closest enemy unit or enemy spawn location, build proxy pylon, and chrono boost nexus or cybercore. The bot should also be able to handle situations when there are no nexuses left. The bot should be run on the \"(2)CatalystLE\" map against a Protoss computer opponent with easy difficulty.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 36, "repo_full_name": "avslab__basilisk", "instruction": "Generate code that imports necessary libraries and modules, including the 'Controller', 'RetentionPolicy', and various 'Dispersions' from the 'Basilisk.utilities.MonteCarlo' module, as well as a scenario module named 'scenario_AttFeedback'. The code should define a function 'run' that takes a boolean parameter 'show_plots'. Inside this function, it should create a Monte Carlo simulation controller, set its simulation and execution functions, execution count, archive directory, seed dispersion, thread count, verbosity, variable casting, and dispersion magnitude file. It should also define a list of dispersions and add them to the Monte Carlo controller. Then, it should create a retention policy, add message logs to it, set its data callback, and add it to the Monte Carlo controller. The function should execute the simulations, execute callbacks if 'show_plots' is True, and return. The code should also define a function 'displayPlots' that takes 'data' and 'retentionPolicy' as parameters, extracts time and states from the data, and plots the states against time. Finally, it should call the 'run' function with 'True' as the argument if the script is run as the main program.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Controller:\n    \"\"\"\n    The MonteCarloController class is used to run a monte carlo simulation.\n    It is used to execute multiple runs of a simulation with varying initial parameters. Data from each run is retained\n    in order to analyze differences in the simulation runs and the parameters used.\n    \"\"\"\n\n    def __init__(self):\n        self.executionCount = 0\n        self.ICrunFlag = False\n        self.icDirectory = \"\"\n        self.archiveDir = None\n        self.varCast = None\n        self.numProcess = mp.cpu_count()\n\n        self.simParams = SimulationParameters(\n            creationFunction=None,\n            executionFunction=None,\n            configureFunction=None,\n            retentionPolicies=[],\n            shouldArchiveParameters=False,\n            shouldDisperseSeeds=False,\n            dispersions=[],\n            filename=\"\",\n            icfilename=\"\"\n        )\n\n    def setShowProgressBar(self, value):\n        \"\"\"\n        To enable or disable progress bar to show simulation progress\n        Args:\n            value: boolean value, decide to show/hide progress bar\n        \"\"\"\n        self.simParams.showProgressBar = value\n\n    @staticmethod\n    def load(runDirectory):\n        \"\"\"\n        Load a previously completed MonteCarlo simulation\n        Args:\n            The path to the MonteCarlo.data file that contains the archived MonteCarlo run\n        \"\"\"\n        filename = os.path.abspath(runDirectory) + \"/MonteCarlo.data\"\n\n        with gzip.open(filename) as pickledData:\n            data = pickle.load(pickledData)\n            if data.simParams.verbose:\n                print(\"Loading montecarlo at\", filename)\n            data.multiProcManager = mp.Manager()\n            data.dataOutQueue = data.multiProcManager.Queue()\n            data.dataWriter = DataWriter(data.dataOutQueue)\n            data.dataWriter.daemon = False\n            return data\n\n    def setExecutionFunction(self, newModule):\n        \"\"\"\n        Set an execution function that executes a simulation instance.\n\n        Args:\n            executionFunction: (sim: SimulationBaseClass) => None\n                A function with one parameter, a simulation instance.\n                The function will be called after the creationFunction and configurationFunction in each simulation run.\n                It must execute the simulation.\n                Its return value is not used.\n        \"\"\"\n        self.simParams.executionFunction = newModule\n\n    def setConfigureFunction(self, newModule):\n        \"\"\"\n        Set an execution function that executes a simulation instance.\n\n        Args:\n            executionFunction: (sim: SimulationBaseClass) => None\n                A function with one parameter, a simulation instance.\n                The function will be called after the creationFunction and configurationFunction in each simulation run.\n                It must execute the simulation.\n                Its return value is not used.\n        \"\"\"\n        self.simParams.configureFunction = newModule\n\n    def setSimulationFunction(self, newObject):\n        \"\"\"\n        Set the function that creates the simulation instance.\n\n        Args:\n            creationFunction: () => SimulationBaseClass\n                A function with no parameters, that returns a simulation instance.\n        \"\"\"\n        self.simParams.creationFunction = newObject\n\n    def setShouldDisperseSeeds(self, seedDisp):\n        \"\"\"\n        Disperse the RNG seeds of each run in the MonteCarlo\n\n        Args:\n            seedDisp: bool\n                Whether to disperse the RNG seeds in each run of the simulation\n        \"\"\"\n        self.simParams.shouldDisperseSeeds = seedDisp\n\n    def setExecutionCount(self, newCount):\n        \"\"\"\n        Set the number of runs for the MonteCarlo simulation\n\n        Args:\n            newCount: int\n                The number of runs to use for the simulation\n        \"\"\"\n        self.executionCount = newCount\n\n    def addDispersion(self, disp):\n        \"\"\"\n        Add a dispersion to the simulation.\n\n        Args:\n            disp: Dispersion\n                The dispersion to add to the simulation.\n        \"\"\"\n        self.simParams.dispersions.append(disp)\n\n    def addRetentionPolicy(self, policy):\n        \"\"\"\n        Add a retention policy to the simulation.\n\n        Args:\n            disp: RetentionPolicy\n                The retention policy to add to the simulation.\n                This defines variables to be logged and saved\n        \"\"\"\n        self.simParams.retentionPolicies.append(policy)\n\n    def setThreadCount(self, threads):\n        \"\"\"\n        Set the number of threads to use for the monte carlo simulation\n\n        Args:\n            threads: int\n                Number of threads to execute the montecarlo run on.\n        \"\"\"\n        self.numProcess = threads\n\n    def setVerbose(self, verbose):\n        \"\"\"\n        Use verbose output for this MonteCarlo run\n\n        Args:\n            verbose: bool\n                Whether to print verbose information during this MonteCarlo sim.\n        \"\"\"\n        self.simParams.verbose = verbose\n\n    def setDispMagnitudeFile(self, magnitudes):\n        \"\"\"\n        Save .txt with the magnitude of each dispersion in % or sigma away from mean\n\n        Args:\n            magnitudes: bool\n                Whether to save extra files for analysis.\n        \"\"\"\n        self.simParams.saveDispMag = magnitudes\n\n    def setShouldArchiveParameters(self, shouldArchiveParameters):\n        self.simParams.shouldArchiveParameters = shouldArchiveParameters\n\n    def setArchiveDir(self, dirName):\n        \"\"\"\n        Set-up archives for this MonteCarlo run\n\n        Args:\n            dirName: string\n                The name of the directory to archive runs in.\n                None, if no archive desired.\n        \"\"\"\n        self.archiveDir = os.path.abspath(dirName) + \"/\"\n        self.simParams.shouldArchiveParameters = dirName is not None\n        self.simParams.filename = self.archiveDir\n\n    def setVarCast(self, varCast):\n        \"\"\"\n        Set the variable type to downcast the data to\n\n        :param varCast: 'float', 'integer', 'signed', 'unsigned' (see pandas.to_numeric documentation)\n        :return:\n        \"\"\"\n        self.varCast = varCast\n\n    def setICDir(self, dirName):\n        \"\"\"\n        Set-up archives containing IC data\n\n        Args:\n            dirName: string\n                The name of the directory to archive runs in.\n                None, if no archive desired.\n        \"\"\"\n        self.icDirectory = os.path.abspath(dirName) + \"/\"\n        self.simParams.shouldArchiveParameters = True\n        self.simParams.icfilename = self.icDirectory\n\n    def setICRunFlag(self, bool):\n        \"\"\"\n        Set the number of threads to use for the monte carlo simulation\n\n        Args:\n            threads: int\n                Number of threads to execute the montecarlo run on.\n        \"\"\"\n        self.ICrunFlag = bool\n\n    def getRetainedData(self, case):\n        \"\"\"\n        Get the data that was retained for a run, or list of runs.\n\n        Args:\n            cases: int The desired case to get data from.\n        Returns:\n            The retained data for that run is returned.\n        \"\"\"\n        if self.ICrunFlag:\n            oldRunDataFile = self.icDirectory + \"run\" + str(case) + \".data\"\n        else:\n            oldRunDataFile = self.archiveDir + \"run\" + str(case) + \".data\"\n\n        with gzip.open(oldRunDataFile) as pickledData:\n            data = pickle.load(pickledData)\n            return data\n\n    def getRetainedDatas(self, cases):\n        \"\"\"\n        Get the data that was retained for a list of runs.\n\n        Args:\n            cases: int[] The desired cases to get data from.\n        Returns:\n            A generator is returned, which will yield, in-order, the retained data for each of these cases\n        \"\"\"\n\n        for case in cases:\n            yield self.getRetainedData(case)  # call this method recursively, yielding the result\n\n    def getParameters(self, caseNumber):\n        \"\"\"\n        Get the parameters used for a particular run of the montecarlo\n\n        :param caseNumber: The number of the run to get the parameters used for.\n        :type caseNumber: int\n\n        :return: A dictionary of the parameters of the simulation\n                 For example:\n                 {\"keyForSim\": parameterValue, 'TaskList[0].TaskModels[0].RNGSeed': 1674764759}\n        \"\"\"\n        if self.ICrunFlag:\n            filename = self.icDirectory + \"run\" + str(caseNumber) + \".json\"\n        else:\n            filename = self.archiveDir + \"run\" + str(caseNumber) + \".json\"\n        with open(filename, \"r\") as dispersionFile:\n            dispersions = json.load(dispersionFile)\n            return dispersions\n\n    def reRunCases(self, caseList):\n        \"\"\"\n        Rerun some cases from a MonteCarlo run. Does not run in parallel\n\n        Args:\n            caseList: int[]\n                The list of runs to repeat, a list of numbers.\n        Returns:\n            failures: int[]\n                The list of failed runs.\n        \"\"\"\n        # the list of failures\n        failed = []\n\n        for caseNumber in caseList:\n            if self.simParams.verbose:\n                print(\"Rerunning\", caseNumber)\n\n            oldRunFile = self.archiveDir + \"run\" + str(caseNumber) + \".json\"\n            if not os.path.exists(oldRunFile):\n                print(\"ERROR re-running case: \" + oldRunFile)\n                continue\n\n            # use old simulation parameters, modified slightly.\n            simParams = copy.deepcopy(self.simParams)\n            simParams.index = caseNumber\n            # don't redisperse seeds, we want to use the ones saved in the oldRunFile\n            simParams.shouldDisperseSeeds = False\n            # don't retain any data so remove all retention policies\n            simParams.retentionPolicies = []\n\n            with open(oldRunFile, \"r\") as runParameters:\n                simParams.modifications = json.load(runParameters)\n\n            # execute simulation with dispersion\n            executor = SimulationExecutor()\n            success = executor([simParams, self.dataOutQueue])\n\n            if not success:\n                print(\"Error re-executing run\", caseNumber)\n                failed.append(caseNumber)\n\n        if len(failed) > 0:\n            failed.sort()\n            print(\"Failed rerunning cases:\", failed)\n\n        return failed\n\n    def runInitialConditions(self, caseList):\n        \"\"\"\n        Run initial conditions given in a file\n\n        Args:\n            caseList: int[]\n                The list of runs to repeat, a list of numbers.\n        Returns:\n            failures: int[]\n                The list of failed runs.\n        \"\"\"\n        # the list of failures\n        failed = []\n\n        assert self.icDirectory != \"\", \"No initial condition directory was given\"\n        assert self.ICrunFlag is not False, \"IC run flag was not set\"\n\n        if self.simParams.verbose:\n            print(\"Beginning simulation with {0} runs on {1} threads\".format(self.executionCount, self.numProcess))\n\n        if self.simParams.shouldArchiveParameters:\n            if not os.path.exists(self.icDirectory):\n                print(\"Cannot run initial conditions: the directory given does not exist\")\n\n            if self.simParams.verbose:\n                print(\"Archiving a copy of this simulation before running it in 'MonteCarlo.data'\")\n            try:\n                with gzip.open(self.icDirectory + \"MonteCarlo.data\", \"w\") as pickleFile:\n                    pickle.dump(self, pickleFile)  # dump this controller object into a file.\n            except Exception as e:\n                print(\"Unknown exception while trying to pickle monte-carlo-controller... \\ncontinuing...\\n\\n\", e)\n\n        # Create Queue, but don't ever start it.\n        self.multiProcManager = mp.Manager()\n        self.dataOutQueue = self.multiProcManager.Queue()\n        self.dataWriter = DataWriter(self.dataOutQueue)\n        self.dataWriter.daemon = False\n\n        # If archiving the rerun data -- make sure not to delete the original data!\n        if self.archiveDir is not None:\n            if self.archiveDir != self.icDirectory:\n                if os.path.exists(self.archiveDir):\n                    shutil.rmtree(self.archiveDir)\n                os.mkdir(self.archiveDir)\n                self.dataWriter.setLogDir(self.archiveDir)\n                self.dataWriter.start()\n            else:\n                print(\"ERROR: The archive directory is set as the icDirectory. Proceeding would have overwriten all data \" \\\n                      \"within: \" + self.archiveDir + \" with the select rerun cases! Exiting.\\n\")\n                sys.exit(\"Change the archive directory to a new location when rerunning cases.\")\n        else:\n            print(\"No archive data specified; no data will be logged to dataframes\")\n\n        jobsFinished = 0  # keep track of what simulations have finished\n\n        # The simulation executor is responsible for executing simulation given a simulation's parameters\n        # It is called within worker threads with each worker's simulation parameters\n        simulationExecutor = SimulationExecutor()\n        #\n        progressBar = SimulationProgressBar(len(caseList), self.simParams.showProgressBar)\n        if self.numProcess == 1:  # don't make child thread\n            if self.simParams.verbose:\n                print(\"Executing sequentially...\")\n            i = 0\n            for i in range(len(caseList)):\n                simGenerator = self.generateICSims(caseList[i:i+1])\n                for sim in simGenerator:\n                    try:\n                        simulationExecutor([sim,  self.dataOutQueue])\n                    except:\n                        failed.append(i)\n                i += 1\n                progressBar.update(i)\n        else:\n            numSims = len(caseList)\n            if self.numProcess > numSims:\n                print(\"Fewer MCs spawned than processes assigned (%d < %d). Changing processes count to %d.\" % (numSims, self.numProcess, numSims))\n                self.numProcess = numSims\n            for i in range(numSims//self.numProcess):\n                # If number of sims doesn't factor evenly into the number of processes:\n                if numSims % self.numProcess != 0 and i == len(list(range(numSims//self.numProcess)))-1:\n                    offset = numSims % self.numProcess\n                else:\n                    offset = 0\n\n                simGenerator = self.generateICSims(caseList[self.numProcess*i:self.numProcess*(i+1)+offset])\n                pool = mp.Pool(self.numProcess)\n                try:\n                    # yields results *as* the workers finish jobs\n                    for result in pool.imap_unordered(simulationExecutor, [(x, self.dataOutQueue) for x in simGenerator]):\n                        if result[0] is not True:  # workers return True on success\n                            failed.append(result[1])  # add failed jobs to the list of failures\n                            print(\"Job\", result[1], \"failed...\")\n\n                        jobsFinished += 1\n                        progressBar.update(jobsFinished)\n                    pool.close()\n                except KeyboardInterrupt as e:\n                    print(\"Ctrl-C was hit, closing pool\")\n                    # failed.extend(range(jobsFinished, numSims))  # fail all potentially running jobs...\n                    pool.terminate()\n                    raise e\n                except Exception as e:\n                    print(\"Unknown exception while running simulations:\", e)\n                    # failed.extend(range(jobsFinished, numSims))  # fail all potentially running jobs...\n                    traceback.print_exc()\n                    pool.terminate()\n                finally:\n                    pool.join()\n\n        progressBar.markComplete()\n        progressBar.close()\n        # If the data was archiving, close the queue.\n        if self.archiveDir is not None and self.archiveDir != self.icDirectory:\n            while not self.dataOutQueue.empty():\n               time.sleep(1)\n            self.dataOutQueue.put((None, None, True))\n            time.sleep(5)\n\n        # if there are failures\n        if len(failed) > 0:\n            failed.sort()\n\n            if self.simParams.verbose:\n                print(\"Failed\", failed, \"saving to 'failures.txt'\")\n\n            if self.simParams.shouldArchiveParameters:\n                # write a file that contains log of failed runs\n                with open(self.icDirectory + \"failures.txt\", \"w\") as failFile:\n                    failFile.write(str(failed))\n\n        return failed\n\n    def generateICSims(self, caseList):\n        \"\"\"\n        Generator function to clone a baseSimulation for IC run\n\n        Args:\n            baseSimulation: SimulationParams\n                A base simulation to clone.\n            numSims: int[]\n                The desired runs to generate.\n        Returns:\n            generator<SimulationParams>\n                A generator that yields that number of cloned simulations\n        \"\"\"\n\n        # make a list of simulations to execute by cloning the base-simulation and\n        # changing each clone's index and filename to make a list of\n        # simulations to execute\n        for caseNumber in caseList:\n            if self.simParams.verbose:\n                print(\"Running IC \", caseNumber)\n\n            oldRunFile = self.icDirectory + \"run\" + str(caseNumber) + \".json\"\n            if not os.path.exists(oldRunFile):\n                print(\"ERROR running IC case: \" + oldRunFile)\n                continue\n\n            # use old simulation parameters, modified slightly.\n            simParams = copy.deepcopy(self.simParams)\n            simParams.index = caseNumber\n            # don't redisperse seeds, we want to use the ones saved in the oldRunFile\n            simParams.shouldDisperseSeeds = False\n\n            simParams.icfilename = self.icDirectory + \"run\" + str(caseNumber)\n            with open(oldRunFile, \"r\") as runParameters:\n                simParams.modifications = json.load(runParameters)\n\n            yield simParams\n\n    def generateSims(self, simNumList):\n        \"\"\"\n        Generator function to clone a baseSimulation\n\n        Args:\n            baseSimulation: SimulationParams\n                A base simulation to clone.\n            numSims: int[]\n                The desired runs to generate.\n        Returns:\n            generator<SimulationParams>\n                A generator that yields that number of cloned simulations\n        \"\"\"\n\n        # make a list of simulations to execute by cloning the base-simulation and\n        # changing each clone's index and filename to make a list of\n        # simulations to execute\n        for i in simNumList:\n            simClone = copy.deepcopy(self.simParams)\n            simClone.index = i\n            simClone.filename += \"run\" + str(i)\n\n            yield simClone\n\n    def executeCallbacks(self, rng=None, retentionPolicies=[]):\n        \"\"\"\n        Execute retention policy callbacks after running a monteCarlo sim.\n\n        Args:\n            rng: A list of simulations to execute callbacks on\n            retentionPolicies: the retention policies to execute\n        \"\"\"\n\n        if rng is None:\n            rng = list(range(self.executionCount))\n\n        if retentionPolicies == []:\n            retentionPolicies = self.simParams.retentionPolicies\n\n        for simIndex in rng:\n            data = self.getRetainedData(simIndex)\n            for retentionPolicy in retentionPolicies:\n                retentionPolicy.executeCallback(data)\n\n    def executeSimulations(self):\n        \"\"\"\n        Execute simulations in parallel\n\n        :return: failed: int[]\n                 A list of the indices of all failed simulation runs.\n        \"\"\"\n\n        if self.simParams.verbose:\n            print(\"Beginning simulation with {0} runs on {1} threads\".format(self.executionCount, self.numProcess))\n\n        if self.simParams.shouldArchiveParameters:\n            if os.path.exists(self.archiveDir):\n                shutil.rmtree(self.archiveDir, ignore_errors=True)\n            os.mkdir(self.archiveDir)\n            if self.simParams.verbose:\n                print(\"Archiving a copy of this simulation before running it in 'MonteCarlo.data'\")\n            try:\n                with gzip.open(self.archiveDir + \"MonteCarlo.data\", \"wb\") as pickleFile:\n                    pickle.dump(self, pickleFile)  # dump this controller object into a file.\n            except Exception as e:\n                print(\"Unknown exception while trying to pickle monte-carlo-controller... \\ncontinuing...\\n\\n\", e)\n\n        self.multiProcManager = mp.Manager()\n        self.dataOutQueue = self.multiProcManager.Queue()\n        self.dataWriter = DataWriter(self.dataOutQueue)\n        self.dataWriter.daemon = False\n\n        numSims = self.executionCount\n\n        # start data writer process\n        self.dataWriter.setLogDir(self.archiveDir)\n        self.dataWriter.setVarCast(self.varCast)\n        self.dataWriter.start()\n\n        # Avoid building a full list of all simulations to run in memory,\n        # instead only generating simulations right before they are needed by a waiting worker\n        # This is accomplished using a generator and pool.imap, -- simulations are only built\n        # when they are about to be passed to a worker, avoiding memory overhead of first building simulations\n        # There is a system-dependent chunking behavior, sometimes 10-20 are generated at a time.\n        # simGenerator = self.generateSims(range(numSims))\n        failed = []  # keep track of the indices of failed simulations\n        jobsFinished = 0  # keep track of what simulations have finished\n\n        # The simulation executor is responsible for executing simulation given a simulation's parameters\n        # It is called within worker threads with each worker's simulation parameters\n        simulationExecutor = SimulationExecutor()\n\n        progressBar = SimulationProgressBar(numSims, self.simParams.showProgressBar)\n\n        # The outermost for-loop for both the serial and multiprocessed sim generator is not necessary. It\n        # is a temporary fix to a memory leak which is assumed to be a result of the simGenerator not collecting\n        # garbage properly. # TODO: Find a more permenant solution to the leak.\n\n        if self.numProcess == 1:  # don't make child thread\n            if self.simParams.verbose:\n                print(\"Executing sequentially...\")\n            i = 0\n            for i in range(numSims):\n                simGenerator = self.generateSims(list(range(i,i+1)))\n                for sim in simGenerator:\n                    try:\n                        run_ok = simulationExecutor([sim, self.dataOutQueue])[0]\n                    except:\n                        failed.append(i)\n                    else:\n                        if not run_ok:\n                            failed.append(i)\n                    i += 1\n                    progressBar.update(i)\n        else:\n            if self.numProcess > numSims:\n                print(\"Fewer MCs spawned than processes assigned (%d < %d). Changing processes count to %d.\" % (numSims, self.numProcess, numSims))\n                self.numProcess = numSims\n            for i in range(numSims//self.numProcess):\n                # If number of sims doesn't factor evenly into the number of processes:\n                if numSims % self.numProcess != 0 and i == len(list(range(numSims//self.numProcess)))-1:\n                    offset = numSims % self.numProcess\n                else:\n                    offset = 0\n                simGenerator = self.generateSims(list(range(self.numProcess*i, self.numProcess*(i+1)+offset)))\n                pool = mp.Pool(self.numProcess)\n                try:\n                    # yields results *as* the workers finish jobs\n                    for result in pool.imap_unordered(simulationExecutor, [(x, self.dataOutQueue) for x in simGenerator]):\n                        if result[0] is not True:  # workers return True on success\n                            failed.append(result[1])  # add failed jobs to the list of failures\n                            print(\"Job\", result[1], \"failed...\")\n\n                        jobsFinished += 1\n                        progressBar.update(jobsFinished)\n                    pool.close()\n                except KeyboardInterrupt as e:\n                    print(\"Ctrl-C was hit, closing pool\")\n                    failed.extend(list(range(jobsFinished, numSims)))  # fail all potentially running jobs...\n                    pool.terminate()\n                    raise e\n                except Exception as e:\n                    print(\"Unknown exception while running simulations:\", e)\n                    failed.extend(list(range(jobsFinished, numSims)))  # fail all potentially running jobs...\n                    traceback.print_exc()\n                    pool.terminate()\n                finally:\n                    # Wait until all data is logged from the spawned runs before proceeding with the next set.\n                    pool.join()\n\n        progressBar.markComplete()\n        progressBar.close()\n        # Wait until all data logging is finished before concatenation dataframes and shutting down the pool\n        while not self.dataOutQueue.empty():\n           time.sleep(1)\n        self.dataOutQueue.put((None, None, True))\n        time.sleep(5)\n\n        # if there are failures\n        if len(failed) > 0:\n            failed.sort()\n\n            if self.simParams.verbose:\n                print(\"Failed\", failed, \"saving to 'failures.txt'\")\n\n            if self.simParams.shouldArchiveParameters:\n                # write a file that contains log of failed runs\n                with open(self.archiveDir + \"failures.txt\", \"w\") as failFile:\n                    failFile.write(str(failed))\n\n        return failed\n\n# --- Snippet Separator ---\n\ndef test_MonteCarloSimulation(show_plots):\n    # Test a montecarlo simulation\n    dirName = os.path.abspath(os.path.dirname(__file__)) + \"/tmp_montecarlo_test\"\n    monteCarlo = Controller()\n    monteCarlo.setShouldDisperseSeeds(True)\n    monteCarlo.setExecutionFunction(myExecutionFunction)\n    monteCarlo.setSimulationFunction(myCreationFunction)\n    monteCarlo.setExecutionCount(NUMBER_OF_RUNS)\n    monteCarlo.setThreadCount(PROCESSES)\n    monteCarlo.setVerbose(True)\n    monteCarlo.setArchiveDir(dirName)\n\n    # Add some dispersions\n    disp1Name = 'TaskList[0].TaskModels[0].hub.sigma_BNInit'\n    disp2Name = 'TaskList[0].TaskModels[0].hub.omega_BN_BInit'\n    disp3Name = 'TaskList[0].TaskModels[0].hub.mHub'\n    disp4Name = 'TaskList[0].TaskModels[0].hub.r_BcB_B'\n    disp5Name = 'TaskList[0].TaskModels[0].hub.r_CN_NInit'\n    disp6Name = 'TaskList[0].TaskModels[0].hub.v_CN_NInit'\n    dispDict = {}\n    dispDict[\"mu\"] = 0.3986004415E+15\n    dispDict[\"a\"] = [\"normal\", 42000 * 1E3, 2000 * 1E3]\n    dispDict[\"e\"] = [\"uniform\", 0, 0.5]\n    dispDict[\"i\"] = [\"uniform\", -80, 80]\n    dispDict[\"Omega\"] = None\n    dispDict[\"omega\"] = [\"uniform\", 80, 90]\n    dispDict[\"f\"] = [\"uniform\", 0, 359]\n    monteCarlo.addDispersion(OrbitalElementDispersion(disp5Name, disp6Name, dispDict))\n    monteCarlo.addDispersion(UniformEulerAngleMRPDispersion(disp1Name))\n    monteCarlo.addDispersion(NormalVectorCartDispersion(disp2Name, 0.0, 0.75 / 3.0 * np.pi / 180))\n    monteCarlo.addDispersion(UniformDispersion(disp3Name, ([1300.0 - 812.3, 1500.0 - 812.3])))\n    monteCarlo.addDispersion(\n        NormalVectorCartDispersion(disp4Name, [0.0, 0.0, 1.0], [0.05 / 3.0, 0.05 / 3.0, 0.1 / 3.0]))\n\n    # Add retention policy\n    retentionPolicy = RetentionPolicy()\n    retentionPolicy.addMessageLog(retainedMessageName, [var1, var2])\n    retentionPolicy.setDataCallback(myDataCallback)\n    monteCarlo.addRetentionPolicy(retentionPolicy)\n\n    failures = monteCarlo.executeSimulations()\n\n    assert len(failures) == 0, \"No runs should fail\"\n\n    # Test loading data from runs from disk\n    monteCarloLoaded = Controller.load(dirName)\n\n    retainedData = monteCarloLoaded.getRetainedData(NUMBER_OF_RUNS-1)\n    assert retainedData is not None, \"Retained data should be available after execution\"\n    assert \"messages\" in retainedData, \"Retained data should retain messages\"\n    assert retainedMessageName + \".r_BN_N\" in retainedData[\"messages\"], \"Retained messages should exist\"\n    assert retainedMessageName + \".v_BN_N\" in retainedData[\"messages\"], \"Retained messages should exist\"\n\n    # rerun the case and it should be the same, because we dispersed random seeds\n    oldOutput = retainedData[\"messages\"][retainedMessageName + \".r_BN_N\"]\n\n    failed = monteCarloLoaded.reRunCases([NUMBER_OF_RUNS-1])\n    assert len(failed) == 0, \"Should rerun case successfully\"\n\n    retainedData = monteCarloLoaded.getRetainedData(NUMBER_OF_RUNS-1)\n    newOutput = retainedData[\"messages\"][retainedMessageName + \".r_BN_N\"]\n    for k1, v1 in enumerate(oldOutput):\n        for k2, v2 in enumerate(v1):\n            assert np.fabs(oldOutput[k1][k2] - newOutput[k1][k2]) < .001, \\\n            \"Outputs shouldn't change on runs if random seeds are same\"\n\n    # test the initial parameters were saved from runs, and they differ between runs\n    params1 = monteCarloLoaded.getParameters(NUMBER_OF_RUNS-1)\n    params2 = monteCarloLoaded.getParameters(NUMBER_OF_RUNS-2)\n    assert \"TaskList[0].TaskModels[0].RNGSeed\" in params1, \"random number seed should be applied\"\n    for dispName in [disp1Name, disp2Name, disp3Name, disp4Name]:\n        assert dispName in params1, \"dispersion should be applied\"\n        # assert two different runs had different parameters.\n        assert params1[dispName] != params2[dispName], \"dispersion should be different in each run\"\n\n    monteCarloLoaded.executeCallbacks()\n    if show_plots:\n        plt.show()\n\n    shutil.rmtree(dirName)\n    assert not os.path.exists(dirName), \"No leftover data should exist after the test\"\n\n# --- Snippet Separator ---\n\nclass SimulationExecutor:\n    \"\"\"\n    This class is used to execute a simulation in a worker thread.\n    To use, create an instance of this class, and then call the instance with the simulation parameters to run them in::\n\n        executor = SimulationExecutor()\n        simParams = SimulationParameters()\n        successFlag = executor(simParams)\n\n    This class can be used to execute a simulation on a different thread, by using this class as the processes target.\n    \"\"\"\n    #\n\n    @classmethod\n    def __call__(cls, params):\n        \"\"\"\n        In each worker process, we execute this function (by calling this object)\n\n        Args:\n            params [simParams, data out queue]:\n                A SimulationParameters object for the simulation to be executed and the output data queue\n                for the data writer.\n        Returns:\n            success: bool\n                (True, simParams.index) if simulation run was successful\n                (False, simParams.index) if simulation run was unsuccessful\n        \"\"\"\n        simParams = params[0]\n        dataOutQueue = params[1]\n\n        try:\n            signal.signal(signal.SIGINT, signal.SIG_IGN)  # On ctrl-c ignore the signal... let the parent deal with it.\n\n            # must make new random seed on each new thread.\n            np.random.seed(simParams.index * 10)\n            random.seed(simParams.index * 10)\n\n            # create the users sim by calling their supplied creationFunction\n            simInstance = simParams.creationFunction()\n\n            # build a list of the parameter and random seed modifications to make\n            modifications = simParams.modifications\n            magnitudes = simParams.dispersionMag\n\n            # we may want to disperse random seeds\n            if simParams.shouldDisperseSeeds:\n                # generate the random seeds for the model (but don't apply them yet)\n                # Note: This sets the RNGSeeds before all other modifications\n                randomSeedDispersions = cls.disperseSeeds(simInstance)\n                for name, value in randomSeedDispersions.items():\n                    modifications[name] = value\n\n            # used if rerunning ICs from a .json file, modifications will contain the\n            # RNGSeeds that need to be set before selfInit()\n            cls.populateSeeds(simInstance, modifications)\n\n            # we may want to disperse parameters\n            for disp in simParams.dispersions:\n                try:\n                    name = disp.getName()\n                    if name not in modifications:  # could be using a saved parameter.\n                        modifications[name] = disp.generateString(simInstance)\n                        if simParams.saveDispMag:\n                            magnitudes[name] = disp.generateMagString()\n                except TypeError:\n                    # This accomodates dispersion variables that are co-dependent\n                    disp.generate()\n                    for i in range(1, disp.numberOfSubDisps+1):\n                        name = disp.getName(i)\n                        if name not in modifications:  # could be using a saved parameter.\n                            modifications[name] = disp.generateString(i, simInstance)\n                            if simParams.saveDispMag:\n                                magnitudes[name] = disp.generateMagString()\n\n            # if archiving, this run's parameters and random seeds are saved in its own json file\n            if simParams.shouldArchiveParameters:\n                # save the dispersions and random seeds for this run\n                if simParams.icfilename != \"\":\n                    with open(simParams.icfilename + \".json\", 'w') as outfile:\n                        json.dump(modifications, outfile)\n                else:\n                    with open(simParams.filename + \".json\", 'w') as outfile:\n                        json.dump(modifications, outfile)\n                    if simParams.saveDispMag:\n                        with open(simParams.filename + \"mag.txt\", 'w') as outfileMag:\n                            for k in sorted(magnitudes.keys()):\n                                outfileMag.write(\"'%s':'%s', \\n\" % (k, magnitudes[k]))\n\n            if simParams.configureFunction is not None:\n                if simParams.verbose:\n                    print(\"Configuring sim\")\n                simParams.configureFunction(simInstance)\n\n            # apply the dispersions and the random seeds\n            for variable, value in list(modifications.items()):\n                disperseStatement = \"simInstance.\" + variable + \"=\" + value\n                if simParams.verbose:\n                    print(\"Executing parameter modification -> \", disperseStatement)\n                exec(disperseStatement)\n\n            # setup data logging\n            if len(simParams.retentionPolicies) > 0:\n                if simParams.verbose:\n                    print(\"Adding retained data\")\n                RetentionPolicy.addRetentionPoliciesToSim(simInstance, simParams.retentionPolicies)\n\n            if simParams.verbose:\n                print(\"Executing simulation\")\n            # execute the simulation, with the user-supplied executionFunction\n            try:\n                simParams.executionFunction(simInstance)\n            except TypeError:\n                simParams.executionFunction(simInstance, simParams.filename)\n\n            if len(simParams.retentionPolicies) > 0:\n                if simParams.icfilename != \"\":\n                    retentionFile = simParams.icfilename + \".data\"\n                else:\n                    retentionFile = simParams.filename + \".data\"\n\n                if simParams.verbose:\n                    print(\"Retaining data for run in\", retentionFile)\n\n                retainedData = RetentionPolicy.getDataForRetention(simInstance, simParams.retentionPolicies)\n                dataOutQueue.put((retainedData, simParams.index, None))\n                time.sleep(1)\n\n                with gzip.open(retentionFile, \"w\") as archive:\n                    retainedData[\"index\"] = simParams.index # add run index\n                    pickle.dump(retainedData, archive)\n\n            if simParams.verbose:\n                print(\"Terminating simulation\")\n\n            if simParams.verbose:\n                print(\"Thread\", os.getpid(), \"Job\", simParams.index, \"finished successfully\")\n\n            return (True, simParams.index)  # this function returns true only if the simulation was successful\n\n        except Exception as e:\n            print(\"Error in worker thread\", e)\n            traceback.print_exc()\n            return (False, simParams.index)  # there was an error\n\n    @staticmethod\n    def disperseSeeds(simInstance):\n        \"\"\"\n        Disperses the RNG seeds of all the tasks in the sim, and returns a statement that contains the seeds.\n        Example return dictionary::\n\n             {\n                '.TaskList[0].TaskModels[1]': 1934586,\n                '.TaskList[0].TaskModels[2]': 3450093,\n                '.TaskList[1].TaskModels[0]': 2221934,\n                '.TaskList[2].TaskModels[0]': 1123244\n             }\n\n        :param simInstance: A basilisk simulation to set random seeds on\n        :type simInstance: SimulationBaseClass\n        :return: A dictionary with the random seeds that should be applied to the sim\n                        \"\"\"\n\n        randomSeeds = {}\n        for i, task in enumerate(simInstance.TaskList):\n            for j, model in enumerate(task.TaskModels):\n                taskVar = 'TaskList[' + str(i) + '].TaskModels' + '[' + str(j) + '].RNGSeed'\n                rand = str(random.randint(0, 1 << 32 - 1))\n                try:\n                    execStatement = \"simInstance.\" + taskVar + \"=\" + str(rand)\n                    exec(execStatement)  # if this fails don't add to the list of modification\n                    randomSeeds[taskVar] = rand\n                except:\n                    pass\n        return randomSeeds\n\n    @staticmethod\n    def populateSeeds(simInstance, modifications):\n        \"\"\"\n        only populate the RNG seeds of all the tasks in the sim\n\n        Args:\n            simInstance: SimulationBaseClass\n                A basilisk simulation to set random seeds on\n            modifications:\n                A dictionary containing RNGSeeds to be populate for the sim, among other sim modifications.\n        \"\"\"\n        for variable, value in modifications.items():\n            if \".RNGSeed\" in variable:\n                rngStatement = \"simInstance.\" + variable + \"=\" + value\n                exec(rngStatement)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary libraries and modules, including the 'Controller', 'RetentionPolicy', and various 'Dispersions' from the 'Basilisk.utilities.MonteCarlo' module, as well as a scenario module named 'scenario_AttFeedback'. The code should define a function 'run' that takes a boolean parameter 'show_plots'. Inside this function, it should create a Monte Carlo simulation controller, set its simulation and execution functions, execution count, archive directory, seed dispersion, thread count, verbosity, variable casting, and dispersion magnitude file. It should also define a list of dispersions and add them to the Monte Carlo controller. Then, it should create a retention policy, add message logs to it, set its data callback, and add it to the Monte Carlo controller. The function should execute the simulations, execute callbacks if 'show_plots' is True, and return. The code should also define a function 'displayPlots' that takes 'data' and 'retentionPolicy' as parameters, extracts time and states from the data, and plots the states against time. Finally, it should call the 'run' function with 'True' as the argument if the script is run as the main program.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 37, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that performs the following tasks using the blenderproc library:\n\n1. Parse command line arguments for a house.json file path, a chair object path, and an optional output directory.\n2. Initialize blenderproc.\n3. Load objects from the house.json file into the scene using a label mapping from a csv file.\n4. Load a chair object from the provided path and replace all chair objects in the scene with this chair object. The replacement should ignore collisions with floor objects and copy properties from the original objects. The pose of the new chair objects should be randomly rotated around the z-axis.\n5. Filter out invalid objects from the scene.\n6. Make all Suncg objects in the scene emit light.\n7. Initialize a point sampler for sampling locations inside the loaded house and a bvh tree containing all mesh objects.\n8. Sample camera poses inside the house, ensuring that obstacles are at least 1 meter away from the camera and the view covers at least 40% of the scene. Add these camera poses to the scene.\n9. Enable normal, depth, and segmentation rendering. Add an alpha channel to textures.\n10. Render the scene and write the rendered data to a .hdf5 file in the specified output directory.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def replace_objects(objects_to_be_replaced: List[MeshObject], objects_to_replace_with: List[MeshObject],\n                    ignore_collision_with: Optional[List[MeshObject]] = None, replace_ratio: float = 1,\n                    copy_properties: bool = True, max_tries: int = 100,\n                    relative_pose_sampler: Callable[[MeshObject], None] = None):\n    \"\"\"\n    Replaces mesh objects with another mesh objects and scales them accordingly, the replaced objects and the\n    objects to replace with in following steps:\n\n    1. Randomly select a subset of objects_to_be_replaced.\n    2. For each of these objects, sample other objects from objects_to_replace_with and try to replace them.\n    3. In each try, the poses of the objects are aligned and a check for collisions with other objects is done.\n    4. An object is skipped if max_tries is reached.\n\n    :param objects_to_be_replaced: Objects, which should be removed from the scene.\n    :param objects_to_replace_with: Objects, which will be tried to be added to the scene.\n    :param ignore_collision_with: Objects, which are not checked for collisions with.\n    :param replace_ratio: Ratio of objects in the original scene, which will be replaced.\n    :param copy_properties: Copies the custom properties of the objects_to_be_replaced to the objects_to_replace_with.\n    :param max_tries: Maximum number of tries to replace one object.\n    :param relative_pose_sampler: A function that randomly perturbs the pose of the object to replace with\n                                  (after it has been aligned to the object to replace).\n    \"\"\"\n    if ignore_collision_with is None:\n        ignore_collision_with = []\n\n    # Hide new objects from renderers until they are added\n    for obj in objects_to_replace_with:\n        obj.hide()\n\n    check_collision_with = []\n    for obj in get_all_mesh_objects():\n        if obj not in ignore_collision_with:\n            check_collision_with.append(obj)\n\n    # amount of replacements depends on the amount of objects and the replace ratio\n    objects_to_be_replaced = random.sample(objects_to_be_replaced, k=int(len(objects_to_be_replaced) * replace_ratio))\n    if len(objects_to_be_replaced) == 0:\n        print(\"Warning: The amount of objects, which should be replace is zero!\")\n\n    # Go over all objects we should replace\n    for current_object_to_be_replaced in objects_to_be_replaced:\n        print(current_object_to_be_replaced.get_name())\n        # Do at most max_tries to replace the object with a random object from  objects_to_replace_with\n        tries = 0\n        while tries < max_tries:\n            current_object_to_replace_with = np.random.choice(objects_to_replace_with)\n            if _ObjectReplacer.replace(current_object_to_be_replaced, current_object_to_replace_with,\n                                       check_collision_with, relative_pose_sampler=relative_pose_sampler):\n\n                # Duplicate the added object to be able to add it again\n                duplicate_new_object = current_object_to_replace_with.duplicate()\n\n                # Copy properties to the newly duplicated object\n                if copy_properties:\n                    for key, value in current_object_to_be_replaced.get_all_cps().items():\n                        duplicate_new_object.set_cp(key, value)\n\n                duplicate_new_object.hide(False)\n\n                print('Replaced ', current_object_to_be_replaced.get_name(), ' by ', duplicate_new_object.get_name())\n\n                # Delete the original object and remove it from the list\n                check_collision_with.remove(current_object_to_be_replaced)\n                current_object_to_be_replaced.delete()\n                break\n            tries += 1\n\n        if tries == max_tries:\n            print(\"Could not replace \" + current_object_to_be_replaced.get_name())\n\n# --- Snippet Separator ---\n\ndef load_blend(path: str, obj_types: Optional[Union[List[str], str]] = None, name_regrex: Optional[str] = None,\n               data_blocks: Union[List[str], str] = \"objects\", link: bool = False) -> List[Entity]:\n    \"\"\"\n    Loads entities (everything that can be stored in a .blend file's folders, see Blender's documentation for\n    bpy.types.ID for more info) that match a name pattern from a specified .blend file's section/data_block.\n\n    :param path: Path to a .blend file.\n    :param obj_types: The type of objects to load. This parameter is only relevant when `data_blocks`\n                      is set to `\"objects\"`. Available options are: ['mesh', 'curve', 'hair', 'armature',\n                      'empty', 'light', 'camera']\n    :param name_regrex: Regular expression representing a name pattern of entities' (everything that can be\n                        stored in a .blend file's folders, see Blender's documentation for bpy.types.ID\n                        for more info) names.\n    :param data_blocks: The data block or a list of data blocks which should be loaded from the given .blend file.\n                        Available options are: ['armatures', 'cameras', 'curves', 'hairs', 'images', 'lights',\n                        'materials', 'meshes', 'objects', 'textures']\n    :param link: whether to link instead of append data blocks from .blend file. Linked objects can not be modified.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    if obj_types is None:\n        obj_types = [\"mesh\", \"empty\"]\n    # get a path to a .blend file\n    path = resolve_path(path)\n    data_blocks = _BlendLoader.validate_and_standardizes_configured_list(data_blocks, _BlendLoader.valid_data_blocks,\n                                                                         \"data block\")\n    obj_types = _BlendLoader.validate_and_standardizes_configured_list(obj_types, _BlendLoader.valid_object_types,\n                                                                       \"object type\")\n\n    # Remember which orphans existed beforehand\n    orphans_before = collect_all_orphan_data_blocks()\n\n    # Start importing blend file. All objects that should be imported need to be copied from \"data_from\" to \"data_to\"\n    with bpy.data.libraries.load(path, link=link) as (data_from, data_to):\n        for data_block in data_blocks:\n            # Verify that the given data block is valid\n            if hasattr(data_from, data_block):\n                # Find all entities of this data block that match the specified pattern\n                data_to_entities = []\n                for entity_name in getattr(data_from, data_block):\n                    if not name_regrex or re.fullmatch(name_regrex, entity_name) is not None:\n                        data_to_entities.append(entity_name)\n                # Import them\n                setattr(data_to, data_block, data_to_entities)\n                print(\"Imported \" + str(len(data_to_entities)) + \" \" + data_block)\n            else:\n                raise Exception(\"No such data block: \" + data_block)\n\n    # Go over all imported objects again\n    loaded_objects: List[Entity] = []\n    for data_block in data_blocks:\n        # Some adjustments that only affect objects\n        if data_block == \"objects\":\n            for obj in getattr(data_to, data_block):\n                # Check that the object type is desired\n                if obj.type.lower() in obj_types:\n                    # Link objects to the scene\n                    bpy.context.collection.objects.link(obj)\n                    loaded_objects.append(convert_to_entity_subclass(obj))\n\n                    # If a camera was imported\n                    if obj.type == 'CAMERA':\n                        # Make it the active camera in the scene\n                        bpy.context.scene.camera = obj\n\n                        # Find the maximum frame number of its key frames\n                        max_keyframe = -1\n                        if obj.animation_data is not None:\n                            fcurves = obj.animation_data.action.fcurves\n                            for curve in fcurves:\n                                keyframe_points = curve.keyframe_points\n                                for keyframe in keyframe_points:\n                                    max_keyframe = max(max_keyframe, keyframe.co[0])\n\n                        # Set frame_end to the next free keyframe\n                        bpy.context.scene.frame_end = max_keyframe + 1\n                else:\n                    # Remove object again if its type is not desired\n                    bpy.data.objects.remove(obj, do_unlink=True)\n            print(\"Selected \" + str(len(loaded_objects)) + \" of the loaded objects by type\")\n        else:\n            loaded_objects.extend(getattr(data_to, data_block))\n\n    # As some loaded objects were deleted again due to their type, we need also to remove the dependent\n    # data blocks that were also loaded and are now orphans\n    _BlendLoader.purge_added_orphans(orphans_before, data_to)\n    return loaded_objects\n\n# --- Snippet Separator ---\n\ndef render(output_dir: Optional[str] = None, file_prefix: str = \"rgb_\", output_key: Optional[str] = \"colors\",\n           load_keys: Optional[Set[str]] = None, return_data: bool = True,\n           keys_with_alpha_channel: Optional[Set[str]] = None,\n           verbose: bool = False) -> Dict[str, Union[np.ndarray, List[np.ndarray]]]:\n    \"\"\" Render all frames.\n\n    This will go through all frames from scene.frame_start to scene.frame_end and render each of them.\n\n    :param output_dir: The directory to write files to, if this is None the temporary directory is used. \\\n                       The temporary directory is usually in the shared memory (only true for linux).\n    :param file_prefix: The prefix to use for writing the images.\n    :param output_key: The key to use for registering the output.\n    :param load_keys: Set of output keys to load when available\n    :param return_data: Whether to load and return generated data.\n    :param keys_with_alpha_channel: A set containing all keys whose alpha channels should be loaded.\n    :param verbose: If True, more details about the rendering process are printed.\n    :return: dict of lists of raw renderer output. Keys can be 'distance', 'colors', 'normals'\n    \"\"\"\n    if output_dir is None:\n        output_dir = Utility.get_temporary_directory()\n    if load_keys is None:\n        load_keys = {'colors', 'distance', 'normals', 'diffuse', 'depth', 'segmap'}\n        keys_with_alpha_channel = {'colors'} if bpy.context.scene.render.film_transparent else None\n\n    if output_key is not None:\n        Utility.add_output_entry({\n            \"key\": output_key,\n            \"path\": os.path.join(output_dir, file_prefix) + \"%04d\" +\n                    map_file_format_to_file_ending(bpy.context.scene.render.image_settings.file_format),\n            \"version\": \"2.0.0\"\n        })\n        load_keys.add(output_key)\n\n    bpy.context.scene.render.filepath = os.path.join(output_dir, file_prefix)\n\n    # Skip if there is nothing to render\n    if bpy.context.scene.frame_end != bpy.context.scene.frame_start:\n        if len(get_all_blender_mesh_objects()) == 0:\n            raise Exception(\"There are no mesh-objects to render, \"\n                            \"please load an object before invoking the renderer.\")\n        # Print what is rendered\n        total_frames = bpy.context.scene.frame_end - bpy.context.scene.frame_start\n        if load_keys:\n            registered_output_keys = [output[\"key\"] for output in Utility.get_registered_outputs()]\n            keys_to_render = sorted([key for key in load_keys if key in registered_output_keys])\n            print(f\"Rendering {total_frames} frames of {', '.join(keys_to_render)}...\")\n\n        # As frame_end is pointing to the next free frame, decrease it by one, as\n        # blender will render all frames in [frame_start, frame_ned]\n        bpy.context.scene.frame_end -= 1\n\n        # Define pipe to communicate blenders debug messages to progress bar\n        pipe_out, pipe_in = os.pipe()\n        begin = time.time()\n        with stdout_redirected(pipe_in, enabled=not verbose) as stdout:\n            with _render_progress_bar(pipe_out, pipe_in, stdout, total_frames, enabled=not verbose):\n                bpy.ops.render.render(animation=True, write_still=True)\n\n        # Close Pipes to prevent having unclosed file handles\n        try:\n            os.close(pipe_out)\n        except OSError:\n            pass\n        try:\n            os.close(pipe_in)\n        except OSError:\n            pass\n\n        print(f\"Finished rendering after {time.time() - begin:.3f} seconds\")\n        # Revert changes\n        bpy.context.scene.frame_end += 1\n    else:\n        raise RuntimeError(\"No camera poses have been registered, therefore nothing can be rendered. A camera \"\n                           \"pose can be registered via bproc.camera.add_camera_pose().\")\n\n    return _WriterUtility.load_registered_outputs(load_keys, keys_with_alpha_channel) if return_data else {}\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks using the blenderproc library:\n\n1. Parse command line arguments for a house.json file path, a chair object path, and an optional output directory.\n2. Initialize blenderproc.\n3. Load objects from the house.json file into the scene using a label mapping from a csv file.\n4. Load a chair object from the provided path and replace all chair objects in the scene with this chair object. The replacement should ignore collisions with floor objects and copy properties from the original objects. The pose of the new chair objects should be randomly rotated around the z-axis.\n5. Filter out invalid objects from the scene.\n6. Make all Suncg objects in the scene emit light.\n7. Initialize a point sampler for sampling locations inside the loaded house and a bvh tree containing all mesh objects.\n8. Sample camera poses inside the house, ensuring that obstacles are at least 1 meter away from the camera and the view covers at least 40% of the scene. Add these camera poses to the scene.\n9. Enable normal, depth, and segmentation rendering. Add an alpha channel to textures.\n10. Render the scene and write the rendered data to a .hdf5 file in the specified output directory.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 38, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation environment using the seed-emulator library. The environment should include three types of autonomous systems (AS): transit, stub, and utility. \n\nFor the transit AS, create two internet exchanges with specific display names, three internal networks, and four routers linked in a linear structure. \n\nFor the stub AS, create three different systems. The first two should have an internal network, a router, and two host nodes. The first host node should have additional software installed and a new account created. The third system should be created using a utility function and further customized.\n\nEstablish BGP peering by creating an Ebgp layer and setting up the transit AS as the internet service provider for all the stub ASes. Also, set up direct peering between two of the stub ASes.\n\nCreate a web service layer with two web service nodes and bind these virtual nodes to physical nodes. \n\nAdd all the created layers to the emulator and save it to a component file. Render the emulator and change the display names for the nodes hosting the web services.\n\nFinally, compile the emulator using Docker, specifying custom images from DockerHub and local sources. Generate Docker files and copy the base container image to the output folder.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def makeEmulatorBaseWith5StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int,\n    services: List[Service]):\n    \"\"\"!\n    @brief create a new stub AS.\n\n    @param emu reference to the Emulator object.\n    @param base reference to the base layer.\n    @param asn ASN for the newly created AS.\n    @param exchange IXP ID for new newly created AS to join.\n    @param list of instances of Service to install on hosts. One host will be\n    created for each.\n    \"\"\"\n\n    # Create AS and internal network\n    stub_as = base.createAutonomousSystem(asn)\n    stub_as.createNetwork('net0')\n\n    # Create a BGP router \n    # Attach the router to both the internal and external networks\n    router = stub_as.createRouter('router0')\n    router.joinNetwork('net0')\n    router.joinNetwork('ix{}'.format(exchange))\n\n    # Create a host node for each specified service\n    createHostsOnNetwork(emu, stub_as, 'net0', services)\n\n# --- Snippet Separator ---\n\ndef makeEmulatorBaseWith10StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 160, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 161, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 162, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 163, 104, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 164, 104, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(103, [3],  [160, 161, 162], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(104, [3, 4], [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [4],  [163], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [12], [164], PeerRelationship.Provider)\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulation environment using the seed-emulator library. The environment should include three types of autonomous systems (AS): transit, stub, and utility. \n\nFor the transit AS, create two internet exchanges with specific display names, three internal networks, and four routers linked in a linear structure. \n\nFor the stub AS, create three different systems. The first two should have an internal network, a router, and two host nodes. The first host node should have additional software installed and a new account created. The third system should be created using a utility function and further customized.\n\nEstablish BGP peering by creating an Ebgp layer and setting up the transit AS as the internet service provider for all the stub ASes. Also, set up direct peering between two of the stub ASes.\n\nCreate a web service layer with two web service nodes and bind these virtual nodes to physical nodes. \n\nAdd all the created layers to the emulator and save it to a component file. Render the emulator and change the display names for the nodes hosting the web services.\n\nFinally, compile the emulator using Docker, specifying custom images from DockerHub and local sources. Generate Docker files and copy the base container image to the output folder.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 39, "repo_full_name": "geodynamics__burnman", "instruction": "Generate code that demonstrates how to create different minerals, compute seismic velocities, and compare them to a seismic reference model using the BurnMan library. The code should include four examples of creating composite minerals: two minerals mixed in simple mole fractions, a mix of three minerals, using preset solutions, and defining a custom solution. The code should also include the computation of seismic velocities and other properties by supplying BurnMan with a list of minerals and their molar abundances. The results should be compared to a seismic model, and the misfit between the computed and reference values should be calculated. Finally, the code should plot the computed and reference values of Vs, Vphi, density, and geotherm against pressure. The plots should be saved as an image file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def misfit(phase_1_fraction):\n        # Here we define the rock as before.\n        phase_2_fraction = 1.0 - phase_1_fraction\n        rock = burnman.Composite(\n            [minerals.SLB_2011.stishovite(), minerals.SLB_2011.wuestite()],\n            [phase_1_fraction, phase_2_fraction],\n        )\n\n        # Just as in step 1, we want to set which equation of state we use,\n        # then call rock.evaluate(), which evaluates the\n        # seismic velocities and density at the requested\n        # pressures and temperatures\n        rock.set_method(\"slb3\")\n        density, vphi, vs = rock.evaluate(\n            [\"density\", \"v_phi\", \"v_s\"], pressure, temperature\n        )\n\n        # Since we will call this misfit function many times, we may be interested\n        # in a status report.  These lines print some debug output so we\n        # can keep track of what the script is doing.\n        print(\"Calculations are done for:\")\n        rock.debug_print()\n\n        # Here we integrate an L2 difference with depth between our calculated seismic\n        # profiles and PREM.  We then return those misfits.\n        [vs_err, vphi_err, rho_err] = burnman.utils.math.compare_l2(\n            depths, [vs, vphi, density], [seis_vs, seis_vphi, seis_rho]\n        )\n\n        return vs_err, vphi_err, rho_err\n\n# --- Snippet Separator ---\n\ndef unroll(self):\n        \"\"\"\n        Unroll this material into a list of :class:`burnman.Mineral` and their molar\n        fractions. All averaging schemes then operate on this list of minerals.\n        Note that the return value of this function may depend on the current\n        state (temperature, pressure).\n\n        .. note:: Needs to be implemented in derived classes.\n\n        :returns: A list of molar fractions which should sum to 1.0,\n            and a list of :class:`burnman.Mineral` objects\n            containing the minerals in the material.\n        :rtype: tuple\n        \"\"\"\n        raise NotImplementedError(\"need to implement unroll() in derived class!\")\n\n# --- Snippet Separator ---\n\ndef equilibrium_temperature(\n    minerals, stoichiometry, pressure, temperature_initial_guess=1000.0\n):\n    \"\"\"\n    Given a list of minerals, their reaction stoichiometries\n    and a pressure of interest, compute the\n    equilibrium temperature of the reaction.\n\n    :param minerals: List of minerals involved in the reaction.\n    :type minerals: list of :class:`burnman.Mineral`\n\n    :param stoichiometry: Reaction stoichiometry for the minerals provided.\n        Reactants and products should have the opposite signs [mol].\n    :type stoichiometry: list of floats\n\n    :param pressure: Pressure of interest [Pa].\n    :type pressure: float\n\n    :param temperature_initial_guess: Initial temperature guess [K].\n    :type temperature_initial_guess: float\n\n    :returns: The equilibrium temperature of the reaction [K].\n    :rtype: float\n    \"\"\"\n\n    def eqm(T, P):\n        gibbs = 0.0\n        for i, mineral in enumerate(minerals):\n            mineral.set_state(P, T[0])\n            gibbs = gibbs + mineral.gibbs * stoichiometry[i]\n        return gibbs\n\n    temperature = fsolve(eqm, [temperature_initial_guess], args=(pressure))[0]\n\n    return temperature\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that demonstrates how to create different minerals, compute seismic velocities, and compare them to a seismic reference model using the BurnMan library. The code should include four examples of creating composite minerals: two minerals mixed in simple mole fractions, a mix of three minerals, using preset solutions, and defining a custom solution. The code should also include the computation of seismic velocities and other properties by supplying BurnMan with a list of minerals and their molar abundances. The results should be compared to a seismic model, and the misfit between the computed and reference values should be calculated. Finally, the code should plot the computed and reference values of Vs, Vphi, density, and geotherm against pressure. The plots should be saved as an image file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 40, "repo_full_name": "microsoft__nni", "instruction": "Generate code that performs the following tasks using the nni library:\n\n1. Fine-tunes a ResNet18 model on the Cifar10 dataset for 30 epochs and evaluates its accuracy.\n2. Creates a teacher model by duplicating the fine-tuned model.\n3. Creates a pruner using the TaylorPruner and AGPPruner classes, with a configuration list that targets Conv2d operations and has a sparse ratio of 0.5. The pruner is set to run for 100 training steps and 30 total times.\n4. Creates a quantizer using the QATQuantizer class, with a configuration list that targets Conv2d and BatchNorm2d operations and uses int8 quantization. The quantizer starts at the 100th step.\n5. Creates a distiller using the DynamicLayerwiseDistiller class, with a configuration list that targets Conv2d operations and uses the mean squared error method. The distiller uses the teacher model for predictions.\n6. Compresses the model using the distiller for 60 iterations of 100 steps each.\n7. Speeds up the model using the ModelSpeedup class and the masks from the pruner.\n8. Evaluates the accuracy of the compressed model.\n9. Simulates quantization by updating the calibration configuration of the model using the Quantizer class.\n10. Evaluates the accuracy of the compressed and quantized model.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Compressor:\n    \"\"\"\n    Common base class for all compressors.\n\n    This class is designed for other base classes.\n    Algorithms should inherit ``Pruner`` or ``Quantizer`` instead.\n\n\n    Attributes\n    ----------\n    bound_model : tf.keras.Model\n        Compressed user model.\n    wrappers : list of tf.keras.Model\n        A wrapper is an instrumented TF ``Layer``, in ``Model`` format.\n        The list is ordered by preorder traversal.\n\n    Parameters\n    ----------\n    LayerWrapperClass : a class derive from Model\n        The class used to instrument layers.\n    model : tf.keras.Model\n        The user model to be compressed.\n    config_list : list of JSON object\n        User configuration. The format is detailed in tutorial.\n    \"\"\"\n\n    def __init__(self, LayerWrapperClass, model, config_list):\n        assert isinstance(model, tf.keras.Model)\n        if isinstance(model, tf.keras.Sequential):\n            raise ValueError('NNI model compression does not support `Sequential` model for now')\n        self.validate_config(model, config_list)\n\n        self.bound_model = model\n        self.wrappers = []\n\n        for layer_info in _detect_layers_to_compress(model, config_list):\n            self.wrappers.append(LayerWrapperClass(layer_info, self))\n        if not self.wrappers:\n            _logger.warning('Nothing is configured to compress, please check your model and config list')\n\n        _instrument_model(model, self.wrappers)\n\n    def set_wrappers_attribute(self, name, value):\n        \"\"\"\n        Call ``setattr`` on all wrappers.\n        \"\"\"\n        for wrapper in self.wrappers:\n            setattr(wrapper, name, value)\n\n# --- Snippet Separator ---\n\nclass SlimPruner(OneshotPruner):\n    \"\"\"\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model to be pruned\n    config_list : list\n        Supported keys:\n            - sparsity : This is to specify the sparsity operations to be compressed to.\n            - op_types : Only BatchNorm2d is supported in Slim Pruner.\n    optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n    \"\"\"\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, pruning_algorithm='slim', optimizer=optimizer)\n\n    def validate_config(self, model, config_list):\n        schema = CompressorSchema([{\n            'sparsity': And(float, lambda n: 0 < n < 1),\n            'op_types': ['BatchNorm2d'],\n            Optional('op_names'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n        if len(config_list) > 1:\n            logger.warning('Slim pruner only supports 1 configuration')\n\n# --- Snippet Separator ---\n\nclass Pruner(Compressor):\n    \"\"\"\n    Base class for pruning algorithms.\n\n    End users should use ``compress`` and callback APIs (WIP) to prune their models.\n\n    The underlying model is instrumented upon initialization of pruner object.\n    So if you want to pre-train the model, train it before creating pruner object.\n\n    The compressed model can only execute in eager mode.\n\n    Algorithm developers should override ``calc_masks`` method to specify pruning strategy.\n\n    Parameters\n    ----------\n    model : tf.keras.Model\n        The user model to prune.\n    config_list : list of JSON object\n        User configuration. The format is detailed in tutorial.\n    \"\"\"\n    def __init__(self, model, config_list):\n        super().__init__(PrunerLayerWrapper, model, config_list)\n        #self.callback = PrunerCallback(self)\n\n    def compress(self):\n        \"\"\"\n        Apply compression on a pre-trained model.\n\n        If you want to prune the model during training, use callback API (WIP) instead.\n\n        Returns\n        -------\n        tf.keras.Model\n            The compressed model, for convenience. This is exactly the same object to constructor argument.\n        \"\"\"\n        self._update_mask()\n        return self.bound_model\n\n    def calc_masks(self, wrapper, **kwargs):\n        \"\"\"\n        Abstract method to be overridden by algorithm. End users should ignore it.\n\n        If the callback is set up, this method will be invoked at end of each training minibatch.\n        If not, it will only be called when end user invokes ``compress``.\n\n        Parameters\n        ----------\n        wrapper : PrunerLayerWrapper\n            The instrumented layer.\n        **kwargs\n            Reserved for forward compatibility.\n\n        Returns\n        -------\n        dict of (str, tf.Tensor), or None\n            The key is weight ``Variable``'s name. The value is a mask ``Tensor`` of weight's shape and dtype.\n            If a weight's key does not appear in the return value, that weight will not be pruned.\n            Returning ``None`` means the mask is not changed since last time.\n            Weight names are globally unique, e.g. `model/conv_1/kernel:0`.\n        \"\"\"\n        # TODO: maybe it should be able to calc on weight-granularity, beside from layer-granularity\n        raise NotImplementedError(\"Pruners must overload calc_masks()\")\n\n    def _update_mask(self):\n        for wrapper_idx, wrapper in enumerate(self.wrappers):\n            masks = self.calc_masks(wrapper, wrapper_idx=wrapper_idx)\n            if masks is not None:\n                wrapper.masks = masks\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks using the nni library:\n\n1. Fine-tunes a ResNet18 model on the Cifar10 dataset for 30 epochs and evaluates its accuracy.\n2. Creates a teacher model by duplicating the fine-tuned model.\n3. Creates a pruner using the TaylorPruner and AGPPruner classes, with a configuration list that targets Conv2d operations and has a sparse ratio of 0.5. The pruner is set to run for 100 training steps and 30 total times.\n4. Creates a quantizer using the QATQuantizer class, with a configuration list that targets Conv2d and BatchNorm2d operations and uses int8 quantization. The quantizer starts at the 100th step.\n5. Creates a distiller using the DynamicLayerwiseDistiller class, with a configuration list that targets Conv2d operations and uses the mean squared error method. The distiller uses the teacher model for predictions.\n6. Compresses the model using the distiller for 60 iterations of 100 steps each.\n7. Speeds up the model using the ModelSpeedup class and the masks from the pruner.\n8. Evaluates the accuracy of the compressed model.\n9. Simulates quantization by updating the calibration configuration of the model using the Quantizer class.\n10. Evaluates the accuracy of the compressed and quantized model.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 41, "repo_full_name": "unidata__metpy", "instruction": "Generate code that uses the MetPy library to create an advanced sounding plot. The code should first import necessary libraries and modules. Then, it should load a sample dataset, clean it by dropping rows with NaN values in specific columns, and assign units to the data. After preparing the data, the code should create a new figure with a specific aspect ratio and plot the data using normal plotting functions. It should also set custom labels for the x and y axes. The code should calculate the lifted condensation level (LCL) and plot it as a black dot, and calculate the full parcel profile and add it to the plot as a black line. It should shade areas of Convective Available Potential Energy (CAPE) and Convective Inhibition (CIN). Finally, the code should add special lines to the plot and display it.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def surface_based_cape_cin(pressure, temperature, dewpoint):\n    r\"\"\"Calculate surface-based CAPE and CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile for a surface-based parcel. CIN is integrated\n    between the surface and LFC, CAPE is integrated between the LFC and EL (or top of\n    sounding). Intersection points of the measured temperature profile and parcel profile are\n    logarithmically interpolated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile. The first entry should be the starting\n        (surface) observation, with the array going from high to low pressure.\n\n    temperature : `pint.Quantity`\n        Temperature profile corresponding to the `pressure` profile\n\n    dewpoint : `pint.Quantity`\n        Dewpoint profile corresponding to the `pressure` profile\n\n    Returns\n    -------\n    `pint.Quantity`\n        Surface based Convective Available Potential Energy (CAPE)\n\n    `pint.Quantity`\n        Surface based Convective Inhibition (CIN)\n\n    See Also\n    --------\n    cape_cin, parcel_profile\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    \"\"\"\n    pressure, temperature, dewpoint = _remove_nans(pressure, temperature, dewpoint)\n    p, t, td, profile = parcel_profile_with_lcl(pressure, temperature, dewpoint)\n    return cape_cin(p, t, td, profile)\n\n# --- Snippet Separator ---\n\ndef cape_cin(pressure, temperature, dewpoint, parcel_profile, which_lfc='bottom',\n             which_el='top'):\n    r\"\"\"Calculate CAPE and CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile and parcel path. CIN is integrated between the surface and\n    LFC, CAPE is integrated between the LFC and EL (or top of sounding). Intersection points\n    of the measured temperature profile and parcel profile are logarithmically interpolated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure level(s) of interest, in order from highest to\n        lowest pressure\n\n    temperature : `pint.Quantity`\n        Atmospheric temperature corresponding to pressure\n\n    dewpoint : `pint.Quantity`\n        Atmospheric dewpoint corresponding to pressure\n\n    parcel_profile : `pint.Quantity`\n        Temperature profile of the parcel\n\n    which_lfc : str\n        Choose which LFC to integrate from. Valid options are 'top', 'bottom', 'wide',\n        and 'most_cape'. Default is 'bottom'.\n\n    which_el : str\n        Choose which EL to integrate to. Valid options are 'top', 'bottom', 'wide',\n        and 'most_cape'. Default is 'top'.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Convective Available Potential Energy (CAPE)\n\n    `pint.Quantity`\n        Convective Inhibition (CIN)\n\n    Examples\n    --------\n    >>> from metpy.calc import cape_cin, dewpoint_from_relative_humidity, parcel_profile\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # compture parcel temperature\n    >>> prof = parcel_profile(p, T[0], Td[0]).to('degC')\n    >>> # calculate surface based CAPE/CIN\n    >>> cape_cin(p, T, Td, prof)\n    (<Quantity(4703.77306, 'joule / kilogram')>, <Quantity(0, 'joule / kilogram')>)\n\n    See Also\n    --------\n    lfc, el\n\n    Notes\n    -----\n    Formula adopted from [Hobbs1977]_.\n\n    .. math:: \\text{CAPE} = -R_d \\int_{LFC}^{EL}\n            (T_{{v}_{parcel}} - T_{{v}_{env}}) d\\text{ln}(p)\n\n    .. math:: \\text{CIN} = -R_d \\int_{SFC}^{LFC}\n            (T_{{v}_{parcel}} - T_{{v}_{env}}) d\\text{ln}(p)\n\n\n    * :math:`CAPE` is convective available potential energy\n    * :math:`CIN` is convective inhibition\n    * :math:`LFC` is pressure of the level of free convection\n    * :math:`EL` is pressure of the equilibrium level\n    * :math:`SFC` is the level of the surface or beginning of parcel path\n    * :math:`R_d` is the gas constant\n    * :math:`g` is gravitational acceleration\n    * :math:`T_{{v}_{parcel}}` is the parcel virtual temperature\n    * :math:`T_{{v}_{env}}` is environment virtual temperature\n    * :math:`p` is atmospheric pressure\n\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``dewpt`` parameter to ``dewpoint``\n\n    \"\"\"\n    pressure, temperature, dewpoint, parcel_profile = _remove_nans(pressure, temperature,\n                                                                   dewpoint, parcel_profile)\n\n    pressure_lcl, _ = lcl(pressure[0], temperature[0], dewpoint[0])\n    below_lcl = pressure > pressure_lcl\n\n    # The mixing ratio of the parcel comes from the dewpoint below the LCL, is saturated\n    # based on the temperature above the LCL\n    parcel_mixing_ratio = np.where(below_lcl, saturation_mixing_ratio(pressure, dewpoint),\n                                   saturation_mixing_ratio(pressure, temperature))\n\n    # Convert the temperature/parcel profile to virtual temperature\n    temperature = virtual_temperature_from_dewpoint(pressure, temperature, dewpoint)\n    parcel_profile = virtual_temperature(parcel_profile, parcel_mixing_ratio)\n\n    # Calculate LFC limit of integration\n    lfc_pressure, _ = lfc(pressure, temperature, dewpoint,\n                          parcel_temperature_profile=parcel_profile, which=which_lfc)\n\n    # If there is no LFC, no need to proceed.\n    if np.isnan(lfc_pressure):\n        return units.Quantity(0, 'J/kg'), units.Quantity(0, 'J/kg')\n    else:\n        lfc_pressure = lfc_pressure.magnitude\n\n    # Calculate the EL limit of integration\n    el_pressure, _ = el(pressure, temperature, dewpoint,\n                        parcel_temperature_profile=parcel_profile, which=which_el)\n\n    # No EL and we use the top reading of the sounding.\n    if np.isnan(el_pressure):\n        el_pressure = pressure[-1].magnitude\n    else:\n        el_pressure = el_pressure.magnitude\n\n    # Difference between the parcel path and measured temperature profiles\n    y = (parcel_profile - temperature).to(units.degK)\n\n    # Estimate zero crossings\n    x, y = _find_append_zero_crossings(np.copy(pressure), y)\n\n    # CAPE\n    # Only use data between the LFC and EL for calculation\n    p_mask = _less_or_close(x.m, lfc_pressure) & _greater_or_close(x.m, el_pressure)\n    x_clipped = x[p_mask].magnitude\n    y_clipped = y[p_mask].magnitude\n    cape = (mpconsts.Rd\n            * units.Quantity(np.trapz(y_clipped, np.log(x_clipped)), 'K')).to(units('J/kg'))\n\n    # CIN\n    # Only use data between the surface and LFC for calculation\n    p_mask = _greater_or_close(x.m, lfc_pressure)\n    x_clipped = x[p_mask].magnitude\n    y_clipped = y[p_mask].magnitude\n    cin = (mpconsts.Rd\n           * units.Quantity(np.trapz(y_clipped, np.log(x_clipped)), 'K')).to(units('J/kg'))\n\n    # Set CIN to 0 if it's returned as a positive value (#1190)\n    if cin > units.Quantity(0, 'J/kg'):\n        cin = units.Quantity(0, 'J/kg')\n    return cape, cin\n\n# --- Snippet Separator ---\n\ndef most_unstable_cape_cin(pressure, temperature, dewpoint, **kwargs):\n    r\"\"\"Calculate most unstable CAPE/CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile and most unstable parcel path. CIN is integrated between the\n    surface and LFC, CAPE is integrated between the LFC and EL (or top of sounding).\n    Intersection points of the measured temperature profile and parcel profile are\n    logarithmically interpolated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure profile\n\n    temperature : `pint.Quantity`\n        Temperature profile\n\n    dewpoint : `pint.Quantity`\n        Dew point profile\n\n    kwargs\n        Additional keyword arguments to pass to `most_unstable_parcel`\n\n    Returns\n    -------\n    `pint.Quantity`\n        Most unstable Convective Available Potential Energy (CAPE)\n\n    `pint.Quantity`\n        Most unstable Convective Inhibition (CIN)\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, most_unstable_cape_cin\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # calculate most unstbale CAPE/CIN\n    >>> most_unstable_cape_cin(p, T, Td)\n    (<Quantity(4703.77306, 'joule / kilogram')>, <Quantity(0, 'joule / kilogram')>)\n\n    See Also\n    --------\n    cape_cin, most_unstable_parcel, parcel_profile\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    \"\"\"\n    pressure, temperature, dewpoint = _remove_nans(pressure, temperature, dewpoint)\n    _, _, _, parcel_idx = most_unstable_parcel(pressure, temperature, dewpoint, **kwargs)\n    p, t, td, mu_profile = parcel_profile_with_lcl(pressure[parcel_idx:],\n                                                   temperature[parcel_idx:],\n                                                   dewpoint[parcel_idx:])\n    return cape_cin(p, t, td, mu_profile)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the MetPy library to create an advanced sounding plot. The code should first import necessary libraries and modules. Then, it should load a sample dataset, clean it by dropping rows with NaN values in specific columns, and assign units to the data. After preparing the data, the code should create a new figure with a specific aspect ratio and plot the data using normal plotting functions. It should also set custom labels for the x and y axes. The code should calculate the lifted condensation level (LCL) and plot it as a black dot, and calculate the full parcel profile and add it to the plot as a black line. It should shade areas of Convective Available Potential Energy (CAPE) and Convective Inhibition (CIN). Finally, the code should add special lines to the plot and display it.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 42, "repo_full_name": "deepmind__acme", "instruction": "Generate code that creates a CRR agent for a specified environment using the Acme library. The agent should be configured with various parameters such as batch size, evaluation period, number of demonstration episodes, random seed, learning rates, discount, target update period, and whether to use SARSA target or not. The environment and dataset names should also be configurable. The code should also include a function to add next action extras to the transitions. The main function should create the environment, get the demonstrations dataset, create the networks to optimize, create the learner, define the evaluator network, create the actor and the environment loop, and finally run the environment loop.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def run_offline_experiment(experiment: config.OfflineExperimentConfig,\n                           eval_every: int = 100,\n                           num_eval_episodes: int = 1):\n  \"\"\"Runs a simple, single-threaded training loop using the default evaluators.\n\n  It targets simplicity of the code and so only the basic features of the\n  OfflineExperimentConfig are supported.\n\n  Arguments:\n    experiment: Definition and configuration of the agent to run.\n    eval_every: After how many learner steps to perform evaluation.\n    num_eval_episodes: How many evaluation episodes to execute at each\n      evaluation step.\n  \"\"\"\n\n  key = jax.random.PRNGKey(experiment.seed)\n\n  # Create the environment and get its spec.\n  environment = experiment.environment_factory(experiment.seed)\n  environment_spec = experiment.environment_spec or specs.make_environment_spec(\n      environment)\n\n  # Create the networks and policy.\n  networks = experiment.network_factory(environment_spec)\n\n  # Parent counter allows to share step counts between train and eval loops and\n  # the learner, so that it is possible to plot for example evaluator's return\n  # value as a function of the number of training episodes.\n  parent_counter = counting.Counter(time_delta=0.)\n\n  # Create the demonstrations dataset.\n  dataset_key, key = jax.random.split(key)\n  dataset = experiment.demonstration_dataset_factory(dataset_key)\n\n  # Create the learner.\n  learner_key, key = jax.random.split(key)\n  learner = experiment.builder.make_learner(\n      random_key=learner_key,\n      networks=networks,\n      dataset=dataset,\n      logger_fn=experiment.logger_factory,\n      environment_spec=environment_spec,\n      counter=counting.Counter(parent_counter, prefix='learner', time_delta=0.))\n\n  # Define the evaluation loop.\n  eval_loop = None\n  if num_eval_episodes > 0:\n    # Create the evaluation actor and loop.\n    eval_counter = counting.Counter(\n        parent_counter, prefix='evaluator', time_delta=0.)\n    eval_logger = experiment.logger_factory('evaluator',\n                                            eval_counter.get_steps_key(), 0)\n    eval_key, key = jax.random.split(key)\n    eval_actor = experiment.builder.make_actor(\n        random_key=eval_key,\n        policy=experiment.builder.make_policy(networks, environment_spec, True),\n        environment_spec=environment_spec,\n        variable_source=learner)\n    eval_loop = acme.EnvironmentLoop(\n        environment,\n        eval_actor,\n        counter=eval_counter,\n        logger=eval_logger,\n        observers=experiment.observers)\n\n  checkpointer = None\n  if experiment.checkpointing is not None:\n    checkpointing = experiment.checkpointing\n    checkpointer = savers.Checkpointer(\n        objects_to_save={'learner': learner, 'counter': parent_counter},\n        time_delta_minutes=checkpointing.time_delta_minutes,\n        directory=checkpointing.directory,\n        add_uid=checkpointing.add_uid,\n        max_to_keep=checkpointing.max_to_keep,\n        keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n        checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n    )\n\n  max_num_learner_steps = (\n      experiment.max_num_learner_steps -\n      parent_counter.get_counts().get('learner_steps', 0))\n\n  # Run the training loop.\n  if eval_loop:\n    eval_loop.run(num_eval_episodes)\n  steps = 0\n  while steps < max_num_learner_steps:\n    learner_steps = min(eval_every, max_num_learner_steps - steps)\n    for _ in range(learner_steps):\n      learner.step()\n      if checkpointer is not None:\n        checkpointer.save()\n    if eval_loop:\n      eval_loop.run(num_eval_episodes)\n    steps += learner_steps\n\n# --- Snippet Separator ---\n\ndef run_experiment(experiment: config.ExperimentConfig,\n                   eval_every: int = 100,\n                   num_eval_episodes: int = 1):\n  \"\"\"Runs a simple, single-threaded training loop using the default evaluators.\n\n  It targets simplicity of the code and so only the basic features of the\n  ExperimentConfig are supported.\n\n  Arguments:\n    experiment: Definition and configuration of the agent to run.\n    eval_every: After how many actor steps to perform evaluation.\n    num_eval_episodes: How many evaluation episodes to execute at each\n      evaluation step.\n  \"\"\"\n\n  key = jax.random.PRNGKey(experiment.seed)\n\n  # Create the environment and get its spec.\n  environment = experiment.environment_factory(experiment.seed)\n  environment_spec = experiment.environment_spec or specs.make_environment_spec(\n      environment)\n\n  # Create the networks and policy.\n  networks = experiment.network_factory(environment_spec)\n  policy = config.make_policy(\n      experiment=experiment,\n      networks=networks,\n      environment_spec=environment_spec,\n      evaluation=False)\n\n  # Create the replay server and grab its address.\n  replay_tables = experiment.builder.make_replay_tables(environment_spec,\n                                                        policy)\n\n  # Disable blocking of inserts by tables' rate limiters, as this function\n  # executes learning (sampling from the table) and data generation\n  # (inserting into the table) sequentially from the same thread\n  # which could result in blocked insert making the algorithm hang.\n  replay_tables, rate_limiters_max_diff = _disable_insert_blocking(\n      replay_tables)\n\n  replay_server = reverb.Server(replay_tables, port=None)\n  replay_client = reverb.Client(f'localhost:{replay_server.port}')\n\n  # Parent counter allows to share step counts between train and eval loops and\n  # the learner, so that it is possible to plot for example evaluator's return\n  # value as a function of the number of training episodes.\n  parent_counter = counting.Counter(time_delta=0.)\n\n  dataset = experiment.builder.make_dataset_iterator(replay_client)\n  # We always use prefetch as it provides an iterator with an additional\n  # 'ready' method.\n  dataset = utils.prefetch(dataset, buffer_size=1)\n\n  # Create actor, adder, and learner for generating, storing, and consuming\n  # data respectively.\n  # NOTE: These are created in reverse order as the actor needs to be given the\n  # adder and the learner (as a source of variables).\n  learner_key, key = jax.random.split(key)\n  learner = experiment.builder.make_learner(\n      random_key=learner_key,\n      networks=networks,\n      dataset=dataset,\n      logger_fn=experiment.logger_factory,\n      environment_spec=environment_spec,\n      replay_client=replay_client,\n      counter=counting.Counter(parent_counter, prefix='learner', time_delta=0.))\n\n  adder = experiment.builder.make_adder(replay_client, environment_spec, policy)\n\n  actor_key, key = jax.random.split(key)\n  actor = experiment.builder.make_actor(\n      actor_key, policy, environment_spec, variable_source=learner, adder=adder)\n\n  # Create the environment loop used for training.\n  train_counter = counting.Counter(\n      parent_counter, prefix='actor', time_delta=0.)\n  train_logger = experiment.logger_factory('actor',\n                                           train_counter.get_steps_key(), 0)\n\n  checkpointer = None\n  if experiment.checkpointing is not None:\n    checkpointing = experiment.checkpointing\n    checkpointer = savers.Checkpointer(\n        objects_to_save={'learner': learner, 'counter': parent_counter},\n        time_delta_minutes=checkpointing.time_delta_minutes,\n        directory=checkpointing.directory,\n        add_uid=checkpointing.add_uid,\n        max_to_keep=checkpointing.max_to_keep,\n        keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n        checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n    )\n\n  # Replace the actor with a LearningActor. This makes sure that every time\n  # that `update` is called on the actor it checks to see whether there is\n  # any new data to learn from and if so it runs a learner step. The rate\n  # at which new data is released is controlled by the replay table's\n  # rate_limiter which is created by the builder.make_replay_tables call above.\n  actor = _LearningActor(actor, learner, dataset, replay_tables,\n                         rate_limiters_max_diff, checkpointer)\n\n  train_loop = acme.EnvironmentLoop(\n      environment,\n      actor,\n      counter=train_counter,\n      logger=train_logger,\n      observers=experiment.observers)\n\n  max_num_actor_steps = (\n      experiment.max_num_actor_steps -\n      parent_counter.get_counts().get(train_counter.get_steps_key(), 0))\n\n  if num_eval_episodes == 0:\n    # No evaluation. Just run the training loop.\n    train_loop.run(num_steps=max_num_actor_steps)\n    return\n\n  # Create the evaluation actor and loop.\n  eval_counter = counting.Counter(\n      parent_counter, prefix='evaluator', time_delta=0.)\n  eval_logger = experiment.logger_factory('evaluator',\n                                          eval_counter.get_steps_key(), 0)\n  eval_policy = config.make_policy(\n      experiment=experiment,\n      networks=networks,\n      environment_spec=environment_spec,\n      evaluation=True)\n  eval_actor = experiment.builder.make_actor(\n      random_key=jax.random.PRNGKey(experiment.seed),\n      policy=eval_policy,\n      environment_spec=environment_spec,\n      variable_source=learner)\n  eval_loop = acme.EnvironmentLoop(\n      environment,\n      eval_actor,\n      counter=eval_counter,\n      logger=eval_logger,\n      observers=experiment.observers)\n\n  steps = 0\n  while steps < max_num_actor_steps:\n    eval_loop.run(num_episodes=num_eval_episodes)\n    num_steps = min(eval_every, max_num_actor_steps - steps)\n    steps += train_loop.run(num_steps=num_steps)\n  eval_loop.run(num_episodes=num_eval_episodes)\n\n  environment.close()\n\n# --- Snippet Separator ---\n\ndef evaluator(\n      self,\n      variable_source: acme.VariableSource,\n      counter: counting.Counter,\n  ):\n    \"\"\"The evaluation process.\"\"\"\n\n    # Create environment and target networks to act with.\n    environment = self._environment_factory(True)\n    agent_networks = self._network_factory(self._environment_spec)\n\n    # Create a stochastic behavior policy.\n    evaluator_network = snt.Sequential([\n        agent_networks['observation'],\n        agent_networks['policy'],\n        networks.StochasticMeanHead(),\n    ])\n\n    # Create the variable client responsible for keeping the actor up-to-date.\n    variable_client = tf2_variable_utils.VariableClient(\n        variable_source,\n        variables={'policy': evaluator_network.variables},\n        update_period=1000)\n\n    # Make sure not to evaluate a random actor by assigning variables before\n    # running the environment loop.\n    variable_client.update_and_wait()\n\n    # Create the agent.\n    evaluator = actors.FeedForwardActor(\n        policy_network=evaluator_network, variable_client=variable_client)\n\n    # Create logger and counter.\n    counter = counting.Counter(counter, 'evaluator')\n    logger = loggers.make_default_logger(\n        'evaluator', time_delta=self._log_every)\n\n    # Create the run loop and return it.\n    return acme.EnvironmentLoop(environment, evaluator, counter, logger)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a CRR agent for a specified environment using the Acme library. The agent should be configured with various parameters such as batch size, evaluation period, number of demonstration episodes, random seed, learning rates, discount, target update period, and whether to use SARSA target or not. The environment and dataset names should also be configurable. The code should also include a function to add next action extras to the transitions. The main function should create the environment, get the demonstrations dataset, create the networks to optimize, create the learner, define the evaluator network, create the actor and the environment loop, and finally run the environment loop.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 43, "repo_full_name": "pysteps__pysteps", "instruction": "Generate code that performs ensemble verification of a probabilistic extrapolation nowcast using MeteoSwiss radar data. The code should read precipitation field data, upscale it to 2 km resolution, and convert it to rain rate. It should also log-transform the data and handle missing values. The code should estimate the motion field and perform an ensemble nowcast using the STEPS approach. It should then back-transform the nowcast to rain rates and plot some of the realizations. Finally, the code should verify the probabilistic forecasts using the ROC curve, reliability diagrams, and rank histograms.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def forecast(precip, velocity, num_timesteps,\n             extrap_method=\"semilagrangian\", extrap_kwargs=None,\n             measure_time=False):\n    \"\"\"Generate a nowcast by applying a simple advection-based extrapolation to\n    the given precipitation field.\n\n    .. _ndarray: http://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html\n\n    Parameters\n    ----------\n    precip : array-like\n      Two-dimensional array of shape (m,n) containing the input precipitation\n      field.\n    velocity : array-like\n      Array of shape (2,m,n) containing the x- and y-components of the\n      advection field. The velocities are assumed to represent one time step\n      between the inputs.\n    num_timesteps : int\n      Number of time steps to forecast.\n    extrap_method : str, optional\n      Name of the extrapolation method to use. See the documentation of\n      pysteps.extrapolation.interface.\n    extrap_kwargs : dict, , optional\n      Optional dictionary that is expanded into keyword arguments for the\n      extrapolation method.\n    measure_time : bool, optional\n      If True, measure, print, and return the computation time.\n\n    Returns\n    -------\n    out : ndarray_\n      Three-dimensional array of shape (num_timesteps, m, n) containing a time\n      series of nowcast precipitation fields. The time series starts from\n      t0 + timestep, where timestep is taken from the advection field velocity.\n      If *measure_time* is True, the return value is a two-element tuple\n      containing this array and the computation time (seconds).\n\n    See also\n    --------\n\n    pysteps.extrapolation.interface\n\n\n    \"\"\"\n    _check_inputs(precip, velocity)\n\n    if extrap_kwargs is None:\n        extrap_kwargs = dict()\n\n    if measure_time:\n        print(\"Computing extrapolation nowcast from a \"\n              f\"{precip.shape[0]:d}x{precip.shape[1]:d} input grid... \",\n              end=\"\")\n\n    if measure_time:\n        start_time = time.time()\n\n    extrapolation_method = extrapolation.get_method(extrap_method)\n\n    precip_forecast = extrapolation_method(precip, velocity, num_timesteps,\n                                           **extrap_kwargs)\n\n    if measure_time:\n        computation_time = time.time() - start_time\n        print(f\"{computation_time:.2f} seconds.\")\n\n    if measure_time:\n        return precip_forecast, computation_time\n    else:\n        return precip_forecast\n\n# --- Snippet Separator ---\n\ndef add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))\n\n# --- Snippet Separator ---\n\nclass StationPlotLayout(dict):\n    r\"\"\"Make a layout to encapsulate plotting using `StationPlot`.\n\n    This class keeps a collection of offsets, plot formats, etc. for a parameter based\n    on its name. This then allows a dictionary of data (or any object that allows looking\n    up of arrays based on a name) to be passed to `plot()` to plot the data all at once.\n\n    See Also\n    --------\n    StationPlot\n\n    \"\"\"\n\n    class PlotTypes(Enum):\n        r\"\"\"Different plotting types for the layout.\n\n        Controls how items are displayed (e.g. converting values to symbols).\n        \"\"\"\n\n        value = 1\n        symbol = 2\n        text = 3\n        barb = 4\n\n    def add_value(self, location, name, fmt='.0f', units=None, **kwargs):\n        r\"\"\"Add a numeric value to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. The conversion of the data values to\n        a string is controlled by `fmt`. The units required for plotting can also\n        be passed in using `units`, which will cause the data to be converted before\n        plotting.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        fmt : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_text\n\n        \"\"\"\n        self[location] = (self.PlotTypes.value, name, (fmt, units, kwargs))\n\n    def add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))\n\n    def add_text(self, location, name, **kwargs):\n        r\"\"\"Add a text field to the  station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted directly as text with no conversion\n        applied.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple(float, float)\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.text, name, kwargs)\n\n    def add_barb(self, u_name, v_name, units=None, **kwargs):\n        r\"\"\"Add a wind barb to the center of the station layout.\n\n        This specifies that u- and v-component data should be pulled from the data\n        container using the keys `u_name` and `v_name`, respectively, and plotted as\n        a wind barb at the center of the station plot. If `units` are given, both\n        components will be converted to these units.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or line width.\n\n        Parameters\n        ----------\n        u_name : str\n            The name of the parameter for the u-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        v_name : str\n            The name of the parameter for the v-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's\n            :meth:`~matplotlib.axes.Axes.barbs` function.\n\n        See Also\n        --------\n        add_symbol, add_text, add_value\n\n        \"\"\"\n        # Not sure if putting the v_name as a plot-specific option is appropriate,\n        # but it seems simpler than making name code in plot handle tuples\n        self['barb'] = (self.PlotTypes.barb, (u_name, v_name), (units, kwargs))\n\n    def names(self):\n        \"\"\"Get the list of names used by the layout.\n\n        Returns\n        -------\n        list[str]\n            the list of names of variables used by the layout\n\n        \"\"\"\n        ret = []\n        for item in self.values():\n            if item[0] == self.PlotTypes.barb:\n                ret.extend(item[1])\n            else:\n                ret.append(item[1])\n        return ret\n\n    def plot(self, plotter, data_dict):\n        \"\"\"Plot a collection of data using this layout for a station plot.\n\n        This function iterates through the entire specified layout, pulling the fields named\n        in the layout from `data_dict` and plotting them using `plotter` as specified\n        in the layout. Fields present in the layout, but not in `data_dict`, are ignored.\n\n        Parameters\n        ----------\n        plotter : StationPlot\n            :class:`StationPlot` to use to plot the data. This controls the axes,\n            spacing, station locations, etc.\n        data_dict : dict[str, array-like]\n            Data container that maps a name to an array of data. Data from this object\n            will be used to fill out the station plot.\n\n        \"\"\"\n        def coerce_data(dat, u):\n            try:\n                return dat.to(u).magnitude\n            except AttributeError:\n                return dat\n\n        for loc, info in self.items():\n            typ, name, args = info\n            if typ == self.PlotTypes.barb:\n                # Try getting the data\n                u_name, v_name = name\n                u_data = data_dict.get(u_name)\n                v_data = data_dict.get(v_name)\n\n                # Plot if we have the data\n                if not (v_data is None or u_data is None):\n                    units, kwargs = args\n                    plotter.plot_barb(coerce_data(u_data, units), coerce_data(v_data, units),\n                                      **kwargs)\n            else:\n                # Check that we have the data for this location\n                data = data_dict.get(name)\n                if data is not None:\n                    # If we have it, hand it to the appropriate method\n                    if typ == self.PlotTypes.value:\n                        fmt, units, kwargs = args\n                        plotter.plot_parameter(loc, coerce_data(data, units), fmt, **kwargs)\n                    elif typ == self.PlotTypes.symbol:\n                        mapper, kwargs = args\n                        plotter.plot_symbol(loc, data, mapper, **kwargs)\n                    elif typ == self.PlotTypes.text:\n                        plotter.plot_text(loc, data, **args)\n\n    def __repr__(self):\n        \"\"\"Return string representation of layout.\"\"\"\n        return ('{'\n                + ', '.join(f'{loc}: ({info[0].name}, {info[1]}, ...)'\n                            for loc, info in sorted(self.items()))\n                + '}')\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs ensemble verification of a probabilistic extrapolation nowcast using MeteoSwiss radar data. The code should read precipitation field data, upscale it to 2 km resolution, and convert it to rain rate. It should also log-transform the data and handle missing values. The code should estimate the motion field and perform an ensemble nowcast using the STEPS approach. It should then back-transform the nowcast to rain rates and plot some of the realizations. Finally, the code should verify the probabilistic forecasts using the ROC curve, reliability diagrams, and rank histograms.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 44, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that performs a PCB analysis using the PyAEDT library. The code should start by importing necessary libraries and setting up project files and paths. It should download a specific project file and set up a temporary project directory. Then, it should open an EDB project, create a cutout on selected nets, and export it to Q3D. The code should identify pin locations on the components to define where to assign sources and sinks for Q3D and append Z elevation. After that, it should save and close the EDB, open it in Hfss 3D Layout to generate the 3D model, and export the layout to Q3D. The code should then launch the newly created Q3D project, plot it, and assign sources and sinks on nets using the previously calculated positions. It should create a setup and a frequency sweep from DC to 2GHz, analyze the project, compute ACL and ACR solutions, plot them, and finally, release the desktop.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Q3d(QExtractor, object):\n    \"\"\"Provides the Q3D application interface.\n\n    This class allows you to create an instance of Q3D and link to an\n    existing project or create a new one.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open. The default is ``None``, in which\n        case an attempt is made to get an active project. If no\n        projects are present, an empty project is created.\n    designname : str, optional\n        Name of the design to select. The default is ``None``, in\n        which case an attempt is made to get an active design. If no\n        designs are present, an empty design is created.\n    solution_type : str, optional\n        Solution type to apply to the design. The default is\n        ``None``, in which case the default type is applied.\n    setup_name : str, optional\n        Name of the setup to use as the nominal. The default is\n        ``None``, in which case the active setup is used or nothing\n        is used.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``, in which case\n        the active version or latest installed version is used.\n        This parameter is ignored when Script is launched within AEDT.\n    non_graphical : bool, optional\n        Whether to launch AEDT in non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in graphical mode.\n        This parameter is ignored when a script is launched within AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine. The default is ``True``. This parameter is ignored when Script is launched within AEDT.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit. The default is ``False``.\n    student_version : bool, optional\n        Whether to open the AEDT student version. The default is ``False``.\n        This parameter is ignored when Script is launched within AEDT.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of Q3D and connect to an existing Q3D\n    design or create a new Q3D design if one does not exist.\n\n    >>> from pyaedt import Q3d\n    >>> app = Q3d()\n\n    \"\"\"\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        setup_name=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=False,\n        close_on_exit=False,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n        QExtractor.__init__(\n            self,\n            \"Q3D Extractor\",\n            projectname,\n            designname,\n            solution_type,\n            setup_name,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine,\n            port,\n            aedt_process_id,\n        )\n        self.MATRIXOPERATIONS = MATRIXOPERATIONSQ3D()\n\n    @property\n    def nets(self):\n        \"\"\"Return the list of available nets in a Q3D project.\n\n        Returns\n        -------\n        list\n\n        References\n        ----------\n\n        >>> oModule.ListNets\n        \"\"\"\n        nets_data = list(self.oboundary.ListNets())\n        net_names = []\n        for i in nets_data:\n            if isinstance(i, (list, tuple)):\n                net_names.append(i[0].split(\":\")[1])\n        return net_names\n\n    @pyaedt_function_handler()\n    def net_sources(self, net_name):\n        \"\"\"Check if a net has sources and return a list of source names.\n\n        Parameters\n        ----------\n        net_name : str\n            Name of the net to search for.\n\n        Returns\n        -------\n        List\n            List of source names.\n\n        Examples\n        --------\n        >>> from pyaedt import Q3d\n        >>> q3d = Q3d(\"my_project\")\n        >>> net = q3d.net_sources(\"Net1\")\n        \"\"\"\n        sources = []\n        net_id = -1\n        for i in self.boundaries:\n            if i.type == \"SignalNet\" and i.name == net_name and i.props.get(\"ID\", None) is not None:\n                net_id = i.props.get(\"ID\", None)  # pragma: no cover\n                break  # pragma: no cover\n        for i in self.boundaries:\n            if i.type == \"Source\":\n                if i.props.get(\"Net\", None) == net_name or i.props.get(\"Net\", None) == net_id:\n                    sources.append(i.name)\n\n        return sources\n\n    @pyaedt_function_handler()\n    def net_sinks(self, net_name):\n        \"\"\"Check if a net has sinks and returns a list of sink names.\n\n        Parameters\n        ----------\n        net_name : str\n            Name of the net to search for.\n\n        Returns\n        -------\n        List\n            List of sink names.\n\n        Examples\n        --------\n        >>> from pyaedt import Q3d\n        >>> q3d = Q3d(\"my_project\")\n        >>> net = q3d.net_sinks(\"Net1\")\n        \"\"\"\n        sinks = []\n        net_id = -1\n        for i in self.boundaries:\n            if i.type == \"SignalNet\" and i.name == net_name and i.props.get(\"ID\", None) is not None:\n                net_id = i.props.get(\"ID\", None)  # pragma: no cover\n                break  # pragma: no cover\n        for i in self.boundaries:\n            if i.type == \"Sink\" and i.props.get(\"Net\", None) == net_name or i.props.get(\"Net\", None) == net_id:\n                sinks.append(i.name)\n        return sinks\n\n    @pyaedt_function_handler()\n    def auto_identify_nets(self):\n        \"\"\"Automatically identify nets.\n\n        Returns\n        -------\n        bool\n            ``True`` when successful, ``False`` when failed.\n\n        References\n        ----------\n\n        >>> oModule.AutoIdentifyNets\n        \"\"\"\n        original_nets = [i for i in self.nets]\n        self.oboundary.AutoIdentifyNets()\n        new_nets = [i for i in self.nets if i not in original_nets]\n        for net in new_nets:\n            objects = self.modeler.convert_to_selections(\n                [int(i) for i in list(self.oboundary.GetExcitationAssignment(net))], True\n            )\n            props = OrderedDict({\"Objects\": objects})\n            bound = BoundaryObject(self, net, props, \"SignalNet\")\n            self.boundaries.append(bound)\n        if new_nets:\n            self.logger.info(\"{} Nets have been identified: {}\".format(len(new_nets), \", \".join(new_nets)))\n        else:\n            self.logger.info(\"No new nets identified\")\n        return True\n\n    @pyaedt_function_handler()\n    def assign_net(self, objects, net_name=None, net_type=\"Signal\"):\n        \"\"\"Assign a net to a list of objects.\n\n        Parameters\n        ----------\n        objects : list, str\n            List of objects to assign the net to. It can be a single object.\n        net_name : str, optional\n            Name of the net. The default is ```None``, in which case the\n            default name is used.\n        net_type : str, bool\n            Type of net to create. Options are ``\"Signal\"``, ``\"Ground\"`` and ``\"Floating\"``.\n            The default is ``\"Signal\"``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Source object.\n\n        References\n        ----------\n\n        >>> oModule.AssignSignalNet\n        >>> oModule.AssignGroundNet\n        >>> oModule.AssignFloatingNet\n\n        Examples\n        --------\n        >>> from pyaedt import Q3d\n        >>> q3d = Q3d()\n        >>> box = q3d.modeler.create_box([30, 30, 30], [10, 10, 10], name=\"mybox\")\n        >>> net_name = \"my_net\"\n        >>> net = q3d.assign_net(box, net_name)\n        \"\"\"\n        objects = self.modeler.convert_to_selections(objects, True)\n        if not net_name:\n            net_name = generate_unique_name(\"Net\")\n        props = OrderedDict({\"Objects\": objects})\n        type_bound = \"SignalNet\"\n        if net_type.lower() == \"ground\":\n            type_bound = \"GroundNet\"\n        elif net_type.lower() == \"floating\":\n            type_bound = \"FloatingNet\"\n        bound = BoundaryObject(self, net_name, props, type_bound)\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_source_to_objectface(self, object_name, axisdir=0, source_name=None, net_name=None):\n        \"\"\"Generate a source on a face of an object.\n\n        The face ID is selected based on the axis direction. It is the face that\n        has the maximum/minimum in this axis direction.\n\n        Parameters\n        ----------\n        object_name : str, int\n            Name of the object or face id.\n        axisdir : int, optional\n            Initial axis direction. Options are ``0`` to ``5``. The default is ``0``.\n        source_name : str, optional\n            Name of the source. The default is ``None``.\n        net_name : str, optional\n            Name of the net. The default is ``None``, in which case ``object_name`` is considered.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Source object.\n\n        References\n        ----------\n\n        >>> oModule.AssignSource\n        \"\"\"\n        object_name = self.modeler.convert_to_selections(object_name, True)[0]\n        if isinstance(object_name, int):\n            a = object_name\n        else:\n            a = self.modeler._get_faceid_on_axis(object_name, axisdir)\n        if not source_name:\n            source_name = generate_unique_name(\"Source\")\n        if not net_name:\n            net_name = object_name\n        if a:\n            props = OrderedDict(\n                {\"Faces\": [a], \"ParentBndID\": object_name, \"TerminalType\": \"ConstantVoltage\", \"Net\": net_name}\n            )\n            bound = BoundaryObject(self, source_name, props, \"Source\")\n            if bound.create():\n                self.boundaries.append(bound)\n                return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_source_to_sheet(self, sheetname, objectname=None, netname=None, sourcename=None):\n        \"\"\"Generate a source on a sheet.\n\n        Parameters\n        ----------\n        sheetname : str\n            Name of the sheet to create the source on.\n        objectname :  str, optional\n            Name of the parent object. The default is ``None``.\n        netname : str, optional\n            Name of the net. The default is ``None``.\n        sourcename : str,  optional\n            Name of the source. The default is ``None``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Source object.\n\n        References\n        ----------\n\n        >>> oModule.AssignSource\n        \"\"\"\n        if not sourcename:\n            sourcename = generate_unique_name(\"Source\")\n        sheetname = self.modeler.convert_to_selections(sheetname, True)\n        props = OrderedDict({\"Objects\": [sheetname]})\n        if objectname:\n            props[\"ParentBndID\"] = objectname\n        props[\"TerminalType\"] = \"ConstantVoltage\"\n        if netname:\n            props[\"Net\"] = netname\n        props = OrderedDict({\"Objects\": sheetname, \"TerminalType\": \"ConstantVoltage\", \"Net\": netname})\n        bound = BoundaryObject(self, sourcename, props, \"Source\")\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_sink_to_objectface(self, object_name, axisdir=0, sink_name=None, net_name=None):\n        \"\"\"Generate a sink on a face of an object.\n\n        The face ID is selected based on the axis direction. It is the face that has\n        the maximum/minimum in this axis direction.\n\n        Parameters\n        ----------\n        object_name : str, int\n            Name of the object or face id.\n        axisdir : int, optional\n            Initial axis direction. Options are ``0`` to ``5``. The default is ``0``.\n        sink_name : str, optional\n            Name of the sink. The default is ``None``.\n        net_name : str, optional\n            Name of the net. The default is ``None``, in which case ``object_name`` is considered.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Sink object.\n\n        References\n        ----------\n\n        >>> oModule.AssignSink\n        \"\"\"\n        object_name = self.modeler.convert_to_selections(object_name, True)[0]\n        if isinstance(object_name, int):\n            a = object_name\n            object_name = self.modeler.oeditor.GetObjectNameByFaceID(a)\n        else:\n            a = self.modeler._get_faceid_on_axis(object_name, axisdir)\n        if not sink_name:\n            sink_name = generate_unique_name(\"Sink\")\n        if not net_name:\n            net_name = object_name\n        if a:\n            props = OrderedDict(\n                {\"Faces\": [a], \"ParentBndID\": object_name, \"TerminalType\": \"ConstantVoltage\", \"Net\": net_name}\n            )\n            bound = BoundaryObject(self, sink_name, props, \"Sink\")\n            if bound.create():\n                self.boundaries.append(bound)\n                return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_sink_to_sheet(self, sheetname, objectname=None, netname=None, sinkname=None):\n        \"\"\"Generate a sink on a sheet.\n\n        Parameters\n        ----------\n        sheetname :\n            Name of the sheet to create the sink on.\n        objectname : str, optional\n            Name of the parent object. The default is ``None``.\n        netname : str, optional\n            Name of the net. The default is ``None``.\n        sinkname : str, optional\n            Name of the sink. The default is ``None``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Source object.\n\n        References\n        ----------\n\n        >>> oModule.AssignSink\n        \"\"\"\n        if not sinkname:\n            sinkname = generate_unique_name(\"Source\")\n        sheetname = self.modeler.convert_to_selections(sheetname, True)\n        props = OrderedDict({\"Objects\": [sheetname]})\n        if objectname:\n            props[\"ParentBndID\"] = objectname\n        props[\"TerminalType\"] = \"ConstantVoltage\"\n        if netname:\n            props[\"Net\"] = netname\n\n        props = OrderedDict({\"Objects\": sheetname, \"TerminalType\": \"ConstantVoltage\", \"Net\": netname})\n        bound = BoundaryObject(self, sinkname, props, \"Sink\")\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n    @pyaedt_function_handler()\n    def create_frequency_sweep(self, setupname, units, freqstart, freqstop, freqstep=None, sweepname=None):\n        \"\"\"Create a frequency sweep.\n\n        Parameters\n        ----------\n        setupname : str\n            Name of the setup that is attached to the sweep.\n        units : str\n            Units of the frequency. For example, ``\"MHz\"`` or\n            ``\"GHz\"``. The default is ``\"GHz\"``.\n        freqstart :\n            Starting frequency of the sweep.\n        freqstop :\n            Stopping frequency of the sweep.\n        freqstep : optional\n            Frequency step point.\n        sweepname : str, optional\n            Name of the sweep. The default is ``None``.\n\n        Returns\n        -------\n        bool\n            ``True`` when successful, ``False`` when failed.\n\n        References\n        ----------\n\n        >>> oModule.InsertSweep\n        \"\"\"\n        if sweepname is None:\n            sweepname = generate_unique_name(\"Sweep\")\n\n        if setupname not in self.setup_names:\n            return False\n        for i in self.setups:\n            if i.name == setupname:\n                setupdata = i\n                for sw in setupdata.sweeps:\n                    if sweepname == sw.name:\n                        self.logger.warning(\"Sweep %s is already present. Rename and retry.\", sweepname)\n                        return False\n                sweepdata = setupdata.add_sweep(sweepname, \"Discrete\")\n                sweepdata.props[\"RangeStart\"] = str(freqstart) + \"GHz\"\n                if not freqstop:\n                    freqstop = freqstart\n                if not freqstep:\n                    freqstep = (freqstop - freqstart) / 11\n                    if freqstep == 0:\n                        freqstep = freqstart\n                sweepdata.props[\"RangeEnd\"] = str(freqstop) + \"GHz\"\n                sweepdata.props[\"RangeStep\"] = str(freqstep) + \"GHz\"\n                sweepdata.props[\"SaveFields\"] = False\n                sweepdata.props[\"SaveRadFields\"] = False\n                sweepdata.props[\"Type\"] = \"Interpolating\"\n                sweepdata.props[\"RangeType\"] = \"LinearStep\"\n                sweepdata.update()\n                return sweepdata\n        return False\n\n    @pyaedt_function_handler()\n    def create_discrete_sweep(\n        self, setupname, freqstart, freqstop=None, freqstep=None, units=\"GHz\", sweepname=None, savefields=False\n    ):\n        \"\"\"Create a discrete sweep with a single frequency value.\n\n        Parameters\n        ----------\n        setupname : str\n            Name of the setup that the sweeps belongs to.\n        freqstart : float\n            Starting point for the discrete frequency.\n        freqstop : float, optional\n            Stopping point for the discrete frequency. If ``None``,\n            a single-point sweep is performed.\n        freqstep : float, optional\n            Step point for the discrete frequency. If ``None``,\n            11 points are created.\n        units : str, optional\n            Units of the discrete frequency. For example, ``\"MHz\"`` or\n            ``\"GHz\"``. The default is ``\"GHz\"``.\n        sweepname : str, optional\n            Name of the sweep.\n        savefields : bool, optional\n            Whether to save fields. The default is ``False``.\n\n        Returns\n        -------\n        SweepQ3D\n            Sweep option.\n\n        References\n        ----------\n\n        >>> oModule.InsertSweep\n        \"\"\"\n        if sweepname is None:\n            sweepname = generate_unique_name(\"Sweep\")\n\n        if setupname not in self.setup_names:\n            return False\n        for i in self.setups:\n            if i.name == setupname:\n                setupdata = i\n                for sw in setupdata.sweeps:\n                    if sweepname == sw.name:\n                        self.logger.warning(\"Sweep %s already present. Rename and retry.\", sweepname)\n                        return False\n                sweepdata = setupdata.add_sweep(sweepname, \"Discrete\")\n                sweepdata.props[\"RangeStart\"] = str(freqstart) + \"GHz\"\n                if not freqstop:\n                    freqstop = freqstart\n                if not freqstep:\n                    freqstep = (freqstop - freqstart) / 11\n                    if freqstep == 0:\n                        freqstep = freqstart\n                sweepdata.props[\"RangeEnd\"] = str(freqstop) + \"GHz\"\n                sweepdata.props[\"RangeStep\"] = str(freqstep) + \"GHz\"\n                sweepdata.props[\"SaveFields\"] = savefields\n                sweepdata.props[\"SaveRadFields\"] = False\n                sweepdata.props[\"Type\"] = \"Discrete\"\n                sweepdata.props[\"RangeType\"] = \"LinearStep\"\n                sweepdata.update()\n                return sweepdata\n        return False\n\n# --- Snippet Separator ---\n\ndef add_subcircuit_dynamic_link(\n        self,\n        pyaedt_app=None,\n        solution_name=None,\n        extrusion_length=None,\n        enable_cable_modeling=True,\n        default_matrix=\"Original\",\n        tline_port=\"\",\n        comp_name=None,\n    ):\n        \"\"\"Add a subcircuit from `HFSS`, `Q3d` or `2D Extractor` in circuit design.\n\n        Parameters\n        ----------\n        pyaedt_app : :class:`pyaedt.q3d.Q3d` or :class:`pyaedt.q3d.Q2d` or :class:`pyaedt.q3d.Hfss`.\n            pyaedt application object to include. It could be an Hfss object, a Q3d object or a Q2d.\n        solution_name : str, optional\n            Name of the solution and sweep. The default is ``\"Setup1 : Sweep\"``.\n        extrusion_length : float, str, optional\n            Extrusion length for 2D Models (q2d or Hfss) in model units. Default is `None`.\n        enable_cable_modeling : bool, optional\n            Either if the Hfss Cable modeling has to be enabled for 2D subcircuits.\n        default_matrix : str, optional\n            Matrix to link to the subcircuit. Default to `\"Original\"`. It only applies to 2D Extractor and Q3D.\n        tline_port : str, optional\n            Port to be used for tramsission line. Only applies to Hfss.\n        comp_name : str, optional\n            Component name.\n\n        Returns\n        -------\n        :class:`pyaedt.modeler.Object3d.CircuitComponent`\n            Circuit Component Object.\n\n        References\n        ----------\n\n        >>> oModelManager.Add\n        >>> oComponentManager.Add\n        >>> oDesign.AddCompInstance\n        >>> oDesign.AddDynamicLink\n        \"\"\"\n        if not comp_name:\n            comp_name = generate_unique_name(pyaedt_app.design_name)\n        source_project_path = pyaedt_app.project_file\n        source_design_name = pyaedt_app.design_name\n        # matrix = None\n        # if pyaedt_app.design_type == \"HFSS\":\n        #     pin_names = pyaedt_app.get_excitations_name()\n        # elif pyaedt_app.design_type == \"Q3D Extractor\":\n        #     excts = list(pyaedt_app.oboundary.GetExcitations())\n        #     i = 0\n        #     sources = []\n        #     sinks = []\n        #     while i < len(excts):\n        #         if excts[i + 1] == \"Source\":\n        #             sources.append(excts[i])\n        #         elif excts[i + 1] == \"Sink\":\n        #             sinks.append(excts[i])\n        #         i += 2\n        #     pin_names = sources + sinks\n        #     matrix = [\"NAME:Reduce Matrix Choices\"] + list(pyaedt_app.omatrix.ListReduceMatrixes())\n        # elif pyaedt_app.design_type == \"2D Extractor\":\n        #     excts = list(pyaedt_app.oboundary.GetExcitations())\n        #     pins = []\n        #     i = 0\n        #     while i < len(excts):\n        #         if excts[i + 1] != \"ReferenceGround\":\n        #             pins.append(excts[i])\n        #         i += 2\n        #     pin_names = [i + \"_in\" for i in pins]\n        #     pin_names.append(\"Input_ref\")\n        #     pin_names.extend([i + \"_out\" for i in pins])\n        #     pin_names.append(\"Output_ref\")\n        #     matrix = [\"NAME:Reduce Matrix Choices\"] + list(pyaedt_app.omatrix.ListReduceMatrixes())\n        # variables = {}\n        # for k, v in pyaedt_app.variable_manager.variables.items():\n        #     variables[k] = v.evaluated_value\n        if not solution_name:\n            solution_name = pyaedt_app.nominal_sweep\n        # comp = self._add_subcircuit_link(\n        #     comp_name=comp_name,\n        #     pin_names=pin_names,\n        #     source_project_path=source_project_path,\n        #     source_design_name=source_design_name,\n        #     solution_name=solution_name,\n        #     image_subcircuit_path=\"\",\n        #     model_type=pyaedt_app.design_type,\n        #     variables=variables,\n        #     extrusion_length_q2d=extrusion_length,\n        #     matrix=matrix,\n        #     enable_cable_modeling=enable_cable_modeling,\n        #     default_matrix=default_matrix,\n        # )\n\n        self._app.odesign.AddDynamicLink(\n            source_design_name,\n            source_project_path,\n            comp_name,\n            solution_name,\n            tline_port,\n            default_matrix,\n            enable_cable_modeling,\n            \"Pyaedt Dynamic Link\",\n        )\n        self.refresh_all_ids()\n        for el in self.components:\n            if comp_name in self.components[el].composed_name:\n                if extrusion_length:\n                    self.components[el].set_property(\"Length\", self.arg_with_dim(extrusion_length))\n                if tline_port and extrusion_length:\n                    self.components[el].set_property(\"TLineLength\", self.arg_with_dim(extrusion_length))\n                return self.components[el]\n        return False\n\n# --- Snippet Separator ---\n\nclass Emit(FieldAnalysisEmit, object):\n    \"\"\"Provides the Emit application interface.\n\n    .. note::\n       This object creates only a skeleton for an empty design.\n       It has very limited functionalities, and no methods\n       are implemented yet.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open.  The default is ``None``. If\n        ``None``, try to get an active project and, if no projects are\n        present, create an empty project.\n    designname : str, optional\n        Name of the design to select. The default is ``None``. If\n        ``None``, try to get an active design and, if no designs are\n        present, create an empty design.\n    solution_type : str, optional\n        Solution type to apply to the design. The default is ``None``, in which\n        case the default type is applied.\n    setup_name : str, optional\n       Name of the setup to use as the nominal. The default is\n       ``None``, in which case the active setup is used or nothing is\n       used.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``, in which case\n        the active setup is used or the latest installed version is\n        used.\n    non_graphical : bool, optional\n        Whether to launch AEDT in non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in graphical mode.\n        This parameter is ignored when a script is launched within AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine.  The default is ``True``.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit. The default is ``True``.\n    student_version : bool, optional\n        Whether to open the AEDT student version. The default is ``False``.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of Emit and connect to an existing Emit\n    design or create a new Emit design if one does not exist.\n\n    >>> from pyaedt import Emit\n    >>> app = Emit()\n\n    Create a instance of Emit and link to a project named\n    ``\"projectname\"``. If this project does not exist, create one with\n    this name.\n\n    >>> app = Emit(projectname)\n\n    Create an instance of Emit and link to a design named\n    ``\"designname\"`` in a project named ``\"projectname\"``.\n\n    >>> app = Emit(projectname,designame)\n\n    Create an instance of Emit and open the specified project,\n    which is named ``Myfile.aedt``.\n\n    >>> app = Emit(\"myfile.aedt\")\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        setup_name=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=True,\n        close_on_exit=True,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n        \"\"\"Constructor.\"\"\"\n        FieldAnalysisEmit.__init__(\n            self,\n            \"EMIT\",\n            projectname,\n            designname,\n            solution_type,\n            setup_name,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine=machine,\n            port=port,\n            aedt_process_id=aedt_process_id,\n        )\n\n    def __enter__(self):\n        return self\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a PCB analysis using the PyAEDT library. The code should start by importing necessary libraries and setting up project files and paths. It should download a specific project file and set up a temporary project directory. Then, it should open an EDB project, create a cutout on selected nets, and export it to Q3D. The code should identify pin locations on the components to define where to assign sources and sinks for Q3D and append Z elevation. After that, it should save and close the EDB, open it in Hfss 3D Layout to generate the 3D model, and export the layout to Q3D. The code should then launch the newly created Q3D project, plot it, and assign sources and sinks on nets using the previously calculated positions. It should create a setup and a frequency sweep from DC to 2GHz, analyze the project, compute ACL and ACR solutions, plot them, and finally, release the desktop.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 45, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that initializes a parser with arguments for paths to the bop datasets parent directory, cc textures, output directory, and the number of scenes to generate. The code should initialize the blenderproc library and load bop objects into the scene from the 'itodd' and 'tless' datasets. It should also load BOP dataset intrinsics and set shading and hide objects. \n\nThe code should create a room using primitive planes and enable rigidbody for these planes. It should also create a light plane and a point light. The code should load cc textures and define a function to sample 6-DoF poses. \n\nThe code should enable depth rendering without antialiasing and set the maximum amount of samples for color rendering. For each scene, the code should sample bop objects, randomize materials, set physics, sample two light sources, assign a random cc texture to room planes, sample object poses, check collisions, simulate physics and fix final poses. \n\nThe code should create a BVH tree for camera obstacle checks and generate camera poses while ensuring that obstacles are at least 0.3 meter away from the camera. The code should render the pipeline and write data in bop format. After each scene, the code should disable rigidbody and hide objects.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def load_bop_scene(bop_dataset_path: str, scene_id: int, model_type: str = \"\", cam_type: str = \"\",\n                   split: str = \"test\", source_frame: Optional[List[str]] = None,\n                   mm2m: Optional[bool] = None, object_model_unit: str = 'm') -> List[MeshObject]:\n    \"\"\" Replicate a BOP scene from the given dataset: load scene objects, object poses, camera intrinsics and\n        extrinsics\n\n    - Interfaces with the bob_toolkit, allows loading of train, val and test splits\n    - Relative camera poses are loaded/computed with respect to a reference model\n    - Sets real camera intrinsics\n\n    :param bop_dataset_path: Full path to a specific bop dataset e.g. /home/user/bop/tless.\n    :param scene_id: Specify BOP dataset scene to synthetically replicate. Default: -1 (no scene is replicated,\n                     only BOP Objects are loaded).\n    :param model_type: Optionally, specify type of BOP model.  Available: [reconst, cad or eval].\n    :param cam_type: Camera type. If not defined, dataset-specific default camera type is used.\n    :param split: Optionally, test or val split depending on BOP dataset.\n    :param source_frame: Can be used if the given positions and rotations are specified in frames different from the\n                         blender frame. Has to be a list of three strings. Example: ['X', '-Z', 'Y']:\n                         Point (1,2,3) will be transformed to (1, -3, 2). Default: [\"X\", \"-Y\", \"-Z\"],\n                         Available: ['X', 'Y', 'Z', '-X', '-Y', '-Z'].\n    :param mm2m: Specify whether to convert poses and models to meters (deprecated).\n    :param object_model_unit: The unit the object model is in. Object model will be scaled to meters. This does not\n                              affect the annotation units. Available: ['m', 'dm', 'cm', 'mm'].\n    :return: The list of loaded mesh objects.\n    \"\"\"\n\n    bop_path, bop_dataset_name = _BopLoader.setup_bop_toolkit(bop_dataset_path)\n\n    # This import is done inside to avoid having the requirement that BlenderProc depends on the bop_toolkit\n    # pylint: disable=import-outside-toplevel\n    from bop_toolkit_lib import dataset_params, inout\n\n    # pylint: enable=import-outside-toplevel\n\n    if source_frame is None:\n        source_frame = [\"X\", \"-Y\", \"-Z\"]\n\n    model_p = dataset_params.get_model_params(bop_path, bop_dataset_name, model_type=model_type if model_type else None)\n    try:\n        split_p = dataset_params.get_split_params(bop_path, bop_dataset_name, split=split,\n                                                  split_type=cam_type if cam_type else None)\n    except ValueError as e:\n        raise RuntimeError(f\"Wrong path or {split} split does not exist in {bop_dataset_path}.\") from e\n    sc_gt = inout.load_scene_gt(split_p['scene_gt_tpath'].format(**{'scene_id': scene_id}))\n    sc_camera = inout.load_json(split_p['scene_camera_tpath'].format(**{'scene_id': scene_id}))\n\n    assert object_model_unit in ['m', 'dm', 'cm', 'mm'], (f\"Invalid object model unit: `{object_model_unit}`. \"\n                                                          f\"Supported are 'm', 'dm', 'cm', 'mm'\")\n    scale = {'m': 1., 'dm': 0.1, 'cm': 0.01, 'mm': 0.001}[object_model_unit]\n    if mm2m is not None:\n        warnings.warn(\"WARNING: `mm2m` is deprecated, please use `object_model_unit='mm'` instead!\")\n        scale = 0.001\n\n    for i, (cam_id, insts) in enumerate(sc_gt.items()):\n        cam_K, cam_H_m2c_ref = _BopLoader.get_ref_cam_extrinsics_intrinsics(sc_camera, cam_id, insts, scale)\n\n        if i == 0:\n            # define world = first camera\n            cam_H_m2w_ref = cam_H_m2c_ref.copy()\n\n            cur_objs = []\n            # load scene objects and set their poses\n            for inst in insts:\n                cur_objs.append(_BopLoader.load_mesh(inst['obj_id'], model_p, bop_dataset_name, scale))\n                _BopLoader.set_object_pose(cur_objs[-1], inst, scale)\n\n        cam_H_c2w = _BopLoader.compute_camera_to_world_trafo(cam_H_m2w_ref, cam_H_m2c_ref, source_frame)\n        # set camera intrinsics\n        CameraUtility.set_intrinsics_from_K_matrix(cam_K, split_p['im_size'][0], split_p['im_size'][1])\n\n        # set camera extrinsics as next frame\n        frame_id = CameraUtility.add_camera_pose(cam_H_c2w)\n\n        # Add key frame for camera shift, as it changes from frame to frame in the tless replication\n        cam = bpy.context.scene.camera.data\n        cam.keyframe_insert(data_path='shift_x', frame=frame_id)\n        cam.keyframe_insert(data_path='shift_y', frame=frame_id)\n\n        # Copy object poses to key frame (to be sure)\n        for cur_obj in cur_objs:\n            _BopLoader.insert_key_frames(cur_obj, frame_id)\n\n    return cur_objs\n\n# --- Snippet Separator ---\n\ndef replace_objects(objects_to_be_replaced: List[MeshObject], objects_to_replace_with: List[MeshObject],\n                    ignore_collision_with: Optional[List[MeshObject]] = None, replace_ratio: float = 1,\n                    copy_properties: bool = True, max_tries: int = 100,\n                    relative_pose_sampler: Callable[[MeshObject], None] = None):\n    \"\"\"\n    Replaces mesh objects with another mesh objects and scales them accordingly, the replaced objects and the\n    objects to replace with in following steps:\n\n    1. Randomly select a subset of objects_to_be_replaced.\n    2. For each of these objects, sample other objects from objects_to_replace_with and try to replace them.\n    3. In each try, the poses of the objects are aligned and a check for collisions with other objects is done.\n    4. An object is skipped if max_tries is reached.\n\n    :param objects_to_be_replaced: Objects, which should be removed from the scene.\n    :param objects_to_replace_with: Objects, which will be tried to be added to the scene.\n    :param ignore_collision_with: Objects, which are not checked for collisions with.\n    :param replace_ratio: Ratio of objects in the original scene, which will be replaced.\n    :param copy_properties: Copies the custom properties of the objects_to_be_replaced to the objects_to_replace_with.\n    :param max_tries: Maximum number of tries to replace one object.\n    :param relative_pose_sampler: A function that randomly perturbs the pose of the object to replace with\n                                  (after it has been aligned to the object to replace).\n    \"\"\"\n    if ignore_collision_with is None:\n        ignore_collision_with = []\n\n    # Hide new objects from renderers until they are added\n    for obj in objects_to_replace_with:\n        obj.hide()\n\n    check_collision_with = []\n    for obj in get_all_mesh_objects():\n        if obj not in ignore_collision_with:\n            check_collision_with.append(obj)\n\n    # amount of replacements depends on the amount of objects and the replace ratio\n    objects_to_be_replaced = random.sample(objects_to_be_replaced, k=int(len(objects_to_be_replaced) * replace_ratio))\n    if len(objects_to_be_replaced) == 0:\n        print(\"Warning: The amount of objects, which should be replace is zero!\")\n\n    # Go over all objects we should replace\n    for current_object_to_be_replaced in objects_to_be_replaced:\n        print(current_object_to_be_replaced.get_name())\n        # Do at most max_tries to replace the object with a random object from  objects_to_replace_with\n        tries = 0\n        while tries < max_tries:\n            current_object_to_replace_with = np.random.choice(objects_to_replace_with)\n            if _ObjectReplacer.replace(current_object_to_be_replaced, current_object_to_replace_with,\n                                       check_collision_with, relative_pose_sampler=relative_pose_sampler):\n\n                # Duplicate the added object to be able to add it again\n                duplicate_new_object = current_object_to_replace_with.duplicate()\n\n                # Copy properties to the newly duplicated object\n                if copy_properties:\n                    for key, value in current_object_to_be_replaced.get_all_cps().items():\n                        duplicate_new_object.set_cp(key, value)\n\n                duplicate_new_object.hide(False)\n\n                print('Replaced ', current_object_to_be_replaced.get_name(), ' by ', duplicate_new_object.get_name())\n\n                # Delete the original object and remove it from the list\n                check_collision_with.remove(current_object_to_be_replaced)\n                current_object_to_be_replaced.delete()\n                break\n            tries += 1\n\n        if tries == max_tries:\n            print(\"Could not replace \" + current_object_to_be_replaced.get_name())\n\n# --- Snippet Separator ---\n\ndef add_intersecting_spot_lights_to_camera_poses(clip_start: float, clip_end: float,\n                                                 perform_look_at_intersection_check: bool = True,\n                                                 perform_look_at_pose_visibility_check: bool = True,\n                                                 light_pose_sampling: Optional[Callable[[np.ndarray],\n                                                                                        np.ndarray]] = None,\n                                                 look_at_pose_sampling: Optional[Callable[[np.ndarray, np.ndarray],\n                                                                                          np.ndarray]] = None,\n                                                 max_tries_per_cam_pose: int = 10000) -> Light:\n    \"\"\" This functions adds spotlights which intersect with the camera pose. This is useful to get a greater variety\n    in lighting situations then the general full illumination from all sides.\n\n    The spotlights location is defined by the `light_pose_sampling` parameter, it gets the eight coordinates of the\n    camera frustum vertices. This camera frustum starts at `clip_start` and ends at `clip_end`. It should return a\n    single 3D point. This point is then checked to not be in the camera frustum, if it is inside a new point will be\n    sampled.\n\n    After the defining a suitable light position, a look at pose is sampled via the `look_pose_sampling` function.\n    It uses the same eight coordinates of the camera frustum and the current sampled light position to return a look at\n    pose.\n\n    If the `perform_look_at_intersection_check` value is set an intersection check between the light position and\n    the look at location is done, which ensures that no object is between these two points.\n    Similarly, for the `perform_look_at_pose_visibility_check`, a newly sampled light pose does not have an intersecting\n    object between this sampled pose and the camera location.\n\n    :param clip_start: The distance between the camera pose and the near clipping plane, used for the sampling of a\n                       light and look at location\n    :param clip_end: The distance between the camera pose and the far clipping plane, used for the sampling of a\n                     light and look at location\n    :param perform_look_at_intersection_check: If this is True an intersection check between the light pose and the look\n                                               at pose is done, if an object is inbetween both poses are discarded.\n    :param perform_look_at_pose_visibility_check: If this is True, an intersection check between the look at pose and\n                                                  the camera location is done, to ensure that the light is visible and\n                                                  not hidden.\n    :param light_pose_sampling: This function samples a new 3D light pose based on the eight 3D coordinates of the\n                                camera frustum. If this is None, the `_default_light_pose_sampling` is used.\n    :param look_at_pose_sampling: This function samples a new 3D look at pose based on the eight 3D coordinates of the\n                                  camera frustum and the currently sampled light pose. If this is None,\n                                  the `_default_look_at_pose_sampling` is used.\n    :param max_tries_per_cam_pose: The amount of maximum tries per camera pose for finding a new light pose with\n                                   look at pose.\n    :return: The newly generated light\n    \"\"\"\n\n    if bpy.context.scene.frame_start == bpy.context.scene.frame_end:\n        raise RuntimeError(\"A camera poses has to be set first!\")\n\n    if light_pose_sampling is None:\n        light_pose_sampling = _default_light_pose_sampling\n\n    if look_at_pose_sampling is None:\n        look_at_pose_sampling = _default_look_at_pose_sampling\n\n    new_light = Light(light_type=\"SPOT\")\n    new_light.set_energy(10000)\n    # create a bvh tree to quickly check if an object is in the line of sight\n    bvh_tree = None\n    if perform_look_at_pose_visibility_check or perform_look_at_intersection_check:\n        bvh_tree = create_bvh_tree_multi_objects(get_all_mesh_objects())\n\n    # iterate over each camera pose\n    for frame_id in range(bpy.context.scene.frame_start, bpy.context.scene.frame_end):\n        # set the current camera frame for all functions\n        with KeyFrame(frame_id):\n            # get the vertices of the camera frustum\n            vertices = get_camera_frustum(clip_start=clip_start, clip_end=clip_end)\n            found_pose = False\n            for _ in range(max_tries_per_cam_pose):\n                # sample a new light position\n                sampled_pose = light_pose_sampling(vertices)\n                # sample a look at pose\n                look_at_point = look_at_pose_sampling(vertices, sampled_pose)\n                # check that the sampled pose is not inside the camera frustum and the look at point is\n                if not is_point_inside_camera_frustum(sampled_pose) and is_point_inside_camera_frustum(look_at_point):\n                    # check if an object is between the look at pose and the camera pose\n                    if perform_look_at_pose_visibility_check:\n                        cam_location = get_camera_pose()[:3, 3]\n                        look_dir = cam_location - look_at_point\n                        _, _, _, dist = bvh_tree.ray_cast(look_at_point, look_dir, np.linalg.norm(look_dir))\n                        if dist is not None:\n                            # if an object is between the light pose and the camera sample a new light pose\n                            continue\n                    # check if an object is between the sample point and the look at point\n                    if perform_look_at_intersection_check:\n                        look_dir = look_at_point - sampled_pose\n                        _, _, _, dist = bvh_tree.ray_cast(sampled_pose, look_dir, np.linalg.norm(look_dir))\n                        if dist is not None:\n                            # skip this light position as it collides with something\n                            continue\n\n                    # calculate the rotation matrix\n                    forward_vec = look_at_point - sampled_pose\n                    rotation_matrix = rotation_from_forward_vec(forward_vec)\n\n                    # save the pose and rotation\n                    new_light.set_location(sampled_pose)\n                    new_light.set_rotation_mat(rotation_matrix)\n                    found_pose = True\n                    break\n            if not found_pose:\n                raise RuntimeError(\"No pose found, increase the start and end clip or increase the amount of tries.\")\n    return new_light\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that initializes a parser with arguments for paths to the bop datasets parent directory, cc textures, output directory, and the number of scenes to generate. The code should initialize the blenderproc library and load bop objects into the scene from the 'itodd' and 'tless' datasets. It should also load BOP dataset intrinsics and set shading and hide objects. \n\nThe code should create a room using primitive planes and enable rigidbody for these planes. It should also create a light plane and a point light. The code should load cc textures and define a function to sample 6-DoF poses. \n\nThe code should enable depth rendering without antialiasing and set the maximum amount of samples for color rendering. For each scene, the code should sample bop objects, randomize materials, set physics, sample two light sources, assign a random cc texture to room planes, sample object poses, check collisions, simulate physics and fix final poses. \n\nThe code should create a BVH tree for camera obstacle checks and generate camera poses while ensuring that obstacles are at least 0.3 meter away from the camera. The code should render the pipeline and write data in bop format. After each scene, the code should disable rigidbody and hide objects.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 46, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that performs two tasks using the pyscf library. The first task is to transform a Full Configuration Interaction (FCI) wavefunction with respect to orbital rotation/transformation. This involves creating two molecules with different atomic configurations, calculating their FCI energies, and then transforming the wavefunction of the first molecule to match the second one. The second task is to transfer a FCI wavefunction from a smaller orbital space to a larger one. This involves creating a molecule with a specific atomic configuration, calculating its FCI energy, and then expanding the wavefunction to a larger orbital space. The code should also compare the transformed wavefunction with the one obtained from the FCI solver and check if they are close. Finally, the code should transform the FCI wavefunction using a different method and compare the results with the previous transformation.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def kernel(self, mo_coeff=None, ci0=None, verbose=None):\n        '''\n        Returns:\n            Five elements, they are\n            total energy,\n            active space CI energy,\n            the active space FCI wavefunction coefficients or DMRG wavefunction ID,\n            the MCSCF canonical orbital coefficients,\n            the MCSCF canonical orbital coefficients.\n\n        They are attributes of mcscf object, which can be accessed by\n        .e_tot, .e_cas, .ci, .mo_coeff, .mo_energy\n        '''\n        if mo_coeff is None:\n            mo_coeff = self.mo_coeff\n        else:\n            self.mo_coeff = mo_coeff\n        if ci0 is None:\n            ci0 = self.ci\n        log = logger.new_logger(self, verbose)\n\n        if self.verbose >= logger.WARN:\n            self.check_sanity()\n        self.dump_flags(log)\n\n        self.e_tot, self.e_cas, self.ci = \\\n                kernel(self, mo_coeff, ci0=ci0, verbose=log)\n\n        if self.canonicalization:\n            self.canonicalize_(mo_coeff, self.ci,\n                               sort=self.sorting_mo_energy,\n                               cas_natorb=self.natorb, verbose=log)\n        elif self.natorb:\n            # FIXME (pyscf-2.0): Whether to transform natural orbitals in\n            # active space when this flag is enabled?\n            log.warn('The attribute .natorb of mcscf object affects only the '\n                     'orbital canonicalization.\\n'\n                     'If you would like to get natural orbitals in active space '\n                     'without touching core and external orbitals, an explicit '\n                     'call to mc.cas_natorb_() is required')\n\n        if getattr(self.fcisolver, 'converged', None) is not None:\n            self.converged = numpy.all(self.fcisolver.converged)\n            if self.converged:\n                log.info('CASCI converged')\n            else:\n                log.info('CASCI not converged')\n        else:\n            self.converged = True\n        self._finalize()\n        return self.e_tot, self.e_cas, self.ci, self.mo_coeff, self.mo_energy\n\n# --- Snippet Separator ---\n\ndef kernel(self, mo_coeff=None, ci0=None, callback=None, _kern=kernel):\n        '''\n        Returns:\n            Five elements, they are\n            total energy,\n            active space CI energy,\n            the active space FCI wavefunction coefficients or DMRG wavefunction ID,\n            the MCSCF canonical orbital coefficients,\n            the MCSCF canonical orbital coefficients.\n\n        They are attributes of mcscf object, which can be accessed by\n        .e_tot, .e_cas, .ci, .mo_coeff, .mo_energy\n        '''\n        if mo_coeff is None:\n            mo_coeff = self.mo_coeff\n        else: # overwrite self.mo_coeff because it is needed in many methods of this class\n            self.mo_coeff = mo_coeff\n        if callback is None: callback = self.callback\n\n        if self.verbose >= logger.WARN:\n            self.check_sanity()\n        self.dump_flags()\n\n        self.converged, self.e_tot, self.e_cas, self.ci, \\\n                self.mo_coeff, self.mo_energy = \\\n                _kern(self, mo_coeff,\n                      tol=self.conv_tol, conv_tol_grad=self.conv_tol_grad,\n                      ci0=ci0, callback=callback, verbose=self.verbose)\n        logger.note(self, 'CASSCF energy = %.15g', self.e_tot)\n        self._finalize()\n        return self.e_tot, self.e_cas, self.ci, self.mo_coeff, self.mo_energy\n\n# --- Snippet Separator ---\n\ndef transform_ci_for_orbital_rotation(ci, norb, nelec, u):\n    '''\n    Transform CI coefficients (dimension conserved) to the representation in\n    new one-particle basis.  Solving CI problem for Hamiltonian h1, h2 defined\n    in old basis,\n    CI_old = fci.kernel(h1, h2, ...)\n    Given orbital rotation u, the CI problem can be either solved by\n    transforming the Hamiltonian, or transforming the coefficients.\n    CI_new = fci.kernel(u^T*h1*u, ...) = transform_ci_for_orbital_rotation(CI_old, u)\n\n    Args:\n        u : a squared 2D array or a list of 2D array\n            the orbital rotation to transform the old one-particle basis to new\n            one-particle basis\n    '''\n    if isinstance(u, numpy.ndarray) and u.ndim == 2:\n        assert u.shape == (norb, norb)\n    else:\n        assert u[0].shape == (norb, norb) and u[1].shape == (norb, norb)\n    return transform_ci(ci, nelec, u)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs two tasks using the pyscf library. The first task is to transform a Full Configuration Interaction (FCI) wavefunction with respect to orbital rotation/transformation. This involves creating two molecules with different atomic configurations, calculating their FCI energies, and then transforming the wavefunction of the first molecule to match the second one. The second task is to transfer a FCI wavefunction from a smaller orbital space to a larger one. This involves creating a molecule with a specific atomic configuration, calculating its FCI energy, and then expanding the wavefunction to a larger orbital space. The code should also compare the transformed wavefunction with the one obtained from the FCI solver and check if they are close. Finally, the code should transform the FCI wavefunction using a different method and compare the results with the previous transformation.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 47, "repo_full_name": "simpeg__simpeg", "instruction": "Generate code that performs a 3D DC inversion of a dipole-dipole array using the SimPEG library. The model should consist of two spheres, one conductive and the other resistive, compared to the background. The inversion should be restrained to the Core Mesh using an Active Cells mapping combined with an exponential mapping to invert in log conductivity space. The code should also include the creation of a synthetic Dipole-Dipole Survey and a Tikhonov Inversion. Finally, the code should generate a plot of the ground truth and the inverted model, both vertically and horizontally.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def make_forward_dipole(dipole, bem, info, trans=None, n_jobs=1, verbose=None):\n    \"\"\"Convert dipole object to source estimate and calculate forward operator\n\n    The instance of Dipole is converted to a discrete source space,\n    which is then combined with a BEM or a sphere model and\n    the sensor information in info to form a forward operator.\n\n    The source estimate object (with the forward operator) can be projected to\n    sensor-space using :func:`mne.simulation.evoked.simulate_evoked`.\n\n    Note that if the (unique) time points of the dipole object are unevenly\n    spaced, the first output will be a list of single-timepoint source\n    estimates.\n\n    Parameters\n    ----------\n    dipole : instance of Dipole\n        Dipole object containing position, orientation and amplitude of\n        one or more dipoles. Multiple simultaneous dipoles may be defined by\n        assigning them identical times.\n    bem : str | dict\n        The BEM filename (str) or a loaded sphere model (dict).\n    info : instance of Info\n        The measurement information dictionary. It is sensor-information etc.,\n        e.g., from a real data file.\n    trans : str | None\n        The head<->MRI transform filename. Must be provided unless BEM\n        is a sphere model.\n    n_jobs : int\n        Number of jobs to run in parallel (used in making forward solution).\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see mne.verbose).\n\n    Returns\n    -------\n    fwd : instance of Forward\n        The forward solution corresponding to the source estimate(s).\n    stc : instance of VolSourceEstimate | list of VolSourceEstimate\n        The dipoles converted to a discrete set of points and associated\n        time courses. If the time points of the dipole are unevenly spaced,\n        a list of single-timepoint source estimates are returned.\n\n    See Also\n    --------\n    mne.simulation.simulate_evoked\n\n    Notes\n    -----\n    .. versionadded:: 0.12.0\n    \"\"\"\n    # Make copies to avoid mangling original dipole\n    times = dipole.times.copy()\n    pos = dipole.pos.copy()\n    amplitude = dipole.amplitude.copy()\n    ori = dipole.ori.copy()\n\n    # Convert positions to discrete source space (allows duplicate rr & nn)\n    # NB information about dipole orientation enters here, then no more\n    sources = dict(rr=pos, nn=ori)\n    # Dipole objects must be in the head frame\n    sp = _make_discrete_source_space(sources, coord_frame='head')\n    src = SourceSpaces([sp])  # dict with working_dir, command_line not nec\n\n    # Forward operator created for channels in info (use pick_info to restrict)\n    # Use defaults for most params, including min_dist\n    fwd = make_forward_solution(info, trans, src, bem, fname=None,\n                                n_jobs=n_jobs, verbose=verbose)\n    # Convert from free orientations to fixed (in-place)\n    convert_forward_solution(fwd, surf_ori=False, force_fixed=True,\n                             copy=False, verbose=None)\n\n    # Check for omissions due to proximity to inner skull in\n    # make_forward_solution, which will result in an exception\n    if fwd['src'][0]['nuse'] != len(pos):\n        inuse = fwd['src'][0]['inuse'].astype(np.bool)\n        head = ('The following dipoles are outside the inner skull boundary')\n        msg = len(head) * '#' + '\\n' + head + '\\n'\n        for (t, pos) in zip(times[np.logical_not(inuse)],\n                            pos[np.logical_not(inuse)]):\n            msg += '    t={:.0f} ms, pos=({:.0f}, {:.0f}, {:.0f}) mm\\n'.\\\n                format(t * 1000., pos[0] * 1000.,\n                       pos[1] * 1000., pos[2] * 1000.)\n        msg += len(head) * '#'\n        logger.error(msg)\n        raise ValueError('One or more dipoles outside the inner skull.')\n\n    # multiple dipoles (rr and nn) per time instant allowed\n    # uneven sampling in time returns list\n    timepoints = np.unique(times)\n    if len(timepoints) > 1:\n        tdiff = np.diff(timepoints)\n        if not np.allclose(tdiff, tdiff[0]):\n            warn('Unique time points of dipoles unevenly spaced: returned '\n                 'stc will be a list, one for each time point.')\n            tstep = -1.0\n        else:\n            tstep = tdiff[0]\n    elif len(timepoints) == 1:\n        tstep = 0.001\n\n    # Build the data matrix, essentially a block-diagonal with\n    # n_rows: number of dipoles in total (dipole.amplitudes)\n    # n_cols: number of unique time points in dipole.times\n    # amplitude with identical value of times go together in one col (others=0)\n    data = np.zeros((len(amplitude), len(timepoints)))  # (n_d, n_t)\n    row = 0\n    for tpind, tp in enumerate(timepoints):\n        amp = amplitude[in1d(times, tp)]\n        data[row:row + len(amp), tpind] = amp\n        row += len(amp)\n\n    if tstep > 0:\n        stc = VolSourceEstimate(data, vertices=fwd['src'][0]['vertno'],\n                                tmin=timepoints[0],\n                                tstep=tstep, subject=None)\n    else:  # Must return a list of stc, one for each time point\n        stc = []\n        for col, tp in enumerate(timepoints):\n            stc += [VolSourceEstimate(data[:, col][:, np.newaxis],\n                                      vertices=fwd['src'][0]['vertno'],\n                                      tmin=tp, tstep=0.001, subject=None)]\n    return fwd, stc\n\n# --- Snippet Separator ---\n\nclass Dipole(object):\n    \"\"\"Dipole class for sequential dipole fits\n\n    .. note:: This class should usually not be instantiated directly,\n              instead :func:`mne.read_dipole` should be used.\n\n    Used to store positions, orientations, amplitudes, times, goodness of fit\n    of dipoles, typically obtained with Neuromag/xfit, mne_dipole_fit\n    or certain inverse solvers. Note that dipole position vectors are given in\n    the head coordinate frame.\n\n    Parameters\n    ----------\n    times : array, shape (n_dipoles,)\n        The time instants at which each dipole was fitted (sec).\n    pos : array, shape (n_dipoles, 3)\n        The dipoles positions (m) in head coordinates.\n    amplitude : array, shape (n_dipoles,)\n        The amplitude of the dipoles (nAm).\n    ori : array, shape (n_dipoles, 3)\n        The dipole orientations (normalized to unit length).\n    gof : array, shape (n_dipoles,)\n        The goodness of fit.\n    name : str | None\n        Name of the dipole.\n\n    See Also\n    --------\n    read_dipole\n    DipoleFixed\n\n    Notes\n    -----\n    This class is for sequential dipole fits, where the position\n    changes as a function of time. For fixed dipole fits, where the\n    position is fixed as a function of time, use :class:`mne.DipoleFixed`.\n    \"\"\"\n    def __init__(self, times, pos, amplitude, ori, gof, name=None):\n        self.times = np.array(times)\n        self.pos = np.array(pos)\n        self.amplitude = np.array(amplitude)\n        self.ori = np.array(ori)\n        self.gof = np.array(gof)\n        self.name = name\n\n    def __repr__(self):\n        s = \"n_times : %s\" % len(self.times)\n        s += \", tmin : %s\" % np.min(self.times)\n        s += \", tmax : %s\" % np.max(self.times)\n        return \"<Dipole  |  %s>\" % s\n\n    def save(self, fname):\n        \"\"\"Save dipole in a .dip file\n\n        Parameters\n        ----------\n        fname : str\n            The name of the .dip file.\n        \"\"\"\n        fmt = \"  %7.1f %7.1f %8.2f %8.2f %8.2f %8.3f %8.3f %8.3f %8.3f %6.1f\"\n        # NB CoordinateSystem is hard-coded as Head here\n        with open(fname, 'wb') as fid:\n            fid.write('# CoordinateSystem \"Head\"\\n'.encode('utf-8'))\n            fid.write('#   begin     end   X (mm)   Y (mm)   Z (mm)'\n                      '   Q(nAm)  Qx(nAm)  Qy(nAm)  Qz(nAm)    g/%\\n'\n                      .encode('utf-8'))\n            t = self.times[:, np.newaxis] * 1000.\n            gof = self.gof[:, np.newaxis]\n            amp = 1e9 * self.amplitude[:, np.newaxis]\n            out = np.concatenate((t, t, self.pos / 1e-3, amp,\n                                  self.ori * amp, gof), axis=-1)\n            np.savetxt(fid, out, fmt=fmt)\n            if self.name is not None:\n                fid.write(('## Name \"%s dipoles\" Style \"Dipoles\"'\n                           % self.name).encode('utf-8'))\n\n    def crop(self, tmin=None, tmax=None):\n        \"\"\"Crop data to a given time interval\n\n        Parameters\n        ----------\n        tmin : float | None\n            Start time of selection in seconds.\n        tmax : float | None\n            End time of selection in seconds.\n        \"\"\"\n        sfreq = None\n        if len(self.times) > 1:\n            sfreq = 1. / np.median(np.diff(self.times))\n        mask = _time_mask(self.times, tmin, tmax, sfreq=sfreq)\n        for attr in ('times', 'pos', 'gof', 'amplitude', 'ori'):\n            setattr(self, attr, getattr(self, attr)[mask])\n\n    def copy(self):\n        \"\"\"Copy the Dipoles object\n\n        Returns\n        -------\n        dip : instance of Dipole\n            The copied dipole instance.\n        \"\"\"\n        return deepcopy(self)\n\n    @verbose\n    def plot_locations(self, trans, subject, subjects_dir=None,\n                       bgcolor=(1, 1, 1), opacity=0.3,\n                       brain_color=(1, 1, 0), fig_name=None,\n                       fig_size=(600, 600), mode='cone',\n                       scale_factor=0.1e-1, colors=None, verbose=None):\n        \"\"\"Plot dipole locations as arrows\n\n        Parameters\n        ----------\n        trans : dict\n            The mri to head trans.\n        subject : str\n            The subject name corresponding to FreeSurfer environment\n            variable SUBJECT.\n        subjects_dir : None | str\n            The path to the freesurfer subjects reconstructions.\n            It corresponds to Freesurfer environment variable SUBJECTS_DIR.\n            The default is None.\n        bgcolor : tuple of length 3\n            Background color in 3D.\n        opacity : float in [0, 1]\n            Opacity of brain mesh.\n        brain_color : tuple of length 3\n            Brain color.\n        fig_name : tuple of length 2\n            Mayavi figure name.\n        fig_size : tuple of length 2\n            Mayavi figure size.\n        mode : str\n            Should be ``'cone'`` or ``'sphere'`` to specify how the\n            dipoles should be shown.\n        scale_factor : float\n            The scaling applied to amplitudes for the plot.\n        colors: list of colors | None\n            Color to plot with each dipole. If None defaults colors are used.\n        verbose : bool, str, int, or None\n            If not None, override default verbose level (see mne.verbose).\n\n        Returns\n        -------\n        fig : instance of mlab.Figure\n            The mayavi figure.\n        \"\"\"\n        from .viz import plot_dipole_locations\n        dipoles = []\n        for t in self.times:\n            dipoles.append(self.copy())\n            dipoles[-1].crop(t, t)\n        return plot_dipole_locations(\n            dipoles, trans, subject, subjects_dir, bgcolor, opacity,\n            brain_color, fig_name, fig_size, mode, scale_factor,\n            colors)\n\n    def plot_amplitudes(self, color='k', show=True):\n        \"\"\"Plot the dipole amplitudes as a function of time\n\n        Parameters\n        ----------\n        color: matplotlib Color\n            Color to use for the trace.\n        show : bool\n            Show figure if True.\n\n        Returns\n        -------\n        fig : matplotlib.figure.Figure\n            The figure object containing the plot.\n        \"\"\"\n        from .viz import plot_dipole_amplitudes\n        return plot_dipole_amplitudes([self], [color], show)\n\n    def __getitem__(self, idx_slice):\n        \"\"\"Handle indexing\"\"\"\n        if isinstance(idx_slice, int):  # make sure attributes stay 2d\n            idx_slice = [idx_slice]\n\n        selected_times = self.times[idx_slice].copy()\n        selected_pos = self.pos[idx_slice, :].copy()\n        selected_amplitude = self.amplitude[idx_slice].copy()\n        selected_ori = self.ori[idx_slice, :].copy()\n        selected_gof = self.gof[idx_slice].copy()\n        selected_name = self.name\n\n        new_dipole = Dipole(selected_times, selected_pos,\n                            selected_amplitude, selected_ori,\n                            selected_gof, selected_name)\n        return new_dipole\n\n    def __len__(self):\n        \"\"\"Handle len function\"\"\"\n        return self.pos.shape[0]\n\n# --- Snippet Separator ---\n\nclass DipoleFixed(object):\n    \"\"\"Dipole class for fixed-position dipole fits\n\n    .. note:: This class should usually not be instantiated directly,\n              instead :func:`mne.read_dipole` should be used.\n\n    Parameters\n    ----------\n    info : instance of Info\n        The measurement info.\n    data : array, shape (n_channels, n_times)\n        The dipole data.\n    times : array, shape (n_times,)\n        The time points.\n    nave : int\n        Number of averages.\n    aspect_kind : int\n        The kind of data.\n    first : int\n        First sample.\n    last : int\n        Last sample.\n    comment : str\n        The dipole comment.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see mne.verbose).\n\n    See Also\n    --------\n    read_dipole\n    Dipole\n\n    Notes\n    -----\n    This class is for fixed-position dipole fits, where the position\n    (and maybe orientation) is static over time. For sequential dipole fits,\n    where the position can change a function of time, use :class:`mne.Dipole`.\n\n    .. versionadded:: 0.12\n    \"\"\"\n    @verbose\n    def __init__(self, info, data, times, nave, aspect_kind, first, last,\n                 comment, verbose=None):\n        self.info = info\n        self.nave = nave\n        self._aspect_kind = aspect_kind\n        self.kind = _aspect_rev.get(str(aspect_kind), 'Unknown')\n        self.first = first\n        self.last = last\n        self.comment = comment\n        self.times = times\n        self.data = data\n        self.verbose = verbose\n\n    @property\n    def ch_names(self):\n        return self.info['ch_names']\n\n    @verbose\n    def save(self, fname, verbose=None):\n        \"\"\"Save dipole in a .fif file\n\n        Parameters\n        ----------\n        fname : str\n            The name of the .fif file. Must end with ``'.fif'`` or\n            ``'.fif.gz'`` to make it explicit that the file contains\n            dipole information in FIF format.\n        verbose : bool, str, int, or None\n            If not None, override default verbose level (see mne.verbose).\n        \"\"\"\n        check_fname(fname, 'DipoleFixed', ('-dip.fif', '-dip.fif.gz'),\n                    ('.fif', '.fif.gz'))\n        _write_evokeds(fname, self, check=False)\n\n    def plot(self, show=True):\n        \"\"\"Plot dipole data\n\n        Parameters\n        ----------\n        show : bool\n            Call pyplot.show() at the end or not.\n\n        Returns\n        -------\n        fig : instance of matplotlib.figure.Figure\n            The figure containing the time courses.\n        \"\"\"\n        return _plot_evoked(self, picks=None, exclude=(), unit=True, show=show,\n                            ylim=None, xlim='tight', proj=False, hline=None,\n                            units=None, scalings=None, titles=None, axes=None,\n                            gfp=False, window_title=None, spatial_colors=False,\n                            plot_type=\"butterfly\", selectable=False)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a 3D DC inversion of a dipole-dipole array using the SimPEG library. The model should consist of two spheres, one conductive and the other resistive, compared to the background. The inversion should be restrained to the Core Mesh using an Active Cells mapping combined with an exponential mapping to invert in log conductivity space. The code should also include the creation of a synthetic Dipole-Dipole Survey and a Tikhonov Inversion. Finally, the code should generate a plot of the ground truth and the inverted model, both vertically and horizontally.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 48, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that estimates randoms from a list mode file and compares the result with the original delayed coincidences. The code should be able to handle command-line options for the path to data files, listmode file, sinograms file prefix, randoms file, raw data template, scanning time interval, reconstruction engine, acquisition data storage scheme, and an option for non-interactive mode. The code should import necessary modules and set up the listmode-to-sinograms converter object with the appropriate input, output, and template files. It should also set the time interval and flags for storing delayed coincidences. After setting up the converter, the code should process the data, get access to the sinograms, and estimate the randoms from the delayeds via Maximum Likelihood estimation. The estimated randoms should be written to a file. The code should also copy the acquisition data into Python arrays and print out the acquisition data dimensions, total number of delayed coincidences and estimated randoms, and max values. If not in non-interactive mode, the code should display a single sinogram. The code should be wrapped in a main function and handle any errors that occur during execution.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class ListmodeToSinograms(object):\n    \"\"\"\n    Class for listmode-to-sinogram converter.\n\n    This class reads list mode data and produces corresponding *sinograms*,\n    i.e. histogrammed data in the format of PETAcquisitionData.\n\n    It has two main functions:\n      - process() can be used to read prompts and/or delayed coincidences to\n        produce a single PETAcquisitionData.\n        Two conversion flags decide what is to be done with 3 possible cases:\n        - `store_prompts`=`true`, `store_delayeds`=`false`:\n        only prompts stored\n        - `store_prompts`=`false`, `store_delayeds`=`true`:\n        only delayeds stored\n        - `store_prompts`=`true`, `store_delayeds`=`true`:\n        prompts-delayeds stored\n        Clearly, enabling the `store_delayeds` option only makes sense if the\n        data was acquired accordingly.\n      - estimate_randoms() can be used to get a relatively noiseless estimate\n        of the random coincidences.\n\n    Currently, the randoms are estimated from the delayed coincidences using\n    the following strategy:\n       1. singles (one per detector) are estimated using a Maximum Likelihood\n          estimator\n       2. randoms-from-singles are computed per detector-pair via the usual\n          product formula. These are then added together for all detector pairs\n          in a certain histogram-bin in the data (accommodating for view\n          mashing and axial compression).\n\n    The actual algorithm is described in\n\n    D. Hogg, K. Thielemans, S. Mustafovic, and T. J. Spinks,\n    \"A study of bias for various iterative reconstruction methods in PET,\"\n    in 2002 IEEE Nuclear Science Symposium Conference Record, vol. 3. IEEE,\n    Nov. 2002, pp. 1519-1523 (http://dx.doi.org/10.1109/nssmic.2002.1239610).\n    \"\"\"\n\n    def __init__(self, file=None):\n        \"\"\"init.\"\"\"\n        self.handle = None\n        self.name = 'ListmodeToSinograms'\n        if file is None:\n            self.handle = pystir.cSTIR_newObject(self.name)\n        else:\n            self.handle = pystir.cSTIR_objectFromFile(self.name, file)\n        self.output = None\n\n    def __del__(self):\n        \"\"\"del.\"\"\"\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n    def set_input(self, lm_file):\n        \"\"\"Sets the listmode file name.\"\"\"\n        parms.set_char_par(self.handle, self.name, 'input', lm_file)\n\n    def set_output_prefix(self, sino_file):\n        \"\"\"Sets the sinograms file names prefix.\"\"\"\n        parms.set_char_par(self.handle, self.name, 'output', sino_file)\n\n    def set_template(self, templ):\n        \"\"\"Sets the sinograms template.\n\n        templ: either file name or AcquisitionData\n        \"\"\"\n        if type(templ) == type('a'):\n            parms.set_char_par(self.handle, self.name, 'template_file', templ)\n        else:\n            parms.set_parameter(self.handle, self.name, 'template', templ.handle)\n\n    def set_time_interval(self, start, stop):\n        \"\"\"Sets the time interval.\n\n        Only data scanned during this time interval will be converted.\n        \"\"\"\n        interval = numpy.ndarray((2,), dtype=numpy.float32)\n        interval[0] = start\n        interval[1] = stop\n        try_calling(pystir.cSTIR_setListmodeToSinogramsInterval(\n            self.handle, interval.ctypes.data))\n\n    def flag_on(self, flag):\n        \"\"\"Switches on (sets to 'true') a conversion flag.\n\n        (see conversion flags description above).\n        \"\"\"\n        try_calling(pystir.cSTIR_setListmodeToSinogramsFlag(\n            self.handle, flag, 1))\n\n    def flag_off(self, flag):\n        \"\"\"Switches off (sets to 'false') a conversion flag.\n\n        (see conversion flags description above).\n        \"\"\"\n        try_calling(pystir.cSTIR_setListmodeToSinogramsFlag(\n            self.handle, flag, 0))\n\n    def set_up(self):\n        \"\"\"Sets up the conversion.\"\"\"\n        try_calling(\n            pystir.cSTIR_setupListmodeToSinogramsConverter(self.handle))\n\n    def process(self):\n        \"\"\"Performs the conversion.\"\"\"\n        self.output = AcquisitionData()\n        self.output.handle = pystir.cSTIR_convertListmodeToSinograms(\n            self.handle)\n        check_status(self.output.handle)\n\n    def get_output(self):\n        \"\"\"Returns the sinograms as an AcquisitionData object.\"\"\"\n        if self.output is None:\n            raise error('Conversion to sinograms not done')\n        return self.output\n\n    def estimate_randoms(self):\n        \"\"\"Returns an estimate of the randoms as an AcquisitionData object.\"\"\"\n        randoms = AcquisitionData()\n        randoms.handle = pystir.cSTIR_computeRandoms(self.handle)\n        check_status(randoms.handle)\n        return randoms\n\n    def get_time_at_which_num_prompts_exceeds_threshold(self, threshold):\n        \"\"\"Returns the time at which the number of prompts exceeds <threshold>.\n\n        Returns -1 if no corresponding time is found.\n        \"\"\"\n        h = pystir.cSTIR_lm_num_prompts_exceeds_threshold(\n            self.handle, float(threshold))\n        check_status(h, inspect.stack()[1])\n        v = pyiutil.floatDataFromHandle(h)\n        pyiutil.deleteDataHandle(h)\n        return v\n\n# --- Snippet Separator ---\n\nclass Run(CodesTask):\n    \"\"\"\n    The 'Run' class for plasmod transport solver.\n\n    Parameters\n    ----------\n    params:\n        The bluemira parameters for the task. Note that this task does\n        not apply any mappings to the ParameterFrame, so they should\n        already be set. Most likely by a solver.\n    input_file:\n        The path to the plasmod input file.\n    output_file:\n        The path to which the plasmod scalar output file should be\n        written.\n    profiles_file:\n        The path to which the plasmod profiles output file should be\n        written.\n    directory:\n        The directory to run the code in\n    binary:\n        The name of, or path to, the plasmod binary. If this is not an\n        absolute path, the binary must be on the system path.\n    \"\"\"\n\n    params: PlasmodSolverParams\n\n    def __init__(\n        self,\n        params: PlasmodSolverParams,\n        input_file: str,\n        output_file: str,\n        profiles_file: str,\n        directory: str = \"./\",\n        binary=PLASMOD_BINARY,\n    ):\n        super().__init__(params, PLASMOD_NAME)\n        self.binary = binary\n        self.input_file = input_file\n        self.output_file = output_file\n        self.profiles_file = profiles_file\n        self.directory = directory\n\n    def run(self):\n        \"\"\"\n        Run the plasmod shell task.\n\n        Runs plasmod on the command line using the given input files and\n        output path.\n\n        Raises\n        ------\n        CodesError\n            If the subprocess returns a non-zero exit code or raises an\n            OSError (e.g., the plasmod binary does not exist).\n        \"\"\"\n        bluemira_print(f\"Running '{PLASMOD_NAME}' systems code\")\n        command = [self.binary, self.input_file, self.output_file, self.profiles_file]\n        with working_dir(self.directory):\n            try:\n                self._run_subprocess(command)\n            except OSError as os_error:\n                raise CodesError(f\"Failed to run plasmod: {os_error}\") from os_error\n\n# --- Snippet Separator ---\n\nclass AcquisitionData(DataContainer):\n    \"\"\"Class for PET acquisition data.\"\"\"\n\n    def __init__(self, src=None, span=1, max_ring_diff=-1, view_mash_factor=1, tof_mash_factor=1):\n        \"\"\"Creates new AcquisitionData.\n\n        Can create object from a file or another AcquisitionData object.\n        src:  file name (Python str) or AcquisitionData object or scanner name\n        \"\"\"\n        self.handle = None\n        self.name = 'AcquisitionData'\n        self.read_only = False\n        self.src = None\n        if src is None:\n            return\n        if isinstance(src, str):\n            i = src.find('.')\n            if i > -1:\n                # src is a file name\n                self.handle = pystir.cSTIR_objectFromFile(\n                    'AcquisitionData', src)\n                self.read_only = self.get_storage_scheme() == 'file'\n                self.src = 'file'\n            else:\n                # src is a scanner name\n                self.handle = pystir.cSTIR_acquisitionDataFromScannerInfo(\n                    src, span, max_ring_diff, view_mash_factor, tof_mash_factor)\n                if pyiutil.executionStatus(self.handle) != 0:\n                    msg = pyiutil.executionError(self.handle)\n                    if msg == 'Unknown scanner':\n                        raise error(\n                            'Unknown scanner ' + src +\n                            ' or missing raw data file extension')\n                self.src = 'scanner'\n        elif isinstance(src, AcquisitionData):\n            # src is AcquisitionData\n            if src.handle is None:\n                raise AssertionError()\n            self.handle = pystir.cSTIR_acquisitionDataFromTemplate(src.handle)\n            self.src = 'template'\n        else:\n            raise error('Wrong source in AcquisitionData constructor')\n        check_status(self.handle)\n\n    def __del__(self):\n        \"\"\"del.\"\"\"\n        # print('deleting AcquisitionData object originated from ', self.src)\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n    @staticmethod\n    def set_storage_scheme(scheme):\n        \"\"\"Sets acquisition data storage scheme.\n\n        scheme = 'file' (default):\n            all acquisition data generated from now on will be kept in\n            scratch files deleted after the user's script terminates\n        scheme = 'memory':\n            all acquisition data generated from now on will be kept in RAM\n            (avoid if data is very large)\n        \"\"\"\n        try_calling(pystir.cSTIR_setAcquisitionDataStorageScheme(scheme))\n\n    @staticmethod\n    def get_storage_scheme():\n        \"\"\"Returns acquisition data storage scheme.\"\"\"\n        handle = pystir.cSTIR_getAcquisitionDataStorageScheme()\n        check_status(handle)\n        scheme = pyiutil.charDataFromHandle(handle)\n        pyiutil.deleteDataHandle(handle)\n        return scheme\n\n    def same_object(self):\n        \"\"\"See DataContainer method.\"\"\"\n        return AcquisitionData()\n\n    def read_from_file(self, filename):  # 'read_from_file' is misleading\n        \"\"\"\n        Reads data from file.\n\n        Replaces the current content of the object.\n        \"\"\"\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n        self.handle = pystir.cSTIR_objectFromFile('AcquisitionData', filename)\n        check_status(self.handle)\n        self.read_only = True\n\n    def create_uniform_image(self, value=0, xy=None):\n        \"\"\"Crates uniform image.\n\n        Creates ImageData object containing PET image of z-dimension\n        and voxel sizes compatible with the scanner geometry stored\n        in this AcquisitionData object and assigns a given value\n        to all voxels;\n        value: a Python float.\n        xy   : x and y dimensions tuple (if None, set by STIR)\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        image = ImageData()\n        if xy is None:\n            image.handle = pystir.cSTIR_imageFromAcquisitionData(self.handle)\n        elif isinstance(xy, tuple):\n            image.handle = pystir.cSTIR_imageFromAcquisitionDataAndNxNy(\n                self.handle, xy[1], xy[0])\n        elif isinstance(xy, int):\n            image.handle = pystir.cSTIR_imageFromAcquisitionDataAndNxNy(\n                self.handle, xy, xy)\n        else:\n            raise error('Wrong second argument in create_uniform_image')\n        check_status(image.handle)\n        image.fill(value)\n        return image\n\n    def dimensions(self):\n        \"\"\"Returns a tuple of the data dimensions.\n\n        Contains:\n        - number of TOF bins\n        - number of sinograms\n        - number of views\n        - number of tangential positions.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        dim = numpy.ndarray((MAX_ACQ_DIMS,), dtype=cpp_int_dtype())\n        try_calling(pystir.cSTIR_getAcquisitionDataDimensions(\n            self.handle, dim.ctypes.data))\n        dim = dim[:4]\n        return tuple(dim[::-1])\n\n    def get_tof_mash_factor(self):\n        '''Returns TOF mashing factor.'''\n        return parms.int_par(self.handle, 'AcquisitionData', 'tof_mash_factor')\n\n    def as_array(self):\n        \"\"\"Returns bin values as ndarray.\n\n        Return a copy of acquisition data stored in this object as a\n        NumPy ndarray of 4 dimensions (in default C ordering of data):\n        - number of TOF bins\n        - number of sinograms\n        - number of views\n        - number of tangential positions.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        array = numpy.ndarray(self.dimensions(), dtype=numpy.float32)\n        try_calling(pystir.cSTIR_getAcquisitionData(\n            self.handle, array.ctypes.data))\n        return array\n\n    def fill(self, value):\n        \"\"\"Fills the object with values.\n\n        value:  either NumPy ndarray or another AcquisitionData object\n                or Python float.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        if self.read_only:\n            raise error(\n                'Cannot fill read-only object, consider filling a clone')\n        if isinstance(value, numpy.ndarray):\n            dims = self.dimensions()\n            shape = value.shape\n            if shape != dims:\n                msg = 'cannot fill AcquisitionData of size %s' \\\n                      + ' with data of size %s'\n                raise ValueError(msg % (repr(dims), repr(shape)))\n            if value.dtype is numpy.dtype('float32'):\n                # print('keeping dtype float32')\n                v = value\n            else:\n                # print('changing dtype to float32')\n                v = value.astype(numpy.float32)\n            if not v.flags['C_CONTIGUOUS']:\n                v = numpy.ascontiguousarray(v)\n            try_calling(pystir.cSTIR_setAcquisitionData(\n                self.handle, v.ctypes.data))\n        elif isinstance(value, AcquisitionData):\n            if value.handle is None:\n                raise AssertionError()\n            try_calling(pystir.cSTIR_fillAcquisitionDataFromAcquisitionData(\n                self.handle, value.handle))\n        elif isinstance(value, float):\n            try_calling(pystir.cSTIR_fillAcquisitionData(self.handle, value))\n        elif isinstance(value, (Integral,numpy.number)):\n            try_calling(pystir.cSTIR_fillAcquisitionData(\n                self.handle, float(value)))\n        else:\n            raise TypeError('Wrong fill value.' + \\\n                ' Should be numpy.ndarray, AcquisitionData, float or int, got {}'\\\n                .format(type(value)))\n        return self\n\n    def get_uniform_copy(self, value=0):\n        \"\"\"Returns a copy of this object filled with given value.\n\n        Returns a true copy of this object filled with a given value;\n        value:  a Python float.\n        \"\"\"\n        ad = AcquisitionData(self)\n        ad.fill(value)\n        ad.src = 'copy'\n        return ad\n\n    def rebin(self, num_segments_to_combine,\n              num_views_to_combine=1, num_tang_poss_to_trim=0,\n              do_normalisation=True, max_in_segment_num_to_process=-1,\n              num_tof_bins_to_combine=1):\n        \"\"\"Re-bins the data to lower resolution.\n\n        Keyword arguments:\n\t\tnum_segments_to_combine -- combines multiple oblique 'segments' together. If set to the\n\t\t    total number of segments, this corresponds to SSRB. Another example is if the input data\n\t\t\thas 'span=1', the output span will be equal to the \\c num_segments_to_combine.\n\t\tnum_views_to_combine -- combines neighbouring views. Needs to be a divisor of the total\n\t\t    number of views in the data.\n\t\tnum_tang_poss_to_trim -- removes a number of tangential positions (horizontal direction\n\t\t    in the sinogram) at each end\n\t\tdo_normalisation -- if True, averages the data, otherwise it adds the data. Often\n\t\t    the latter is required for emission data (as it preserves Poisson statistics),\n\t\t\twhile the former should be used for corrected data (or for attenuation correction factors).\n\t\tmax_in_segment_num_to_process -- by default all input data are used. If set to a non-negative\n\t\t    number, it will remove the most oblique segments.\n\t\tnum_tof_bins_to_combine -- number of TOF bins to combine.\n        \"\"\"\n        ad = AcquisitionData()\n        ad.handle = pystir.cSTIR_rebinnedAcquisitionData(\n            self.handle,\n            num_segments_to_combine, num_views_to_combine,\n            num_tang_poss_to_trim, do_normalisation,\n            max_in_segment_num_to_process, num_tof_bins_to_combine)\n        check_status(ad.handle)\n        return ad\n\n    def show(self, sino=None, tof=0, title=None):\n        '''Displays selected sinograms.'''\n        if self.handle is None:\n            raise AssertionError()\n        if not HAVE_PYLAB:\n            print('pylab not found')\n            return\n        data = self.as_array()\n        if tof <0 or tof >= data.shape[0]:\n            raise IndexError('TOF bin index out of range')\n        nz = data.shape[1]\n        if isinstance(sino, (Integral,numpy.integer)):\n            if sino < 0 or sino >= nz:\n                raise IndexError('Slice index out of range')\n            show_2D_array('sinogram %d' % sino, data[tof, sino, :, :])\n            return\n        elif sino is None:\n            ns = nz\n            sino = range(nz)\n        else:\n            try:\n                ns = len(sino)\n            except:\n                raise error('wrong sinograms list')\n        if title is None:\n            title = 'Selected sinograms'\n        if ns >= 16:\n            tiles = (4, 4)\n        else:\n            tiles = None\n        f = 0\n        while f < ns:\n            t = min(f + 16, ns)\n            show_3D_array(\n                data[0, :, :, :],\n                index=sino[f: t], tile_shape=tiles,\n                label='sinogram',\n                xlabel='tang.pos', ylabel='view',\n                suptitle=title, show=(t == ns))\n            f = t\n\n    def allocate(self, value=0, **kwargs):\n        \"\"\"Alias to get_uniform_copy.\n\n        CIL/SIRF compatibility\n        \"\"\"\n        if value in ['random', 'random_int']:\n            out = self.get_uniform_copy()\n            shape = out.as_array().shape\n            seed = kwargs.get('seed', None)\n            if seed is not None:\n                numpy.random.seed(seed)\n            if value == 'random':\n                out.fill(numpy.random.random_sample(shape))\n            elif value == 'random_int':\n                max_value = kwargs.get('max_value', 100)\n                out.fill(numpy.random.randint(max_value,size=shape))\n        elif value is None:\n            out = self.get_uniform_copy(0)\n        else:\n            out = self.get_uniform_copy(value)\n        return out\n\n    def get_info(self):\n        \"\"\"Returns the AcquisitionData's metadata as Python str.\"\"\"\n        handle = pystir.cSTIR_get_ProjDataInfo(self.handle)\n        check_status(handle)\n        info = pyiutil.charDataFromHandle(handle)\n        pyiutil.deleteDataHandle(handle)\n        return info\n\n    def get_subset(self, views):\n        \"\"\"Returns the subset of self data formed by specified views\n\n        views: array of views (will be converted to numpy ndarray)\n        \"\"\"\n        # Ensure the array passed to C++ is a contiguous array of C++ int's\n        v = cpp_int_array(views)\n        n = len(views)\n        subset = AcquisitionData()\n        subset.handle = pystir.cSTIR_get_subset(self.handle, n, v.ctypes.data)\n        check_status(subset.handle)\n        return subset\n\n    @property\n    def shape(self):\n        return self.dimensions()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that estimates randoms from a list mode file and compares the result with the original delayed coincidences. The code should be able to handle command-line options for the path to data files, listmode file, sinograms file prefix, randoms file, raw data template, scanning time interval, reconstruction engine, acquisition data storage scheme, and an option for non-interactive mode. The code should import necessary modules and set up the listmode-to-sinograms converter object with the appropriate input, output, and template files. It should also set the time interval and flags for storing delayed coincidences. After setting up the converter, the code should process the data, get access to the sinograms, and estimate the randoms from the delayeds via Maximum Likelihood estimation. The estimated randoms should be written to a file. The code should also copy the acquisition data into Python arrays and print out the acquisition data dimensions, total number of delayed coincidences and estimated randoms, and max values. If not in non-interactive mode, the code should display a single sinogram. The code should be wrapped in a main function and handle any errors that occur during execution.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 49, "repo_full_name": "pyqtgraph__pyqtgraph", "instruction": "Generate code that creates a custom graph using the pyqtgraph library. The graph should be displayed in a GraphicsLayoutWidget with the title 'pyqtgraph example: CustomGraphItem'. The graph should be a subclass of GraphItem and include methods for setting data, updating the graph, handling mouse drag events, and responding to clicks on the graph. The graph should be populated with nodes at specified positions, connected by lines with specified styles, and each node should be labeled with a text. The nodes should be draggable, and the graph should update in real time as nodes are dragged. The program should print a message to the console when a node is clicked. The graph should be displayed when the script is run.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Graph:\n    \"\"\"\n    A graph class.\n\n    The graph class stores the nodes and edges of the graph in a sparse\n    array (equivalently to face_nodes in the Grid class).\n\n    Attributes:\n        node_connections (sps.csc-matrix): Should be given at construction.\n            node_node connections. Matrix size: num_nodes x num_nodes.\n            node_connections[i,j] should be true if there is an edge\n            connecting node i and j.\n        regions (int) the number of regions. A region is a set of nodes\n            that can be reached by traversing the graph. Two nodes are\n            int different regions if they can not be reached by traversing\n            the graph.\n        color (int) the color of each region. Initialized as (NaN). By\n            calling color_nodes() all nodes in a region are given the\n            same colors and nodes in different regions are given different\n            colors.\n    \"\"\"\n\n    def __init__(self, node_connections):\n        if node_connections.getformat() != 'csr':\n            self.node_connections = node_connections.tocsr()\n        else:\n            self.node_connections = node_connections\n        self.regions = 0\n        self.color = np.array([np.NaN] * node_connections.shape[1])\n\n    def color_nodes(self):\n        \"\"\"\n        Color the nodes in each region\n        \"\"\"\n        color = 0\n        while self.regions <= self.node_connections.shape[1]:\n            start = np.ravel(np.argwhere(np.isnan(self.color)))\n            if start.size != 0:\n                self.bfs(start[0], color)\n                color += 1\n                self.regions += 1\n            else:\n                return\n        raise RuntimeWarning('number of regions can not be greater than '\n                             'number of nodes')\n\n    def bfs(self, start, color):\n        \"\"\"\n        Breadth first search\n        \"\"\"\n        visited, queue = [], [start]\n        while queue:\n            node = queue.pop(0)\n            if node not in visited:\n                visited.append(node)\n                neighbours = sparse_mat.slice_indices(\n                    self.node_connections, node)\n                queue.extend(neighbours)\n        self.color[visited] = color\n\n# --- Snippet Separator ---\n\ndef part_graph(graph, nparts=2,\n    tpwgts=None, ubvec=None, recursive=False, **opts):\n    \"\"\"\n    Perform graph partitioning using k-way or recursive methods.\n\n    Returns a 2-tuple `(objval, parts)`, where `parts` is a list of\n    partition indices corresponding and `objval` is the value of\n    the objective function that was minimized (either the edge cuts\n    or the total volume).\n\n    :param graph: may be a NetworkX graph, an adjacency list, or a :class:`METIS_Graph`\n      named tuple. To use the named tuple approach, you'll need to\n      read the METIS manual for the meanings of the fields.\n\n      See :func:`networkx_to_metis` for help and details on how the\n      graph is converted and how node/edge weights and sizes can\n      be specified.\n\n      See :func:`adjlist_to_metis` for information on the use of adjacency lists.\n      The extra ``nodew`` and ``nodesz`` keyword arguments of that function may be given\n      directly to this function and will be forwarded to the converter.\n      Alternatively, a dictionary can be provided as ``graph`` and its items\n      will be passed as keyword arguments.\n    :param nparts: The target number of partitions. You might get fewer.\n    :param tpwgts: Target partition weights. For each partition, there should\n      be one (float) weight for each node constraint. That is, if `nparts` is 3 and\n      each node of the graph has two weights, then tpwgts might look like this::\n\n        [(0.5, 0.1), (0.25, 0.8), (0.25, 0.1)]\n\n      This list may be provided flattened. The internal tuples are for convenience.\n      The partition weights for each constraint must sum to 1.\n    :param ubvec: The load imalance tolerance for each node constraint. Should be\n      a list of floating point values each greater than 1.\n\n    :param recursive: Determines whether the partitioning should be done by\n      direct k-way cuts or by a series of recursive cuts. These correspond to\n      :c:func:`METIS_PartGraphKway` and :c:func:`METIS_PartGraphRecursive` in\n      METIS's C API.\n\n    Any additional METIS options may be specified as keyword parameters.\n\n    For k-way clustering, the appropriate options are::\n\n        objtype   = 'cut' or 'vol'\n        ctype     = 'rm' or 'shem'\n        iptype    = 'grow', 'random', 'edge', 'node'\n        rtype     = 'fm', 'greedy', 'sep2sided', 'sep1sided'\n        ncuts     = integer, number of cut attempts (default = 1)\n        niter     = integer, number of iterations (default = 10)\n        ufactor   = integer, maximum load imbalance of (1+x)/1000\n        minconn   = bool, minimize degree of subdomain graph\n        contig    = bool, force contiguous partitions\n        seed      = integer, RNG seed\n        numbering = 0 (C-style) or 1 (Fortran-style) indices\n        dbglvl    = Debug flag bitfield\n\n    For recursive clustering, the appropraite options are::\n\n        ctype     = 'rm' or 'shem'\n        iptype    = 'grow', 'random', 'edge', 'node'\n        rtype     = 'fm', 'greedy', 'sep2sided', 'sep1sided'\n        ncuts     = integer, number of cut attempts (default = 1)\n        niter     = integer, number of iterations (default = 10)\n        ufactor   = integer, maximum load imbalance of (1+x)/1000\n        seed      = integer, RNG seed\n        numbering = 0 (C-style) or 1 (Fortran-style) indices\n        dbglvl    = Debug flag bitfield\n\n    See the METIS manual for specific meaning of each option.\n    \"\"\"\n\n    if networkx and isinstance(graph, networkx.Graph):\n        graph = networkx_to_metis(graph)\n    elif isinstance(graph, list):\n        nodesz = opts.pop('nodesz', None)\n        nodew  = opts.pop('nodew', None)\n        graph = adjlist_to_metis(graph, nodew, nodesz)\n    elif isinstance(graph, dict):\n        # Check if this has METIS_Graph fields or an adjlist\n        if 'nvtxs' in graph:\n            graph = METIS_Graph(**graph)\n        elif 'adjlist' in graph:\n            graph = adjlist_to_metis(**graph)\n\n    options = METIS_Options(**opts)\n    if tpwgts and not isinstance(tpwgts, ctypes.Array):\n        if isinstance(tpwgts[0], (tuple, list)):\n            tpwgts = reduce(op.add, tpwgts)\n        tpwgts = (real_t*len(tpwgts))(*tpwgts)\n    if ubvec and not isinstance(ubvec, ctypes.Array):\n        ubvec = (real_t*len(ubvect))(*ubvec)\n\n    if tpwgts: assert len(tpwgts) == nparts * graph.ncon\n    if ubvec: assert len(ubvec) == graph.ncon\n\n    nparts_var = idx_t(nparts)\n\n    objval = idx_t()\n    partition = (idx_t*graph.nvtxs.value)()\n\n    args = (byref(graph.nvtxs), byref(graph.ncon), graph.xadj,\n            graph.adjncy, graph.vwgt, graph.vsize, graph.adjwgt,\n            byref(nparts_var), tpwgts, ubvec, options.array,\n            byref(objval), partition)\n    if recursive:\n        _METIS_PartGraphRecursive(*args)\n    else:\n        _METIS_PartGraphKway(*args)\n\n    return objval.value, list(partition)\n\n# --- Snippet Separator ---\n\ndef get_vertical_line_to_graph(self, x, graph, line_class=Line, **line_kwargs):\n        \"\"\"\n        This method returns a Vertical line from the x-axis to\n        the corresponding point on the graph/curve.\n\n        Parameters\n        ----------\n        x : int, float\n            The x-value at which the line should be placed/calculated.\n\n        graph : ParametricFunction\n            The graph on which the line should extend to.\n\n        line_class : Line and similar\n            The type of line that should be used.\n            Defaults to Line.\n\n        **line_kwargs\n            Any valid keyword arguments of the object passed in \"line_class\"\n            If line_class is Line, any valid keyword arguments of Line are allowed.\n\n        Return\n        ------\n        An object of type passed in \"line_class\"\n            Defaults to Line\n        \"\"\"\n        if \"color\" not in line_kwargs:\n            line_kwargs[\"color\"] = graph.get_color()\n        return line_class(\n            self.coords_to_point(x, 0),\n            self.input_to_graph_point(x, graph),\n            **line_kwargs,\n        )\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a custom graph using the pyqtgraph library. The graph should be displayed in a GraphicsLayoutWidget with the title 'pyqtgraph example: CustomGraphItem'. The graph should be a subclass of GraphItem and include methods for setting data, updating the graph, handling mouse drag events, and responding to clicks on the graph. The graph should be populated with nodes at specified positions, connected by lines with specified styles, and each node should be labeled with a text. The nodes should be draggable, and the graph should update in real time as nodes are dragged. The program should print a message to the console when a node is clicked. The graph should be displayed when the script is run.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 50, "repo_full_name": "deepmind__acme", "instruction": "Generate code that sets up and runs a behavioral cloning (BC) experiment on a specified environment using the Acme library. The code should define several flags for configuring the experiment, such as the environment name, number of demonstrations, learning steps, batch size, learning rate, dropout rate, and network parameters. It should also include functions to create a demonstration dataset factory, an environment factory, and a network factory. The network should be a multi-layer perceptron (MLP) with ReLU activation and dropout. The code should also include a function to build the experiment configuration, which uses the previously defined factories and the BC builder. Finally, the code should include a main function that builds the experiment configuration and runs the experiment, either in a distributed or single-threaded manner.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def run_experiment(experiment: config.ExperimentConfig,\n                   eval_every: int = 100,\n                   num_eval_episodes: int = 1):\n  \"\"\"Runs a simple, single-threaded training loop using the default evaluators.\n\n  It targets simplicity of the code and so only the basic features of the\n  ExperimentConfig are supported.\n\n  Arguments:\n    experiment: Definition and configuration of the agent to run.\n    eval_every: After how many actor steps to perform evaluation.\n    num_eval_episodes: How many evaluation episodes to execute at each\n      evaluation step.\n  \"\"\"\n\n  key = jax.random.PRNGKey(experiment.seed)\n\n  # Create the environment and get its spec.\n  environment = experiment.environment_factory(experiment.seed)\n  environment_spec = experiment.environment_spec or specs.make_environment_spec(\n      environment)\n\n  # Create the networks and policy.\n  networks = experiment.network_factory(environment_spec)\n  policy = config.make_policy(\n      experiment=experiment,\n      networks=networks,\n      environment_spec=environment_spec,\n      evaluation=False)\n\n  # Create the replay server and grab its address.\n  replay_tables = experiment.builder.make_replay_tables(environment_spec,\n                                                        policy)\n\n  # Disable blocking of inserts by tables' rate limiters, as this function\n  # executes learning (sampling from the table) and data generation\n  # (inserting into the table) sequentially from the same thread\n  # which could result in blocked insert making the algorithm hang.\n  replay_tables, rate_limiters_max_diff = _disable_insert_blocking(\n      replay_tables)\n\n  replay_server = reverb.Server(replay_tables, port=None)\n  replay_client = reverb.Client(f'localhost:{replay_server.port}')\n\n  # Parent counter allows to share step counts between train and eval loops and\n  # the learner, so that it is possible to plot for example evaluator's return\n  # value as a function of the number of training episodes.\n  parent_counter = counting.Counter(time_delta=0.)\n\n  dataset = experiment.builder.make_dataset_iterator(replay_client)\n  # We always use prefetch as it provides an iterator with an additional\n  # 'ready' method.\n  dataset = utils.prefetch(dataset, buffer_size=1)\n\n  # Create actor, adder, and learner for generating, storing, and consuming\n  # data respectively.\n  # NOTE: These are created in reverse order as the actor needs to be given the\n  # adder and the learner (as a source of variables).\n  learner_key, key = jax.random.split(key)\n  learner = experiment.builder.make_learner(\n      random_key=learner_key,\n      networks=networks,\n      dataset=dataset,\n      logger_fn=experiment.logger_factory,\n      environment_spec=environment_spec,\n      replay_client=replay_client,\n      counter=counting.Counter(parent_counter, prefix='learner', time_delta=0.))\n\n  adder = experiment.builder.make_adder(replay_client, environment_spec, policy)\n\n  actor_key, key = jax.random.split(key)\n  actor = experiment.builder.make_actor(\n      actor_key, policy, environment_spec, variable_source=learner, adder=adder)\n\n  # Create the environment loop used for training.\n  train_counter = counting.Counter(\n      parent_counter, prefix='actor', time_delta=0.)\n  train_logger = experiment.logger_factory('actor',\n                                           train_counter.get_steps_key(), 0)\n\n  checkpointer = None\n  if experiment.checkpointing is not None:\n    checkpointing = experiment.checkpointing\n    checkpointer = savers.Checkpointer(\n        objects_to_save={'learner': learner, 'counter': parent_counter},\n        time_delta_minutes=checkpointing.time_delta_minutes,\n        directory=checkpointing.directory,\n        add_uid=checkpointing.add_uid,\n        max_to_keep=checkpointing.max_to_keep,\n        keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n        checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n    )\n\n  # Replace the actor with a LearningActor. This makes sure that every time\n  # that `update` is called on the actor it checks to see whether there is\n  # any new data to learn from and if so it runs a learner step. The rate\n  # at which new data is released is controlled by the replay table's\n  # rate_limiter which is created by the builder.make_replay_tables call above.\n  actor = _LearningActor(actor, learner, dataset, replay_tables,\n                         rate_limiters_max_diff, checkpointer)\n\n  train_loop = acme.EnvironmentLoop(\n      environment,\n      actor,\n      counter=train_counter,\n      logger=train_logger,\n      observers=experiment.observers)\n\n  max_num_actor_steps = (\n      experiment.max_num_actor_steps -\n      parent_counter.get_counts().get(train_counter.get_steps_key(), 0))\n\n  if num_eval_episodes == 0:\n    # No evaluation. Just run the training loop.\n    train_loop.run(num_steps=max_num_actor_steps)\n    return\n\n  # Create the evaluation actor and loop.\n  eval_counter = counting.Counter(\n      parent_counter, prefix='evaluator', time_delta=0.)\n  eval_logger = experiment.logger_factory('evaluator',\n                                          eval_counter.get_steps_key(), 0)\n  eval_policy = config.make_policy(\n      experiment=experiment,\n      networks=networks,\n      environment_spec=environment_spec,\n      evaluation=True)\n  eval_actor = experiment.builder.make_actor(\n      random_key=jax.random.PRNGKey(experiment.seed),\n      policy=eval_policy,\n      environment_spec=environment_spec,\n      variable_source=learner)\n  eval_loop = acme.EnvironmentLoop(\n      environment,\n      eval_actor,\n      counter=eval_counter,\n      logger=eval_logger,\n      observers=experiment.observers)\n\n  steps = 0\n  while steps < max_num_actor_steps:\n    eval_loop.run(num_episodes=num_eval_episodes)\n    num_steps = min(eval_every, max_num_actor_steps - steps)\n    steps += train_loop.run(num_steps=num_steps)\n  eval_loop.run(num_episodes=num_eval_episodes)\n\n  environment.close()\n\n# --- Snippet Separator ---\n\ndef run_offline_experiment(experiment: config.OfflineExperimentConfig,\n                           eval_every: int = 100,\n                           num_eval_episodes: int = 1):\n  \"\"\"Runs a simple, single-threaded training loop using the default evaluators.\n\n  It targets simplicity of the code and so only the basic features of the\n  OfflineExperimentConfig are supported.\n\n  Arguments:\n    experiment: Definition and configuration of the agent to run.\n    eval_every: After how many learner steps to perform evaluation.\n    num_eval_episodes: How many evaluation episodes to execute at each\n      evaluation step.\n  \"\"\"\n\n  key = jax.random.PRNGKey(experiment.seed)\n\n  # Create the environment and get its spec.\n  environment = experiment.environment_factory(experiment.seed)\n  environment_spec = experiment.environment_spec or specs.make_environment_spec(\n      environment)\n\n  # Create the networks and policy.\n  networks = experiment.network_factory(environment_spec)\n\n  # Parent counter allows to share step counts between train and eval loops and\n  # the learner, so that it is possible to plot for example evaluator's return\n  # value as a function of the number of training episodes.\n  parent_counter = counting.Counter(time_delta=0.)\n\n  # Create the demonstrations dataset.\n  dataset_key, key = jax.random.split(key)\n  dataset = experiment.demonstration_dataset_factory(dataset_key)\n\n  # Create the learner.\n  learner_key, key = jax.random.split(key)\n  learner = experiment.builder.make_learner(\n      random_key=learner_key,\n      networks=networks,\n      dataset=dataset,\n      logger_fn=experiment.logger_factory,\n      environment_spec=environment_spec,\n      counter=counting.Counter(parent_counter, prefix='learner', time_delta=0.))\n\n  # Define the evaluation loop.\n  eval_loop = None\n  if num_eval_episodes > 0:\n    # Create the evaluation actor and loop.\n    eval_counter = counting.Counter(\n        parent_counter, prefix='evaluator', time_delta=0.)\n    eval_logger = experiment.logger_factory('evaluator',\n                                            eval_counter.get_steps_key(), 0)\n    eval_key, key = jax.random.split(key)\n    eval_actor = experiment.builder.make_actor(\n        random_key=eval_key,\n        policy=experiment.builder.make_policy(networks, environment_spec, True),\n        environment_spec=environment_spec,\n        variable_source=learner)\n    eval_loop = acme.EnvironmentLoop(\n        environment,\n        eval_actor,\n        counter=eval_counter,\n        logger=eval_logger,\n        observers=experiment.observers)\n\n  checkpointer = None\n  if experiment.checkpointing is not None:\n    checkpointing = experiment.checkpointing\n    checkpointer = savers.Checkpointer(\n        objects_to_save={'learner': learner, 'counter': parent_counter},\n        time_delta_minutes=checkpointing.time_delta_minutes,\n        directory=checkpointing.directory,\n        add_uid=checkpointing.add_uid,\n        max_to_keep=checkpointing.max_to_keep,\n        keep_checkpoint_every_n_hours=checkpointing.keep_checkpoint_every_n_hours,\n        checkpoint_ttl_seconds=checkpointing.checkpoint_ttl_seconds,\n    )\n\n  max_num_learner_steps = (\n      experiment.max_num_learner_steps -\n      parent_counter.get_counts().get('learner_steps', 0))\n\n  # Run the training loop.\n  if eval_loop:\n    eval_loop.run(num_eval_episodes)\n  steps = 0\n  while steps < max_num_learner_steps:\n    learner_steps = min(eval_every, max_num_learner_steps - steps)\n    for _ in range(learner_steps):\n      learner.step()\n      if checkpointer is not None:\n        checkpointer.save()\n    if eval_loop:\n      eval_loop.run(num_eval_episodes)\n    steps += learner_steps\n\n# --- Snippet Separator ---\n\nclass QlibRecorder:\n    \"\"\"\n    A global system that helps to manage the experiments.\n    \"\"\"\n\n    def __init__(self, exp_manager):\n        self.exp_manager = exp_manager\n\n    def __repr__(self):\n        return \"{name}(manager={manager})\".format(name=self.__class__.__name__, manager=self.exp_manager)\n\n    @contextmanager\n    def start(\n        self,\n        experiment_name: Optional[Text] = None,\n        recorder_name: Optional[Text] = None,\n        uri: Optional[Text] = None,\n        resume: bool = False,\n    ):\n        \"\"\"\n        Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:\n\n        .. code-block:: Python\n\n            # start new experiment and recorder\n            with R.start('test', 'recorder_1'):\n                model.fit(dataset)\n                R.log...\n                ... # further operations\n\n            # resume previous experiment and recorder\n            with R.start('test', 'recorder_1', resume=True): # if users want to resume recorder, they have to specify the exact same name for experiment and recorder.\n                ... # further operations\n\n        Parameters\n        ----------\n        experiment_name : str\n            name of the experiment one wants to start.\n        recorder_name : str\n            name of the recorder under the experiment one wants to start.\n        uri : str\n            The tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.\n            The default uri is set in the qlib.config. Note that this uri argument will not change the one defined in the config file.\n            Therefore, the next time when users call this function in the same experiment,\n            they have to also specify this argument with the same value. Otherwise, inconsistent uri may occur.\n        resume : bool\n            whether to resume the specific recorder with given name under the given experiment.\n        \"\"\"\n        run = self.start_exp(experiment_name, recorder_name, uri, resume)\n        try:\n            yield run\n        except Exception as e:\n            self.end_exp(Recorder.STATUS_FA)  # end the experiment if something went wrong\n            raise e\n        self.end_exp(Recorder.STATUS_FI)\n\n    def start_exp(self, experiment_name=None, recorder_name=None, uri=None, resume=False):\n        \"\"\"\n        Lower level method for starting an experiment. When use this method, one should end the experiment manually\n        and the status of the recorder may not be handled properly. Here is the example code:\n\n        .. code-block:: Python\n\n            R.start_exp(experiment_name='test', recorder_name='recorder_1')\n            ... # further operations\n            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)\n\n\n        Parameters\n        ----------\n        experiment_name : str\n            the name of the experiment to be started\n        recorder_name : str\n            name of the recorder under the experiment one wants to start.\n        uri : str\n            the tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.\n            The default uri are set in the qlib.config.\n        resume : bool\n            whether to resume the specific recorder with given name under the given experiment.\n\n        Returns\n        -------\n        An experiment instance being started.\n        \"\"\"\n        return self.exp_manager.start_exp(experiment_name, recorder_name, uri, resume)\n\n    def end_exp(self, recorder_status=Recorder.STATUS_FI):\n        \"\"\"\n        Method for ending an experiment manually. It will end the current active experiment, as well as its\n        active recorder with the specified `status` type. Here is the example code of the method:\n\n        .. code-block:: Python\n\n            R.start_exp(experiment_name='test')\n            ... # further operations\n            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)\n\n        Parameters\n        ----------\n        status : str\n            The status of a recorder, which can be SCHEDULED, RUNNING, FINISHED, FAILED.\n        \"\"\"\n        self.exp_manager.end_exp(recorder_status)\n\n    def search_records(self, experiment_ids, **kwargs):\n        \"\"\"\n        Get a pandas DataFrame of records that fit the search criteria.\n\n        The arguments of this function are not set to be rigid, and they will be different with different implementation of\n        ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the\n        example code of the this method with the ``MLflowExpManager``:\n\n        .. code-block:: Python\n\n            R.log_metrics(m=2.50, step=0)\n            records = R.search_runs([experiment_id], order_by=[\"metrics.m DESC\"])\n\n        Parameters\n        ----------\n        experiment_ids : list\n            list of experiment IDs.\n        filter_string : str\n            filter query string, defaults to searching all runs.\n        run_view_type : int\n            one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType).\n        max_results  : int\n            the maximum number of runs to put in the dataframe.\n        order_by : list\n            list of columns to order by (e.g., “metrics.rmse”).\n\n        Returns\n        -------\n        A pandas.DataFrame of records, where each metric, parameter, and tag\n        are expanded into their own columns named metrics.*, params.*, and tags.*\n        respectively. For records that don't have a particular metric, parameter, or tag, their\n        value will be (NumPy) Nan, None, or None respectively.\n        \"\"\"\n        return self.exp_manager.search_records(experiment_ids, **kwargs)\n\n    def list_experiments(self):\n        \"\"\"\n        Method for listing all the existing experiments (except for those being deleted.)\n\n        .. code-block:: Python\n\n            exps = R.list_experiments()\n\n        Returns\n        -------\n        A dictionary (name -> experiment) of experiments information that being stored.\n        \"\"\"\n        return self.exp_manager.list_experiments()\n\n    def list_recorders(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for listing all the recorders of experiment with given id or name.\n\n        If user doesn't provide the id or name of the experiment, this method will try to retrieve the default experiment and\n        list all the recorders of the default experiment. If the default experiment doesn't exist, the method will first\n        create the default experiment, and then create a new recorder under it. (More information about the default experiment\n        can be found `here <../component/recorder.html#qlib.workflow.exp.Experiment>`_).\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            recorders = R.list_recorders(experiment_name='test')\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n\n        Returns\n        -------\n        A dictionary (id -> recorder) of recorder information that being stored.\n        \"\"\"\n        return self.get_exp(experiment_id, experiment_name).list_recorders()\n\n    def get_exp(self, experiment_id=None, experiment_name=None, create: bool = True) -> Experiment:\n        \"\"\"\n        Method for retrieving an experiment with given id or name. Once the `create` argument is set to\n        True, if no valid experiment is found, this method will create one for you. Otherwise, it will\n        only retrieve a specific experiment or raise an Error.\n\n        - If '`create`' is True:\n\n            - If `active experiment` exists:\n\n                - no id or name specified, return the active experiment.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name, and the experiment is set to be active.\n\n            - If `active experiment` not exists:\n\n                - no id or name specified, create a default experiment, and the experiment is set to be active.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given name or the default experiment, and the experiment is set to be active.\n\n        - Else If '`create`' is False:\n\n            - If ``active experiment` exists:\n\n                - no id or name specified, return the active experiment.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n            - If `active experiment` not exists:\n\n                - no id or name specified. If the default experiment exists, return it, otherwise, raise Error.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                exp = R.get_exp()\n                recorders = exp.list_recorders()\n\n            # Case 2\n            with R.start('test'):\n                exp = R.get_exp('test1')\n\n            # Case 3\n            exp = R.get_exp() -> a default experiment.\n\n            # Case 4\n            exp = R.get_exp(experiment_name='test')\n\n            # Case 5\n            exp = R.get_exp(create=False) -> the default experiment if exists.\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n        create : boolean\n            an argument determines whether the method will automatically create a new experiment\n            according to user's specification if the experiment hasn't been created before.\n\n        Returns\n        -------\n        An experiment instance with given id or name.\n        \"\"\"\n        return self.exp_manager.get_exp(experiment_id, experiment_name, create)\n\n    def delete_exp(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for deleting the experiment with given id or name. At least one of id or name must be given,\n        otherwise, error will occur.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            R.delete_exp(experiment_name='test')\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n        \"\"\"\n        self.exp_manager.delete_exp(experiment_id, experiment_name)\n\n    def get_uri(self):\n        \"\"\"\n        Method for retrieving the uri of current experiment manager.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            uri = R.get_uri()\n\n        Returns\n        -------\n        The uri of current experiment manager.\n        \"\"\"\n        return self.exp_manager.uri\n\n    def set_uri(self, uri: Optional[Text]):\n        \"\"\"\n        Method to reset the current uri of current experiment manager.\n        \"\"\"\n        self.exp_manager.set_uri(uri)\n\n    def get_recorder(self, recorder_id=None, recorder_name=None, experiment_name=None):\n        \"\"\"\n        Method for retrieving a recorder.\n\n        - If `active recorder` exists:\n\n            - no id or name specified, return the active recorder.\n\n            - if id or name is specified, return the specified recorder.\n\n        - If `active recorder` not exists:\n\n            - no id or name specified, raise Error.\n\n            - if id or name is specified, and the corresponding experiment_name must be given, return the specified recorder. Otherwise, raise Error.\n\n        The recorder can be used for further process such as `save_object`, `load_object`, `log_params`,\n        `log_metrics`, etc.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                recorder = R.get_recorder()\n\n            # Case 2\n            with R.start('test'):\n                recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')\n\n            # Case 3\n            recorder = R.get_recorder() -> Error\n\n            # Case 4\n            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d') -> Error\n\n            # Case 5\n            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d', experiment_name='test')\n\n        Parameters\n        ----------\n        recorder_id : str\n            id of the recorder.\n        recorder_name : str\n            name of the recorder.\n        experiment_name : str\n            name of the experiment.\n\n        Returns\n        -------\n        A recorder instance.\n        \"\"\"\n        return self.get_exp(experiment_name=experiment_name, create=False).get_recorder(\n            recorder_id, recorder_name, create=False\n        )\n\n    def delete_recorder(self, recorder_id=None, recorder_name=None):\n        \"\"\"\n        Method for deleting the recorders with given id or name. At least one of id or name must be given,\n        otherwise, error will occur.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            R.delete_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')\n\n        Parameters\n        ----------\n        recorder_id : str\n            id of the experiment.\n        recorder_name : str\n            name of the experiment.\n        \"\"\"\n        self.get_exp().delete_recorder(recorder_id, recorder_name)\n\n    def save_objects(self, local_path=None, artifact_path=None, **kwargs):\n        \"\"\"\n        Method for saving objects as artifacts in the experiment to the uri. It supports either saving\n        from a local file/directory, or directly saving objects. User can use valid python's keywords arguments\n        to specify the object to be saved as well as its name (name: value).\n\n        - If `active recorder` exists: it will save the objects through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment, and a new recorder and save objects under it.\n\n        .. note::\n\n            If one wants to save objects with a specific recorder. It is recommended to first get the specific recorder through `get_recorder` API and use the recorder the save objects. The supported arguments are the same as this method.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                pred = model.predict(dataset)\n                R.save_objects(**{\"pred.pkl\": pred}, artifact_path='prediction')\n\n            # Case 2\n            with R.start('test'):\n                R.save_objects(local_path='results/pred.pkl')\n\n        Parameters\n        ----------\n        local_path : str\n            if provided, them save the file or directory to the artifact URI.\n        artifact_path : str\n            the relative path for the artifact to be stored in the URI.\n        \"\"\"\n        self.get_exp().get_recorder().save_objects(local_path, artifact_path, **kwargs)\n\n    def log_params(self, **kwargs):\n        \"\"\"\n        Method for logging parameters during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will log parameters through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log parameters under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.log_params(learning_rate=0.01)\n\n            # Case 2\n            R.log_params(learning_rate=0.01)\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().log_params(**kwargs)\n\n    def log_metrics(self, step=None, **kwargs):\n        \"\"\"\n        Method for logging metrics during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will log metrics through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log metrics under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.log_metrics(train_loss=0.33, step=1)\n\n            # Case 2\n            R.log_metrics(train_loss=0.33, step=1)\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().log_metrics(step, **kwargs)\n\n    def set_tags(self, **kwargs):\n        \"\"\"\n        Method for setting tags for a recorder. In addition to using ``R``, one can also set the tag to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will set tags through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.set_tags(release_version=\"2.2.0\")\n\n            # Case 2\n            R.set_tags(release_version=\"2.2.0\")\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().set_tags(**kwargs)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up and runs a behavioral cloning (BC) experiment on a specified environment using the Acme library. The code should define several flags for configuring the experiment, such as the environment name, number of demonstrations, learning steps, batch size, learning rate, dropout rate, and network parameters. It should also include functions to create a demonstration dataset factory, an environment factory, and a network factory. The network should be a multi-layer perceptron (MLP) with ReLU activation and dropout. The code should also include a function to build the experiment configuration, which uses the previously defined factories and the BC builder. Finally, the code should include a main function that builds the experiment configuration and runs the experiment, either in a distributed or single-threaded manner.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 51, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that performs a CCSD (Coupled Cluster with Single and Double excitations) calculation with k-point sampling for a system of two carbon atoms in a cell using the pyscf library. The code should first build the cell with the given atomic coordinates, basis, pseudopotential, lattice vectors, and unit. Then, it should perform KHF and KCCSD calculations with 2x2x2 k-points and print the total energy per unit cell. Next, it should perform KHF and KCCSD calculations for a single k-point and print the total energy per unit cell. \n\nAfterwards, the code should perform a single k-point calculation using the RHF method, run RCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the RCCSD energy based on CCSD density matrices. \n\nNext, the code should convert the RHF object to a UHF object, run UCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the UCCSD energy based on CCSD density matrices. \n\nFinally, the code should convert the UHF object to a GHF object, run GCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the GCCSD energy based on CCSD density matrices.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def add_mm_charges(scf_method, atoms_or_coords, charges, unit=None):\n    '''Embedding the one-electron (non-relativistic) potential generated by MM\n    point charges into QM Hamiltonian.\n\n    The total energy includes the regular QM energy, the interaction between\n    the nuclei in QM region and the MM charges, and the static Coulomb\n    interaction between the electron density and the MM charges. It does not\n    include the static Coulomb interactions of the MM point charges, the MM\n    energy, the vdw interaction or other bonding/non-bonding effects between\n    QM region and MM particles.\n\n    Args:\n        scf_method : a HF or DFT object\n\n        atoms_or_coords : 2D array, shape (N,3)\n            MM particle coordinates\n        charges : 1D array\n            MM particle charges\n    Kwargs:\n        unit : str\n            Bohr, AU, Ang (case insensitive). Default is the same to mol.unit\n\n    Returns:\n        Same method object as the input scf_method with modified 1e Hamiltonian\n\n    Note:\n        1. if MM charge and X2C correction are used together, function mm_charge\n        needs to be applied after X2C decoration (.x2c method), eg\n        mf = mm_charge(scf.RHF(mol).x2c()), [(0.5,0.6,0.8)], [-0.5]).\n        2. Once mm_charge function is applied on the SCF object, it\n        affects all the post-HF calculations eg MP2, CCSD, MCSCF etc\n\n    Examples:\n\n    >>> mol = gto.M(atom='H 0 0 0; F 0 0 1', basis='ccpvdz', verbose=0)\n    >>> mf = mm_charge(dft.RKS(mol), [(0.5,0.6,0.8)], [-0.3])\n    >>> mf.kernel()\n    -101.940495711284\n    '''\n    mol = scf_method.mol\n    if unit is None:\n        unit = mol.unit\n    mm_mol = mm_mole.create_mm_mol(atoms_or_coords, charges, unit)\n    return qmmm_for_scf(scf_method, mm_mol)\n\n# --- Snippet Separator ---\n\ndef dip_moment(cell, dm_kpts, unit='Debye', verbose=logger.NOTE,\n               grids=None, rho=None, kpts=np.zeros((1,3))):\n    ''' Dipole moment in the unit cell.\n\n    Args:\n         cell : an instance of :class:`Cell`\n\n         dm_kpts (two lists of ndarrays) : KUHF density matrices of k-points\n\n    Return:\n        A list: the dipole moment on x, y and z components\n    '''\n    dm_kpts = dm_kpts[0] + dm_kpts[1]\n    return khf.dip_moment(cell, dm_kpts, unit, verbose, grids, rho, kpts)\n\n# --- Snippet Separator ---\n\ndef point_data_to_cell_data(self, pass_point_data=False, progress_bar=False):\n        \"\"\"Transform point data into cell data.\n\n        Point data are specified per node and cell data specified within cells.\n        Optionally, the input point data can be passed through to the output.\n\n        See also: :func:`pyvista.DataSetFilters.cell_data_to_point_data`\n\n        Parameters\n        ----------\n        pass_point_data : bool, default: False\n            If enabled, pass the input point data through to the output.\n\n        progress_bar : bool, default: False\n            Display a progress bar to indicate progress.\n\n        Returns\n        -------\n        pyvista.DataSet\n            Dataset with the point data transformed into cell data.\n            Return type matches input.\n\n        Examples\n        --------\n        Color cells by their z coordinates.  First, create point\n        scalars based on z-coordinates of a sample sphere mesh.  Then\n        convert this point data to cell data.  Use a low resolution\n        sphere for emphasis of cell valued data.\n\n        First, plot these values as point values to show the\n        difference between point and cell data.\n\n        >>> import pyvista\n        >>> sphere = pyvista.Sphere(theta_resolution=10, phi_resolution=10)\n        >>> sphere['Z Coordinates'] = sphere.points[:, 2]\n        >>> sphere.plot()\n\n        Now, convert these values to cell data and then plot it.\n\n        >>> import pyvista\n        >>> sphere = pyvista.Sphere(theta_resolution=10, phi_resolution=10)\n        >>> sphere['Z Coordinates'] = sphere.points[:, 2]\n        >>> sphere = sphere.point_data_to_cell_data()\n        >>> sphere.plot()\n\n        \"\"\"\n        alg = _vtk.vtkPointDataToCellData()\n        alg.SetInputDataObject(self)\n        alg.SetPassPointData(pass_point_data)\n        _update_alg(alg, progress_bar, 'Transforming point data into cell data')\n        active_scalars = None\n        if not isinstance(self, pyvista.MultiBlock):\n            active_scalars = self.active_scalars_name\n        return _get_output(alg, active_scalars=active_scalars)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a CCSD (Coupled Cluster with Single and Double excitations) calculation with k-point sampling for a system of two carbon atoms in a cell using the pyscf library. The code should first build the cell with the given atomic coordinates, basis, pseudopotential, lattice vectors, and unit. Then, it should perform KHF and KCCSD calculations with 2x2x2 k-points and print the total energy per unit cell. Next, it should perform KHF and KCCSD calculations for a single k-point and print the total energy per unit cell. \n\nAfterwards, the code should perform a single k-point calculation using the RHF method, run RCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the RCCSD energy based on CCSD density matrices. \n\nNext, the code should convert the RHF object to a UHF object, run UCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the UCCSD energy based on CCSD density matrices. \n\nFinally, the code should convert the UHF object to a GHF object, run GCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the GCCSD energy based on CCSD density matrices.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 52, "repo_full_name": "vispy__vispy", "instruction": "Generate code that simulates fireworks using the vispy library in Python. The simulation should create a series of explosions that last one second, with each explosion being unique. The visualization during the explosion should be highly optimized using a Vertex Buffer Object (VBO). The code should include a class named 'Canvas' that inherits from 'app.Canvas' and includes methods for initializing the simulation, drawing the simulation, resizing the simulation, timing the simulation, and creating a new explosion. The code should also include vertex and fragment shaders written in GLSL. The simulation should be interactive and the window size should be 800x600 pixels. The code should run the simulation when executed.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class SimulationParameters():\n    \"\"\"\n    This class represents the run parameters for a simulation, with information including\n\n     - a function that creates the simulation\n     - a function that executes the simulation\n     - the dispersions to use on that simulation\n     - parameters describing the data to be retained for a simulation\n     - whether randomized seeds should be applied to the simulation\n     - whether data should be archived\n    \"\"\"\n\n    def __init__(self, creationFunction, executionFunction, configureFunction,\n                 retentionPolicies, dispersions, shouldDisperseSeeds,\n                 shouldArchiveParameters, filename, icfilename, index=None, verbose=False, modifications={},\n                 showProgressBar=False):\n        self.index = index\n        self.creationFunction = creationFunction\n        self.executionFunction = executionFunction\n        self.configureFunction = configureFunction\n        self.retentionPolicies = retentionPolicies\n        self.dispersions = dispersions\n        self.shouldDisperseSeeds = shouldDisperseSeeds\n        self.shouldArchiveParameters = shouldArchiveParameters\n        self.filename = filename\n        self.icfilename = icfilename\n        self.verbose = verbose\n        self.modifications = modifications\n        self.dispersionMag = {}\n        self.saveDispMag = False\n        self.showProgressBar = showProgressBar\n\n# --- Snippet Separator ---\n\ndef simulate_physics_and_fix_final_poses(min_simulation_time: float = 4.0, max_simulation_time: float = 40.0,\n                                         check_object_interval: float = 2.0,\n                                         object_stopped_location_threshold: float = 0.01,\n                                         object_stopped_rotation_threshold: float = 0.1, substeps_per_frame: int = 10,\n                                         solver_iters: int = 10, verbose: bool = False):\n    \"\"\" Simulates the current scene and in the end fixes the final poses of all active objects.\n\n    The simulation is run for at least `min_simulation_time` seconds and at a maximum `max_simulation_time` seconds.\n    Every `check_object_interval` seconds, it is checked if the maximum object movement in the last second is below a\n    given threshold. If that is the case, the simulation is stopped.\n\n    After performing the simulation, the simulation cache is removed, the rigid body components are disabled and the\n    pose of the active objects is set to their final pose in the simulation.\n\n    :param min_simulation_time: The minimum number of seconds to simulate.\n    :param max_simulation_time: The maximum number of seconds to simulate.\n    :param check_object_interval: The interval in seconds at which all objects should be checked if they are still\n                                  moving. If all objects have stopped moving, then the simulation will be stopped.\n    :param object_stopped_location_threshold: The maximum difference per second and per coordinate in the rotation\n                                              Euler vector that is allowed such that an object is still recognized\n                                              as 'stopped moving'.\n    :param object_stopped_rotation_threshold: The maximum difference per second and per coordinate in the rotation\n                                              Euler vector that is allowed such that an object is still recognized\n                                              as 'stopped moving'.\n    :param substeps_per_frame: Number of simulation steps taken per frame.\n    :param solver_iters: Number of constraint solver iterations made per simulation step.\n    :param verbose: If True, more details during the physics simulation are printed.\n    \"\"\"\n    # Undo changes made in the simulation like origin adjustment and persisting the object's scale\n    with UndoAfterExecution():\n        # Run simulation and remember poses before and after\n        obj_poses_before_sim = _PhysicsSimulation.get_pose()\n        origin_shifts = simulate_physics(min_simulation_time, max_simulation_time, check_object_interval,\n                                         object_stopped_location_threshold, object_stopped_rotation_threshold,\n                                         substeps_per_frame, solver_iters, verbose)\n        obj_poses_after_sim = _PhysicsSimulation.get_pose()\n\n        # Make sure to remove the simulation cache as we are only interested in the final poses\n        bpy.ops.ptcache.free_bake({\"point_cache\": bpy.context.scene.rigidbody_world.point_cache})\n\n    # Fix the pose of all objects to their pose at the end of the simulation (also revert origin shift)\n    for obj in get_all_mesh_objects():\n        if obj.has_rigidbody_enabled():\n            # Skip objects that have parents with compound rigid body component\n            has_compound_parent = obj.get_parent() is not None and isinstance(obj.get_parent(), MeshObject) \\\n                                  and obj.get_parent().get_rigidbody() is not None \\\n                                  and obj.get_parent().get_rigidbody().collision_shape == \"COMPOUND\"\n            if obj.get_rigidbody().type == \"ACTIVE\" and not has_compound_parent:\n                # compute relative object rotation before and after simulation\n                R_obj_before_sim = mathutils.Euler(obj_poses_before_sim[obj.get_name()]['rotation']).to_matrix()\n                R_obj_after = mathutils.Euler(obj_poses_after_sim[obj.get_name()]['rotation']).to_matrix()\n                R_obj_rel = R_obj_before_sim @ R_obj_after.transposed()\n                # Apply relative rotation to origin shift\n                origin_shift = R_obj_rel.transposed() @ mathutils.Vector(origin_shifts[obj.get_name()])\n\n                # Fix pose of object to the one it had at the end of the simulation\n                obj.set_location(obj_poses_after_sim[obj.get_name()]['location'] - origin_shift)\n                obj.set_rotation_euler(obj_poses_after_sim[obj.get_name()]['rotation'])\n\n    # Deactivate the simulation so it does not influence object positions\n    bpy.context.scene.rigidbody_world.enabled = False\n    bpy.context.view_layer.update()\n\n# --- Snippet Separator ---\n\ndef simulate_physics(min_simulation_time: float = 4.0, max_simulation_time: float = 40.0,\n                     check_object_interval: float = 2.0, object_stopped_location_threshold: float = 0.01,\n                     object_stopped_rotation_threshold: float = 0.1, substeps_per_frame: int = 10,\n                     solver_iters: int = 10, verbose: bool = False) -> dict:\n    \"\"\" Simulates the current scene.\n\n    The simulation is run for at least `min_simulation_time` seconds and at a maximum `max_simulation_time` seconds.\n    Every `check_object_interval` seconds, it is checked if the maximum object movement in the last second is below\n    a given threshold. If that is the case, the simulation is stopped.\n\n    The origin of all objects is set to their center of mass in this function which is necessary to achieve a realistic\n    simulation in blender (see https://blender.stackexchange.com/questions/167488/physics-not-working-as-expected)\n    Also the scale of each participating object is persisted as scale != 1 can make the simulation unstable.\n\n    :param min_simulation_time: The minimum number of seconds to simulate.\n    :param max_simulation_time: The maximum number of seconds to simulate.\n    :param check_object_interval: The interval in seconds at which all objects should be checked if they are still\n                                  moving. If all objects have stopped moving, then the simulation will be stopped.\n    :param object_stopped_location_threshold: The maximum difference per second and per coordinate in the rotation\n                                              Euler vector that is allowed such that an object is still recognized\n                                              as 'stopped moving'.\n    :param object_stopped_rotation_threshold: The maximum difference per second and per coordinate in the rotation\n                                              Euler vector that is allowed such that an object is still recognized\n                                              as 'stopped moving'.\n    :param substeps_per_frame: Number of simulation steps taken per frame.\n    :param solver_iters: Number of constraint solver iterations made per simulation step.\n    :param verbose: If True, more details during the physics simulation are printed.\n    :return: A dict containing for every active object the shift that was added to their origins.\n    \"\"\"\n    # Shift the origin of all objects to their center of mass to make the simulation more realistic\n    origin_shift = {}\n    for obj in get_all_mesh_objects():\n        if obj.has_rigidbody_enabled():\n            prev_origin = obj.get_origin()\n            new_origin = obj.set_origin(mode=\"CENTER_OF_VOLUME\")\n            origin_shift[obj.get_name()] = new_origin - prev_origin\n\n            # Persist mesh scaling as having a scale != 1 can make the simulation unstable\n            obj.persist_transformation_into_mesh(location=False, rotation=False, scale=True)\n\n    # Configure simulator\n    bpy.context.scene.rigidbody_world.substeps_per_frame = substeps_per_frame\n    bpy.context.scene.rigidbody_world.solver_iterations = solver_iters\n\n    # Perform simulation\n    _PhysicsSimulation.do_simulation(min_simulation_time, max_simulation_time, check_object_interval,\n                                     object_stopped_location_threshold, object_stopped_rotation_threshold,\n                                     verbose)\n\n    return origin_shift\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that simulates fireworks using the vispy library in Python. The simulation should create a series of explosions that last one second, with each explosion being unique. The visualization during the explosion should be highly optimized using a Vertex Buffer Object (VBO). The code should include a class named 'Canvas' that inherits from 'app.Canvas' and includes methods for initializing the simulation, drawing the simulation, resizing the simulation, timing the simulation, and creating a new explosion. The code should also include vertex and fragment shaders written in GLSL. The simulation should be interactive and the window size should be 800x600 pixels. The code should run the simulation when executed.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 53, "repo_full_name": "imsy-dkfz__simpa", "instruction": "Generate code that uses the simpa library to create a simulation of a tissue structure. The tissue structure should include a background, a muscle layer, an epidermis layer, and two blood vessels. The simulation should be set up with specific global parameters such as volume dimensions, spacing, and wavelengths. The code should also include a function to create the tissue structure with specific properties for each component. The simulation should be run for all wavelengths specified and include a linear unmixing component. The results of the simulation and the linear unmixing should be loaded and visualized.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def run(self):\n\n        self.logger.info(\"Performing linear spectral unmixing...\")\n\n        if Tags.WAVELENGTHS in self.component_settings:\n            self.wavelengths = self.component_settings[Tags.WAVELENGTHS]\n        else:\n            if Tags.WAVELENGTHS in self.global_settings:\n                self.wavelengths = self.global_settings[Tags.WAVELENGTHS]\n            else:\n                msg = \"Was not able to get wavelengths from component_settings or global_settings.\"\n                self.logger.critical(msg)\n                raise AssertionError(msg)\n\n        if len(self.wavelengths) < 2:\n            msg = \"Linear unmixing should be performed with at least two wavelengths!\"\n            self.logger.critical(msg)\n            raise AssertionError(msg)\n\n        # Build internal list of spectra based on Tags.LINEAR_UNMIXING_SPECTRA\n        self.build_chromophore_spectra_dict()\n\n        # check if absorption dictionary contains any spectra\n        if self.chromophore_spectra_dict == {}:\n            raise KeyError(\"Linear unmixing must be performed for at least one chromophore. \"\n                           \"Please specify at least one chromophore in the component settings by setting \"\n                           \"the corresponding tag.\")\n\n        # check if non-negative contraint should be used for linear unmixing\n        non_negative = False\n        if Tags.LINEAR_UNMIXING_NON_NEGATIVE in self.component_settings:\n            non_negative = Tags.LINEAR_UNMIXING_NON_NEGATIVE\n\n        # create the absorption matrix needed by FLUPAI\n        # the matrix should have the shape [#global wavelengths, #chromophores]\n        self.absorption_matrix = self.create_absorption_matrix()\n        self.logger.debug(f\"The absorption matrix has shape {np.shape(self.absorption_matrix)}.\")\n\n        # perform fast linear unmixing FLUPAI\n        # the result saved in self.chromophore_concentrations is a list with the unmixed images\n        # containing the chromophore concentration\n        self.chromophore_concentrations = self.flupai(non_negative=non_negative)\n        self.logger.debug(f\"The unmixing result has shape {np.shape(self.chromophore_concentrations)}.\")\n\n        # split results to create dictionary which contains linear unmixing result for each chromophore\n        for index, chromophore in enumerate(self.chromophore_spectra_dict.keys()):\n            self.chromophore_concentrations_dict[chromophore] = self.chromophore_concentrations[index]\n        self.logger.info(f\"The chromophore concentration was computed for chromophores: \"\n                         f\"{self.chromophore_concentrations_dict.keys()}\")\n\n        # compute blood oxygen saturation if selected\n        save_dict = {\n            \"chromophore_concentrations\": self.chromophore_concentrations_dict,\n            \"wavelengths\": self.wavelengths\n        }\n        if Tags.LINEAR_UNMIXING_COMPUTE_SO2 in self.component_settings:\n            if self.component_settings[Tags.LINEAR_UNMIXING_COMPUTE_SO2]:\n                self.logger.info(\"Blood oxygen saturation is calculated and saved.\")\n                save_dict[\"sO2\"] = self.calculate_sO2()\n\n        # save linear unmixing result in hdf5\n        save_data_field(save_dict, self.global_settings[Tags.SIMPA_OUTPUT_PATH],\n                        Tags.LINEAR_UNMIXING_RESULT, wavelength=None)\n\n        self.logger.info(\"Performing linear spectral unmixing......[Done]\")\n\n# --- Snippet Separator ---\n\nclass LinearUnmixing(MultispectralProcessingAlgorithm):\n    \"\"\"\n    Performs linear spectral unmixing (LU) using Fast Linear Unmixing for PhotoAcoustic Imaging (FLUPAI)\n    on the defined data field for each chromophore specified in the component settings.\n\n    If tag LINEAR_UNMIXING_NON_NEGATIVE is set to True non-negative linear unmixing is performed, which solves the\n    KKT (Karush-Kuhn-Tucker) conditions for the non-negative least squares problem.\n\n    This component saves a dictionary containing the chromophore concentrations and corresponding wavelengths for\n    each chromophore. If the tag LINEAR_UNMIXING_COMPUTE_SO2 is set True the blood oxygen saturation\n    is saved as well, however, this is only possible if the chromophores oxy- and deoxyhemoglobin are specified.\n    IMPORTANT:\n    Linear unmixing should only be performed with at least two wavelengths:\n    e.g. Tags.WAVELENGTHS: [750, 800]\n\n    Parameters:\n    Tags.DATA_FIELD (required)\n    Tags.LINEAR_UNMIXING_SPECTRA (required)\n    Tags.WAVELENGTHS (default: None, if None, then settings[Tags.WAVELENGTHS] will be used.)\n    Tags.LINEAR_UNMIXING_COMPUTE_SO2 (default: False)\n    Tags.LINEAR_UNMIXING_NON_NEGATIVE (default: False)\n    global_settings (required)\n    component_settings_key (required)\n    \"\"\"\n\n    def __init__(self, global_settings, component_settings_key: str):\n        super(LinearUnmixing, self).__init__(global_settings=global_settings,\n                                             component_settings_key=component_settings_key)\n\n        self.chromophore_spectra_dict = {}  # dictionary containing the spectrum for each chromophore and wavelength\n        self.absorption_matrix = []  # endmember matrix needed in LU\n        self.pseudo_inverse_absorption_matrix = []\n\n        self.chromophore_concentrations = []  # list of LU results\n        self.chromophore_concentrations_dict = {}  # dictionary of LU results\n        self.wavelengths = []  # list of wavelengths\n\n    def run(self):\n\n        self.logger.info(\"Performing linear spectral unmixing...\")\n\n        if Tags.WAVELENGTHS in self.component_settings:\n            self.wavelengths = self.component_settings[Tags.WAVELENGTHS]\n        else:\n            if Tags.WAVELENGTHS in self.global_settings:\n                self.wavelengths = self.global_settings[Tags.WAVELENGTHS]\n            else:\n                msg = \"Was not able to get wavelengths from component_settings or global_settings.\"\n                self.logger.critical(msg)\n                raise AssertionError(msg)\n\n        if len(self.wavelengths) < 2:\n            msg = \"Linear unmixing should be performed with at least two wavelengths!\"\n            self.logger.critical(msg)\n            raise AssertionError(msg)\n\n        # Build internal list of spectra based on Tags.LINEAR_UNMIXING_SPECTRA\n        self.build_chromophore_spectra_dict()\n\n        # check if absorption dictionary contains any spectra\n        if self.chromophore_spectra_dict == {}:\n            raise KeyError(\"Linear unmixing must be performed for at least one chromophore. \"\n                           \"Please specify at least one chromophore in the component settings by setting \"\n                           \"the corresponding tag.\")\n\n        # check if non-negative contraint should be used for linear unmixing\n        non_negative = False\n        if Tags.LINEAR_UNMIXING_NON_NEGATIVE in self.component_settings:\n            non_negative = Tags.LINEAR_UNMIXING_NON_NEGATIVE\n\n        # create the absorption matrix needed by FLUPAI\n        # the matrix should have the shape [#global wavelengths, #chromophores]\n        self.absorption_matrix = self.create_absorption_matrix()\n        self.logger.debug(f\"The absorption matrix has shape {np.shape(self.absorption_matrix)}.\")\n\n        # perform fast linear unmixing FLUPAI\n        # the result saved in self.chromophore_concentrations is a list with the unmixed images\n        # containing the chromophore concentration\n        self.chromophore_concentrations = self.flupai(non_negative=non_negative)\n        self.logger.debug(f\"The unmixing result has shape {np.shape(self.chromophore_concentrations)}.\")\n\n        # split results to create dictionary which contains linear unmixing result for each chromophore\n        for index, chromophore in enumerate(self.chromophore_spectra_dict.keys()):\n            self.chromophore_concentrations_dict[chromophore] = self.chromophore_concentrations[index]\n        self.logger.info(f\"The chromophore concentration was computed for chromophores: \"\n                         f\"{self.chromophore_concentrations_dict.keys()}\")\n\n        # compute blood oxygen saturation if selected\n        save_dict = {\n            \"chromophore_concentrations\": self.chromophore_concentrations_dict,\n            \"wavelengths\": self.wavelengths\n        }\n        if Tags.LINEAR_UNMIXING_COMPUTE_SO2 in self.component_settings:\n            if self.component_settings[Tags.LINEAR_UNMIXING_COMPUTE_SO2]:\n                self.logger.info(\"Blood oxygen saturation is calculated and saved.\")\n                save_dict[\"sO2\"] = self.calculate_sO2()\n\n        # save linear unmixing result in hdf5\n        save_data_field(save_dict, self.global_settings[Tags.SIMPA_OUTPUT_PATH],\n                        Tags.LINEAR_UNMIXING_RESULT, wavelength=None)\n\n        self.logger.info(\"Performing linear spectral unmixing......[Done]\")\n\n    def build_chromophore_spectra_dict(self):\n        \"\"\"\n        This function builds the absorption spectra dictionary for each chromophore using SIMPAs spectral library\n        and saves the result in self.chromophore_spectra_dict.\n        This function might have to change drastically if the design of the spectral library changes in the future!\n        \"\"\"\n\n        if Tags.LINEAR_UNMIXING_SPECTRA in self.component_settings:\n            spectra = self.component_settings[Tags.LINEAR_UNMIXING_SPECTRA]\n            if len(spectra) < 2:\n                raise AssertionError(f\"Need at least two endmembers for unmixing! You provided {len(spectra)}.\")\n            for spectrum in spectra:\n                self.create_chromophore_spectra_entry(spectrum)\n        else:\n            raise AssertionError(\"Tried to unmix without spectra definitions. Make sure that the\"\n                                 \" Tags.LINEAR_UNMIXING_SPECTRA tag is set in the linear unmixing settings.\")\n\n    def create_chromophore_spectra_entry(self, spectrum: Spectrum):\n        \"\"\"\n        This function builds the spectra for a chromophore specified by tag and name and saves it in\n        self.chromophore_spectra_dict and creates a dictionary containing the corresponding wavelengths.\n        The name must match the ones used in the spectral library of SIMPA.\n        \"\"\"\n\n        self.chromophore_spectra_dict[spectrum.spectrum_name] = [spectrum.get_value_for_wavelength(wavelength)\n                                                                 for wavelength in self.wavelengths]\n\n    def create_absorption_matrix(self) -> np.ndarray:\n        \"\"\"\n        Method that returns the absorption (endmember) matrix needed for linear unmixing.\n\n        :return: absorption matrix\n        \"\"\"\n\n        numberWavelengths = len(self.wavelengths)\n        numberChromophores = len(self.chromophore_spectra_dict.keys())\n\n        # prepare matrix\n        endmemberMatrix = np.zeros((numberWavelengths, numberChromophores))\n\n        # write absorption data for each chromophore and the corresponding wavelength into an array (matrix)\n        for index, key in enumerate(self.chromophore_spectra_dict.keys()):\n            for wave in range(numberWavelengths):\n                endmemberMatrix[wave][index] = self.chromophore_spectra_dict[key][wave]\n\n        return endmemberMatrix\n\n    def flupai(self, non_negative=False) -> list:\n        \"\"\"\n        Fast Linear Unmixing for PhotoAcoustic Imaging (FLUPAI) is based on\n        SVD decomposition with a pseudo inverse, which is equivalent to a least squares\n        ansatz for linear spectral unmixing of multi-spectral photoacoustic images.\n\n        :return: list with unmixed images containing the chromophore concentration.\n        :raise: SystemExit.\n        \"\"\"\n\n        # reshape image data to [number of wavelength, number of pixel]\n        dims_raw = np.shape(self.data)\n        try:\n            reshapedData = np.reshape(self.data, (dims_raw[0], -1))\n        except Exception:\n            self.logger.critical(f\"FLUPAI failed probably caused by wrong input dimensions of {dims_raw}!\")\n            raise ValueError(\"Reshaping of input data failed. FLUPAI expects a 4 dimensional numpy array, \"\n                             \"where the first dimension represents the wavelengths and the second, third and fourth \"\n                             \"dimension are representing a single wavelength PA image.\")\n\n        # if non_negative is False, matmul of x = PI * b with x chromophore information,\n        # PI pseudo inverse with absorber information and b containing the measured pixel,\n        # else non-negative least squares is performed.\n        try:\n            if non_negative:\n                output = []\n                for i in range(np.shape(reshapedData)[1]):\n                    foo, ris = nnls(np.array(self.absorption_matrix), reshapedData[:, i])\n                    output.append(foo)\n\n                output = np.swapaxes(output, axis1=0, axis2=1)\n            else:\n                self.pseudo_inverse_absorption_matrix = linalg.pinv(self.absorption_matrix)\n                output = np.matmul(self.pseudo_inverse_absorption_matrix, reshapedData)\n\n        except Exception as e:\n            self.logger.critical(f\"Matrix multiplication failed probably caused by mismatching dimensions of absorption\"\n                                 f\"matrix ({len(self.absorption_matrix[1])}) and \"\n                                 f\"input data ({dims_raw[0]})!\")\n            print(e)\n            raise ValueError(\"Absorption matrix and input data must have matching sizes...\")\n\n    # write output into list of images containing the chromophore information\n        numberChromophores = np.shape(output)[0]\n        chromophores_concentrations = []\n        for chromophore in range(numberChromophores):\n            chromophores_concentrations.append(np.reshape(output[chromophore, :], (dims_raw[1:])))\n        return chromophores_concentrations\n\n    def calculate_sO2(self) -> np.ndarray:\n        \"\"\"\n        Function calculates sO2 (blood oxygen saturation) values for given concentrations\n        of oxyhemoglobin and deoxyhemoglobin. Of course this is only possible if the concentrations of both\n        chromophores were calculated by this component/were specified in settings.\n        \"\"\"\n\n        try:\n            concentration_oxy = self.chromophore_concentrations_dict[\"Oxyhemoglobin\"]\n            concentration_deoxy = self.chromophore_concentrations_dict[\"Deoxyhemoglobin\"]\n\n            sO2 = concentration_oxy / (concentration_oxy + concentration_deoxy)\n            # if total hemoglobin is zero handle NaN by setting sO2 to zero\n            where_are_NaNs = np.isnan(sO2)\n            sO2[where_are_NaNs] = 0\n            return sO2\n\n        except Exception:\n            raise KeyError(\"Chromophores oxy- and/or deoxyhemoglobin were not specified in component settings, \"\n                           \"so so2 cannot be calculated!\")\n\n# --- Snippet Separator ---\n\nclass SegmentationClasses:\n    \"\"\"\n    The segmentation classes define which \"tissue types\" are modelled in the simulation volumes.\n    \"\"\"\n    GENERIC = -1\n    AIR = 0\n    MUSCLE = 1\n    BONE = 2\n    BLOOD = 3\n    EPIDERMIS = 4\n    DERMIS = 5\n    FAT = 6\n    ULTRASOUND_GEL = 7\n    WATER = 8\n    HEAVY_WATER = 9\n    COUPLING_ARTIFACT = 10\n    MEDIPRENE = 11\n    SOFT_TISSUE = 12\n    LYMPH_NODE = 13\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the simpa library to create a simulation of a tissue structure. The tissue structure should include a background, a muscle layer, an epidermis layer, and two blood vessels. The simulation should be set up with specific global parameters such as volume dimensions, spacing, and wavelengths. The code should also include a function to create the tissue structure with specific properties for each component. The simulation should be run for all wavelengths specified and include a linear unmixing component. The results of the simulation and the linear unmixing should be loaded and visualized.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 54, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that uses the avalanche library to implement an online continual learning scenario with the Replay strategy. The code should first set up the necessary configurations and transformations. Then, it should create an online continual learning scenario using the MNIST dataset for training and testing. A SimpleMLP model should be created and some metrics for evaluation should be chosen. The code should then create a Replay strategy instance with a ReservoirSamplingBuffer storage policy. Finally, the code should implement a training loop where it trains on the online train stream of the scenario and evaluates on the test stream. The results of the evaluation should be stored in a list. The code should also include an argument parser to select the cuda device to use.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def __init__(self, model: Module, optimizer: Optimizer,\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        BaseStrategy is the super class of all task-based continual learning\n        strategies. It implements a basic training loop and callback system\n        that allows to execute code at each experience of the training loop.\n        Plugins can be used to implement callbacks to augment the training\n        loop with additional behavior (e.g. a memory buffer for replay).\n\n        **Scenarios**\n        This strategy supports several continual learning scenarios:\n\n        * class-incremental scenarios (no task labels)\n        * multi-task scenarios, where task labels are provided)\n        * multi-incremental scenarios, where the same task may be revisited\n\n        The exact scenario depends on the data stream and whether it provides\n        the task labels.\n\n        **Training loop**\n        The training loop is organized as follows::\n            train\n                train_exp  # for each experience\n                    adapt_train_dataset\n                    train_dataset_adaptation\n                    make_train_dataloader\n                    train_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        **Evaluation loop**\n        The evaluation loop is organized as follows::\n            eval\n                eval_exp  # for each experience\n                    adapt_eval_dataset\n                    eval_dataset_adaptation\n                    make_eval_dataloader\n                    eval_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device where the model will be allocated.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience and before training on the first experience.\n                if >0: calls `eval` every `eval_every` epochs, at the end\n                    of all the epochs for a single experience and before\n                    training on the first experience.\n        \"\"\"\n        self._criterion = criterion\n\n        self.model: Module = model\n        \"\"\" PyTorch model. \"\"\"\n\n        self.optimizer = optimizer\n        \"\"\" PyTorch optimizer. \"\"\"\n\n        self.train_epochs: int = train_epochs\n        \"\"\" Number of training epochs. \"\"\"\n\n        self.train_mb_size: int = train_mb_size\n        \"\"\" Training mini-batch size. \"\"\"\n\n        self.eval_mb_size: int = train_mb_size if eval_mb_size is None \\\n            else eval_mb_size\n        \"\"\" Eval mini-batch size. \"\"\"\n\n        self.device = device\n        \"\"\" PyTorch device where the model will be allocated. \"\"\"\n\n        self.plugins = [] if plugins is None else plugins\n        \"\"\" List of `StrategyPlugin`s. \"\"\"\n\n        if evaluator is None:\n            evaluator = EvaluationPlugin()\n        self.plugins.append(evaluator)\n        self.evaluator = evaluator\n        \"\"\" EvaluationPlugin used for logging and metric computations. \"\"\"\n\n        self.eval_every = eval_every\n        \"\"\" Frequency of the evaluation during training. \"\"\"\n\n        ###################################################################\n        # State variables. These are updated during the train/eval loops. #\n        ###################################################################\n        self.training_exp_counter = 0\n        \"\"\" Counts the number of training steps. +1 at the end of each \n        experience. \"\"\"\n\n        self.epoch: Optional[int] = None\n        \"\"\" Epoch counter. \"\"\"\n\n        self.experience = None\n        \"\"\" Current experience. \"\"\"\n\n        self.adapted_dataset = None\n        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n        append data to it (e.g. for replay). \n\n        .. note:: \n            This dataset may contain samples from different experiences. If you \n            want the original data for the current experience  \n            use :attr:`.BaseStrategy.experience`.\n        \"\"\"\n\n        self.dataloader = None\n        \"\"\" Dataloader. \"\"\"\n\n        self.mb_it = None\n        \"\"\" Iteration counter. Reset at the start of a new epoch. \"\"\"\n\n        self.mbatch = None\n        \"\"\" Current mini-batch. \"\"\"\n\n        self.mb_output = None\n        \"\"\" Model's output computed on the current mini-batch. \"\"\"\n\n        self.loss = None\n        \"\"\" Loss of the current mini-batch. \"\"\"\n\n        self.is_training: bool = False\n        \"\"\" True if the strategy is in training mode. \"\"\"\n\n        self.current_eval_stream = None\n        \"\"\"User-provided evaluation stream on `eval` call\"\"\"\n\n        self._stop_training = False\n\n        self._warn_for_disabled_plugins_callbacks()\n        self._warn_for_disabled_metrics_callbacks()\n\n# --- Snippet Separator ---\n\nclass JointTraining(BaseStrategy):\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger):\n        \"\"\"\n        JointTraining performs joint training (also called offline training) on\n        the entire stream of data. This means that it is not a continual\n        learning strategy but it can be used as an \"offline\" upper bound for\n        them.\n\n        .. warnings also::\n            Currently :py:class:`JointTraining` adapts its own dataset.\n            Please check that the plugins you are using do not implement\n            :py:meth:`adapt_trainin_dataset`. Otherwise, they are incompatible\n            with :py:class:`JointTraining`.\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device to run the model.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        \"\"\"\n        super().__init__(model, optimizer, criterion, train_mb_size,\n                         train_epochs, eval_mb_size, device, plugins, evaluator)\n        # JointTraining can be trained only once.\n        self._is_fitted = False\n\n    def train(self, experiences: Union[Experience, Sequence[Experience]],\n              eval_streams: Optional[Sequence[Union[Experience,\n                                                    Sequence[\n                                                        Experience]]]] = None,\n              **kwargs):\n        \"\"\" Training loop. if experiences is a single element trains on it.\n        If it is a sequence, trains the model on each experience in order.\n        This is different from joint training on the entire stream.\n        It returns a dictionary with last recorded value for each metric.\n\n        :param experiences: single Experience or sequence.\n        :param eval_streams: list of streams for evaluation.\n            If None: use training experiences for evaluation.\n            Use [] if you do not want to evaluate during training.\n\n        :return: dictionary containing last recorded value for\n            each metric name.\n        \"\"\"\n        self.is_training = True\n        self.model.train()\n        self.model.to(self.device)\n\n        if self._is_fitted:\n            raise AlreadyTrainedError(\n                \"JointTraining can be trained only once. \"\n                \"Please call the train method once on the entire stream.\"\n            )\n\n        # Normalize training and eval data.\n        if isinstance(experiences, Experience):\n            experiences = [experiences]\n        if eval_streams is None:\n            eval_streams = [experiences]\n        for i, exp in enumerate(eval_streams):\n            if isinstance(exp, Experience):\n                eval_streams[i] = [exp]\n\n        self._experiences = experiences\n        self.before_training(**kwargs)\n        for exp in experiences:\n            self.train_exp(exp, eval_streams, **kwargs)\n            # Joint training only needs a single step because\n            # it concatenates all the data at once.\n            break\n        self.after_training(**kwargs)\n\n        res = self.evaluator.get_last_metrics()\n        self._is_fitted = True\n        return res\n\n    def train_dataset_adaptation(self, **kwargs):\n        \"\"\" Concatenates all the datastream. \"\"\"\n        self.adapted_dataset = self._experiences[0].dataset\n        for exp in self._experiences[1:]:\n            cat_data = AvalancheConcatDataset([self.adapted_dataset,\n                                               exp.dataset])\n            self.adapted_dataset = cat_data\n        self.adapted_dataset = self.adapted_dataset.train()\n\n# --- Snippet Separator ---\n\nclass DynamicModule(Module):\n    \"\"\"\n        Dynamic Modules are Avalanche modules that can be incrementally\n        expanded to allow architectural modifications (multi-head\n        classifiers, progressive networks, ...).\n\n        Compared to pytoch Modules, they provide an additional method,\n        `model_adaptation`, which adapts the model given data from the\n        current experience.\n    \"\"\"\n\n    def adaptation(self, dataset: AvalancheDataset = None):\n        \"\"\" Adapt the module (freeze units, add units...) using the current\n        data. Optimizers must be updated after the model adaptation.\n\n        Avalanche strategies call this method to adapt the architecture\n        *before* processing each experience. Strategies also update the\n        optimizer automatically.\n\n        .. warning::\n            As a general rule, you should NOT use this method to train the\n            model. The dataset should be used only to check conditions which\n            require the model's adaptation, such as the discovery of new\n            classes or tasks.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        if self.training:\n            self.train_adaptation(dataset)\n        else:\n            self.eval_adaptation(dataset)\n\n    def train_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at training time.\n\n        Avalanche strategies automatically call this method *before* training\n        on each experience.\n        \"\"\"\n        pass\n\n    def eval_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at evaluation time.\n\n        Avalanche strategies automatically call this method *before* evaluating\n        on each experience.\n\n        .. warning::\n            This method receives the experience's data at evaluation time\n            because some dynamic models need it for adaptation. For example,\n            an incremental classifier needs to be expanded even at evaluation\n            time if new classes are available. However, you should **never**\n            use this data to **train** the module's parameters.\n        \"\"\"\n        pass\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the avalanche library to implement an online continual learning scenario with the Replay strategy. The code should first set up the necessary configurations and transformations. Then, it should create an online continual learning scenario using the MNIST dataset for training and testing. A SimpleMLP model should be created and some metrics for evaluation should be chosen. The code should then create a Replay strategy instance with a ReservoirSamplingBuffer storage policy. Finally, the code should implement a training loop where it trains on the online train stream of the scenario and evaluates on the test stream. The results of the evaluation should be stored in a list. The code should also include an argument parser to select the cuda device to use.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 55, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a simulation using the dd4hep library in Python. The code should include a function to display help information and a main function to run the simulation. The simulation should include setting up the logger, parsing command line arguments, setting up the Geant4 kernel and detector description, configuring the user interface, loading a specific geometry, setting up the magnetic field tracking, random generator, event actions, I/O, and various generator actions. The code should also handle simulation particles, setup detectors, build the physics list, add special particle types and a global range cut. If visualization is enabled, the code should include commands for visualization. Finally, the code should configure, initialize, run, and terminate the kernel.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def run(self):\n    \"\"\"setup the geometry and dd4hep and geant4 and do what was asked to be done\"\"\"\n    import ROOT\n    ROOT.PyConfig.IgnoreCommandLineOptions = True\n\n    import DDG4\n    import dd4hep\n\n    self.printLevel = getOutputLevel(self.printLevel)\n\n    kernel = DDG4.Kernel()\n    dd4hep.setPrintLevel(self.printLevel)\n\n    for compactFile in self.compactFile:\n      kernel.loadGeometry(str(\"file:\" + os.path.abspath(compactFile)))\n    detectorDescription = kernel.detectorDescription()\n\n    DDG4.importConstants(detectorDescription)\n\n  # ----------------------------------------------------------------------------------\n\n    # simple = DDG4.Geant4( kernel, tracker='Geant4TrackerAction',calo='Geant4CalorimeterAction')\n    # geant4 = DDG4.Geant4( kernel, tracker='Geant4TrackerCombineAction',calo='Geant4ScintillatorCalorimeterAction')\n    geant4 = DDG4.Geant4(kernel, tracker=self.action.tracker, calo=self.action.calo)\n\n    geant4.printDetectors()\n\n    if self.runType == \"vis\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=True, macro=self.macroFile)\n    elif self.runType == \"qt\":\n      uiaction = geant4.setupUI(typ=\"qt\", vis=True, macro=self.macroFile)\n    elif self.runType == \"run\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=self.macroFile, ui=False)\n    elif self.runType == \"shell\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=True)\n    elif self.runType == \"batch\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=False)\n    else:\n      logger.error(\"unknown runType\")\n      exit(1)\n\n    # User Configuration for the Geant4Phases\n    uiaction.ConfigureCommands = self.ui._commandsConfigure\n    uiaction.InitializeCommands = self.ui._commandsInitialize\n    uiaction.PostRunCommands = self.ui._commandsPostRun\n    uiaction.PreRunCommands = self.ui._commandsPreRun\n    uiaction.TerminateCommands = self.ui._commandsTerminate\n\n    kernel.NumEvents = self.numberOfEvents\n\n    # -----------------------------------------------------------------------------------\n    # setup the magnetic field:\n    self.__setMagneticFieldOptions(geant4)\n\n    # configure geometry creation\n    self.geometry.constructGeometry(kernel, geant4, self.output.geometry)\n\n    # ----------------------------------------------------------------------------------\n    # Configure Run actions\n    run1 = DDG4.RunAction(kernel, 'Geant4TestRunAction/RunInit')\n    kernel.registerGlobalAction(run1)\n    kernel.runAction().add(run1)\n\n    # Configure the random seed, do it before the I/O because we might change the seed!\n    self.random.initialize(DDG4, kernel, self.output.random)\n\n    # Configure the output file format and plugin\n    self.outputConfig.initialize(dd4hepsimulation=self, geant4=geant4)\n\n    actionList = []\n\n    if self.enableGun:\n      gun = DDG4.GeneratorAction(kernel, \"Geant4ParticleGun/\" + \"Gun\")\n      self.gun.setOptions(gun)\n      gun.Standalone = False\n      gun.Mask = 1\n      actionList.append(gun)\n      self.__applyBoostOrSmear(kernel, actionList, 1)\n      logger.info(\"++++ Adding DD4hep Particle Gun ++++\")\n\n    if self.enableG4Gun:\n      # GPS Create something\n      self._g4gun = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/Gun\")\n      self._g4gun.Uses = 'G4ParticleGun'\n      self._g4gun.Mask = 2\n      logger.info(\"++++ Adding Geant4 Particle Gun ++++\")\n      actionList.append(self._g4gun)\n\n    if self.enableG4GPS:\n      # GPS Create something\n      self._g4gps = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/GPS\")\n      self._g4gps.Uses = 'G4GeneralParticleSource'\n      self._g4gps.Mask = 3\n      logger.info(\"++++ Adding Geant4 General Particle Source ++++\")\n      actionList.append(self._g4gps)\n\n    start = 4\n    for index, plugin in enumerate(self.inputConfig.userInputPlugin, start=start):\n      gen = plugin(self)\n      gen.Mask = index\n      start = index + 1\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n      logger.info(\"++++ Adding User Plugin %s ++++\", gen.Name)\n\n    for index, inputFile in enumerate(self.inputFiles, start=start):\n      if inputFile.endswith(\".slcio\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/LCIO%d\" % index)\n        gen.Parameters = self.lcio.getParameters()\n        gen.Input = \"LCIOFileReader|\" + inputFile\n      elif inputFile.endswith(\".stdhep\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/STDHEP%d\" % index)\n        gen.Input = \"LCIOStdHepReader|\" + inputFile\n      elif inputFile.endswith(\".HEPEvt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/HEPEvt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtShort|\" + inputFile\n      elif inputFile.endswith(\".hepevt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepevt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtLong|\" + inputFile\n      elif inputFile.endswith(tuple([\".hepmc\"] + HEPMC3_SUPPORTED_EXTENSIONS)):\n        if self.hepmc3.useHepMC3:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Parameters = self.hepmc3.getParameters()\n          gen.Input = \"HEPMC3FileReader|\" + inputFile\n        else:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Input = \"Geant4EventReaderHepMC|\" + inputFile\n      elif inputFile.endswith(\".pairs\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/GuineaPig%d\" % index)\n        gen.Input = \"Geant4EventReaderGuineaPig|\" + inputFile\n        gen.Parameters = self.guineapig.getParameters()\n      else:\n        # this should never happen because we already check at the top, but in case of some LogicError...\n        raise RuntimeError(\"Unknown input file type: %s\" % inputFile)\n      gen.Sync = self.skipNEvents\n      gen.Mask = index\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n\n    if actionList:\n      self._buildInputStage(geant4, actionList, output_level=self.output.inputStage,\n                            have_mctruth=self._enablePrimaryHandler())\n\n    # ================================================================================================\n\n    # And handle the simulation particles.\n    part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n    kernel.generatorAction().adopt(part)\n    # part.SaveProcesses = ['conv','Decay']\n    part.SaveProcesses = self.part.saveProcesses\n    part.MinimalKineticEnergy = self.part.minimalKineticEnergy\n    part.KeepAllParticles = self.part.keepAllParticles\n    part.PrintEndTracking = self.part.printEndTracking\n    part.PrintStartTracking = self.part.printStartTracking\n    part.MinDistToParentVertex = self.part.minDistToParentVertex\n    part.OutputLevel = self.output.part\n    part.enableUI()\n\n    if self.part.enableDetailedHitsAndParticleInfo:\n      self.part.setDumpDetailedParticleInfo(kernel, DDG4)\n\n    self.part.setupUserParticleHandler(part, kernel, DDG4)\n\n    # =================================================================================\n\n    # Setup global filters for use in sensitive detectors\n    try:\n      self.filter.setupFilters(kernel)\n    except RuntimeError as e:\n      logger.error(\"%s\", e)\n      exit(1)\n\n    # =================================================================================\n    # get lists of trackers and calorimeters in detectorDescription\n\n    trk, cal, unk = self.getDetectorLists(detectorDescription)\n\n    for detectors, function, defFilter, abort in [(trk, geant4.setupTracker, self.filter.tracker, False),\n                                                  (cal, geant4.setupCalorimeter, self.filter.calo, False),\n                                                  (unk, geant4.setupDetector, None, True),\n                                                  ]:\n      try:\n        self.__setupSensitiveDetectors(detectors, function, defFilter, abort)\n      except Exception as e:\n        logger.error(\"Failed setting up sensitive detector %s\", e)\n        raise\n\n  # =================================================================================\n    # Now build the physics list:\n    _phys = self.physics.setupPhysics(kernel, name=self.physicsList)\n\n    # add the G4StepLimiterPhysics to activate the max step limits in volumes\n    ph = DDG4.PhysicsList(kernel, 'Geant4PhysicsList/Myphysics')\n    ph.addPhysicsConstructor(str('G4StepLimiterPhysics'))\n    _phys.add(ph)\n\n    dd4hep.setPrintLevel(self.printLevel)\n\n    kernel.configure()\n    kernel.initialize()\n\n    # GPS\n    if self._g4gun is not None:\n      self._g4gun.generator()\n    if self._g4gps is not None:\n      self._g4gps.generator()\n\n    startUpTime, _sysTime, _cuTime, _csTime, _elapsedTime = os.times()\n\n    kernel.run()\n    kernel.terminate()\n\n    totalTimeUser, totalTimeSys, _cuTime, _csTime, _elapsedTime = os.times()\n    if self.printLevel <= 3:\n      logger.info(\"DDSim            INFO  Total Time:   %3.2f s (User), %3.2f s (System)\" %\n                  (totalTimeUser, totalTimeSys))\n      if self.numberOfEvents != 0:\n        eventTime = totalTimeUser - startUpTime\n        perEventTime = eventTime / self.numberOfEvents\n        logger.info(\"DDSim            INFO  StartUp Time: %3.2f s, Event Processing: %3.2f s (%3.2f s/Event) \"\n                    % (startUpTime, eventTime, perEventTime))\n\n# --- Snippet Separator ---\n\nclass DD4hepSimulation(object):\n  \"\"\"Class to hold all the parameters and functions to run simulation\"\"\"\n\n  def __init__(self):\n    self.steeringFile = None\n    self.compactFile = []\n    self.inputFiles = []\n    self.outputFile = defaultOutputFile()\n    self.runType = \"batch\"\n    self.printLevel = 3\n\n    self.numberOfEvents = 0\n    self.skipNEvents = 0\n    self.physicsList = None  # deprecated use physics.list\n    self.crossingAngleBoost = 0.0\n    self.macroFile = ''\n    self.enableGun = False\n    self.enableG4GPS = False\n    self.enableG4Gun = False\n    self._g4gun = None\n    self._g4gps = None\n    self.vertexSigma = [0.0, 0.0, 0.0, 0.0]\n    self.vertexOffset = [0.0, 0.0, 0.0, 0.0]\n    self.enableDetailedShowerMode = False\n\n    self._errorMessages = []\n    self._dumpParameter = False\n    self._dumpSteeringFile = False\n\n    # objects for extended configuration option\n    self.output = Output()\n    self.random = Random()\n    self.gun = Gun()\n    self.part = ParticleHandler()\n    self.field = MagneticField()\n    self.action = Action()\n    self.outputConfig = OutputConfig()\n    self.inputConfig = InputConfig()\n    self.guineapig = GuineaPig()\n    self.lcio = LCIO()\n    self.hepmc3 = HepMC3()\n    self.meta = Meta()\n\n    self.geometry = Geometry()\n    self.filter = Filter()\n    self.physics = Physics()\n    self.ui = UI()\n\n    self._argv = None\n\n  def readSteeringFile(self):\n    \"\"\"Reads a steering file and sets the parameters to that of the\n    DD4hepSimulation object present in the steering file.\n    \"\"\"\n    globs = {}\n    locs = {}\n    if not self.steeringFile:\n      return\n    sFileTemp = self.steeringFile\n    exec(compile(open(self.steeringFile).read(), self.steeringFile, 'exec'), globs, locs)\n    for _name, obj in locs.items():\n      if isinstance(obj, DD4hepSimulation):\n        self.__dict__ = obj.__dict__\n    self.steeringFile = os.path.abspath(sFileTemp)\n\n  def parseOptions(self, argv=None):\n    \"\"\"parse the command line options\"\"\"\n\n    if argv is None:\n      self._argv = list(sys.argv)\n\n    parser = argparse.ArgumentParser(\"Running DD4hep Simulations:\",\n                                     formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument(\"--steeringFile\", \"-S\", action=\"store\", default=self.steeringFile,\n                        help=\"Steering file to change default behaviour\")\n\n    # first we parse just the steering file, but only if we don't want to see the help message\n    if not any(opt in self._argv for opt in ('-h', '--help')):\n      parsed, _unknown = parser.parse_known_args()\n      self.steeringFile = parsed.steeringFile\n      self.readSteeringFile()\n\n    # readSteeringFile will set self._argv to None if there is a steering file\n    if self._argv is None:\n      self._argv = list(argv) if argv else list(sys.argv)\n\n    parser.add_argument(\"--compactFile\", nargs='+', action=\"store\",\n                        default=ConfigHelper.makeList(self.compactFile), type=str,\n                        help=\"The compact XML file, or multiple compact files, if the last one is the closer.\")\n\n    parser.add_argument(\"--runType\", action=\"store\", choices=(\"batch\", \"vis\", \"run\", \"shell\", \"qt\"),\n                        default=self.runType,\n                        help=\"The type of action to do in this invocation\"  # Note: implicit string concatenation\n                        \"\\nbatch: just simulate some events, needs numberOfEvents, and input file or gun\"\n                        \"\\nvis: enable visualisation, run the macroFile if it is set\"\n                        \"\\nqt: enable visualisation in Qt shell, run the macroFile if it is set\"\n                        \"\\nrun: run the macroFile and exit\"\n                        \"\\nshell: enable interactive session\")\n\n    parser.add_argument(\"--inputFiles\", \"-I\", nargs='+', action=\"store\", default=self.inputFiles,\n                        help=\"InputFiles for simulation %s files are supported\" % \", \".join(POSSIBLEINPUTFILES))\n\n    parser.add_argument(\"--outputFile\", \"-O\", action=\"store\", default=self.outputFile,\n                        help=\"Outputfile from the simulation: .slcio, edm4hep.root and .root\"\n                        \" output files are supported\")\n\n    parser.add_argument(\"-v\", \"--printLevel\", action=\"store\", default=self.printLevel, dest=\"printLevel\",\n                        choices=(1, 2, 3, 4, 5, 6, 7, 'VERBOSE', 'DEBUG',\n                                 'INFO', 'WARNING', 'ERROR', 'FATAL', 'ALWAYS'),\n                        type=outputLevelType,\n                        help=\"Verbosity use integers from 1(most) to 7(least) verbose\"\n                        \"\\nor strings: VERBOSE, DEBUG, INFO, WARNING, ERROR, FATAL, ALWAYS\")\n\n    parser.add_argument(\"--numberOfEvents\", \"-N\", action=\"store\", dest=\"numberOfEvents\", default=self.numberOfEvents,\n                        type=int, help=\"number of events to simulate, used in batch mode\")\n\n    parser.add_argument(\"--skipNEvents\", action=\"store\", dest=\"skipNEvents\", default=self.skipNEvents, type=int,\n                        help=\"Skip first N events when reading a file\")\n\n    parser.add_argument(\"--physicsList\", action=\"store\", dest=\"physicsList\", default=self.physicsList,\n                        help=\"Physics list to use in simulation\")\n\n    parser.add_argument(\"--crossingAngleBoost\", action=\"store\", dest=\"crossingAngleBoost\",\n                        default=self.crossingAngleBoost,\n                        type=float, help=\"Lorentz boost for the crossing angle, in radian!\")\n\n    parser.add_argument(\"--vertexSigma\", nargs=4, action=\"store\", dest=\"vertexSigma\",\n                        default=self.vertexSigma, metavar=('X', 'Y', 'Z', 'T'),\n                        type=float, help=\"FourVector of the Sigma for the Smearing of the Vertex position: x y z t\")\n\n    parser.add_argument(\"--vertexOffset\", nargs=4, action=\"store\", dest=\"vertexOffset\",\n                        default=self.vertexOffset, metavar=('X', 'Y', 'Z', 'T'),\n                        type=float, help=\"FourVector of translation for the Smearing of the Vertex position: x y z t\")\n\n    parser.add_argument(\"--macroFile\", \"-M\", action=\"store\", dest=\"macroFile\", default=self.macroFile,\n                        help=\"Macro file to execute for runType 'run' or 'vis'\")\n\n    parser.add_argument(\"--enableGun\", \"-G\", action=\"store_true\", dest=\"enableGun\", default=self.enableGun,\n                        help=\"enable the DDG4 particle gun\")\n\n    parser.add_argument(\"--enableG4GPS\", action=\"store_true\", dest=\"enableG4GPS\", default=self.enableG4GPS,\n                        help=\"enable the Geant4 GeneralParticleSource. Needs a macroFile (runType run)\"\n                        \"or use it with the shell (runType shell)\")\n\n    parser.add_argument(\"--enableG4Gun\", action=\"store_true\", dest=\"enableG4Gun\", default=self.enableG4Gun,\n                        help=\"enable the Geant4 particle gun. Needs a macroFile (runType run)\"\n                        \" or use it with the shell (runType shell)\")\n\n    parser.add_argument(\"--dumpParameter\", \"--dump\", action=\"store_true\", dest=\"dumpParameter\",\n                        default=self._dumpParameter, help=\"Print all configuration Parameters and exit\")\n\n    parser.add_argument(\"--enableDetailedShowerMode\", action=\"store_true\", dest=\"enableDetailedShowerMode\",\n                        default=self.enableDetailedShowerMode,\n                        help=\"use detailed shower mode\")\n\n    parser.add_argument(\"--dumpSteeringFile\", action=\"store_true\", dest=\"dumpSteeringFile\",\n                        default=self._dumpSteeringFile, help=\"print an example steering file to stdout\")\n\n    # output, or do something smarter with fullHelp only for example\n    ConfigHelper.addAllHelper(self, parser)\n    # now parse everything. The default values are now taken from the\n    # steeringFile if they were set so that the steering file parameters can be\n    # overwritten from the command line\n    if ARGCOMPLETEENABLED:\n      argcomplete.autocomplete(parser)\n    parsed = parser.parse_args()\n\n    self._dumpParameter = parsed.dumpParameter\n    self._dumpSteeringFile = parsed.dumpSteeringFile\n\n    self.compactFile = ConfigHelper.makeList(parsed.compactFile)\n    self.inputFiles = parsed.inputFiles\n    self.inputFiles = self.__checkFileFormat(self.inputFiles, POSSIBLEINPUTFILES)\n    self.outputFile = parsed.outputFile\n    self.__checkFileFormat(self.outputFile, ('.root', '.slcio'))\n    self.runType = parsed.runType\n    self.printLevel = self.__checkOutputLevel(parsed.printLevel)\n\n    self.numberOfEvents = parsed.numberOfEvents\n    self.skipNEvents = parsed.skipNEvents\n    self.physicsList = parsed.physicsList\n    self.crossingAngleBoost = parsed.crossingAngleBoost\n    self.macroFile = parsed.macroFile\n    self.enableGun = parsed.enableGun\n    self.enableG4Gun = parsed.enableG4Gun\n    self.enableG4GPS = parsed.enableG4GPS\n    self.enableDetailedShowerMode = parsed.enableDetailedShowerMode\n    self.vertexOffset = parsed.vertexOffset\n    self.vertexSigma = parsed.vertexSigma\n\n    self._consistencyChecks()\n\n    if self.printLevel <= 2:  # VERBOSE or DEBUG\n      logger.setLevel(logging.DEBUG)\n\n    # self.__treatUnknownArgs( parsed, unknown )\n    self.__parseAllHelper(parsed)\n    if self._errorMessages and not (self._dumpParameter or self._dumpSteeringFile):\n      parser.epilog = \"\\n\".join(self._errorMessages)\n      parser.print_help()\n      exit(1)\n\n    if self._dumpParameter:\n      from pprint import pprint\n      logger.info(\"=\" * 80)\n      pprint(vars(self))\n      logger.info(\"=\" * 80)\n      exit(0)\n\n    if self._dumpSteeringFile:\n      self.__printSteeringFile(parser)\n      exit(0)\n\n  def getDetectorLists(self, detectorDescription):\n    ''' get lists of trackers and calorimeters that are defined in detectorDescription (the compact xml file)'''\n    import DDG4\n    trackers, calos, unknown = [], [], []\n    for i in detectorDescription.detectors():\n      det = DDG4.DetElement(i.second.ptr())\n      name = det.name()\n      sd = detectorDescription.sensitiveDetector(name)\n      if sd.isValid():\n        detType = sd.type()\n        logger.info('getDetectorLists - found active detector %s type: %s', name, detType)\n        if any(pat.lower() in detType.lower() for pat in self.action.trackerSDTypes):\n          trackers.append(det.name())\n        elif any(pat.lower() in detType.lower() for pat in self.action.calorimeterSDTypes):\n          calos.append(det.name())\n        else:\n          logger.warning('Unknown sensitive detector type: %s', detType)\n          unknown.append(det.name())\n\n    return trackers, calos, unknown\n\n# ==================================================================================\n\n  def run(self):\n    \"\"\"setup the geometry and dd4hep and geant4 and do what was asked to be done\"\"\"\n    import ROOT\n    ROOT.PyConfig.IgnoreCommandLineOptions = True\n\n    import DDG4\n    import dd4hep\n\n    self.printLevel = getOutputLevel(self.printLevel)\n\n    kernel = DDG4.Kernel()\n    dd4hep.setPrintLevel(self.printLevel)\n\n    for compactFile in self.compactFile:\n      kernel.loadGeometry(str(\"file:\" + os.path.abspath(compactFile)))\n    detectorDescription = kernel.detectorDescription()\n\n    DDG4.importConstants(detectorDescription)\n\n  # ----------------------------------------------------------------------------------\n\n    # simple = DDG4.Geant4( kernel, tracker='Geant4TrackerAction',calo='Geant4CalorimeterAction')\n    # geant4 = DDG4.Geant4( kernel, tracker='Geant4TrackerCombineAction',calo='Geant4ScintillatorCalorimeterAction')\n    geant4 = DDG4.Geant4(kernel, tracker=self.action.tracker, calo=self.action.calo)\n\n    geant4.printDetectors()\n\n    if self.runType == \"vis\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=True, macro=self.macroFile)\n    elif self.runType == \"qt\":\n      uiaction = geant4.setupUI(typ=\"qt\", vis=True, macro=self.macroFile)\n    elif self.runType == \"run\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=self.macroFile, ui=False)\n    elif self.runType == \"shell\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=True)\n    elif self.runType == \"batch\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=False)\n    else:\n      logger.error(\"unknown runType\")\n      exit(1)\n\n    # User Configuration for the Geant4Phases\n    uiaction.ConfigureCommands = self.ui._commandsConfigure\n    uiaction.InitializeCommands = self.ui._commandsInitialize\n    uiaction.PostRunCommands = self.ui._commandsPostRun\n    uiaction.PreRunCommands = self.ui._commandsPreRun\n    uiaction.TerminateCommands = self.ui._commandsTerminate\n\n    kernel.NumEvents = self.numberOfEvents\n\n    # -----------------------------------------------------------------------------------\n    # setup the magnetic field:\n    self.__setMagneticFieldOptions(geant4)\n\n    # configure geometry creation\n    self.geometry.constructGeometry(kernel, geant4, self.output.geometry)\n\n    # ----------------------------------------------------------------------------------\n    # Configure Run actions\n    run1 = DDG4.RunAction(kernel, 'Geant4TestRunAction/RunInit')\n    kernel.registerGlobalAction(run1)\n    kernel.runAction().add(run1)\n\n    # Configure the random seed, do it before the I/O because we might change the seed!\n    self.random.initialize(DDG4, kernel, self.output.random)\n\n    # Configure the output file format and plugin\n    self.outputConfig.initialize(dd4hepsimulation=self, geant4=geant4)\n\n    actionList = []\n\n    if self.enableGun:\n      gun = DDG4.GeneratorAction(kernel, \"Geant4ParticleGun/\" + \"Gun\")\n      self.gun.setOptions(gun)\n      gun.Standalone = False\n      gun.Mask = 1\n      actionList.append(gun)\n      self.__applyBoostOrSmear(kernel, actionList, 1)\n      logger.info(\"++++ Adding DD4hep Particle Gun ++++\")\n\n    if self.enableG4Gun:\n      # GPS Create something\n      self._g4gun = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/Gun\")\n      self._g4gun.Uses = 'G4ParticleGun'\n      self._g4gun.Mask = 2\n      logger.info(\"++++ Adding Geant4 Particle Gun ++++\")\n      actionList.append(self._g4gun)\n\n    if self.enableG4GPS:\n      # GPS Create something\n      self._g4gps = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/GPS\")\n      self._g4gps.Uses = 'G4GeneralParticleSource'\n      self._g4gps.Mask = 3\n      logger.info(\"++++ Adding Geant4 General Particle Source ++++\")\n      actionList.append(self._g4gps)\n\n    start = 4\n    for index, plugin in enumerate(self.inputConfig.userInputPlugin, start=start):\n      gen = plugin(self)\n      gen.Mask = index\n      start = index + 1\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n      logger.info(\"++++ Adding User Plugin %s ++++\", gen.Name)\n\n    for index, inputFile in enumerate(self.inputFiles, start=start):\n      if inputFile.endswith(\".slcio\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/LCIO%d\" % index)\n        gen.Parameters = self.lcio.getParameters()\n        gen.Input = \"LCIOFileReader|\" + inputFile\n      elif inputFile.endswith(\".stdhep\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/STDHEP%d\" % index)\n        gen.Input = \"LCIOStdHepReader|\" + inputFile\n      elif inputFile.endswith(\".HEPEvt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/HEPEvt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtShort|\" + inputFile\n      elif inputFile.endswith(\".hepevt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepevt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtLong|\" + inputFile\n      elif inputFile.endswith(tuple([\".hepmc\"] + HEPMC3_SUPPORTED_EXTENSIONS)):\n        if self.hepmc3.useHepMC3:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Parameters = self.hepmc3.getParameters()\n          gen.Input = \"HEPMC3FileReader|\" + inputFile\n        else:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Input = \"Geant4EventReaderHepMC|\" + inputFile\n      elif inputFile.endswith(\".pairs\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/GuineaPig%d\" % index)\n        gen.Input = \"Geant4EventReaderGuineaPig|\" + inputFile\n        gen.Parameters = self.guineapig.getParameters()\n      else:\n        # this should never happen because we already check at the top, but in case of some LogicError...\n        raise RuntimeError(\"Unknown input file type: %s\" % inputFile)\n      gen.Sync = self.skipNEvents\n      gen.Mask = index\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n\n    if actionList:\n      self._buildInputStage(geant4, actionList, output_level=self.output.inputStage,\n                            have_mctruth=self._enablePrimaryHandler())\n\n    # ================================================================================================\n\n    # And handle the simulation particles.\n    part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n    kernel.generatorAction().adopt(part)\n    # part.SaveProcesses = ['conv','Decay']\n    part.SaveProcesses = self.part.saveProcesses\n    part.MinimalKineticEnergy = self.part.minimalKineticEnergy\n    part.KeepAllParticles = self.part.keepAllParticles\n    part.PrintEndTracking = self.part.printEndTracking\n    part.PrintStartTracking = self.part.printStartTracking\n    part.MinDistToParentVertex = self.part.minDistToParentVertex\n    part.OutputLevel = self.output.part\n    part.enableUI()\n\n    if self.part.enableDetailedHitsAndParticleInfo:\n      self.part.setDumpDetailedParticleInfo(kernel, DDG4)\n\n    self.part.setupUserParticleHandler(part, kernel, DDG4)\n\n    # =================================================================================\n\n    # Setup global filters for use in sensitive detectors\n    try:\n      self.filter.setupFilters(kernel)\n    except RuntimeError as e:\n      logger.error(\"%s\", e)\n      exit(1)\n\n    # =================================================================================\n    # get lists of trackers and calorimeters in detectorDescription\n\n    trk, cal, unk = self.getDetectorLists(detectorDescription)\n\n    for detectors, function, defFilter, abort in [(trk, geant4.setupTracker, self.filter.tracker, False),\n                                                  (cal, geant4.setupCalorimeter, self.filter.calo, False),\n                                                  (unk, geant4.setupDetector, None, True),\n                                                  ]:\n      try:\n        self.__setupSensitiveDetectors(detectors, function, defFilter, abort)\n      except Exception as e:\n        logger.error(\"Failed setting up sensitive detector %s\", e)\n        raise\n\n  # =================================================================================\n    # Now build the physics list:\n    _phys = self.physics.setupPhysics(kernel, name=self.physicsList)\n\n    # add the G4StepLimiterPhysics to activate the max step limits in volumes\n    ph = DDG4.PhysicsList(kernel, 'Geant4PhysicsList/Myphysics')\n    ph.addPhysicsConstructor(str('G4StepLimiterPhysics'))\n    _phys.add(ph)\n\n    dd4hep.setPrintLevel(self.printLevel)\n\n    kernel.configure()\n    kernel.initialize()\n\n    # GPS\n    if self._g4gun is not None:\n      self._g4gun.generator()\n    if self._g4gps is not None:\n      self._g4gps.generator()\n\n    startUpTime, _sysTime, _cuTime, _csTime, _elapsedTime = os.times()\n\n    kernel.run()\n    kernel.terminate()\n\n    totalTimeUser, totalTimeSys, _cuTime, _csTime, _elapsedTime = os.times()\n    if self.printLevel <= 3:\n      logger.info(\"DDSim            INFO  Total Time:   %3.2f s (User), %3.2f s (System)\" %\n                  (totalTimeUser, totalTimeSys))\n      if self.numberOfEvents != 0:\n        eventTime = totalTimeUser - startUpTime\n        perEventTime = eventTime / self.numberOfEvents\n        logger.info(\"DDSim            INFO  StartUp Time: %3.2f s, Event Processing: %3.2f s (%3.2f s/Event) \"\n                    % (startUpTime, eventTime, perEventTime))\n\n  def __setMagneticFieldOptions(self, geant4):\n    \"\"\" create and configure the magnetic tracking setup \"\"\"\n    field = geant4.addConfig('Geant4FieldTrackingSetupAction/MagFieldTrackingSetup')\n    field.stepper = self.field.stepper\n    field.equation = self.field.equation\n    field.eps_min = self.field.eps_min\n    field.eps_max = self.field.eps_max\n    field.min_chord_step = self.field.min_chord_step\n    field.delta_chord = self.field.delta_chord\n    field.delta_intersection = self.field.delta_intersection\n    field.delta_one_step = self.field.delta_one_step\n    field.largest_step = self.field.largest_step\n\n  def __checkFileFormat(self, fileNames, extensions):\n    \"\"\"check if the fileName is allowed, note that the filenames are case\n    sensitive, and in case of hepevt we depend on this to identify short and long versions of the content\n    \"\"\"\n    if isinstance(fileNames, str):\n      fileNames = [fileNames]\n    if not all(fileName.endswith(tuple(extensions)) for fileName in fileNames):\n      self._errorMessages.append(\"ERROR: Unknown fileformat for file: %s\" % fileNames)\n    is_hepmc3_extension = any(fileName.endswith(tuple(HEPMC3_SUPPORTED_EXTENSIONS)) for fileName in fileNames)\n    if not self.hepmc3.useHepMC3 and is_hepmc3_extension:\n      self._errorMessages.append(\"ERROR: HepMC3 files or compressed HepMC2 require the use of HepMC3 library\")\n    return fileNames\n\n  def __applyBoostOrSmear(self, kernel, actionList, mask):\n    \"\"\"apply boost or smearing for given mask index\"\"\"\n    import DDG4\n    if self.crossingAngleBoost:\n      lbo = DDG4.GeneratorAction(kernel, \"Geant4InteractionVertexBoost\")\n      lbo.Angle = self.crossingAngleBoost\n      lbo.Mask = mask\n      actionList.append(lbo)\n\n    if any(self.vertexSigma) or any(self.vertexOffset):\n      vSmear = DDG4.GeneratorAction(kernel, \"Geant4InteractionVertexSmear\")\n      vSmear.Offset = self.vertexOffset\n      vSmear.Sigma = self.vertexSigma\n      vSmear.Mask = mask\n      actionList.append(vSmear)\n\n  def __parseAllHelper(self, parsed):\n    \"\"\" parse all the options for the helper \"\"\"\n    parsedDict = vars(parsed)\n    for name, obj in vars(self).items():\n      if isinstance(obj, ConfigHelper):\n        for var in obj.getOptions():\n          key = \"%s.%s\" % (name, var)\n          if key in parsedDict:\n            try:\n              obj.setOption(var, parsedDict[key])\n            except RuntimeError as e:\n              self._errorMessages.append(\"ERROR: %s \" % e)\n              if logger.level <= logging.DEBUG:\n                self._errorMessages.append(traceback.format_exc())\n        obj._checkProperties()\n\n  def __checkOutputLevel(self, level):\n    \"\"\"return outputlevel as int so we don't have to import anything for faster startup\"\"\"\n    try:\n      return outputLevel(level)\n    except ValueError:\n      self._errorMessages.append(\"ERROR: printLevel is neither integer nor string\")\n      return -1\n    except KeyError:\n      self._errorMessages.append(\"ERROR: printLevel '%s' unknown\" % level)\n      return -1\n\n  def __setupSensitiveDetectors(self, detectors, setupFunction, defaultFilter=None,\n                                abortForMissingAction=False,\n                                ):\n    \"\"\"Attach sensitive detector actions for all subdetectors.\n\n    Can be steered with the `Action` ConfigHelpers\n\n    :param detectors: list of detectors\n    :param setupFunction: function used to register the sensitive detector\n    :param defaultFilter: default filter to apply for given types\n    :param abortForMissingAction: if true end program if there is no action found\n    \"\"\"\n    for det in detectors:\n      logger.info('Setting up SD for %s', det)\n      action = None\n      for pattern in self.action.mapActions:\n        if pattern.lower() in det.lower():\n          action = self.action.mapActions[pattern]\n          logger.info('       replace default action with : %s', action)\n          break\n      if abortForMissingAction and action is None:\n        logger.error('Cannot find Action for detector %s. You have to extend \"action.mapAction\"', det)\n        raise RuntimeError(\"Cannot find Action\")\n      seq, act = setupFunction(det, action)\n      self.filter.applyFilters(seq, det, defaultFilter)\n\n      # set detailed hit creation mode for this\n      if self.enableDetailedShowerMode:\n        if isinstance(act, list):\n          for a in act:\n            a.HitCreationMode = 2\n        else:\n          act.HitCreationMode = 2\n\n  def __printSteeringFile(self, parser):\n    \"\"\"print the parameters formated as a steering file\"\"\"\n\n    steeringFileBase = textwrap.dedent(\"\"\"\\\n        from DDSim.DD4hepSimulation import DD4hepSimulation\n        from g4units import mm, GeV, MeV\n        SIM = DD4hepSimulation()\n        \"\"\")\n    steeringFileBase += \"\\n\"\n    optionDict = parser._option_string_actions\n    parameters = vars(self)\n    for parName, parameter in sorted(list(parameters.items()), key=sortParameters):\n      if parName.startswith(\"_\"):\n        continue\n      if isinstance(parameter, ConfigHelper):\n        steeringFileBase += \"\\n\\n\"\n        steeringFileBase += \"################################################################################\\n\"\n        steeringFileBase += \"## %s \\n\" % \"\\n## \".join(parameter.__doc__.splitlines())\n        steeringFileBase += \"################################################################################\\n\"\n        options = parameter.getOptions()\n        for opt, optionsDict in sorted(options.items(), key=sortParameters):\n          if opt.startswith(\"_\"):\n            continue\n          parValue = optionsDict['default']\n          if isinstance(optionsDict.get('help'), str):\n            steeringFileBase += \"\\n## %s\\n\" % \"\\n## \".join(optionsDict.get('help').splitlines())\n          # add quotes if it is a string\n          if isinstance(parValue, str):\n            steeringFileBase += \"SIM.%s.%s = \\\"%s\\\"\\n\" % (parName, opt, parValue)\n          else:\n            steeringFileBase += \"SIM.%s.%s = %s\\n\" % (parName, opt, parValue)\n      else:\n        # get the docstring from the command line parameter\n        optionObj = optionDict.get(\"--\" + parName, None)\n        if isinstance(optionObj, argparse._StoreAction):\n          steeringFileBase += \"## %s\\n\" % \"\\n## \".join(optionObj.help.splitlines())\n        # add quotes if it is a string\n        if isinstance(parameter, str):\n          steeringFileBase += \"SIM.%s = \\\"%s\\\"\" % (parName, str(parameter))\n        else:\n          steeringFileBase += \"SIM.%s = %s\" % (parName, str(parameter))\n        steeringFileBase += \"\\n\"\n    for line in steeringFileBase.splitlines():\n      print(line)\n\n  def _consistencyChecks(self):\n    \"\"\"Check if the requested setup makes sense, or if there is something preventing it from working correctly\n\n    Appends error messages to self._errorMessages\n\n    :returns: None\n    \"\"\"\n\n    if not self.compactFile:\n      self._errorMessages.append(\"ERROR: No geometry compact file provided\")\n\n    if self.runType == \"batch\":\n      if not self.numberOfEvents:\n        self._errorMessages.append(\"ERROR: Batch mode requested, but did not set number of events\")\n      if not (self.inputFiles or self.enableGun or self.inputConfig.userInputPlugin):\n        self._errorMessages.append(\"ERROR: Batch mode requested, but did not set inputFile(s), gun, or userInputPlugin\")\n\n    if self.inputFiles and (self.enableG4Gun or self.enableG4GPS):\n      self._errorMessages.append(\"ERROR: Cannot use both inputFiles and Geant4Gun or GeneralParticleSource\")\n\n    if self.enableGun and (self.enableG4Gun or self.enableG4GPS):\n      self._errorMessages.append(\"ERROR: Cannot use both DD4hepGun and Geant4 Gun or GeneralParticleSource\")\n\n    if self.inputConfig.userInputPlugin and (self.enableG4Gun or self.enableG4GPS):\n      self._errorMessages.append(\"ERROR: Cannot use both userInputPlugin and Geant4 Gun or GeneralParticleSource\")\n\n    if self.numberOfEvents < 0 and not self.inputFiles:\n      self._errorMessages.append(\"ERROR: Negative number of events only sensible for inputFiles\")\n\n  def _enablePrimaryHandler(self):\n    \"\"\" the geant4 Gun or GeneralParticleSource cannot be used together with the PrimaryHandler.\n        Particles would be simulated multiple times\n\n    :returns: True or False\n    \"\"\"\n    enablePrimaryHandler = not (self.enableG4Gun or self.enableG4GPS)\n    if enablePrimaryHandler:\n      logger.info(\"Enabling the PrimaryHandler\")\n    else:\n      logger.info(\"Disabling the PrimaryHandler\")\n    return enablePrimaryHandler\n\n  def _buildInputStage(self, geant4, generator_input_modules, output_level=None, have_mctruth=True):\n    \"\"\"\n    Generic build of the input stage with multiple input modules.\n    Actions executed are:\n    1) Register Generation initialization action\n    2) Append all modules to build the complete input record\n    These modules are readers/particle sources, boosters and/or smearing actions.\n    3) Merge all existing interaction records\n    4) Add the MC truth handler\n    \"\"\"\n    from DDG4 import GeneratorAction\n    ga = geant4.kernel().generatorAction()\n\n    # Register Generation initialization action\n    gen = GeneratorAction(geant4.kernel(), \"Geant4GeneratorActionInit/GenerationInit\")\n    if output_level is not None:\n      gen.OutputLevel = output_level\n    ga.adopt(gen)\n\n    # Now append all modules to build the complete input record\n    # These modules are readers/particle sources, boosters and/or smearing actions\n    for gen in generator_input_modules:\n      gen.enableUI()\n      if output_level is not None:\n        gen.OutputLevel = output_level\n      ga.adopt(gen)\n\n    # Merge all existing interaction records\n    gen = GeneratorAction(geant4.kernel(), \"Geant4InteractionMerger/InteractionMerger\")\n    gen.enableUI()\n    if output_level is not None:\n      gen.OutputLevel = output_level\n    ga.adopt(gen)\n\n    # Finally generate Geant4 primaries\n    if have_mctruth:\n      gen = GeneratorAction(geant4.kernel(), \"Geant4PrimaryHandler/PrimaryHandler\")\n      gen.RejectPDGs = ConfigHelper.makeString(self.physics.rejectPDGs)\n      gen.ZeroTimePDGs = ConfigHelper.makeString(self.physics.zeroTimePDGs)\n      gen.enableUI()\n      if output_level is not None:\n        gen.OutputLevel = output_level\n      ga.adopt(gen)\n    # Puuuhh! All done.\n    return None\n\n# --- Snippet Separator ---\n\ndef run(args):\n    '''\n    Driver for the psyclone-kern tool.\n\n    Parses and checks the command line arguments, calls the appropriate\n    generate function(s) if all is well, catches any errors and outputs the\n    results.\n\n    :param list args: the list of command-line arguments with which \\\n                      psyclone-kern has been invoked.\n    '''\n    # pylint: disable=too-many-statements,too-many-branches\n\n    # Make sure we have the supported APIs defined in the Config singleton,\n    # but postpone loading the config file till the command line was parsed\n    # in case that the user specifies a different config file.\n    Config.get(do_not_load_file=True)\n\n    # We specify the name of the program so that the expected messages are\n    # produced if running within pytest.\n    parser = argparse.ArgumentParser(\n        prog=\"psyclone-kern\",\n        description='Run the PSyclone kernel generator on a particular file.')\n\n    parser.add_argument('-gen', choices=GEN_MODES.keys(), default=\"stub\",\n                        help=\"what to generate for the supplied kernel \"\n                        \"(alg=algorithm layer, stub=kernel-stub \"\n                        \"subroutine). Defaults to stub.\")\n    parser.add_argument('-o', dest='out_file', default=None,\n                        help=\"filename for created code.\")\n    parser.add_argument('-api',\n                        help=f\"choose a particular API from \"\n                        f\"{Config.get().supported_apis}, default \"\n                        f\"'{Config.get().default_api}'.\")\n    parser.add_argument('filename', help='file containing Kernel metadata.')\n\n    # Make the default an empty list so that we can check whether the\n    # user has supplied a value(s) later\n    parser.add_argument(\n        '-I', '--include', default=[], action=\"append\",\n        help='path to Fortran INCLUDE or module files.')\n    parser.add_argument(\n        '-l', '--limit', dest='limit', default='off',\n        choices=['off', 'all', 'output'],\n        help='limit the Fortran line length to 132 characters (default '\n        '\\'%(default)s\\'). Use \\'all\\' to apply limit to both input and '\n        'output Fortran. Use \\'output\\' to apply line-length limit to output '\n        'Fortran only.')\n\n    parser.add_argument(\"--config\", help=\"config file with \"\n                        \"PSyclone specific options.\")\n    parser.add_argument(\n        '--version', '-v', action='version',\n        version=f'psyclone-kern version: {__VERSION__}',\n        help=f'display version information ({__VERSION__})')\n\n    args = parser.parse_args(args)\n\n    # If no config file name is specified, args.config is none\n    # and config will load the default config file.\n    Config.get().load(args.config)\n\n    # Check API, if none is specified, take the setting from the config file\n    if args.api is None:\n        # No command line option, use the one specified in Config - which\n        # is either based on a parameter in the config file, or otherwise\n        # the default:\n        api = Config.get().api\n    elif args.api not in Config.get().supported_apis:\n        print(f\"Unsupported API '{args.api}' specified. Supported APIs are \"\n              f\"{Config.get().supported_apis}.\", file=sys.stderr)\n        sys.exit(1)\n    else:\n        # There is a valid API specified on the command line. Set it\n        # as API in the config object as well.\n        api = args.api\n        Config.get().api = api\n\n    # The Configuration manager checks that the supplied path(s) is/are\n    # valid so protect with a try\n    try:\n        if args.include:\n            Config.get().include_paths = args.include\n        else:\n            # Default is to instruct fparser2 to look in the directory\n            # containing the file being parsed\n            Config.get().include_paths = [\"./\"]\n    except ConfigurationError as err:\n        print(str(err), file=sys.stderr)\n        sys.exit(1)\n\n    try:\n        if args.gen == \"alg\":\n            # Generate algorithm code.\n            if api == \"dynamo0.3\":\n                alg_psyir = LFRicAlg().create_from_kernel(\"test_alg\",\n                                                          args.filename)\n                code = FortranWriter()(alg_psyir)\n            else:\n                print(f\"Algorithm generation from kernel metadata is \"\n                      f\"not yet implemented for API '{api}'\", file=sys.stderr)\n                sys.exit(1)\n        elif args.gen == \"stub\":\n            # Generate kernel stub\n            code = gen_kernel_stub.generate(args.filename, api=api)\n        else:\n            raise InternalError(f\"Expected -gen option to be one of \"\n                                f\"{list(GEN_MODES.keys())} but got {args.gen}\")\n\n    except (IOError, ParseError, GenerationError, RuntimeError) as error:\n        print(\"Error:\", error, file=sys.stderr)\n        sys.exit(1)\n\n    except Exception:   # pylint: disable=broad-except\n        print(\"Error, unexpected exception:\\n\", file=sys.stderr)\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        print(exc_type, file=sys.stderr)\n        print(exc_value, file=sys.stderr)\n        traceback.print_tb(exc_traceback)\n        sys.exit(1)\n\n    if args.limit != \"off\":\n        # Apply line-length limiting to the output code.\n        fll = FortLineLength()\n        code_str = fll.process(str(code))\n    else:\n        code_str = str(code)\n\n    if args.out_file:\n        with io.open(args.out_file, mode='w', encoding='utf-8') as fobj:\n            fobj.write(code_str)\n    else:\n        print(f\"{GEN_MODES[args.gen]}:\\n\", code_str, file=sys.stdout)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a simulation using the dd4hep library in Python. The code should include a function to display help information and a main function to run the simulation. The simulation should include setting up the logger, parsing command line arguments, setting up the Geant4 kernel and detector description, configuring the user interface, loading a specific geometry, setting up the magnetic field tracking, random generator, event actions, I/O, and various generator actions. The code should also handle simulation particles, setup detectors, build the physics list, add special particle types and a global range cut. If visualization is enabled, the code should include commands for visualization. Finally, the code should configure, initialize, run, and terminate the kernel.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 56, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS150, AS151, AS152) with their respective networks and routers. AS150 should be a transit AS with four routers and three networks. AS151 and AS152 should each have a web host and a router, and they should each join a network. AS151 and AS152 should also join an internet exchange. The code should also set up BGP peering between AS150 and AS151, and between AS150 and AS152. Finally, the code should add all the layers to the emulator and dump the emulator's state to a binary file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int,\n    services: List[Service]):\n    \"\"\"!\n    @brief create a new stub AS.\n\n    @param emu reference to the Emulator object.\n    @param base reference to the base layer.\n    @param asn ASN for the newly created AS.\n    @param exchange IXP ID for new newly created AS to join.\n    @param list of instances of Service to install on hosts. One host will be\n    created for each.\n    \"\"\"\n\n    # Create AS and internal network\n    stub_as = base.createAutonomousSystem(asn)\n    stub_as.createNetwork('net0')\n\n    # Create a BGP router \n    # Attach the router to both the internal and external networks\n    router = stub_as.createRouter('router0')\n    router.joinNetwork('net0')\n    router.joinNetwork('ix{}'.format(exchange))\n\n    # Create a host node for each specified service\n    createHostsOnNetwork(emu, stub_as, 'net0', services)\n\n# --- Snippet Separator ---\n\ndef makeTransitAs(base: Base, asn: int, exchanges: List[int],\n    intra_ix_links: List[Tuple[int, int]]) -> AutonomousSystem:\n    \"\"\"!\n    @brief create a transit AS.\n\n    @param base reference to the base layer.\n    @param asn ASN of the newly created AS.\n    @param exchanges list of IXP IDs to join.\n    @param intra_ix_links list of tuple of IXP IDs, to create intra-IX links at.\n\n    @returns transit AS object.\n    \"\"\"\n\n    transit_as = base.createAutonomousSystem(asn)\n\n    routers: Dict[int, Router] = {}\n\n    # Create a BGP router for each internet exchange (for peering purpose)\n    for ix in exchanges:\n        routers[ix] = transit_as.createRouter('r{}'.format(ix))\n        routers[ix].joinNetwork('ix{}'.format(ix))\n\n    # For each pair, create an internal network to connect the BGP routers\n    # from two internet exchanges. There is no need to create a full-mesh\n    # network among the BGP routers. As long as they can reach each other\n    # over a single or multiple hops, it is OK.\n    for (a, b) in intra_ix_links:\n        name = 'net_{}_{}'.format(a, b)\n\n        transit_as.createNetwork(name)\n        routers[a].joinNetwork(name)\n        routers[b].joinNetwork(name)\n\n    return transit_as\n\n# --- Snippet Separator ---\n\ndef makeStubAsWithHosts(emu: Emulator, base: Base, asn: int, exchange: int, hosts_total: int):\n\n    # Create AS and internal network\n    network = \"net0\"\n    stub_as = base.createAutonomousSystem(asn)\n    stub_as.createNetwork(network)\n\n    # Create a BGP router\n    # Attach the router to both the internal and external networks\n    router = stub_as.createRouter('router0')\n    router.joinNetwork(network)\n    router.joinNetwork('ix{}'.format(exchange))\n\n    for counter in range(hosts_total):\n       name = 'host_{}'.format(counter)\n       host = stub_as.createHost(name)\n       host.joinNetwork(network)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS150, AS151, AS152) with their respective networks and routers. AS150 should be a transit AS with four routers and three networks. AS151 and AS152 should each have a web host and a router, and they should each join a network. AS151 and AS152 should also join an internet exchange. The code should also set up BGP peering between AS150 and AS151, and between AS150 and AS152. Finally, the code should add all the layers to the emulator and dump the emulator's state to a binary file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 57, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates a network topology using the seedemu library. The topology should consist of three autonomous systems (AS): AS150, AS2, and AS151. AS150 and AS151 should each have one router and one network, while AS2 should have two routers and one network. AS150 and AS2 should be connected through an internet exchange (IX) 100, and AS2 and AS151 should be connected through IX 101. \n\nAdditionally, create a BGP attacker component that hijacks the prefix of AS151 and joins IX 100. Merge this component with the main simulation. \n\nFinally, establish private peering relationships: between AS150 and AS2 at IX 100, between AS151 and AS2 at IX 101, and between the attacker and AS2 at IX 100. Render and compile the simulation with Docker, managing the network internally.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def makeTransitAs(base: Base, asn: int, exchanges: List[int],\n    intra_ix_links: List[Tuple[int, int]]) -> AutonomousSystem:\n    \"\"\"!\n    @brief create a transit AS.\n\n    @param base reference to the base layer.\n    @param asn ASN of the newly created AS.\n    @param exchanges list of IXP IDs to join.\n    @param intra_ix_links list of tuple of IXP IDs, to create intra-IX links at.\n\n    @returns transit AS object.\n    \"\"\"\n\n    transit_as = base.createAutonomousSystem(asn)\n\n    routers: Dict[int, Router] = {}\n\n    # Create a BGP router for each internet exchange (for peering purpose)\n    for ix in exchanges:\n        routers[ix] = transit_as.createRouter('r{}'.format(ix))\n        routers[ix].joinNetwork('ix{}'.format(ix))\n\n    # For each pair, create an internal network to connect the BGP routers\n    # from two internet exchanges. There is no need to create a full-mesh\n    # network among the BGP routers. As long as they can reach each other\n    # over a single or multiple hops, it is OK.\n    for (a, b) in intra_ix_links:\n        name = 'net_{}_{}'.format(a, b)\n\n        transit_as.createNetwork(name)\n        routers[a].joinNetwork(name)\n        routers[b].joinNetwork(name)\n\n    return transit_as\n\n# --- Snippet Separator ---\n\nclass DefaultEbgpMerger(Merger):\n    \"\"\"!\n    @brief default EBGP layer merging implementation.\n    \"\"\"\n\n    __peeringConflictHandler: Callable[[int, int, int, PeerRelationship, PeerRelationship], PeerRelationship]\n    __xcPeeringConflictHandler: Callable[[int, int, int, PeerRelationship, PeerRelationship], PeerRelationship]\n\n    def __init__(\n        self,\n        onPeeringRelationshipConflict: Callable[[int, int, int, PeerRelationship, PeerRelationship], PeerRelationship] = lambda ix, a, b, relA, relB: relA,\n        onXcPeeringRelationshipConflict: Callable[[int, int, PeerRelationship, PeerRelationship], PeerRelationship] = lambda a, b, relA, relB: relA):\n        \"\"\"!\n        @brief DefaultEbgpMerger constructor.\n        @param onPeeringRelationshipConflict define handler for handling peering\n        relationship conflicts. This should be a function that accepts: (ix,\n        asnA, asnB, peeringRelationshipA, peeringRelationshipB) and return a\n        peering relationship. This defaults to use the peering relationship\n        set in the first emulator.\n        @param onXcPeeringRelationshipConflict define handler for handling\n        peering relationship conflicts. This should be a function that accepts:\n        (asnA, asnB, peeringRelationshipA, peeringRelationshipB) and return a\n        peering relationship. This defaults to use the peering relationship\n        set in the first emulator.\n        \"\"\"\n        super().__init__()\n        self.__peeringConflictHandler = onPeeringRelationshipConflict\n        self.__xcPeeringConflictHandler = onXcPeeringRelationshipConflict\n\n    def getName(self) -> str:\n        return 'DefaultEbgpMerger'\n\n    def getTargetType(self) -> str:\n        return 'EbgpLayer'\n\n    def doMerge(self, objectA: Ebgp, objectB: Ebgp) -> Ebgp:\n        \"\"\"!\n        @brief merge two Ebgp layers.\n\n        @param objectA first Ebgp layer.\n        @param objectB second Ebgp layer.\n\n        @returns merged Ebgp layer.\n        \"\"\"\n\n        new_private = objectA.getPrivatePeerings()\n        new_rs = objectA.getRsPeers()\n        new_xc = objectA.getCrossConnectPeerings()\n\n        for ((ix, a, b), rel) in objectB.getPrivatePeerings().items():\n            if (ix, a, b) in new_private.keys() and new_private[(ix, a, b)] != rel:\n                self._log('Peering relationship conflict for peering in IX{} between AS{} and AS{}: {} != {}, calling handler'.format(\n                    ix, a, b, new_private[(ix, a, b)], rel\n                ))\n                new_private[(ix, a, b)] = self.__peeringConflictHandler(ix, a, b, new_private[(ix, a, b)], rel)\n            else: new_private[(ix, a, b)] = rel\n\n        for (ix, asn) in objectB.getRsPeers():\n            if (ix, asn) not in new_rs: new_rs.append((ix, asn))\n\n        for ((a, b), rel) in objectB.getCrossConnectPeerings().items():\n            if (a, b) in new_xc.keys() and new_private[(a, b)] != rel:\n                self._log('Peering relationship conflict for peering in XC between AS{} and AS{}: {} != {}, calling handler'.format(\n                    a, b, new_xc[(a, b)], rel\n                ))\n                new_xc[(a, b)] = self.__xcPeeringConflictHandler(a, b, new_xc[(a, b)], rel)\n            else: new_xc[(a, b)] = rel\n\n        new_ebgp = Ebgp()\n\n        for ((ix, a, b), rel) in new_private.items(): new_ebgp.addPrivatePeering(ix, a, b, rel)\n        for ((a, b), rel) in new_xc.items(): new_ebgp.addCrossConnectPeering(a, b, rel)\n        for (ix, asn) in new_rs: new_ebgp.addRsPeer(ix, asn)\n\n        return new_ebgp\n\n# --- Snippet Separator ---\n\ndef getInternetExchangeMembers(self, id: int) -> Dict[int, str]:\n        \"\"\"!\n        @brief Get internet exchange members for given IX ID.\n        @param id internet exchange ID provided by getInternetExchanges.\n\n        @returns dict where key is ASN and value is IP address in the exchange.\n        Note that if an AS has multiple addresses in the IX, only one should be\n        returned.\n        \"\"\"\n        raise NotImplementedError('getInternetExchangeMembers not implemented.')\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a network topology using the seedemu library. The topology should consist of three autonomous systems (AS): AS150, AS2, and AS151. AS150 and AS151 should each have one router and one network, while AS2 should have two routers and one network. AS150 and AS2 should be connected through an internet exchange (IX) 100, and AS2 and AS151 should be connected through IX 101. \n\nAdditionally, create a BGP attacker component that hijacks the prefix of AS151 and joins IX 100. Merge this component with the main simulation. \n\nFinally, establish private peering relationships: between AS150 and AS2 at IX 100, between AS151 and AS2 at IX 101, and between the attacker and AS2 at IX 100. Render and compile the simulation with Docker, managing the network internally.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 58, "repo_full_name": "pytorch__torchrec", "instruction": "Generate code that imports necessary modules from the torchrec library and defines two functions: `_get_random_dataloader` and `train`. The `_get_random_dataloader` function should take in the number of embeddings, batch size, and a boolean indicating whether to pin memory, and return a DataLoader object. The `train` function should take in parameters for the number of embeddings, embedding dimension, dense architecture layer sizes, over architecture layer sizes, and learning rate. It should initialize the process group, device, rank, and backend, construct a DLRM model, enable optimizer fusion, distribute the model across devices, overlap communication, compute, and device transfer during training, and finally train the model using a training iterator. The script should call the `train` function if it is the main module.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def __init__(self, model, model_optim, device,\n                 train_loader, valid_loader, label_smoothing=0.1,\n                 n_epochs=120, init_lr=0.025, binary_mode='full_v2',\n                 arch_init_type='normal', arch_init_ratio=1e-3,\n                 arch_optim_lr=1e-3, arch_weight_decay=0,\n                 grad_update_arch_param_every=5, grad_update_steps=1,\n                 warmup=True, warmup_epochs=25,\n                 arch_valid_frequency=1,\n                 load_ckpt=False, ckpt_path=None, arch_path=None):\n        \"\"\"\n        Parameters\n        ----------\n        model : pytorch model\n            the user model, which has mutables\n        model_optim : pytorch optimizer\n            the user defined optimizer\n        device : pytorch device\n            the devices to train/search the model\n        train_loader : pytorch data loader\n            data loader for the training set\n        valid_loader : pytorch data loader\n            data loader for the validation set\n        label_smoothing : float\n            for label smoothing\n        n_epochs : int\n            number of epochs to train/search\n        init_lr : float\n            init learning rate for training the model\n        binary_mode : str\n            the forward/backward mode for the binary weights in mutator\n        arch_init_type : str\n            the way to init architecture parameters\n        arch_init_ratio : float\n            the ratio to init architecture parameters\n        arch_optim_lr : float\n            learning rate of the architecture parameters optimizer\n        arch_weight_decay : float\n            weight decay of the architecture parameters optimizer\n        grad_update_arch_param_every : int\n            update architecture weights every this number of minibatches\n        grad_update_steps : int\n            during each update of architecture weights, the number of steps to train\n        warmup : bool\n            whether to do warmup\n        warmup_epochs : int\n            the number of epochs to do during warmup\n        arch_valid_frequency : int\n            frequency of printing validation result\n        load_ckpt : bool\n            whether load checkpoint\n        ckpt_path : str\n            checkpoint path, if load_ckpt is True, ckpt_path cannot be None\n        arch_path : str\n            the path to store chosen architecture\n        \"\"\"\n        self.model = model\n        self.model_optim = model_optim\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n        self.device = device\n        self.n_epochs = n_epochs\n        self.init_lr = init_lr\n        self.warmup = warmup\n        self.warmup_epochs = warmup_epochs\n        self.arch_valid_frequency = arch_valid_frequency\n        self.label_smoothing = label_smoothing\n\n        self.train_batch_size = train_loader.batch_sampler.batch_size\n        self.valid_batch_size = valid_loader.batch_sampler.batch_size\n        # update architecture parameters every this number of minibatches\n        self.grad_update_arch_param_every = grad_update_arch_param_every\n        # the number of steps per architecture parameter update\n        self.grad_update_steps = grad_update_steps\n        self.binary_mode = binary_mode\n\n        self.load_ckpt = load_ckpt\n        self.ckpt_path = ckpt_path\n        self.arch_path = arch_path\n\n        # init mutator\n        self.mutator = ProxylessNasMutator(model)\n\n        # DataParallel should be put behind the init of mutator\n        self.model = torch.nn.DataParallel(self.model)\n        self.model.to(self.device)\n\n        # iter of valid dataset for training architecture weights\n        self._valid_iter = None\n        # init architecture weights\n        self._init_arch_params(arch_init_type, arch_init_ratio)\n        # build architecture optimizer\n        self.arch_optimizer = torch.optim.Adam(self.mutator.get_architecture_parameters(),\n                                               arch_optim_lr,\n                                               weight_decay=arch_weight_decay,\n                                               betas=(0, 0.999),\n                                               eps=1e-8)\n\n        self.criterion = nn.CrossEntropyLoss()\n        self.warmup_curr_epoch = 0\n        self.train_curr_epoch = 0\n\n# --- Snippet Separator ---\n\nclass SelfSupervisedEmbedding(BaseEmbedding):\n    \"\"\"Implementation of self-supervised embedding models.\n\n    Implements an embedding strategy based on self-supervised learning. A\n    model backbone, self-supervised criterion, optimizer, and dataloader are\n    passed to the constructor. The embedding itself is a pytorch-lightning\n    module.\n\n    The implementation is based on contrastive learning.\n\n    * SimCLR: https://arxiv.org/abs/2002.05709\n    * MoCo: https://arxiv.org/abs/1911.05722\n    * SimSiam: https://arxiv.org/abs/2011.10566\n\n    Attributes:\n        model:\n            A backbone convolutional network with a projection head.\n        criterion:\n            A contrastive loss function.\n        optimizer:\n            A PyTorch optimizer.\n        dataloader:\n            A torchvision dataloader.\n        scheduler:\n            A PyTorch learning rate scheduler.\n\n    Examples:\n        >>> # define a model, criterion, optimizer, and dataloader above\n        >>> import lightly.embedding as embedding\n        >>> encoder = SelfSupervisedEmbedding(\n        >>>     model,\n        >>>     criterion,\n        >>>     optimizer,\n        >>>     dataloader,\n        >>> )\n        >>> # train the self-supervised embedding with default settings\n        >>> encoder.train_embedding()\n        >>> # pass pytorch-lightning trainer arguments as kwargs\n        >>> encoder.train_embedding(max_epochs=10)\n\n    \"\"\"\n\n    def __init__(self,\n                 model: torch.nn.Module,\n                 criterion: torch.nn.Module,\n                 optimizer: torch.optim.Optimizer,\n                 dataloader: torch.utils.data.DataLoader,\n                 scheduler=None):\n\n        super(SelfSupervisedEmbedding, self).__init__(\n            model, criterion, optimizer, dataloader, scheduler)\n\n    def embed(self,\n              dataloader: torch.utils.data.DataLoader,\n              device: torch.device = None,\n              to_numpy: bool = True):\n        \"\"\"Embeds images in a vector space.\n\n        Args:\n            dataloader:\n                A PyTorch dataloader.\n            device:\n                Selected device (`cpu`, `cuda`, see PyTorch documentation)\n            to_numpy:\n                Whether to return the embeddings as numpy array.\n\n        Returns:\n            A tuple consisting of a tensor or ndarray of embeddings\n            with shape n_images x num_ftrs and labels, fnames\n\n        Examples:\n            >>> # embed images in vector space\n            >>> embeddings, labels, fnames = encoder.embed(dataloader)\n\n        \"\"\"\n\n        self.model.eval()\n        embeddings, labels, fnames = None, None, []\n\n        if lightly._is_prefetch_generator_available():\n            pbar = tqdm(BackgroundGenerator(dataloader, max_prefetch=3),\n                        total=len(dataloader))\n        else:\n            pbar = tqdm(dataloader, total=len(dataloader))\n\n        efficiency = 0.\n        embeddings = []\n        labels = []\n        with torch.no_grad():\n\n            start_time = time.time()\n            for (img, label, fname) in pbar:\n\n                img = img.to(device)\n                label = label.to(device)\n\n                fnames += [*fname]\n\n                batch_size = img.shape[0]\n                prepare_time = time.time()\n\n                emb = self.model.backbone(img)\n                emb = emb.detach().reshape(batch_size, -1)\n\n                embeddings.append(emb)\n                labels.append(label)\n\n                process_time = time.time()\n\n                efficiency = \\\n                    (process_time - prepare_time) / (process_time - start_time)\n                pbar.set_description(\n                    \"Compute efficiency: {:.2f}\".format(efficiency))\n                start_time = time.time()\n\n            embeddings = torch.cat(embeddings, 0)\n            labels = torch.cat(labels, 0)\n            if to_numpy:\n                embeddings = embeddings.cpu().numpy()\n                labels = labels.cpu().numpy()\n\n        return embeddings, labels, fnames\n\n# --- Snippet Separator ---\n\nclass DLRM_Transformer(DLRM):\n    \"\"\"\n    Recsys model from \"Deep Learning Recommendation Model for Personalization and\n    Recommendation Systems\" (https://arxiv.org/abs/1906.00091). Processes sparse\n    features by learning pooled embeddings for each feature. On the interaction layer,\n    the relationship between dense features and sparse features is learned through a transformer encoder layer\n    https://arxiv.org/abs/1706.03762.\n    The module assumes all sparse features have the same embedding dimension\n    (i.e. each EmbeddingBagConfig uses the same embedding_dim).\n    The following notation is used throughout the documentation for the models:\n    * F: number of sparse features\n    * D: embedding_dimension of sparse features\n    * B: batch size\n    * num_features: number of dense features\n    Args:\n        embedding_bag_collection (EmbeddingBagCollection): collection of embedding bags\n            used to define `SparseArch`.\n        dense_in_features (int): the dimensionality of the dense input features.\n        dense_arch_layer_sizes (List[int]): the layer sizes for the `DenseArch`.\n        over_arch_layer_sizes (List[int]): the layer sizes for the `OverArch`.\n            The output dimension of the `InteractionArch` should not be manually\n            specified here.\n        nhead: int: Number of multi-attention heads\n        ntransformer_layers: int: Number of transformer encoder layers\n        dense_device (Optional[torch.device]): default compute device.\n    Example::\n        B = 2\n        D = 8\n        eb1_config = EmbeddingBagConfig(\n           name=\"t1\", embedding_dim=D, num_embeddings=100, feature_names=[\"f1\", \"f3\"]\n        )\n        eb2_config = EmbeddingBagConfig(\n           name=\"t2\",\n           embedding_dim=D,\n           num_embeddings=100,\n           feature_names=[\"f2\"],\n        )\n        ebc = EmbeddingBagCollection(tables=[eb1_config, eb2_config])\n        model = DLRM_Transformer(\n           embedding_bag_collection=ebc,\n           dense_in_features=100,\n           dense_arch_layer_sizes=[20],\n           over_arch_layer_sizes=[5, 1],\n        )\n        features = torch.rand((B, 100))\n        #     0       1\n        # 0   [1,2] [4,5]\n        # 1   [4,3] [2,9]\n        # ^\n        # feature\n        sparse_features = KeyedJaggedTensor.from_offsets_sync(\n           keys=[\"f1\", \"f3\"],\n           values=torch.tensor([1, 2, 4, 5, 4, 3, 2, 9]),\n           offsets=torch.tensor([0, 2, 4, 6, 8]),\n        )\n        logits = model(\n           dense_features=features,\n           sparse_features=sparse_features,\n        )\n    \"\"\"\n\n    def __init__(\n        self,\n        embedding_bag_collection: EmbeddingBagCollection,\n        dense_in_features: int,\n        dense_arch_layer_sizes: List[int],\n        over_arch_layer_sizes: List[int],\n        nhead: int = 8,\n        ntransformer_layers: int = 4,\n        dense_device: Optional[torch.device] = None,\n    ) -> None:\n        # initialize DLRM\n        # sparse arch and dense arch are initialized via DLRM\n        super().__init__(\n            embedding_bag_collection,\n            dense_in_features,\n            dense_arch_layer_sizes,\n            over_arch_layer_sizes,\n            dense_device,\n        )\n        embedding_dim: int = embedding_bag_collection.embedding_bag_configs()[\n            0\n        ].embedding_dim\n        num_sparse_features: int = len(self.sparse_arch.sparse_feature_names)\n        self.inter_arch = InteractionTransformerArch(\n            num_sparse_features=num_sparse_features,\n            embedding_dim=embedding_dim,\n            nhead=nhead,\n            ntransformer_layers=ntransformer_layers,\n        )\n        over_in_features: int = (num_sparse_features + 1) * embedding_dim\n        self.over_arch = OverArch(\n            in_features=over_in_features,\n            layer_sizes=over_arch_layer_sizes,\n            device=dense_device,\n        )\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary modules from the torchrec library and defines two functions: `_get_random_dataloader` and `train`. The `_get_random_dataloader` function should take in the number of embeddings, batch size, and a boolean indicating whether to pin memory, and return a DataLoader object. The `train` function should take in parameters for the number of embeddings, embedding dimension, dense architecture layer sizes, over architecture layer sizes, and learning rate. It should initialize the process group, device, rank, and backend, construct a DLRM model, enable optimizer fusion, distribute the model across devices, overlap communication, compute, and device transfer during training, and finally train the model using a training iterator. The script should call the `train` function if it is the main module.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 59, "repo_full_name": "funkelab__gunpowder", "instruction": "Generate code that imports necessary libraries and sets up logging. Then, define a function to train a model for a specified number of iterations. This function should declare array keys for raw intensities, labelled objects, per-voxel affinities, loss weights, predicted affinities, and gradients of the loss with respect to the predicted affinities. \n\nNext, the function should read a configuration file and calculate the input and output sizes in world units. It should then formulate a request for what a batch should contain and a snapshot request for inspection. \n\nThe function should then assemble a training pipeline that includes reading batches from an HDF5 file, normalizing raw data, choosing a random location for each requested batch, applying various augmentations, growing a boundary between labels, converting labels into affinities, balancing labels, pre-caching batches, performing one training iteration for each passing batch, saving the passing batch as an HDF5 file for inspection, and printing profiling stats. \n\nFinally, the function should print a statement indicating the start of training, build the pipeline, request batches for the specified number of iterations, and print a statement indicating the end of training. \n\nThe code should then call this function with a specified number of iterations when run as a main program.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class AddAffinities(BatchFilter):\n    \"\"\"Add an array with affinities for a given label array and neighborhood to\n    the batch. Affinity values are created one for each voxel and entry in the\n    neighborhood list, i.e., for each voxel and each neighbor of this voxel.\n    Values are 1 iff both labels (of the voxel and the neighbor) are equal and\n    non-zero.\n\n    Args:\n\n        affinity_neighborhood (``list`` of array-like):\n\n            List of offsets for the affinities to consider for each voxel.\n\n        labels (:class:`ArrayKey`):\n\n            The array to read the labels from.\n\n        affinities (:class:`ArrayKey`):\n\n            The array to generate containing the affinities.\n\n        labels_mask (:class:`ArrayKey`, optional):\n\n            The array to use as a mask for ``labels``. Affinities connecting at\n            least one masked out label will be masked out in\n            ``affinities_mask``. If not given, ``affinities_mask`` will contain\n            ones everywhere (if requested).\n\n        unlabelled (:class:`ArrayKey`, optional):\n\n            A binary array to indicate unlabelled areas with 0. Affinities from\n            labelled to unlabelled voxels are set to 0, affinities between\n            unlabelled voxels are masked out (they will not be used for\n            training).\n\n        affinities_mask (:class:`ArrayKey`, optional):\n\n            The array to generate containing the affinitiy mask, as derived\n            from parameter ``labels_mask``.\n    \"\"\"\n\n    def __init__(\n        self,\n        affinity_neighborhood,\n        labels,\n        affinities,\n        labels_mask=None,\n        unlabelled=None,\n        affinities_mask=None,\n        dtype=np.uint8,\n    ):\n        self.affinity_neighborhood = np.array(affinity_neighborhood)\n        self.labels = labels\n        self.unlabelled = unlabelled\n        self.labels_mask = labels_mask\n        self.affinities = affinities\n        self.affinities_mask = affinities_mask\n        self.dtype = dtype\n\n    def setup(self):\n        assert self.labels in self.spec, (\n            \"Upstream does not provide %s needed by \" \"AddAffinities\" % self.labels\n        )\n\n        voxel_size = self.spec[self.labels].voxel_size\n\n        dims = self.affinity_neighborhood.shape[1]\n        self.padding_neg = (\n            Coordinate(\n                min([0] + [a[d] for a in self.affinity_neighborhood])\n                for d in range(dims)\n            )\n            * voxel_size\n        )\n\n        self.padding_pos = (\n            Coordinate(\n                max([0] + [a[d] for a in self.affinity_neighborhood])\n                for d in range(dims)\n            )\n            * voxel_size\n        )\n\n        logger.debug(\"padding neg: \" + str(self.padding_neg))\n        logger.debug(\"padding pos: \" + str(self.padding_pos))\n\n        spec = self.spec[self.labels].copy()\n        if spec.roi is not None:\n            spec.roi = spec.roi.grow(self.padding_neg, -self.padding_pos)\n        spec.dtype = self.dtype\n\n        self.provides(self.affinities, spec)\n        if self.affinities_mask:\n            self.provides(self.affinities_mask, spec)\n        self.enable_autoskip()\n\n    def prepare(self, request):\n        deps = BatchRequest()\n\n        # grow labels ROI to accomodate padding\n        labels_roi = request[self.affinities].roi.grow(\n            -self.padding_neg, self.padding_pos\n        )\n        deps[self.labels] = request[self.affinities].copy()\n        deps[self.labels].dtype = None\n        deps[self.labels].roi = labels_roi\n\n        if self.labels_mask:\n            deps[self.labels_mask] = deps[self.labels].copy()\n        if self.unlabelled:\n            deps[self.unlabelled] = deps[self.labels].copy()\n\n        return deps\n\n    def process(self, batch, request):\n        outputs = Batch()\n\n        affinities_roi = request[self.affinities].roi\n\n        logger.debug(\"computing ground-truth affinities from labels\")\n\n        affinities = seg_to_affgraph(\n            batch.arrays[self.labels].data.astype(np.int32), self.affinity_neighborhood\n        ).astype(self.dtype)\n\n        # crop affinities to requested ROI\n        offset = affinities_roi.offset\n        shift = -offset - self.padding_neg\n        crop_roi = affinities_roi.shift(shift)\n        crop_roi /= self.spec[self.labels].voxel_size\n        crop = crop_roi.get_bounding_box()\n\n        logger.debug(\"cropping with \" + str(crop))\n        affinities = affinities[(slice(None),) + crop]\n\n        spec = self.spec[self.affinities].copy()\n        spec.roi = affinities_roi\n        outputs.arrays[self.affinities] = Array(affinities, spec)\n\n        if self.affinities_mask and self.affinities_mask in request:\n            if self.labels_mask:\n                logger.debug(\n                    \"computing ground-truth affinities mask from \" \"labels mask\"\n                )\n                affinities_mask = seg_to_affgraph(\n                    batch.arrays[self.labels_mask].data.astype(np.int32),\n                    self.affinity_neighborhood,\n                )\n                affinities_mask = affinities_mask[(slice(None),) + crop]\n\n            else:\n                affinities_mask = np.ones_like(affinities)\n\n            if self.unlabelled:\n                # 1 for all affinities between unlabelled voxels\n                unlabelled = 1 - batch.arrays[self.unlabelled].data\n                unlabelled_mask = seg_to_affgraph(\n                    unlabelled.astype(np.int32), self.affinity_neighborhood\n                )\n                unlabelled_mask = unlabelled_mask[(slice(None),) + crop]\n\n                # 0 for all affinities between unlabelled voxels\n                unlabelled_mask = 1 - unlabelled_mask\n\n                # combine with mask\n                affinities_mask = affinities_mask * unlabelled_mask\n\n            affinities_mask = affinities_mask.astype(affinities.dtype)\n            outputs.arrays[self.affinities_mask] = Array(affinities_mask, spec)\n\n        else:\n            if self.labels_mask is not None:\n                logger.warning(\n                    \"GT labels does have a mask, but affinities \"\n                    \"mask is not requested.\"\n                )\n\n        # Should probably have a better way of handling arbitrary batch attributes\n        batch.affinity_neighborhood = self.affinity_neighborhood\n\n        return outputs\n\n# --- Snippet Separator ---\n\ndef process(self, batch, request):\n        outputs = Batch()\n\n        affinities_roi = request[self.affinities].roi\n\n        logger.debug(\"computing ground-truth affinities from labels\")\n\n        affinities = seg_to_affgraph(\n            batch.arrays[self.labels].data.astype(np.int32), self.affinity_neighborhood\n        ).astype(self.dtype)\n\n        # crop affinities to requested ROI\n        offset = affinities_roi.offset\n        shift = -offset - self.padding_neg\n        crop_roi = affinities_roi.shift(shift)\n        crop_roi /= self.spec[self.labels].voxel_size\n        crop = crop_roi.get_bounding_box()\n\n        logger.debug(\"cropping with \" + str(crop))\n        affinities = affinities[(slice(None),) + crop]\n\n        spec = self.spec[self.affinities].copy()\n        spec.roi = affinities_roi\n        outputs.arrays[self.affinities] = Array(affinities, spec)\n\n        if self.affinities_mask and self.affinities_mask in request:\n            if self.labels_mask:\n                logger.debug(\n                    \"computing ground-truth affinities mask from \" \"labels mask\"\n                )\n                affinities_mask = seg_to_affgraph(\n                    batch.arrays[self.labels_mask].data.astype(np.int32),\n                    self.affinity_neighborhood,\n                )\n                affinities_mask = affinities_mask[(slice(None),) + crop]\n\n            else:\n                affinities_mask = np.ones_like(affinities)\n\n            if self.unlabelled:\n                # 1 for all affinities between unlabelled voxels\n                unlabelled = 1 - batch.arrays[self.unlabelled].data\n                unlabelled_mask = seg_to_affgraph(\n                    unlabelled.astype(np.int32), self.affinity_neighborhood\n                )\n                unlabelled_mask = unlabelled_mask[(slice(None),) + crop]\n\n                # 0 for all affinities between unlabelled voxels\n                unlabelled_mask = 1 - unlabelled_mask\n\n                # combine with mask\n                affinities_mask = affinities_mask * unlabelled_mask\n\n            affinities_mask = affinities_mask.astype(affinities.dtype)\n            outputs.arrays[self.affinities_mask] = Array(affinities_mask, spec)\n\n        else:\n            if self.labels_mask is not None:\n                logger.warning(\n                    \"GT labels does have a mask, but affinities \"\n                    \"mask is not requested.\"\n                )\n\n        # Should probably have a better way of handling arbitrary batch attributes\n        batch.affinity_neighborhood = self.affinity_neighborhood\n\n        return outputs\n\n# --- Snippet Separator ---\n\nclass GenericTrain(BatchFilter):\n    \"\"\"Generic train node to perform one training iteration for each batch that\n    passes through. This node alone does nothing and should be subclassed for\n    concrete implementations.\n\n    Args:\n\n        inputs (dict): Dictionary from names of input layers in the network to\n            :class:``ArrayKey`` or batch attribute name as string.\n\n        outputs (dict): Dictionary from the names of output layers in the\n            network to :class:``ArrayKey``. New arrays will be generated by\n            this node for each entry (if requested downstream).\n\n        gradients (dict): Dictionary from the names of output layers in the\n            network to :class:``ArrayKey``. New arrays containing the\n            gradient of an output with respect to the loss will be generated by\n            this node for each entry (if requested downstream).\n\n        array_specs (dict, optional): An optional dictionary of\n            :class:`ArrayKey` to :class:`ArraySpec` to set the array specs\n            generated arrays (``outputs`` and ``gradients``). This is useful\n            to set the ``voxel_size``, for example, if they differ from the\n            voxel size of the input arrays. Only fields that are not ``None``\n            in the given :class:`ArraySpec` will be used.\n\n        spawn_subprocess (bool, optional): Whether to run the ``train_step`` in\n            a separate process. Default is false.\n    \"\"\"\n\n    def __init__(\n        self, inputs, outputs, gradients, array_specs=None, spawn_subprocess=False\n    ):\n        self.initialized = False\n\n        self.inputs = inputs\n        self.outputs = outputs\n        self.gradients = gradients\n        self.array_specs = {} if array_specs is None else array_specs\n        self.spawn_subprocess = spawn_subprocess\n\n        self.provided_arrays = list(self.outputs.values()) + list(\n            self.gradients.values()\n        )\n\n    def setup(self):\n        # get common voxel size of inputs, or None if they differ\n        common_voxel_size = None\n        for key in self.inputs.values():\n            if not isinstance(key, ArrayKey):\n                continue\n            if self.spec[key].nonspatial:\n                continue\n\n            voxel_size = self.spec[key].voxel_size\n\n            if common_voxel_size is None:\n                common_voxel_size = voxel_size\n            elif common_voxel_size != voxel_size:\n                common_voxel_size = None\n                break\n\n        # announce provided outputs\n        for key in self.provided_arrays:\n            if key in self.array_specs:\n                spec = self.array_specs[key].copy()\n            else:\n                spec = ArraySpec()\n\n            if spec.voxel_size is None and not spec.nonspatial:\n                assert common_voxel_size is not None, (\n                    \"There is no common voxel size of the inputs, and no \"\n                    \"ArraySpec has been given for %s that defines \"\n                    \"voxel_size.\" % key\n                )\n\n                spec.voxel_size = common_voxel_size\n\n            if spec.interpolatable is None:\n                # default for predictions\n                spec.interpolatable = False\n\n            self.provides(key, spec)\n\n        if self.spawn_subprocess:\n            # start training as a producer pool, so that we can gracefully exit if\n            # anything goes wrong\n            self.worker = ProducerPool([self.__produce_train_batch], queue_size=1)\n            self.batch_in = multiprocessing.Queue(maxsize=1)\n            self.worker.start()\n        else:\n            self.start()\n            self.initialized = True\n\n    def prepare(self, request):\n        deps = BatchRequest()\n        for key in self.inputs.values():\n            deps[key] = request[key]\n        return deps\n\n    def teardown(self):\n        if self.spawn_subprocess:\n            # signal \"stop\"\n            self.batch_in.put((None, None))\n            try:\n                self.worker.get(timeout=2)\n            except NoResult:\n                pass\n            self.worker.stop()\n        else:\n            self.stop()\n\n    def process(self, batch, request):\n        start = time.time()\n\n        if self.spawn_subprocess:\n            self.batch_in.put((batch, request))\n\n            try:\n                out = self.worker.get()\n            except WorkersDied:\n                raise TrainProcessDied()\n\n            for array_key in self.provided_arrays:\n                if array_key in request:\n                    batch.arrays[array_key] = out.arrays[array_key]\n\n            batch.loss = out.loss\n            batch.iteration = out.iteration\n\n        else:\n            self.train_step(batch, request)\n\n        time_of_iteration = time.time() - start\n\n        logger.info(\n            \"Train process: iteration=%d loss=%f time=%f\",\n            batch.iteration,\n            batch.loss,\n            time_of_iteration,\n        )\n\n    def start(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called before the first call to :fun:`train_step`,\n        from the same process that :fun:`train_step` will be called from. Use\n        this to initialize you solver and training hardware.\n        \"\"\"\n        pass\n\n    def train_step(self, batch, request):\n        \"\"\"To be implemented in subclasses.\n\n        In this method, an implementation should perform one training iteration\n        on the given batch. ``batch.loss`` and ``batch.iteration`` should be\n        set. Output arrays should be created according to the given request\n        and added to ``batch``.\"\"\"\n        raise NotImplementedError(\n            \"Class %s does not implement 'train_step'\" % self.name()\n        )\n\n    def stop(self):\n        \"\"\"To be implemented in subclasses.\n\n        This method will be called after the last call to :fun:`train_step`,\n        from the same process that :fun:`train_step` will be called from. Use\n        this to tear down you solver and free training hardware.\n        \"\"\"\n        pass\n\n    def _checkpoint_name(self, basename, iteration):\n        return basename + \"_checkpoint_\" + \"%i\" % iteration\n\n    def _get_latest_checkpoint(self, basename):\n        def atoi(text):\n            return int(text) if text.isdigit() else text\n\n        def natural_keys(text):\n            return [atoi(c) for c in re.split(r\"(\\d+)\", text)]\n\n        checkpoints = glob.glob(basename + \"_checkpoint_*\")\n        checkpoints.sort(key=natural_keys)\n\n        if len(checkpoints) > 0:\n            checkpoint = checkpoints[-1]\n            iteration = int(checkpoint.split(\"_\")[-1])\n            return checkpoint, iteration\n\n        return None, 0\n\n    def __produce_train_batch(self):\n        \"\"\"Process one train batch.\"\"\"\n\n        if not self.initialized:\n            self.start()\n            self.initialized = True\n\n        batch, request = self.batch_in.get()\n\n        # stop signal\n        if batch is None:\n            self.stop()\n            return None\n\n        self.train_step(batch, request)\n\n        return batch\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary libraries and sets up logging. Then, define a function to train a model for a specified number of iterations. This function should declare array keys for raw intensities, labelled objects, per-voxel affinities, loss weights, predicted affinities, and gradients of the loss with respect to the predicted affinities. \n\nNext, the function should read a configuration file and calculate the input and output sizes in world units. It should then formulate a request for what a batch should contain and a snapshot request for inspection. \n\nThe function should then assemble a training pipeline that includes reading batches from an HDF5 file, normalizing raw data, choosing a random location for each requested batch, applying various augmentations, growing a boundary between labels, converting labels into affinities, balancing labels, pre-caching batches, performing one training iteration for each passing batch, saving the passing batch as an HDF5 file for inspection, and printing profiling stats. \n\nFinally, the function should print a statement indicating the start of training, build the pipeline, request batches for the specified number of iterations, and print a statement indicating the end of training. \n\nThe code should then call this function with a specified number of iterations when run as a main program.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 60, "repo_full_name": "avslab__basilisk", "instruction": "Generate code that imports necessary modules and defines a function to rerun a set or subset of Monte Carlo simulations using the Basilisk library. The function should allow for the specification of the scenario name, the number of processes to spawn, and the run numbers to rerun. It should also allow for the addition of new retention policies. The function should set up the Monte Carlo controller, specify the initial conditions directory, the archive directory, the execution count, and whether to disperse seeds or archive parameters. It should also add the specified retention policy and run the initial conditions. If the function is run as the main program, it should call itself.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Controller:\n    \"\"\"\n    The MonteCarloController class is used to run a monte carlo simulation.\n    It is used to execute multiple runs of a simulation with varying initial parameters. Data from each run is retained\n    in order to analyze differences in the simulation runs and the parameters used.\n    \"\"\"\n\n    def __init__(self):\n        self.executionCount = 0\n        self.ICrunFlag = False\n        self.icDirectory = \"\"\n        self.archiveDir = None\n        self.varCast = None\n        self.numProcess = mp.cpu_count()\n\n        self.simParams = SimulationParameters(\n            creationFunction=None,\n            executionFunction=None,\n            configureFunction=None,\n            retentionPolicies=[],\n            shouldArchiveParameters=False,\n            shouldDisperseSeeds=False,\n            dispersions=[],\n            filename=\"\",\n            icfilename=\"\"\n        )\n\n    def setShowProgressBar(self, value):\n        \"\"\"\n        To enable or disable progress bar to show simulation progress\n        Args:\n            value: boolean value, decide to show/hide progress bar\n        \"\"\"\n        self.simParams.showProgressBar = value\n\n    @staticmethod\n    def load(runDirectory):\n        \"\"\"\n        Load a previously completed MonteCarlo simulation\n        Args:\n            The path to the MonteCarlo.data file that contains the archived MonteCarlo run\n        \"\"\"\n        filename = os.path.abspath(runDirectory) + \"/MonteCarlo.data\"\n\n        with gzip.open(filename) as pickledData:\n            data = pickle.load(pickledData)\n            if data.simParams.verbose:\n                print(\"Loading montecarlo at\", filename)\n            data.multiProcManager = mp.Manager()\n            data.dataOutQueue = data.multiProcManager.Queue()\n            data.dataWriter = DataWriter(data.dataOutQueue)\n            data.dataWriter.daemon = False\n            return data\n\n    def setExecutionFunction(self, newModule):\n        \"\"\"\n        Set an execution function that executes a simulation instance.\n\n        Args:\n            executionFunction: (sim: SimulationBaseClass) => None\n                A function with one parameter, a simulation instance.\n                The function will be called after the creationFunction and configurationFunction in each simulation run.\n                It must execute the simulation.\n                Its return value is not used.\n        \"\"\"\n        self.simParams.executionFunction = newModule\n\n    def setConfigureFunction(self, newModule):\n        \"\"\"\n        Set an execution function that executes a simulation instance.\n\n        Args:\n            executionFunction: (sim: SimulationBaseClass) => None\n                A function with one parameter, a simulation instance.\n                The function will be called after the creationFunction and configurationFunction in each simulation run.\n                It must execute the simulation.\n                Its return value is not used.\n        \"\"\"\n        self.simParams.configureFunction = newModule\n\n    def setSimulationFunction(self, newObject):\n        \"\"\"\n        Set the function that creates the simulation instance.\n\n        Args:\n            creationFunction: () => SimulationBaseClass\n                A function with no parameters, that returns a simulation instance.\n        \"\"\"\n        self.simParams.creationFunction = newObject\n\n    def setShouldDisperseSeeds(self, seedDisp):\n        \"\"\"\n        Disperse the RNG seeds of each run in the MonteCarlo\n\n        Args:\n            seedDisp: bool\n                Whether to disperse the RNG seeds in each run of the simulation\n        \"\"\"\n        self.simParams.shouldDisperseSeeds = seedDisp\n\n    def setExecutionCount(self, newCount):\n        \"\"\"\n        Set the number of runs for the MonteCarlo simulation\n\n        Args:\n            newCount: int\n                The number of runs to use for the simulation\n        \"\"\"\n        self.executionCount = newCount\n\n    def addDispersion(self, disp):\n        \"\"\"\n        Add a dispersion to the simulation.\n\n        Args:\n            disp: Dispersion\n                The dispersion to add to the simulation.\n        \"\"\"\n        self.simParams.dispersions.append(disp)\n\n    def addRetentionPolicy(self, policy):\n        \"\"\"\n        Add a retention policy to the simulation.\n\n        Args:\n            disp: RetentionPolicy\n                The retention policy to add to the simulation.\n                This defines variables to be logged and saved\n        \"\"\"\n        self.simParams.retentionPolicies.append(policy)\n\n    def setThreadCount(self, threads):\n        \"\"\"\n        Set the number of threads to use for the monte carlo simulation\n\n        Args:\n            threads: int\n                Number of threads to execute the montecarlo run on.\n        \"\"\"\n        self.numProcess = threads\n\n    def setVerbose(self, verbose):\n        \"\"\"\n        Use verbose output for this MonteCarlo run\n\n        Args:\n            verbose: bool\n                Whether to print verbose information during this MonteCarlo sim.\n        \"\"\"\n        self.simParams.verbose = verbose\n\n    def setDispMagnitudeFile(self, magnitudes):\n        \"\"\"\n        Save .txt with the magnitude of each dispersion in % or sigma away from mean\n\n        Args:\n            magnitudes: bool\n                Whether to save extra files for analysis.\n        \"\"\"\n        self.simParams.saveDispMag = magnitudes\n\n    def setShouldArchiveParameters(self, shouldArchiveParameters):\n        self.simParams.shouldArchiveParameters = shouldArchiveParameters\n\n    def setArchiveDir(self, dirName):\n        \"\"\"\n        Set-up archives for this MonteCarlo run\n\n        Args:\n            dirName: string\n                The name of the directory to archive runs in.\n                None, if no archive desired.\n        \"\"\"\n        self.archiveDir = os.path.abspath(dirName) + \"/\"\n        self.simParams.shouldArchiveParameters = dirName is not None\n        self.simParams.filename = self.archiveDir\n\n    def setVarCast(self, varCast):\n        \"\"\"\n        Set the variable type to downcast the data to\n\n        :param varCast: 'float', 'integer', 'signed', 'unsigned' (see pandas.to_numeric documentation)\n        :return:\n        \"\"\"\n        self.varCast = varCast\n\n    def setICDir(self, dirName):\n        \"\"\"\n        Set-up archives containing IC data\n\n        Args:\n            dirName: string\n                The name of the directory to archive runs in.\n                None, if no archive desired.\n        \"\"\"\n        self.icDirectory = os.path.abspath(dirName) + \"/\"\n        self.simParams.shouldArchiveParameters = True\n        self.simParams.icfilename = self.icDirectory\n\n    def setICRunFlag(self, bool):\n        \"\"\"\n        Set the number of threads to use for the monte carlo simulation\n\n        Args:\n            threads: int\n                Number of threads to execute the montecarlo run on.\n        \"\"\"\n        self.ICrunFlag = bool\n\n    def getRetainedData(self, case):\n        \"\"\"\n        Get the data that was retained for a run, or list of runs.\n\n        Args:\n            cases: int The desired case to get data from.\n        Returns:\n            The retained data for that run is returned.\n        \"\"\"\n        if self.ICrunFlag:\n            oldRunDataFile = self.icDirectory + \"run\" + str(case) + \".data\"\n        else:\n            oldRunDataFile = self.archiveDir + \"run\" + str(case) + \".data\"\n\n        with gzip.open(oldRunDataFile) as pickledData:\n            data = pickle.load(pickledData)\n            return data\n\n    def getRetainedDatas(self, cases):\n        \"\"\"\n        Get the data that was retained for a list of runs.\n\n        Args:\n            cases: int[] The desired cases to get data from.\n        Returns:\n            A generator is returned, which will yield, in-order, the retained data for each of these cases\n        \"\"\"\n\n        for case in cases:\n            yield self.getRetainedData(case)  # call this method recursively, yielding the result\n\n    def getParameters(self, caseNumber):\n        \"\"\"\n        Get the parameters used for a particular run of the montecarlo\n\n        :param caseNumber: The number of the run to get the parameters used for.\n        :type caseNumber: int\n\n        :return: A dictionary of the parameters of the simulation\n                 For example:\n                 {\"keyForSim\": parameterValue, 'TaskList[0].TaskModels[0].RNGSeed': 1674764759}\n        \"\"\"\n        if self.ICrunFlag:\n            filename = self.icDirectory + \"run\" + str(caseNumber) + \".json\"\n        else:\n            filename = self.archiveDir + \"run\" + str(caseNumber) + \".json\"\n        with open(filename, \"r\") as dispersionFile:\n            dispersions = json.load(dispersionFile)\n            return dispersions\n\n    def reRunCases(self, caseList):\n        \"\"\"\n        Rerun some cases from a MonteCarlo run. Does not run in parallel\n\n        Args:\n            caseList: int[]\n                The list of runs to repeat, a list of numbers.\n        Returns:\n            failures: int[]\n                The list of failed runs.\n        \"\"\"\n        # the list of failures\n        failed = []\n\n        for caseNumber in caseList:\n            if self.simParams.verbose:\n                print(\"Rerunning\", caseNumber)\n\n            oldRunFile = self.archiveDir + \"run\" + str(caseNumber) + \".json\"\n            if not os.path.exists(oldRunFile):\n                print(\"ERROR re-running case: \" + oldRunFile)\n                continue\n\n            # use old simulation parameters, modified slightly.\n            simParams = copy.deepcopy(self.simParams)\n            simParams.index = caseNumber\n            # don't redisperse seeds, we want to use the ones saved in the oldRunFile\n            simParams.shouldDisperseSeeds = False\n            # don't retain any data so remove all retention policies\n            simParams.retentionPolicies = []\n\n            with open(oldRunFile, \"r\") as runParameters:\n                simParams.modifications = json.load(runParameters)\n\n            # execute simulation with dispersion\n            executor = SimulationExecutor()\n            success = executor([simParams, self.dataOutQueue])\n\n            if not success:\n                print(\"Error re-executing run\", caseNumber)\n                failed.append(caseNumber)\n\n        if len(failed) > 0:\n            failed.sort()\n            print(\"Failed rerunning cases:\", failed)\n\n        return failed\n\n    def runInitialConditions(self, caseList):\n        \"\"\"\n        Run initial conditions given in a file\n\n        Args:\n            caseList: int[]\n                The list of runs to repeat, a list of numbers.\n        Returns:\n            failures: int[]\n                The list of failed runs.\n        \"\"\"\n        # the list of failures\n        failed = []\n\n        assert self.icDirectory != \"\", \"No initial condition directory was given\"\n        assert self.ICrunFlag is not False, \"IC run flag was not set\"\n\n        if self.simParams.verbose:\n            print(\"Beginning simulation with {0} runs on {1} threads\".format(self.executionCount, self.numProcess))\n\n        if self.simParams.shouldArchiveParameters:\n            if not os.path.exists(self.icDirectory):\n                print(\"Cannot run initial conditions: the directory given does not exist\")\n\n            if self.simParams.verbose:\n                print(\"Archiving a copy of this simulation before running it in 'MonteCarlo.data'\")\n            try:\n                with gzip.open(self.icDirectory + \"MonteCarlo.data\", \"w\") as pickleFile:\n                    pickle.dump(self, pickleFile)  # dump this controller object into a file.\n            except Exception as e:\n                print(\"Unknown exception while trying to pickle monte-carlo-controller... \\ncontinuing...\\n\\n\", e)\n\n        # Create Queue, but don't ever start it.\n        self.multiProcManager = mp.Manager()\n        self.dataOutQueue = self.multiProcManager.Queue()\n        self.dataWriter = DataWriter(self.dataOutQueue)\n        self.dataWriter.daemon = False\n\n        # If archiving the rerun data -- make sure not to delete the original data!\n        if self.archiveDir is not None:\n            if self.archiveDir != self.icDirectory:\n                if os.path.exists(self.archiveDir):\n                    shutil.rmtree(self.archiveDir)\n                os.mkdir(self.archiveDir)\n                self.dataWriter.setLogDir(self.archiveDir)\n                self.dataWriter.start()\n            else:\n                print(\"ERROR: The archive directory is set as the icDirectory. Proceeding would have overwriten all data \" \\\n                      \"within: \" + self.archiveDir + \" with the select rerun cases! Exiting.\\n\")\n                sys.exit(\"Change the archive directory to a new location when rerunning cases.\")\n        else:\n            print(\"No archive data specified; no data will be logged to dataframes\")\n\n        jobsFinished = 0  # keep track of what simulations have finished\n\n        # The simulation executor is responsible for executing simulation given a simulation's parameters\n        # It is called within worker threads with each worker's simulation parameters\n        simulationExecutor = SimulationExecutor()\n        #\n        progressBar = SimulationProgressBar(len(caseList), self.simParams.showProgressBar)\n        if self.numProcess == 1:  # don't make child thread\n            if self.simParams.verbose:\n                print(\"Executing sequentially...\")\n            i = 0\n            for i in range(len(caseList)):\n                simGenerator = self.generateICSims(caseList[i:i+1])\n                for sim in simGenerator:\n                    try:\n                        simulationExecutor([sim,  self.dataOutQueue])\n                    except:\n                        failed.append(i)\n                i += 1\n                progressBar.update(i)\n        else:\n            numSims = len(caseList)\n            if self.numProcess > numSims:\n                print(\"Fewer MCs spawned than processes assigned (%d < %d). Changing processes count to %d.\" % (numSims, self.numProcess, numSims))\n                self.numProcess = numSims\n            for i in range(numSims//self.numProcess):\n                # If number of sims doesn't factor evenly into the number of processes:\n                if numSims % self.numProcess != 0 and i == len(list(range(numSims//self.numProcess)))-1:\n                    offset = numSims % self.numProcess\n                else:\n                    offset = 0\n\n                simGenerator = self.generateICSims(caseList[self.numProcess*i:self.numProcess*(i+1)+offset])\n                pool = mp.Pool(self.numProcess)\n                try:\n                    # yields results *as* the workers finish jobs\n                    for result in pool.imap_unordered(simulationExecutor, [(x, self.dataOutQueue) for x in simGenerator]):\n                        if result[0] is not True:  # workers return True on success\n                            failed.append(result[1])  # add failed jobs to the list of failures\n                            print(\"Job\", result[1], \"failed...\")\n\n                        jobsFinished += 1\n                        progressBar.update(jobsFinished)\n                    pool.close()\n                except KeyboardInterrupt as e:\n                    print(\"Ctrl-C was hit, closing pool\")\n                    # failed.extend(range(jobsFinished, numSims))  # fail all potentially running jobs...\n                    pool.terminate()\n                    raise e\n                except Exception as e:\n                    print(\"Unknown exception while running simulations:\", e)\n                    # failed.extend(range(jobsFinished, numSims))  # fail all potentially running jobs...\n                    traceback.print_exc()\n                    pool.terminate()\n                finally:\n                    pool.join()\n\n        progressBar.markComplete()\n        progressBar.close()\n        # If the data was archiving, close the queue.\n        if self.archiveDir is not None and self.archiveDir != self.icDirectory:\n            while not self.dataOutQueue.empty():\n               time.sleep(1)\n            self.dataOutQueue.put((None, None, True))\n            time.sleep(5)\n\n        # if there are failures\n        if len(failed) > 0:\n            failed.sort()\n\n            if self.simParams.verbose:\n                print(\"Failed\", failed, \"saving to 'failures.txt'\")\n\n            if self.simParams.shouldArchiveParameters:\n                # write a file that contains log of failed runs\n                with open(self.icDirectory + \"failures.txt\", \"w\") as failFile:\n                    failFile.write(str(failed))\n\n        return failed\n\n    def generateICSims(self, caseList):\n        \"\"\"\n        Generator function to clone a baseSimulation for IC run\n\n        Args:\n            baseSimulation: SimulationParams\n                A base simulation to clone.\n            numSims: int[]\n                The desired runs to generate.\n        Returns:\n            generator<SimulationParams>\n                A generator that yields that number of cloned simulations\n        \"\"\"\n\n        # make a list of simulations to execute by cloning the base-simulation and\n        # changing each clone's index and filename to make a list of\n        # simulations to execute\n        for caseNumber in caseList:\n            if self.simParams.verbose:\n                print(\"Running IC \", caseNumber)\n\n            oldRunFile = self.icDirectory + \"run\" + str(caseNumber) + \".json\"\n            if not os.path.exists(oldRunFile):\n                print(\"ERROR running IC case: \" + oldRunFile)\n                continue\n\n            # use old simulation parameters, modified slightly.\n            simParams = copy.deepcopy(self.simParams)\n            simParams.index = caseNumber\n            # don't redisperse seeds, we want to use the ones saved in the oldRunFile\n            simParams.shouldDisperseSeeds = False\n\n            simParams.icfilename = self.icDirectory + \"run\" + str(caseNumber)\n            with open(oldRunFile, \"r\") as runParameters:\n                simParams.modifications = json.load(runParameters)\n\n            yield simParams\n\n    def generateSims(self, simNumList):\n        \"\"\"\n        Generator function to clone a baseSimulation\n\n        Args:\n            baseSimulation: SimulationParams\n                A base simulation to clone.\n            numSims: int[]\n                The desired runs to generate.\n        Returns:\n            generator<SimulationParams>\n                A generator that yields that number of cloned simulations\n        \"\"\"\n\n        # make a list of simulations to execute by cloning the base-simulation and\n        # changing each clone's index and filename to make a list of\n        # simulations to execute\n        for i in simNumList:\n            simClone = copy.deepcopy(self.simParams)\n            simClone.index = i\n            simClone.filename += \"run\" + str(i)\n\n            yield simClone\n\n    def executeCallbacks(self, rng=None, retentionPolicies=[]):\n        \"\"\"\n        Execute retention policy callbacks after running a monteCarlo sim.\n\n        Args:\n            rng: A list of simulations to execute callbacks on\n            retentionPolicies: the retention policies to execute\n        \"\"\"\n\n        if rng is None:\n            rng = list(range(self.executionCount))\n\n        if retentionPolicies == []:\n            retentionPolicies = self.simParams.retentionPolicies\n\n        for simIndex in rng:\n            data = self.getRetainedData(simIndex)\n            for retentionPolicy in retentionPolicies:\n                retentionPolicy.executeCallback(data)\n\n    def executeSimulations(self):\n        \"\"\"\n        Execute simulations in parallel\n\n        :return: failed: int[]\n                 A list of the indices of all failed simulation runs.\n        \"\"\"\n\n        if self.simParams.verbose:\n            print(\"Beginning simulation with {0} runs on {1} threads\".format(self.executionCount, self.numProcess))\n\n        if self.simParams.shouldArchiveParameters:\n            if os.path.exists(self.archiveDir):\n                shutil.rmtree(self.archiveDir, ignore_errors=True)\n            os.mkdir(self.archiveDir)\n            if self.simParams.verbose:\n                print(\"Archiving a copy of this simulation before running it in 'MonteCarlo.data'\")\n            try:\n                with gzip.open(self.archiveDir + \"MonteCarlo.data\", \"wb\") as pickleFile:\n                    pickle.dump(self, pickleFile)  # dump this controller object into a file.\n            except Exception as e:\n                print(\"Unknown exception while trying to pickle monte-carlo-controller... \\ncontinuing...\\n\\n\", e)\n\n        self.multiProcManager = mp.Manager()\n        self.dataOutQueue = self.multiProcManager.Queue()\n        self.dataWriter = DataWriter(self.dataOutQueue)\n        self.dataWriter.daemon = False\n\n        numSims = self.executionCount\n\n        # start data writer process\n        self.dataWriter.setLogDir(self.archiveDir)\n        self.dataWriter.setVarCast(self.varCast)\n        self.dataWriter.start()\n\n        # Avoid building a full list of all simulations to run in memory,\n        # instead only generating simulations right before they are needed by a waiting worker\n        # This is accomplished using a generator and pool.imap, -- simulations are only built\n        # when they are about to be passed to a worker, avoiding memory overhead of first building simulations\n        # There is a system-dependent chunking behavior, sometimes 10-20 are generated at a time.\n        # simGenerator = self.generateSims(range(numSims))\n        failed = []  # keep track of the indices of failed simulations\n        jobsFinished = 0  # keep track of what simulations have finished\n\n        # The simulation executor is responsible for executing simulation given a simulation's parameters\n        # It is called within worker threads with each worker's simulation parameters\n        simulationExecutor = SimulationExecutor()\n\n        progressBar = SimulationProgressBar(numSims, self.simParams.showProgressBar)\n\n        # The outermost for-loop for both the serial and multiprocessed sim generator is not necessary. It\n        # is a temporary fix to a memory leak which is assumed to be a result of the simGenerator not collecting\n        # garbage properly. # TODO: Find a more permenant solution to the leak.\n\n        if self.numProcess == 1:  # don't make child thread\n            if self.simParams.verbose:\n                print(\"Executing sequentially...\")\n            i = 0\n            for i in range(numSims):\n                simGenerator = self.generateSims(list(range(i,i+1)))\n                for sim in simGenerator:\n                    try:\n                        run_ok = simulationExecutor([sim, self.dataOutQueue])[0]\n                    except:\n                        failed.append(i)\n                    else:\n                        if not run_ok:\n                            failed.append(i)\n                    i += 1\n                    progressBar.update(i)\n        else:\n            if self.numProcess > numSims:\n                print(\"Fewer MCs spawned than processes assigned (%d < %d). Changing processes count to %d.\" % (numSims, self.numProcess, numSims))\n                self.numProcess = numSims\n            for i in range(numSims//self.numProcess):\n                # If number of sims doesn't factor evenly into the number of processes:\n                if numSims % self.numProcess != 0 and i == len(list(range(numSims//self.numProcess)))-1:\n                    offset = numSims % self.numProcess\n                else:\n                    offset = 0\n                simGenerator = self.generateSims(list(range(self.numProcess*i, self.numProcess*(i+1)+offset)))\n                pool = mp.Pool(self.numProcess)\n                try:\n                    # yields results *as* the workers finish jobs\n                    for result in pool.imap_unordered(simulationExecutor, [(x, self.dataOutQueue) for x in simGenerator]):\n                        if result[0] is not True:  # workers return True on success\n                            failed.append(result[1])  # add failed jobs to the list of failures\n                            print(\"Job\", result[1], \"failed...\")\n\n                        jobsFinished += 1\n                        progressBar.update(jobsFinished)\n                    pool.close()\n                except KeyboardInterrupt as e:\n                    print(\"Ctrl-C was hit, closing pool\")\n                    failed.extend(list(range(jobsFinished, numSims)))  # fail all potentially running jobs...\n                    pool.terminate()\n                    raise e\n                except Exception as e:\n                    print(\"Unknown exception while running simulations:\", e)\n                    failed.extend(list(range(jobsFinished, numSims)))  # fail all potentially running jobs...\n                    traceback.print_exc()\n                    pool.terminate()\n                finally:\n                    # Wait until all data is logged from the spawned runs before proceeding with the next set.\n                    pool.join()\n\n        progressBar.markComplete()\n        progressBar.close()\n        # Wait until all data logging is finished before concatenation dataframes and shutting down the pool\n        while not self.dataOutQueue.empty():\n           time.sleep(1)\n        self.dataOutQueue.put((None, None, True))\n        time.sleep(5)\n\n        # if there are failures\n        if len(failed) > 0:\n            failed.sort()\n\n            if self.simParams.verbose:\n                print(\"Failed\", failed, \"saving to 'failures.txt'\")\n\n            if self.simParams.shouldArchiveParameters:\n                # write a file that contains log of failed runs\n                with open(self.archiveDir + \"failures.txt\", \"w\") as failFile:\n                    failFile.write(str(failed))\n\n        return failed\n\n# --- Snippet Separator ---\n\ndef runInitialConditions(self, caseList):\n        \"\"\"\n        Run initial conditions given in a file\n\n        Args:\n            caseList: int[]\n                The list of runs to repeat, a list of numbers.\n        Returns:\n            failures: int[]\n                The list of failed runs.\n        \"\"\"\n        # the list of failures\n        failed = []\n\n        assert self.icDirectory != \"\", \"No initial condition directory was given\"\n        assert self.ICrunFlag is not False, \"IC run flag was not set\"\n\n        if self.simParams.verbose:\n            print(\"Beginning simulation with {0} runs on {1} threads\".format(self.executionCount, self.numProcess))\n\n        if self.simParams.shouldArchiveParameters:\n            if not os.path.exists(self.icDirectory):\n                print(\"Cannot run initial conditions: the directory given does not exist\")\n\n            if self.simParams.verbose:\n                print(\"Archiving a copy of this simulation before running it in 'MonteCarlo.data'\")\n            try:\n                with gzip.open(self.icDirectory + \"MonteCarlo.data\", \"w\") as pickleFile:\n                    pickle.dump(self, pickleFile)  # dump this controller object into a file.\n            except Exception as e:\n                print(\"Unknown exception while trying to pickle monte-carlo-controller... \\ncontinuing...\\n\\n\", e)\n\n        # Create Queue, but don't ever start it.\n        self.multiProcManager = mp.Manager()\n        self.dataOutQueue = self.multiProcManager.Queue()\n        self.dataWriter = DataWriter(self.dataOutQueue)\n        self.dataWriter.daemon = False\n\n        # If archiving the rerun data -- make sure not to delete the original data!\n        if self.archiveDir is not None:\n            if self.archiveDir != self.icDirectory:\n                if os.path.exists(self.archiveDir):\n                    shutil.rmtree(self.archiveDir)\n                os.mkdir(self.archiveDir)\n                self.dataWriter.setLogDir(self.archiveDir)\n                self.dataWriter.start()\n            else:\n                print(\"ERROR: The archive directory is set as the icDirectory. Proceeding would have overwriten all data \" \\\n                      \"within: \" + self.archiveDir + \" with the select rerun cases! Exiting.\\n\")\n                sys.exit(\"Change the archive directory to a new location when rerunning cases.\")\n        else:\n            print(\"No archive data specified; no data will be logged to dataframes\")\n\n        jobsFinished = 0  # keep track of what simulations have finished\n\n        # The simulation executor is responsible for executing simulation given a simulation's parameters\n        # It is called within worker threads with each worker's simulation parameters\n        simulationExecutor = SimulationExecutor()\n        #\n        progressBar = SimulationProgressBar(len(caseList), self.simParams.showProgressBar)\n        if self.numProcess == 1:  # don't make child thread\n            if self.simParams.verbose:\n                print(\"Executing sequentially...\")\n            i = 0\n            for i in range(len(caseList)):\n                simGenerator = self.generateICSims(caseList[i:i+1])\n                for sim in simGenerator:\n                    try:\n                        simulationExecutor([sim,  self.dataOutQueue])\n                    except:\n                        failed.append(i)\n                i += 1\n                progressBar.update(i)\n        else:\n            numSims = len(caseList)\n            if self.numProcess > numSims:\n                print(\"Fewer MCs spawned than processes assigned (%d < %d). Changing processes count to %d.\" % (numSims, self.numProcess, numSims))\n                self.numProcess = numSims\n            for i in range(numSims//self.numProcess):\n                # If number of sims doesn't factor evenly into the number of processes:\n                if numSims % self.numProcess != 0 and i == len(list(range(numSims//self.numProcess)))-1:\n                    offset = numSims % self.numProcess\n                else:\n                    offset = 0\n\n                simGenerator = self.generateICSims(caseList[self.numProcess*i:self.numProcess*(i+1)+offset])\n                pool = mp.Pool(self.numProcess)\n                try:\n                    # yields results *as* the workers finish jobs\n                    for result in pool.imap_unordered(simulationExecutor, [(x, self.dataOutQueue) for x in simGenerator]):\n                        if result[0] is not True:  # workers return True on success\n                            failed.append(result[1])  # add failed jobs to the list of failures\n                            print(\"Job\", result[1], \"failed...\")\n\n                        jobsFinished += 1\n                        progressBar.update(jobsFinished)\n                    pool.close()\n                except KeyboardInterrupt as e:\n                    print(\"Ctrl-C was hit, closing pool\")\n                    # failed.extend(range(jobsFinished, numSims))  # fail all potentially running jobs...\n                    pool.terminate()\n                    raise e\n                except Exception as e:\n                    print(\"Unknown exception while running simulations:\", e)\n                    # failed.extend(range(jobsFinished, numSims))  # fail all potentially running jobs...\n                    traceback.print_exc()\n                    pool.terminate()\n                finally:\n                    pool.join()\n\n        progressBar.markComplete()\n        progressBar.close()\n        # If the data was archiving, close the queue.\n        if self.archiveDir is not None and self.archiveDir != self.icDirectory:\n            while not self.dataOutQueue.empty():\n               time.sleep(1)\n            self.dataOutQueue.put((None, None, True))\n            time.sleep(5)\n\n        # if there are failures\n        if len(failed) > 0:\n            failed.sort()\n\n            if self.simParams.verbose:\n                print(\"Failed\", failed, \"saving to 'failures.txt'\")\n\n            if self.simParams.shouldArchiveParameters:\n                # write a file that contains log of failed runs\n                with open(self.icDirectory + \"failures.txt\", \"w\") as failFile:\n                    failFile.write(str(failed))\n\n        return failed\n\n# --- Snippet Separator ---\n\ndef run(self, *args, **kwargs):\n        \"\"\"This method should be the meat of the experiment (needs overriden).\n\n        This is where your experiment code goes.  Note that you should use\n        `self.wait_or_stop()` to pause your experiment between readings, to\n        allow the background thread to be stopped if necessary.\n\n        If you set `self.latest_data`, this may be used to display your\n        results in real time.  You can also use `self.log()` to output text\n        describing the experiment's progress; this may be picked up and \n        displayed graphically or in the console.\n\n        The arguments are passed through from start() to here, so you should\n        either use or ignore them as appropriate.  These are the same args\n        as are passed to run(), so if one of the two functions requires an\n        argument you should make sure the other won't fail if the same\n        argument is passed to it (simple rule: accept *args, **kwargs in\n        both, in addition to any arguments you might have).\n        \"\"\"\n        NotImplementedError(\"The run() method of an Experiment must be overridden!\")\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary modules and defines a function to rerun a set or subset of Monte Carlo simulations using the Basilisk library. The function should allow for the specification of the scenario name, the number of processes to spawn, and the run numbers to rerun. It should also allow for the addition of new retention policies. The function should set up the Monte Carlo controller, specify the initial conditions directory, the archive directory, the execution count, and whether to disperse seeds or archive parameters. It should also add the specified retention policy and run the initial conditions. If the function is run as the main program, it should call itself.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 61, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that performs an iterative reconstruction with radial phase encoding (RPE) data using the SIRF library. The code should include command-line options for specifying the raw data file, path to data files, output file for simulated data, reconstruction engine, and whether to run the reconstruction if non-cartesian code was compiled. It should also include an option for specifying the trajectory type (cartesian, radial, goldenangle or grpe) and whether to show plots. The code should import the necessary engine module from the SIRF library based on the specified engine option. It should then process the command-line options and define a symmetrical operator for cg-optimisation. The code should also define a function for performing the Conjugate Gradient method, which includes computing coil sensitivity maps, setting up the acquisition model, performing backward projection, and implementing the iterative reconstruction. Finally, the code should define a main function that locates the k-space raw data file, reads the acquisition data from an HDF file, pre-processes the acquisition data, sets the trajectory, sorts the processed acquisition data, and performs the reconstruction if the relevant option is set. The code should handle any errors that occur during execution and print an appropriate error message.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def run(args):\n    '''\n    Driver for the psyclone-kern tool.\n\n    Parses and checks the command line arguments, calls the appropriate\n    generate function(s) if all is well, catches any errors and outputs the\n    results.\n\n    :param list args: the list of command-line arguments with which \\\n                      psyclone-kern has been invoked.\n    '''\n    # pylint: disable=too-many-statements,too-many-branches\n\n    # Make sure we have the supported APIs defined in the Config singleton,\n    # but postpone loading the config file till the command line was parsed\n    # in case that the user specifies a different config file.\n    Config.get(do_not_load_file=True)\n\n    # We specify the name of the program so that the expected messages are\n    # produced if running within pytest.\n    parser = argparse.ArgumentParser(\n        prog=\"psyclone-kern\",\n        description='Run the PSyclone kernel generator on a particular file.')\n\n    parser.add_argument('-gen', choices=GEN_MODES.keys(), default=\"stub\",\n                        help=\"what to generate for the supplied kernel \"\n                        \"(alg=algorithm layer, stub=kernel-stub \"\n                        \"subroutine). Defaults to stub.\")\n    parser.add_argument('-o', dest='out_file', default=None,\n                        help=\"filename for created code.\")\n    parser.add_argument('-api',\n                        help=f\"choose a particular API from \"\n                        f\"{Config.get().supported_apis}, default \"\n                        f\"'{Config.get().default_api}'.\")\n    parser.add_argument('filename', help='file containing Kernel metadata.')\n\n    # Make the default an empty list so that we can check whether the\n    # user has supplied a value(s) later\n    parser.add_argument(\n        '-I', '--include', default=[], action=\"append\",\n        help='path to Fortran INCLUDE or module files.')\n    parser.add_argument(\n        '-l', '--limit', dest='limit', default='off',\n        choices=['off', 'all', 'output'],\n        help='limit the Fortran line length to 132 characters (default '\n        '\\'%(default)s\\'). Use \\'all\\' to apply limit to both input and '\n        'output Fortran. Use \\'output\\' to apply line-length limit to output '\n        'Fortran only.')\n\n    parser.add_argument(\"--config\", help=\"config file with \"\n                        \"PSyclone specific options.\")\n    parser.add_argument(\n        '--version', '-v', action='version',\n        version=f'psyclone-kern version: {__VERSION__}',\n        help=f'display version information ({__VERSION__})')\n\n    args = parser.parse_args(args)\n\n    # If no config file name is specified, args.config is none\n    # and config will load the default config file.\n    Config.get().load(args.config)\n\n    # Check API, if none is specified, take the setting from the config file\n    if args.api is None:\n        # No command line option, use the one specified in Config - which\n        # is either based on a parameter in the config file, or otherwise\n        # the default:\n        api = Config.get().api\n    elif args.api not in Config.get().supported_apis:\n        print(f\"Unsupported API '{args.api}' specified. Supported APIs are \"\n              f\"{Config.get().supported_apis}.\", file=sys.stderr)\n        sys.exit(1)\n    else:\n        # There is a valid API specified on the command line. Set it\n        # as API in the config object as well.\n        api = args.api\n        Config.get().api = api\n\n    # The Configuration manager checks that the supplied path(s) is/are\n    # valid so protect with a try\n    try:\n        if args.include:\n            Config.get().include_paths = args.include\n        else:\n            # Default is to instruct fparser2 to look in the directory\n            # containing the file being parsed\n            Config.get().include_paths = [\"./\"]\n    except ConfigurationError as err:\n        print(str(err), file=sys.stderr)\n        sys.exit(1)\n\n    try:\n        if args.gen == \"alg\":\n            # Generate algorithm code.\n            if api == \"dynamo0.3\":\n                alg_psyir = LFRicAlg().create_from_kernel(\"test_alg\",\n                                                          args.filename)\n                code = FortranWriter()(alg_psyir)\n            else:\n                print(f\"Algorithm generation from kernel metadata is \"\n                      f\"not yet implemented for API '{api}'\", file=sys.stderr)\n                sys.exit(1)\n        elif args.gen == \"stub\":\n            # Generate kernel stub\n            code = gen_kernel_stub.generate(args.filename, api=api)\n        else:\n            raise InternalError(f\"Expected -gen option to be one of \"\n                                f\"{list(GEN_MODES.keys())} but got {args.gen}\")\n\n    except (IOError, ParseError, GenerationError, RuntimeError) as error:\n        print(\"Error:\", error, file=sys.stderr)\n        sys.exit(1)\n\n    except Exception:   # pylint: disable=broad-except\n        print(\"Error, unexpected exception:\\n\", file=sys.stderr)\n        exc_type, exc_value, exc_traceback = sys.exc_info()\n        print(exc_type, file=sys.stderr)\n        print(exc_value, file=sys.stderr)\n        traceback.print_tb(exc_traceback)\n        sys.exit(1)\n\n    if args.limit != \"off\":\n        # Apply line-length limiting to the output code.\n        fll = FortLineLength()\n        code_str = fll.process(str(code))\n    else:\n        code_str = str(code)\n\n    if args.out_file:\n        with io.open(args.out_file, mode='w', encoding='utf-8') as fobj:\n            fobj.write(code_str)\n    else:\n        print(f\"{GEN_MODES[args.gen]}:\\n\", code_str, file=sys.stdout)\n\n# --- Snippet Separator ---\n\nclass Run(CodesTask):\n    \"\"\"\n    The 'Run' class for plasmod transport solver.\n\n    Parameters\n    ----------\n    params:\n        The bluemira parameters for the task. Note that this task does\n        not apply any mappings to the ParameterFrame, so they should\n        already be set. Most likely by a solver.\n    input_file:\n        The path to the plasmod input file.\n    output_file:\n        The path to which the plasmod scalar output file should be\n        written.\n    profiles_file:\n        The path to which the plasmod profiles output file should be\n        written.\n    directory:\n        The directory to run the code in\n    binary:\n        The name of, or path to, the plasmod binary. If this is not an\n        absolute path, the binary must be on the system path.\n    \"\"\"\n\n    params: PlasmodSolverParams\n\n    def __init__(\n        self,\n        params: PlasmodSolverParams,\n        input_file: str,\n        output_file: str,\n        profiles_file: str,\n        directory: str = \"./\",\n        binary=PLASMOD_BINARY,\n    ):\n        super().__init__(params, PLASMOD_NAME)\n        self.binary = binary\n        self.input_file = input_file\n        self.output_file = output_file\n        self.profiles_file = profiles_file\n        self.directory = directory\n\n    def run(self):\n        \"\"\"\n        Run the plasmod shell task.\n\n        Runs plasmod on the command line using the given input files and\n        output path.\n\n        Raises\n        ------\n        CodesError\n            If the subprocess returns a non-zero exit code or raises an\n            OSError (e.g., the plasmod binary does not exist).\n        \"\"\"\n        bluemira_print(f\"Running '{PLASMOD_NAME}' systems code\")\n        command = [self.binary, self.input_file, self.output_file, self.profiles_file]\n        with working_dir(self.directory):\n            try:\n                self._run_subprocess(command)\n            except OSError as os_error:\n                raise CodesError(f\"Failed to run plasmod: {os_error}\") from os_error\n\n# --- Snippet Separator ---\n\ndef main(args):\n    '''\n    Parses and checks the command line arguments, calls the generate\n    function if all is well, catches any errors and outputs the\n    results.\n\n    :param args: the list of command-line arguments that PSyclone has \\\n        been invoked with.\n    :type args: List[str]\n\n    '''\n    # pylint: disable=too-many-statements,too-many-branches\n\n    # Make sure we have the supported APIs defined in the Config singleton,\n    # but postpone loading the config file till the command line was parsed\n    # in case that the user specifies a different config file.\n    Config.get(do_not_load_file=True)\n\n    parser = argparse.ArgumentParser(\n        description='Run the PSyclone code generator on a particular file')\n    parser.add_argument('-oalg', help='filename of transformed algorithm code')\n    parser.add_argument(\n        '-opsy', help='filename of generated PSy code')\n    parser.add_argument('-okern',\n                        help='directory in which to put transformed kernels, '\n                        'default is the current working directory.')\n    parser.add_argument('-api',\n                        help=f'choose a particular api from '\n                        f'{str(Config.get().supported_apis)}, '\n                        f'default \\'{Config.get().default_api}\\'.')\n    parser.add_argument('filename', help='algorithm-layer source code')\n    parser.add_argument('-s', '--script', help='filename of a PSyclone'\n                        ' optimisation script')\n    parser.add_argument(\n        '-d', '--directory', default=[], action=\"append\", help='path to a '\n        'root directory structure containing kernel source code. Multiple '\n        'roots can be specified by using multiple -d arguments.')\n    # Make the default an empty list so that we can check whether the\n    # user has supplied a value(s) later\n    parser.add_argument(\n        '-I', '--include', default=[], action=\"append\",\n        help='path to Fortran INCLUDE or module files')\n    parser.add_argument(\n        '-l', '--limit', dest='limit', default='off',\n        choices=['off', 'all', 'output'],\n        help='limit the Fortran line length to 132 characters (default '\n        '\\'%(default)s\\'). Use \\'all\\' to apply limit to both input and '\n        'output Fortran. Use \\'output\\' to apply line-length limit to output '\n        'Fortran only.')\n    parser.add_argument(\n        '-dm', '--dist_mem', dest='dist_mem', action='store_true',\n        help='generate distributed memory code')\n    parser.add_argument(\n        '-nodm', '--no_dist_mem', dest='dist_mem', action='store_false',\n        help='do not generate distributed memory code')\n    parser.add_argument(\n        '--kernel-renaming', default=\"multiple\",\n        choices=configuration.VALID_KERNEL_NAMING_SCHEMES,\n        help=\"Naming scheme to use when re-naming transformed kernels\")\n    parser.add_argument(\n        '--profile', '-p', action=\"append\", choices=Profiler.SUPPORTED_OPTIONS,\n        help=\"Add profiling hooks for either 'kernels' or 'invokes'\")\n    parser.set_defaults(dist_mem=Config.get().distributed_memory)\n\n    parser.add_argument(\"--config\", help=\"Config file with \"\n                        \"PSyclone specific options.\")\n    parser.add_argument(\n        '--version', '-v', action='version',\n        version=f'PSyclone version: {__VERSION__}',\n        help=f'Display version information ({__VERSION__})')\n\n    args = parser.parse_args(args)\n\n    if args.profile:\n        Profiler.set_options(args.profile)\n\n    # If an output directory has been specified for transformed kernels\n    # then check that it is valid\n    if args.okern:\n        if not os.path.exists(args.okern):\n            print(f\"Specified kernel output directory ({args.okern}) does \"\n                  f\"not exist.\", file=sys.stderr)\n            sys.exit(1)\n        if not os.access(args.okern, os.W_OK):\n            print(f\"Cannot write to specified kernel output directory \"\n                  f\"({args.okern}).\", file=sys.stderr)\n            sys.exit(1)\n        kern_out_path = args.okern\n    else:\n        # We write any transformed kernels to the current working directory\n        kern_out_path = os.getcwd()\n\n    # If no config file name is specified, args.config is none\n    # and config will load the default config file.\n    Config.get().load(args.config)\n\n    # Check API, if none is specified, take the setting from the config file\n    if args.api is None:\n        # No command line option, use the one specified in Config - which\n        # is either based on a parameter in the config file, or otherwise\n        # the default:\n        api = Config.get().api\n    elif args.api not in Config.get().supported_apis:\n        print(f\"Unsupported API '{args.api}' specified. Supported APIs are \"\n              f\"{Config.get().supported_apis}.\", file=sys.stderr)\n        sys.exit(1)\n    else:\n        # There is a valid API specified on the command line. Set it\n        # as API in the config object as well.\n        api = args.api\n        Config.get().api = api\n\n    # The Configuration manager checks that the supplied path(s) is/are\n    # valid so protect with a try\n    try:\n        if args.include:\n            Config.get().include_paths = args.include\n        else:\n            # Default is to instruct fparser2 to look in the directory\n            # containing the file being parsed\n            Config.get().include_paths = [\"./\"]\n    except ConfigurationError as err:\n        print(str(err), file=sys.stderr)\n        sys.exit(1)\n\n    try:\n        alg, psy = generate(args.filename, api=api,\n                            kernel_paths=args.directory,\n                            script_name=args.script,\n                            line_length=(args.limit == 'all'),\n                            distributed_memory=args.dist_mem,\n                            kern_out_path=kern_out_path,\n                            kern_naming=args.kernel_renaming)\n    except NoInvokesError:\n        _, exc_value, _ = sys.exc_info()\n        print(f\"Warning: {exc_value}\")\n        # no invoke calls were found in the algorithm file so we do\n        # not need to process it, or generate any psy layer code, so\n        # output the original algorithm file and set the psy file to\n        # be empty\n        with open(args.filename, encoding=\"utf8\") as alg_file:\n            alg = alg_file.read()\n        psy = \"\"\n    except (OSError, IOError, ParseError, GenerationError,\n            RuntimeError):\n        _, exc_value, _ = sys.exc_info()\n        print(exc_value, file=sys.stderr)\n        sys.exit(1)\n    except Exception:  # pylint: disable=broad-except\n        print(\"Error, unexpected exception, please report to the authors:\",\n              file=sys.stderr)\n        traceback.print_exception(*sys.exc_info(), file=sys.stderr)\n        sys.exit(1)\n    if args.limit != 'off':\n        # Limit the line length of the output Fortran to ensure it conforms\n        # to the 132 characters mandated by the standard.\n        fll = FortLineLength()\n        psy_str = fll.process(str(psy))\n        alg_str = fll.process(str(alg))\n    else:\n        psy_str = str(psy)\n        alg_str = str(alg)\n    if args.oalg is not None:\n        with open(args.oalg, mode='w', encoding=\"utf8\") as alg_file:\n            alg_file.write(alg_str)\n    else:\n        print(f\"Transformed algorithm code:\\n{alg_str}\")\n\n    if not psy_str:\n        # empty file so do not output anything\n        pass\n    elif args.opsy is not None:\n        with open(args.opsy, mode='w', encoding=\"utf8\") as psy_file:\n            psy_file.write(psy_str)\n    else:\n        print(f\"Generated psy layer code:\\n{psy_str}\")\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs an iterative reconstruction with radial phase encoding (RPE) data using the SIRF library. The code should include command-line options for specifying the raw data file, path to data files, output file for simulated data, reconstruction engine, and whether to run the reconstruction if non-cartesian code was compiled. It should also include an option for specifying the trajectory type (cartesian, radial, goldenangle or grpe) and whether to show plots. The code should import the necessary engine module from the SIRF library based on the specified engine option. It should then process the command-line options and define a symmetrical operator for cg-optimisation. The code should also define a function for performing the Conjugate Gradient method, which includes computing coil sensitivity maps, setting up the acquisition model, performing backward projection, and implementing the iterative reconstruction. Finally, the code should define a main function that locates the k-space raw data file, reads the acquisition data from an HDF file, pre-processes the acquisition data, sets the trajectory, sorts the processed acquisition data, and performs the reconstruction if the relevant option is set. The code should handle any errors that occur during execution and print an appropriate error message.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 62, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that calculates the triplet and quintet energy gap of an Iron-Porphyrin molecule using DMRG-CASSCF and DMRG-NEVPT2 methods from the pyscf library. The code should first define the DMET active space, then calculate the quintet and triplet energies separately. The active space should include the Fe double d-shell, 4s shell, and the ligand N 2pz orbitals to describe metal-ligand pi bond and pi backbond. The code should also output the active space orbitals to molden format.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def for_dmrg(self):\n        '''Some preprocess for dmrg-nevpt'''\n        if not self._mc.natorb:\n            logger.warn(self, '''\\\nDRMG-MCSCF orbitals are not natural orbitals in active space. It's recommended\nto rerun DMRG-CASCI with mc.natorb before calling DMRG-NEVPT2.\nSee discussions in github issue https://github.com/pyscf/pyscf/issues/698 and\nexample examples/dmrg/32-dmrg_casscf_nevpt2_for_FeS.py''')\n        return self\n\n# --- Snippet Separator ---\n\ndef supercell_response(vind, space, nocc, nmo, double, rot_bloch, log_dest):\n    \"\"\"\n    Retrieves a raw response matrix.\n    Args:\n        vind (Callable): a pyscf matvec routine;\n        space (ndarray): the active space: either for both rows and columns (1D array) or for rows and columns\n        separately (2D array). Basis order: [k, orb=o+v];\n        nocc (ndarray): the numbers of occupied orbitals (frozen and active) per k-point;\n        nmo (ndarray): the total number of orbitals per k-point;\n        double (bool): set to True if `vind` returns the double-sized (i.e. full) matrix;\n        rot_bloch (ndarray): a matrix specifying the rotation from real orbitals returned from pyscf to Bloch\n        functions;\n        log_dest (object): pyscf logging;\n\n    Returns:\n        The TD matrix.\n    \"\"\"\n    if not double:\n        raise NotImplementedError(\"Not implemented for MK-type matrixes\")\n\n    # Full space dims\n    nmo_full = sum(nmo)\n    space = numpy.array(space)\n\n    if space.shape == (nmo_full,):\n        space = numpy.repeat(space[numpy.newaxis, :], 2, axis=0)\n    elif space.shape != (2, nmo_full):\n        raise ValueError(\"The 'space' argument should a 1D array with dimension {:d} or a 2D array with dimensions {},\"\n                         \" found: {}\".format(nmo_full, (2, nmo_full), space.shape))\n\n    return supercell_response_ov(vind, orb2ov(space, nocc, nmo), nocc, nmo, double, rot_bloch, log_dest)\n\n# --- Snippet Separator ---\n\ndef kernel(mf, aolabels, threshold=THRESHOLD, minao=MINAO, with_iao=WITH_IAO,\n           openshell_option=OPENSHELL_OPTION, canonicalize=CANONICALIZE,\n           ncore=0, verbose=None):\n    '''AVAS method to construct mcscf active space.\n    Ref. arXiv:1701.07862 [physics.chem-ph]\n\n    Args:\n        mf : an :class:`SCF` object\n\n        aolabels : string or a list of strings\n            AO labels for AO active space\n\n    Kwargs:\n        threshold : float\n            Tructing threshold of the AO-projector above which AOs are kept in\n            the active space.\n        minao : str\n            A reference AOs for AVAS.\n        with_iao : bool\n            Whether to use IAO localization to construct the reference active AOs.\n        openshell_option : int\n            How to handle singly-occupied orbitals in the active space. The\n            singly-occupied orbitals are projected as part of alpha orbitals\n            if openshell_option=2, or completely kept in active space if\n            openshell_option=3.  See Section III.E option 2 or 3 of the\n            reference paper for more details.\n        canonicalize : bool\n            Orbitals defined in AVAS method are local orbitals.  Symmetrizing\n            the core, active and virtual space.\n        ncore : integer\n            Number of core orbitals to be excluded from the AVAS method.\n\n    Returns:\n        active-space-size, #-active-electrons, orbital-initial-guess-for-CASCI/CASSCF\n\n    Examples:\n\n    >>> from pyscf import gto, scf, mcscf\n    >>> from pyscf.mcscf import avas\n    >>> mol = gto.M(atom='Cr 0 0 0; Cr 0 0 1.6', basis='ccpvtz')\n    >>> mf = scf.RHF(mol).run()\n    >>> ncas, nelecas, mo = avas.avas(mf, ['Cr 3d', 'Cr 4s'])\n    >>> mc = mcscf.CASSCF(mf, ncas, nelecas).run(mo)\n    '''\n    avas_obj = AVAS(mf, aolabels, threshold, minao, with_iao,\n                    openshell_option, canonicalize, ncore, verbose)\n    return avas_obj.kernel()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that calculates the triplet and quintet energy gap of an Iron-Porphyrin molecule using DMRG-CASSCF and DMRG-NEVPT2 methods from the pyscf library. The code should first define the DMET active space, then calculate the quintet and triplet energies separately. The active space should include the Fe double d-shell, 4s shell, and the ligand N 2pz orbitals to describe metal-ligand pi bond and pi backbond. The code should also output the active space orbitals to molden format.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 63, "repo_full_name": "ansys__pydpf-core", "instruction": "Generate code that calculates the average elemental stress on a given volume using the pydpf-core library. The code should first create a model targeting a given result file and get all node IDs in the model to find the minimum amount of surrounding elements to get a minimum volume. Then, it should read the volume by element and find the minimum list of elements by node to get the volume check. After that, the code should create a workflow to compute equivalent stress averaged on elements, apply dot product seqv.volume, sum up those on the list of elements, and divide this sum by the total volume on these elements. Finally, the code should plot equivalent elemental stress and volume averaged elemental equivalent stress, and use the operator with the same algorithm that has been implemented.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def nodal_stress(self, rnum):\n        \"\"\"Retrieves the component stresses for each node in the\n        solution.\n\n        The order of the results corresponds to the sorted node\n        numbering.\n\n        This algorithm, like ANSYS, computes the nodal stress by\n        averaging the stress for each element at each node.  Due to\n        the discontinuities across elements, stresses will vary based\n        on the element they are evaluated from.\n\n        Parameters\n        ----------\n        rnum : int or list\n            Cumulative result number with zero based indexing, or a\n            list containing (step, substep) of the requested result.\n\n        Returns\n        -------\n        nodenum : numpy.ndarray\n            Node numbers of the result.\n\n        stress : numpy.ndarray\n            Stresses at X, Y, Z, XY, YZ, and XZ Sxz averaged at each corner\n            node.\n\n        Notes\n        -----\n        Nodes without a stress value will be NAN.\n        Equivalent ANSYS command: PRNSOL, S\n        \"\"\"\n        return self._nodal_result(rnum, 'ENS')\n\n# --- Snippet Separator ---\n\ndef plot_principal_nodal_stress(self, rnum, stype=None, node_components=None,\n                                    sel_type_all=True, **kwargs):\n        \"\"\"Plot the principal stress at each node in the solution.\n\n        Parameters\n        ----------\n        rnum : int or list\n            Cumulative result number with zero based indexing, or a\n            list containing (step, substep) of the requested result.\n\n        stype : string\n            Stress type to plot.  S1, S2, S3 principal stresses, SINT\n            stress intensity, and SEQV equivalent stress.\n\n            Stress type must be a string from the following list:\n\n            ['S1', 'S2', 'S3', 'SINT', 'SEQV']\n\n        node_components : list, optional\n            Accepts either a string or a list strings of node\n            components to plot.  For example: \n            ``['MY_COMPONENT', 'MY_OTHER_COMPONENT]``\n\n        sel_type_all : bool, optional\n            If node_components is specified, plots those elements\n            containing all nodes of the component.  Default True.\n\n        kwargs : keyword arguments\n            Additional keyword arguments.  See help(pyvista.plot)\n\n        Returns\n        -------\n        cpos : list\n            VTK camera position.\n\n        stress : np.ndarray\n            Array used to plot stress.\n        \"\"\"\n        if stype is None:\n            raise Exception(\"Stress type must be a string from the following list:\\n\" +\n                            \"['1', '2', '3', 'INT', 'EQV']\")\n\n        rnum = self.parse_step_substep(rnum)\n        stress = self.principle_stress_for_plotting(rnum, stype)\n\n        if node_components:\n            grid, ind = self._extract_node_components(node_components, sel_type_all)\n            stress = stress[ind]\n        else:\n            grid = self.grid\n\n        # Generate plot\n        return self._plot_point_scalars(stress, rnum=rnum, grid=grid, **kwargs)\n\n# --- Snippet Separator ---\n\ndef nodal_stress(self, rnum, phase=0, as_complex=False, full_rotor=False):\n        \"\"\"\n        Equivalent ANSYS command: PRNSOL, S\n\n        Retrieves the component stresses for each node in the\n        solution.\n\n        The order of the results corresponds to the sorted node\n        numbering.\n\n        This algorithm, like ANSYS, computes the nodal stress by\n        averaging the stress for each element at each node.  Due to\n        the discontinuities across elements, stresses will vary based\n        on the element they are evaluated from.\n\n        Parameters\n        ----------\n        rnum : int or list\n            Cumulative result number with zero based indexing, or a\n            list containing (step, substep) of the requested result.\n\n        phase : float\n            Phase adjustment of the stress in degrees.\n\n        as_complex : bool, optional\n            Reports stess as a complex result.  Real and imaginary\n            stresses correspond to the stress of the main and repeated\n            sector.  Stress can be \"rotated\" using the phase\n            parameter.\n\n        full_rotor : bool, optional\n            Expands the results to the full rotor when True.  Default\n            False.\n\n        Returns\n        -------\n        nodenum : numpy.ndarray\n            Node numbers of the result.\n\n        stress : numpy.ndarray\n            Stresses at Sx Sy Sz Sxy Syz Sxz averaged at each corner\n            node.  For the corresponding node numbers, see where\n            result is the result object.\n\n        Notes\n        -----\n        Nodes without a stress value will be NAN.\n\n        \"\"\"\n        nnum, stress = super(CyclicResult, self).nodal_stress(rnum)\n        # nnum = nnum[self.mas_ind]\n        # stress = stress[self.mas_ind]\n\n        if self.resultheader['kan'] == 0:  # static result\n            expanded_result = self.expand_cyclic_static(stress, tensor=True)\n        elif self.resultheader['kan'] == 2:  # modal analysis\n            # combine modal solution results\n            hindex_table = self.resultheader['hindex']\n            hindex = hindex_table[rnum]\n\n            # if repeated mode\n            if hindex != 0 and -hindex in hindex_table:\n                if hindex < 0:\n                    rnum_r = rnum - 1\n                else:\n                    rnum_r = rnum + 1\n\n                # get repeated result and combine\n                _, stress_r = super(CyclicResult, self).nodal_stress(rnum_r)\n\n            else:\n                stress_r = np.zeros_like(stress)\n\n            expanded_result = self.expand_cyclic_modal_stress(stress,\n                                                              stress_r,\n                                                              hindex,\n                                                              phase,\n                                                              as_complex,\n                                                              full_rotor)\n        else:\n            raise Exception('Unsupported analysis type')\n\n        return nnum, expanded_result\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that calculates the average elemental stress on a given volume using the pydpf-core library. The code should first create a model targeting a given result file and get all node IDs in the model to find the minimum amount of surrounding elements to get a minimum volume. Then, it should read the volume by element and find the minimum list of elements by node to get the volume check. After that, the code should create a workflow to compute equivalent stress averaged on elements, apply dot product seqv.volume, sum up those on the list of elements, and divide this sum by the total volume on these elements. Finally, the code should plot equivalent elemental stress and volume averaged elemental equivalent stress, and use the operator with the same algorithm that has been implemented.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 64, "repo_full_name": "ansys__pymapdl", "instruction": "Generate code that creates a pressure vessel using the pymapdl library. The code should start by launching MAPDL and setting the units to US Customary system using inches. Then, it should define the materials and element type, and create the geometry of the pressure vessel. After that, the code should create a mesh, apply boundary conditions and pressure, and solve the problem. The results should be post-processed to obtain the von-mises stress for the single static solution. The code should also plot the results and compare them with the results obtained from the legacy file reader. Finally, the code should stop MAPDL.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def compute_pressure(self, points, result):\n        \"\"\"Compute the value of the pressure at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the pressure should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the pressure at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        return 1j * result.omega * result.rho * self.compute_potential(points, results)\n\n# --- Snippet Separator ---\n\nclass BEMSolver:\n    \"\"\"\n    Solver for linear potential flow problems.\n\n    Parameters\n    ----------\n    green_function: AbstractGreenFunction, optional\n        Object handling the computation of the Green function.\n        (default: :class:`~capytaine.green_function.delhommeau.Delhommeau`)\n    engine: MatrixEngine, optional\n        Object handling the building of matrices and the resolution of linear systems with these matrices.\n        (default: :class:`~capytaine.bem.engines.BasicMatrixEngine`)\n\n    Attributes\n    ----------\n    exportable_settings : dict\n        Settings of the solver that can be saved to reinit the same solver later.\n    \"\"\"\n\n    def __init__(self, *, green_function=None, engine=None):\n        self.green_function = Delhommeau() if green_function is None else green_function\n        self.engine = BasicMatrixEngine() if engine is None else engine\n\n        try:\n            self.exportable_settings = {\n                **self.green_function.exportable_settings,\n                **self.engine.exportable_settings\n            }\n        except AttributeError:\n            pass\n\n    def __str__(self):\n        return f\"BEMSolver(engine={self.engine}, green_function={self.green_function})\"\n\n    def __repr__(self):\n        return self.__str__()\n\n    def _repr_pretty_(self, p, cycle):\n        p.text(self.__str__())\n\n    @classmethod\n    def from_exported_settings(settings):\n        raise NotImplementedError\n\n    def solve(self, problem, keep_details=True):\n        \"\"\"Solve the linear potential flow problem.\n\n        Parameters\n        ----------\n        problem: LinearPotentialFlowProblem\n            the problem to be solved\n        keep_details: bool, optional\n            if True, store the sources and the potential on the floating body in the output object\n            (default: True)\n\n        Returns\n        -------\n        LinearPotentialFlowResult\n            an object storing the problem data and its results\n        \"\"\"\n        LOG.info(\"Solve %s.\", problem)\n\n        S, K = self.engine.build_matrices(\n            problem.body.mesh, problem.body.mesh,\n            problem.free_surface, problem.water_depth, problem.wavenumber,\n            self.green_function\n        )\n        sources = self.engine.linear_solver(K, problem.boundary_condition)\n        potential = S @ sources\n        pressure = 1j * problem.omega * problem.rho * potential\n\n        forces = problem.body.integrate_pressure(pressure)\n\n        if not keep_details:\n            result = problem.make_results_container(forces)\n        else:\n            result = problem.make_results_container(forces, sources, potential, pressure)\n\n        LOG.debug(\"Done!\")\n\n        return result\n\n    def solve_all(self, problems, *, n_jobs=1, **kwargs):\n        \"\"\"Solve several problems.\n        Optional keyword arguments are passed to `BEMSolver.solve`.\n\n        Parameters\n        ----------\n        problems: list of LinearPotentialFlowProblem\n            several problems to be solved\n        n_jobs: int, optional (default: 1)\n            the number of jobs to run in parallel using the optional dependency `joblib`\n            By defaults: do not use joblib and solve sequentially.\n\n        Returns\n        -------\n        list of LinearPotentialFlowResult\n            the solved problems\n        \"\"\"\n        if n_jobs == 1:  # force sequential resolution\n            return [self.solve(pb, **kwargs) for pb in sorted(problems)]\n        else:\n            joblib = silently_import_optional_dependency(\"joblib\")\n            if joblib is None:\n                raise ImportError(f\"Setting the `n_jobs` argument to {n_jobs} requires the missing optional dependency 'joblib'.\")\n            groups_of_problems = LinearPotentialFlowProblem._group_for_parallel_resolution(problems)\n            groups_of_results = joblib.Parallel(n_jobs=n_jobs)(joblib.delayed(self.solve_all)(grp, n_jobs=1, **kwargs) for grp in groups_of_problems)\n            results = [res for grp in groups_of_results for res in grp]  # flatten the nested list\n            return results\n\n    def fill_dataset(self, dataset, bodies, *, n_jobs=1, **kwargs):\n        \"\"\"Solve a set of problems defined by the coordinates of an xarray dataset.\n\n        Parameters\n        ----------\n        dataset : xarray Dataset\n            dataset containing the problems parameters: frequency, radiating_dof, water_depth, ...\n        bodies : FloatingBody or list of FloatingBody\n            The body or bodies involved in the problems\n            They should all have different names.\n        n_jobs: int, optional (default: 1)\n            the number of jobs to run in parallel using the optional dependency `joblib`\n            By defaults: do not use joblib and solve sequentially.\n\n        Returns\n        -------\n        xarray Dataset\n        \"\"\"\n        attrs = {'start_of_computation': datetime.now().isoformat(),\n                 **self.exportable_settings}\n        problems = problems_from_dataset(dataset, bodies)\n        if 'theta' in dataset.coords:\n            results = self.solve_all(problems, keep_details=True, n_jobs=n_jobs)\n            kochin = kochin_data_array(results, dataset.coords['theta'])\n            dataset = assemble_dataset(results, attrs=attrs, **kwargs)\n            dataset.update(kochin)\n        else:\n            results = self.solve_all(problems, keep_details=False, n_jobs=n_jobs)\n            dataset = assemble_dataset(results, attrs=attrs, **kwargs)\n        return dataset\n\n\n    def compute_potential(self, points, result):\n        \"\"\"Compute the value of the potential at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the potential should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the potential at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_points(points, keep_mesh=True)\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        S, _ = self.green_function.evaluate(points, result.body.mesh, result.free_surface, result.water_depth, result.wavenumber)\n        potential = S @ result.sources  # Sum the contributions of all panels in the mesh\n        return potential.reshape(output_shape)\n\n\n    def compute_velocity(self, points, result):\n        \"\"\"Compute the value of the velocity vector at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the velocity should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (3,) or (N,, 3) or (nx, ny, nz, 3) or (mesh.nb_faces, 3) depending of the kind of input\n            The value of the velocity at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_points(points, keep_mesh=True)\n\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        _, gradG = self.green_function.evaluate(points, result.body.mesh, result.free_surface, result.water_depth, result.wavenumber,\n                                                early_dot_product=False)\n        velocities = np.einsum('ijk,j->ik', gradG, result.sources)  # Sum the contributions of all panels in the mesh\n        return velocities.reshape((*output_shape, 3))\n\n\n    def compute_pressure(self, points, result):\n        \"\"\"Compute the value of the pressure at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (3,) or (N, 3), or 3-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the pressure should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the pressure at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        return 1j * result.omega * result.rho * self.compute_potential(points, results)\n\n\n    def compute_free_surface_elevation(self, points, result):\n        \"\"\"Compute the value of the free surface elevation at given points for a previously solved potential flow problem.\n\n        Parameters\n        ----------\n        points: array of shape (2,) or (N, 2), or 2-ple of arrays returned by meshgrid, or cpt.Mesh or cpt.CollectionOfMeshes object\n            Coordinates of the point(s) at which the free surface elevation should be computed\n        results: LinearPotentialFlowResult\n            The return of the BEM solver\n\n        Returns\n        -------\n        complex-valued array of shape (1,) or (N,) or (nx, ny, nz) or (mesh.nb_faces,) depending of the kind of input\n            The value of the free surface elevation at the points\n\n        Raises\n        ------\n        Exception: if the :code:`LinearPotentialFlowResult` object given as input does not contain the source distribution.\n        \"\"\"\n        points, output_shape = _normalize_free_surface_points(points, keep_mesh=True)\n\n        fs_elevation = 1j*result.omega/result.g * self.compute_potential(points, result)\n        return fs_elevation.reshape(output_shape)\n\n\n    ## Legacy\n\n    def get_potential_on_mesh(self, result, mesh, chunk_size=50):\n        \"\"\"Compute the potential on a mesh for the potential field of a previously solved problem.\n        Since the interaction matrix does not need to be computed in full to compute the matrix-vector product,\n        only a few lines are evaluated at a time to reduce the memory cost of the operation.\n\n        The newer method :code:`compute_potential` should be prefered in the future.\n\n        Parameters\n        ----------\n        result : LinearPotentialFlowResult\n            the return of the BEM solver\n        mesh : Mesh or CollectionOfMeshes\n            a mesh\n        chunk_size: int, optional\n            Number of lines to compute in the matrix.\n            (legacy, should be passed as an engine setting instead).\n\n        Returns\n        -------\n        array of shape (mesh.nb_faces,)\n            potential on the faces of the mesh\n\n        Raises\n        ------\n        Exception: if the :code:`Result` object given as input does not contain the source distribution.\n        \"\"\"\n        LOG.info(f\"Compute potential on {mesh.name} for {result}.\")\n\n        if result.sources is None:\n            raise Exception(f\"\"\"The values of the sources of {result} cannot been found.\n            They probably have not been stored by the solver because the option keep_details=True have not been set.\n            Please re-run the resolution with this option.\"\"\")\n\n        if chunk_size > mesh.nb_faces:\n            S = self.engine.build_S_matrix(\n                mesh,\n                result.body.mesh,\n                result.free_surface, result.water_depth, result.wavenumber,\n                self.green_function\n            )\n            phi = S @ result.sources\n\n        else:\n            phi = np.empty((mesh.nb_faces,), dtype=np.complex128)\n            for i in range(0, mesh.nb_faces, chunk_size):\n                faces_to_extract = list(range(i, min(i+chunk_size, mesh.nb_faces)))\n                S = self.engine.build_S_matrix(\n                    mesh.extract_faces(faces_to_extract),\n                    result.body.mesh,\n                    result.free_surface, result.water_depth, result.wavenumber,\n                    self.green_function\n                )\n                phi[i:i+chunk_size] = S @ result.sources\n\n        LOG.debug(f\"Done computing potential on {mesh.name} for {result}.\")\n\n        return phi\n\n    def get_free_surface_elevation(self, result, free_surface, keep_details=False):\n        \"\"\"Compute the elevation of the free surface on a mesh for a previously solved problem.\n\n        The newer method :code:`compute_free_surface_elevation` should be prefered in the future.\n\n        Parameters\n        ----------\n        result : LinearPotentialFlowResult\n            the return of the solver\n        free_surface : FreeSurface\n            a meshed free surface\n        keep_details : bool, optional\n            if True, keep the free surface elevation in the LinearPotentialFlowResult (default:False)\n\n        Returns\n        -------\n        array of shape (free_surface.nb_faces,)\n            the free surface elevation on each faces of the meshed free surface\n\n        Raises\n        ------\n        Exception: if the :code:`Result` object given as input does not contain the source distribution.\n        \"\"\"\n        fs_elevation = 1j*result.omega/result.g * self.get_potential_on_mesh(result, free_surface.mesh)\n        if keep_details:\n            result.fs_elevation[free_surface] = fs_elevation\n        return fs_elevation\n\n# --- Snippet Separator ---\n\ndef create_read_in_code(program, psy_data, read_write_info, postfix):\n        '''This function creates the code that reads in the NetCDF file\n        produced during extraction. For each:\n\n        - variable that is read-only, it will declare the symbol and add code\n          that reads in the variable using the PSyData library.\n        - variable that is read and written, it will create code to read in the\n          variable that is read, and create a new variable with the same name\n          and \"_post\" added which is read in to store the values from the\n          NetCDF file after the instrumented region was executed. In the end,\n          the variable that was read and written should have the same value\n          as the corresponding \"_post\" variable.\n        - variable that is written only, it will create a variable with \"_post\"\n          as postfix that reads in the output data from the NetCDF file. It\n          then also declares a variable without postfix (which will be the\n          parameter to the function), allocates it based on the shape of\n          the corresponding \"_post\" variable, and initialises it with 0.\n\n        :param program: the PSyIR Routine to which any code must \\\n            be added. It also contains the symbol table to be used.\n        :type program: :py:class:`psyclone.psyir.nodes.Routine`\n        :param psy_data: the PSyData symbol to be used.\n        :type psy_data: :py:class:`psyclone.psyir.symbols.DataSymbol`\n        :param read_write_info: information about all input and output \\\n            parameters.\n        :type read_write_info: :py:class:`psyclone.psyir.tools.ReadWriteInfo`\n        :param str postfix: a postfix that is added to a variable to \\\n            create the corresponding variable that stores the output \\\n            value from the kernel data file.\n\n        :returns: a list with all output parameters, i.e. variables that \\\n            need to be verified after executing the kernel. Each entry is \\\n            a 2-tuple containing the symbol of the computed variable, and \\\n            the symbol of the variable that contains the value read from \\\n            the file.\n        :rtype: list of 2-tuples of \\\n            :py:class:`psyclone.psyir.symbols.Symbol`\n\n        '''\n        symbol_table = program.scope.symbol_table\n        read_var = f\"{psy_data.name}%ReadVariable\"\n\n        # Collect all output symbols to later create the tests for\n        # correctness. This list stores 2-tuples: first one the\n        # variable that stores the output from the kernel, the second\n        # one the variable that stores the output values read from the\n        # file. The content of these two variables should be identical\n        # at the end.\n        output_symbols = []\n\n        # First handle variables that are read:\n        # -------------------------------------\n        for signature in read_write_info.signatures_read:\n            # Find the right symbol for the variable. Note that all variables\n            # in the input and output list have been detected as being used\n            # when the variable accesses were analysed. Therefore, these\n            # variables have References, and will already have been declared\n            # in the symbol table (in add_all_kernel_symbols).\n            sig_str = str(signature)\n            sym = symbol_table.lookup_with_tag(sig_str)\n            name_lit = Literal(sig_str, CHARACTER_TYPE)\n            ExtractDriverCreator.add_call(program, read_var,\n                                          [name_lit, Reference(sym)])\n\n        # Then handle all variables that are written (note that some\n        # variables might be read and written)\n        for signature in read_write_info.signatures_written:\n            # Find the right symbol for the variable. Note that all variables\n            # in the input and output list have been detected as being used\n            # when the variable accesses were analysed. Therefore, these\n            # variables have References, and will already have been declared\n            # in the symbol table (in add_all_kernel_symbols).\n            sig_str = str(signature)\n            sym = symbol_table.lookup_with_tag(sig_str)\n\n            # The variable is written (and maybe read as well)\n            # ------------------------------------------------\n            # Declare a 'post' variable of the same type and\n            # read in its value.\n            post_name = sig_str+postfix\n            post_sym = symbol_table.new_symbol(post_name,\n                                               symbol_type=DataSymbol,\n                                               datatype=sym.datatype)\n            ExtractDriverCreator.add_call(program, read_var,\n                                          [Literal(post_name, CHARACTER_TYPE),\n                                           Reference(post_sym)])\n\n            # Now if a variable is written to, but not read, the variable\n            # is not allocated. So we need to allocate it and set it to 0.\n            if not read_write_info.is_read(signature):\n                if isinstance(post_sym.datatype, ArrayType):\n                    alloc = IntrinsicCall.create(\n                        IntrinsicCall.Intrinsic.ALLOCATE,\n                        [Reference(sym), (\"mold\", Reference(post_sym))])\n                    program.addchild(alloc)\n                set_zero = Assignment.create(Reference(sym),\n                                             Literal(\"0\", INTEGER_TYPE))\n                program.addchild(set_zero)\n            output_symbols.append((sym, post_sym))\n        return output_symbols\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a pressure vessel using the pymapdl library. The code should start by launching MAPDL and setting the units to US Customary system using inches. Then, it should define the materials and element type, and create the geometry of the pressure vessel. After that, the code should create a mesh, apply boundary conditions and pressure, and solve the problem. The results should be post-processed to obtain the von-mises stress for the single static solution. The code should also plot the results and compare them with the results obtained from the legacy file reader. Finally, the code should stop MAPDL.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 65, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that tests radial transport of n_re with a constant scalar diffusion coefficient using the DREAM library. The code should set up a simulation with specific parameters such as initial and final temperatures, time steps, ion species, electric field, and cold electron temperature. It should also set up a radial grid, time stepper, and ions. The code should then set the E_field and cold electron temperature, and enable the hot tail grid. The code should also set up the transport settings, and finally, run the simulation. The code should also include conditions for different transport modes and whether the hot tail grid is enabled or not.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class ColdElectronTemperature(PrescribedParameter,PrescribedInitialParameter,UnknownQuantity):\n\n    def __init__(self, settings, ttype=TYPE_PRESCRIBED, temperature=None, radius=0, times=0, recombination=RECOMBINATION_RADIATION_NEGLECTED):\n        \"\"\"\n        Constructor.\n        \"\"\"\n        super().__init__(settings=settings)\n\n        self.setType(ttype=ttype)\n\n        self.temperature = None\n        self.radius = None\n        self.times  = None\n\n        self.transport = TransportSettings(kinetic=False)\n        self.recombination = recombination\n\n        if (ttype == TYPE_PRESCRIBED) and (temperature is not None):\n            self.setPrescribedData(temperature=temperature, radius=radius, times=times)\n        elif ttype == TYPE_SELFCONSISTENT:\n            self.setInitialProfile(temperature=temperature, radius=radius)\n\n\n    ###################\n    # SETTERS\n    ###################\n    def setInitialProfile(self, temperature, radius=0):\n        \"\"\"\n        Sets the initial temperature profile T=T(r) for when the temperature is\n        evolved self-consistently.\n\n        :param temperature: Scalar or vector giving the initial temperature profile.\n        :param radius: If ``temperature`` is a vector, contains the corresponding radial grid on which ``temperature`` is defined.\n        \"\"\"\n        _data, _rad = self._setInitialData(data=temperature, radius=radius)\n\n        self.temperature = _data\n        self.radius      = _rad\n        self.times       = None\n\n        self.verifySettingsPrescribedInitialData()\n\n\n    def setPrescribedData(self, temperature, radius=0, times=0):\n        \"\"\"\n        Prescribes a temperature evolution in time and space.\n\n        :param temperature: Scalar, vector or matrix giving the temperature throughout the simulation.\n        :param radius: If ``temperature`` is a function of radius, contains the radial grid on which it is defined.\n        :param times: If ``temperature`` is a function of time, contains the time grid on which it is defined.\n        \"\"\"\n        _t, _rad, _tim = self._setPrescribedData(temperature, radius, times)\n        self.temperature = _t\n        self.radius      = _rad\n        self.times       = _tim\n\n        self.verifySettingsPrescribedData()\n\n\n    def setType(self, ttype):\n        \"\"\"\n        Specifies whether to evolve the electron temperature according to a\n        prescribed function, or self-consistently.\n\n        :param ttype: Type of evolution. Can take one of the following values:\n\n        - ``TYPE_PRESCRIBED``: Evolve according to prescribed function.\n        - ``TYPE_SELFCONSISTENT``: Evolve self-consistently.\n        \"\"\"\n        if ttype == TYPE_PRESCRIBED:\n            self.type = ttype\n        elif ttype == TYPE_SELFCONSISTENT:\n            self.type = ttype\n\n            # Set T=0 if 'setInitialProfile' has not been previously called\n            # (if 'setInitialProfile()' has been called, 'self.radius != None'\n            # and 'self.times == None')\n            if (self.radius) is None or (self.times is not None):\n                self.setInitialProfile(temperature=-1)\n        else:\n            raise EquationException(\"T_cold: Unrecognized cold electron temperature type: {}\".format(self.type))\n\n\n    def setRecombinationRadiation(self, recombination=RECOMBINATION_RADIATION_NEGLECTED):\n        \"\"\"\n        Specify whether or not to include recombination radiation when evolving\n        the temperature self-consistently.\n        \"\"\"\n        self.recombination = recombination\n\n\n    def fromdict(self, data):\n        self.type = data['type']\n\n        if self.type == TYPE_PRESCRIBED:\n            self.temperature = data['data']['x']\n            self.radius = data['data']['r']\n            self.times = data['data']['t']\n        elif self.type == TYPE_SELFCONSISTENT:\n            self.temperature = data['init']['x']\n            self.radius = data['init']['r']\n\n            if 'transport' in data:\n                self.transport.fromdict(data['transport'])\n        else:\n            raise EquationException(\"T_cold: Unrecognized cold electron temperature type: {}\".format(self.type))\n        if 'recombination' in data:\n            self.recombination = data['recombination']\n\n        self.verifySettings()\n\n\n    def todict(self):\n        \"\"\"\n        Returns a Python dictionary containing all settings of\n        this ColdElectrons object.\n        \"\"\"\n        data = { 'type': self.type }\n        data['recombination'] = self.recombination\n        if self.type == TYPE_PRESCRIBED:\n            data['data'] = {\n                'x': self.temperature,\n                'r': self.radius,\n                't': self.times\n            }\n        elif self.type == TYPE_SELFCONSISTENT:\n            data['init'] = {\n                'x': self.temperature,\n                'r': self.radius\n            }\n            data['transport'] = self.transport.todict()\n        else:\n            raise EquationException(\"T_cold: Unrecognized cold electron temperature type: {}\".format(self.type))\n\n        return data\n\n\n    def verifySettings(self):\n        \"\"\"\n        Verify that the settings of this unknown are correctly set.\n        \"\"\"\n        if self.type == TYPE_PRESCRIBED:\n            if type(self.temperature) != np.ndarray:\n                raise EquationException(\"T_cold: Temperature prescribed, but no temperature data provided.\")\n            elif type(self.times) != np.ndarray:\n                raise EquationException(\"T_cold: Temperature prescribed, but no time data provided, or provided in an invalid format.\")\n            elif type(self.radius) != np.ndarray:\n                raise EquationException(\"T_cold: Temperature prescribed, but no radial data provided, or provided in an invalid format.\")\n\n            self.verifySettingsPrescribedData()\n        elif self.type == TYPE_SELFCONSISTENT:\n            if type(self.temperature) != np.ndarray:\n                raise EquationException(\"T_cold: Temperature prescribed, but no temperature data provided.\")\n            elif type(self.radius) != np.ndarray:\n                raise EquationException(\"T_cold: Temperature prescribed, but no radial data provided, or provided in an invalid format.\")\n\n            self.verifySettingsPrescribedInitialData()\n            self.transport.verifySettings()\n        else:\n            raise EquationException(\"T_cold: Unrecognized equation type specified: {}.\".format(self.type))\n\n\n    def verifySettingsPrescribedData(self):\n        self._verifySettingsPrescribedData('T_cold', self.temperature, self.radius, self.times)\n\n    def verifySettingsPrescribedInitialData(self):\n        self._verifySettingsPrescribedInitialData('T_cold', data=self.temperature, radius=self.radius)\n\n# --- Snippet Separator ---\n\ndef setPrescribedData(self, density, radius=0, times=0):\n        \"\"\"\n        Prescribe the cold electron density in time and space.\n\n        :param density: Cold electron density (2D array (nt, nr) or scalar (=> constant and uniform in time and space))\n        :param radius:  Radial grid on which the cold electron density is defined.\n        :param times:   Time grid on which the cold electron density is defined.\n        \"\"\"\n        _data, _rad, _tim = self._setPrescribedData(density, radius, times)\n        self.density = _data\n        self.radius = _rad\n        self.times  = _tim\n\n        self.setType(TYPE_PRESCRIBED)\n\n        self.verifySettingsPrescribedData()\n\n# --- Snippet Separator ---\n\ndef addIon(self, name, Z, iontype=IONS_PRESCRIBED, Z0=None, isotope=0, SPIMolarFraction=-1, opacity_mode=ION_OPACITY_MODE_TRANSPARENT, \n        charged_diffusion_mode=ION_CHARGED_DIFFUSION_MODE_NONE, charged_prescribed_diffusion=None, rChargedPrescribedDiffusion=None, tChargedPrescribedDiffusion=None,\n        neutral_diffusion_mode=ION_NEUTRAL_DIFFUSION_MODE_NONE, neutral_prescribed_diffusion=None, rNeutralPrescribedDiffusion=None, tNeutralPrescribedDiffusion=None,\n        charged_advection_mode=ION_CHARGED_ADVECTION_MODE_NONE, charged_prescribed_advection=None, rChargedPrescribedAdvection=None, tChargedPrescribedAdvection=None,\n        neutral_advection_mode=ION_NEUTRAL_ADVECTION_MODE_NONE, neutral_prescribed_advection=None, rNeutralPrescribedAdvection=None, tNeutralPrescribedAdvection=None,\n        t_transp_expdecay_all_cs = None, t_transp_start_expdecay_all_cs = 0, diffusion_initial_all_cs = None, diffusion_final_all_cs = 0, advection_initial_all_cs = None, advection_final_all_cs = 0, r_expdecay_all_cs = None, t_expdecay_all_cs = None, \n        T=None, n=None, r=None, t=None, tritium=False, hydrogen=False):\n\n        \"\"\"\n        Adds a new ion species to the plasma.\n\n        :param str name:        Name by which the ion species will be referred to.\n        :param int Z:           Ion charge number.\n        :param int isotope:            Ion mass number.\n        :param int iontype:     Method to use for evolving ions in time.\n        :param int Z0:          Charge state to populate (used for populating exactly one charge state for the ion).\n        :param n:               Ion density (can be either a scalar, 1D array or 2D array, depending on the other input parameters)\n        :param float SPIMolarFraction: Molar fraction of the SPI injection (if any). A negative value means that this species is not part of the SPI injection \n        :param numpy.ndarray r: Radial grid on which the input density is defined.\n        :param T:               Ion initial temperature (can be scalar for uniform temperature, otherwise 1D array matching `r` in size)\n        :param numpy.ndarray r: Radial grid on which the input density and temperature is defined.\n        :param numpy.ndarray t: Time grid on which the input density is defined.\n        :param bool tritium:    If ``True``, the ion species is treated as Tritium.\n        :param bool hydrogen:   If ``True``, the ion species is treated as Hydrogen (single proton).\n        \"\"\"\n        if (self.r is not None) and (r is not None) and (np.any(self.r[:] != r[:])):\n            if self.r.size == 1:\n                self.changeRadialGrid(r)\n            else:\n                raise EquationException(\"The radial grid must be the same for all ion species.\")\n        if (self.t is not None) and (t is not None) and (np.any(self.t != t)):\n            raise EquationException(\"The time grid must be the same for all ion species.\")\n\n        if (self.rChargedPrescribedDiffusion is not None) and (rChargedPrescribedDiffusion is not None) and (np.any(self.rChargedPrescribedDiffusion != rChargedPrescribedDiffusion)):\n            raise EquationException(\"The radial grid for the prescribed charged diffusion must be the same for all ion species.\")\n        if (self.tChargedPrescribedDiffusion is not None) and (tChargedPrescribedDiffusion is not None) and (np.any(self.tChargedPrescribedDiffusion != tChargedPrescribedDiffusion)):\n            raise EquationException(\"The time grid for the prescribed charged diffusion must be the same for all ion species.\")\n\n        if (self.rNeutralPrescribedDiffusion is not None) and (rNeutralPrescribedDiffusion is not None) and (np.any(self.rNeutralPrescribedDiffusion != rNeutralPrescribedDiffusion)):\n            raise EquationException(\"The radial grid for the prescribed neutral diffusion must be the same for all ion species.\")\n        if (self.tNeutralPrescribedDiffusion is not None) and (tNeutralPrescribedDiffusion is not None) and (np.any(self.tNeutralPrescribedDiffusion != tNeutralPrescribedDiffusion)):\n            raise EquationException(\"The time grid for the prescribed neutral diffusion must be the same for all ion species.\")\n\n        if T is not None:\n            self.typeTi = IONS_T_I_INCLUDE\n\n        ion = IonSpecies(settings=self.settings, name=name, Z=Z, ttype=iontype, Z0=Z0, isotope=isotope, SPIMolarFraction=SPIMolarFraction, opacity_mode=opacity_mode, \n            charged_diffusion_mode=charged_diffusion_mode, charged_prescribed_diffusion=charged_prescribed_diffusion, rChargedPrescribedDiffusion=rChargedPrescribedDiffusion, tChargedPrescribedDiffusion=tChargedPrescribedDiffusion,\n            neutral_diffusion_mode=neutral_diffusion_mode, neutral_prescribed_diffusion=neutral_prescribed_diffusion, rNeutralPrescribedDiffusion=rNeutralPrescribedDiffusion, tNeutralPrescribedDiffusion=tNeutralPrescribedDiffusion,           \n            charged_advection_mode=charged_advection_mode, charged_prescribed_advection=charged_prescribed_advection, rChargedPrescribedAdvection=rChargedPrescribedAdvection, tChargedPrescribedAdvection=tChargedPrescribedAdvection,\n            neutral_advection_mode=neutral_advection_mode, neutral_prescribed_advection=neutral_prescribed_advection, rNeutralPrescribedAdvection=rNeutralPrescribedAdvection, tNeutralPrescribedAdvection=tNeutralPrescribedAdvection,\n            t_transp_expdecay_all_cs = t_transp_expdecay_all_cs, t_transp_start_expdecay_all_cs = t_transp_start_expdecay_all_cs,\n            diffusion_initial_all_cs = diffusion_initial_all_cs, diffusion_final_all_cs = diffusion_final_all_cs, \n            advection_initial_all_cs = advection_initial_all_cs, advection_final_all_cs = advection_final_all_cs, \n            r_expdecay_all_cs = r_expdecay_all_cs, t_expdecay_all_cs = t_expdecay_all_cs,            \n            T=T, n=n, r=r, t=t, interpr=self.r, interpt=None, tritium=tritium, hydrogen=hydrogen)\n\n        self.ions.append(ion)\n\n        self.r = ion.getR()\n        if ion.getTime() is not None:\n            self.t = ion.getTime()\n\n        if charged_diffusion_mode==ION_CHARGED_DIFFUSION_MODE_PRESCRIBED:\n            self.rChargedPrescribedDiffusion = ion.getRChargedPrescribedDiffusion()\n            self.tChargedPrescribedDiffusion = ion.getTChargedPrescribedDiffusion()\n\n        if neutral_diffusion_mode==ION_NEUTRAL_DIFFUSION_MODE_PRESCRIBED:\n            self.rNeutralPrescribedDiffusion = ion.getRNeutralPrescribedDiffusion()\n            self.tNeutralPrescribedDiffusion = ion.getTNeutralPrescribedDiffusion()\n\n        if charged_advection_mode==ION_CHARGED_ADVECTION_MODE_PRESCRIBED:\n            self.rChargedPrescribedAdvection = ion.getRChargedPrescribedAdvection()\n            self.tChargedPrescribedAdvection = ion.getTChargedPrescribedAdvection()\n\n        if neutral_advection_mode==ION_NEUTRAL_ADVECTION_MODE_PRESCRIBED:\n            self.rNeutralPrescribedAdvection = ion.getRNeutralPrescribedAdvection()\n            self.tNeutralPrescribedAdvection = ion.getTNeutralPrescribedAdvection()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that tests radial transport of n_re with a constant scalar diffusion coefficient using the DREAM library. The code should set up a simulation with specific parameters such as initial and final temperatures, time steps, ion species, electric field, and cold electron temperature. It should also set up a radial grid, time stepper, and ions. The code should then set the E_field and cold electron temperature, and enable the hot tail grid. The code should also set up the transport settings, and finally, run the simulation. The code should also include conditions for different transport modes and whether the hot tail grid is enabled or not.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 66, "repo_full_name": "hewlettpackard__oneview-python", "instruction": "Generate code that interacts with the OneViewClient from the hpeOneView library. The code should establish a connection to the client using a configuration file. It should then create a scope and a user with specific permissions. The code should also create multiple users with different permissions. After creating the users, the code should update the user's password, add a new role to an existing user, update the roles of a user, and remove certain roles from a user. The code should also retrieve a user by their username, retrieve all users, validate if a full name or username is already in use, get the roles associated with a user, and get users by their role. Finally, the code should delete a single user and multiple users.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Users(Resource):\n    \"\"\"\n    Users API client.\n\n    \"\"\"\n\n    URI = '/rest/users'\n\n    def __init__(self, connection, data=None):\n        super(Users, self).__init__(connection, data)\n        self.__default_values = {\n            'type': 'UserAndRoles'\n        }\n\n    def validate_user_name(self, user_name, timeout=-1):\n        \"\"\"\n        Verifies if a userName is already in use.\n\n        Args:\n            user_name (str):\n                The userName to be verified.\n            timeout:\n                Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation in\n                OneView, just stops waiting for its completion.\n\n        Returns: True if user name is in use, False if it is not.\n        \"\"\"\n        uri = self.URI + '/validateLoginName/' + user_name\n        return self.create(uri=uri)\n\n    def validate_full_name(self, full_name, timeout=-1):\n        \"\"\"\n        Verifies if a fullName is already in use.\n\n        Args:\n            full_name (str):\n                The fullName to be verified.\n            timeout:\n                Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation in\n                OneView, just stops waiting for its completion.\n\n        Returns: True if full name is in use, False if it is not.\n        \"\"\"\n        uri = self.URI + '/validateUserName/' + full_name\n        return self.create(uri=uri)\n\n    def change_password(self, resource):\n        \"\"\"\n        Change one's own password\n\n        Args:\n            resource (dict): Object to change password\n            timeout:\n                Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation in\n                OneView, just stops waiting for its completion.\n\n        \"\"\"\n        uri = self._helper.build_uri('changePassword')\n        return self._helper.create(resource, uri)\n\n    def get_role_associated_with_userName(self, userName):\n        \"\"\"\n        Gets a user by userName.\n\n        Args:\n            name (str): userName of the user.\n\n        Returns:\n            dict object: User\n        \"\"\"\n\n        users = self.get_all()\n\n        result = [x for x in users if x['userName'] == userName]\n        resource = result[0] if result else None\n        if resource:\n            uri = self.URI + '/role/' + userName\n            data = self.get_by_uri(uri).data\n            result = data['members']\n            return result\n        else:\n            return None\n\n    def get_by_userName(self, name):\n        \"\"\"\n        Gets a complete json body for username\n\n        Args:\n          name (str): userName of the user\n\n        Returns:\n           dict object: User\n        \"\"\"\n\n        uri = self._helper.build_uri(name)\n        try:\n            resource = self.get_by_uri(uri)\n        except HPEOneViewException:\n            resource = None\n\n        return resource\n\n    def get_user_by_role(self, rolename):\n        \"\"\"\n        Gets all the users associated with this role\n\n        Args:\n          rolename (str): rolename of the user\n\n        Returns:\n          list: User\n        \"\"\"\n\n        rolename = quote(rolename)\n        uri = self.URI + '/roles/users/' + rolename\n        data = self.get_by_uri(uri).data\n        result = []\n        for i in range(0, len(data['members'])):\n            result.append(data[\"members\"][i])\n\n        return result\n\n    def create_multiple_user(self, data):\n        \"\"\"\n        Create a multiple user\n\n        Agrs:\n          data (list): multiple user\n\n        Returns:\n          dict object: User\n        \"\"\"\n\n        uri = self.URI + '?multiResource=true'\n        return self.create(data, uri)\n\n    def update(self, data=None, timeout=-1, custom_headers=None, force=False):\n        \"\"\"\n        Makes a PUT request to update a resource when a request body is required.\n\n        Args:\n            data (dict): Data to update the resource.\n            timeout: Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation\n                in OneView; it just stops waiting for its completion.\n            custom_headers: Allows to add custom HTTP headers.\n            force: Force the update operation.\n\n        Returns:\n            A dict with the updated resource data.\n        \"\"\"\n\n        resource = deepcopy(self.data)\n        resource.update(data)\n\n        self.data = self._helper.update(resource, self.URI, force, timeout, custom_headers)\n\n        return self\n\n    def add_role_to_userName(self, username, data):\n        \"\"\"\n        Add roles to a given user name\n\n        Args:\n          username (str): userName of the user\n          data (list): roles to be added\n\n        Returns:\n          dict object: User\n        \"\"\"\n\n        uri = self.URI + '/' + username + '/roles?multiResource=true'\n        return self.create(data, uri)\n\n    def update_role_to_userName(self, username, data):\n        \"\"\"\n        Update roles to a given user name\n\n        Agrs:\n          username (str): username of the user\n          data (list): roles to be updated\n\n        Return:\n          dict: User\n        \"\"\"\n\n        uri = self.URI + '/' + username + '/roles?multiResource=true'\n        return self._helper.update(data, uri)\n\n    def remove_role_from_username(self, username, data):\n        \"\"\"\n        Removes a specified role from the username\n\n        Args:\n          username (str): username of the user\n          data (str/list): list role to be removed from user\n\n        Return:\n          boolean\n        \"\"\"\n\n        rolelist_query = self.query_filter(data)\n        uri = self.URI + '/roles?filter' + '=\"userName=\\'{}\\'{}'.format(username, rolelist_query)\n        return self._helper.delete(uri)\n\n    def delete_multiple_user(self, data):\n        \"\"\"\n        Delete the multiple users\n\n        Args:\n          data (list): List of users to be deleted\n\n        Returns:\n          None\n        \"\"\"\n\n        uri = self.URI + '?query='\n\n        for i in range(0, len(data)):\n            uri = uri + '(loginname=\\'{}\\')'.format(data[i])\n            if i == len(data) - 1:\n                break\n            uri = uri + quote(' or ')\n        self._helper.delete(uri)\n\n    def query_filter(self, filters):\n\n        formated_filter = ''\n        base_query = \"\\\"&filter=\\\"roleName=\\'{}\\'\\\"\"\n        if isinstance(filters, list):\n            for role in filters:\n                formated_filter += base_query.format(quote(role))\n            return formated_filter\n\n        if isinstance(filters, str):\n            return base_query.format(quote(filters))\n\n# --- Snippet Separator ---\n\ndef get_user_by_role(self, rolename):\n        \"\"\"\n        Gets all the users associated with this role\n\n        Args:\n          rolename (str): rolename of the user\n\n        Returns:\n          list: User\n        \"\"\"\n\n        rolename = quote(rolename)\n        uri = self.URI + '/roles/users/' + rolename\n        data = self.get_by_uri(uri).data\n        result = []\n        for i in range(0, len(data['members'])):\n            result.append(data[\"members\"][i])\n\n        return result\n\n# --- Snippet Separator ---\n\nclass AuthPolicyBuilder(Builder):\n    def __init__(self, orgs, roles, groups, disabled):\n        \"\"\"Build authorization.json.\n\n        Creates and writes authorization.json to the server's startup directory with the authorization policy\n        defining the groups each org is in and the admin client roles which controls the allowed rights. The\n        participant information from project.yml is included in authorization.json with what orgs, groups, and roles\n        are associated with each participant. This builder also checks for errors if the arguments are specified\n        incorrectly.\n\n        Args:\n            orgs: authorization configuration for orgs (it may be helpful to build this section with the UI)\n            roles: authorization configuration for roles (it may be helpful to build this section with the UI)\n            groups: authorization configuration for groups (it may be helpful to build this section with the UI)\n            disabled: if true, all users are super with all privileges\n        \"\"\"\n        self.orgs = orgs\n        self.roles = roles\n        self.groups = groups\n        self.disabled = disabled\n\n    def build(self, project: Project, ctx: dict):\n        authz = {\"version\": \"1.0\"}\n        authz[\"roles\"] = self.roles\n        authz[\"groups\"] = self.groups\n        users = dict()\n        for admin in project.get_participants_by_type(\"admin\", first_only=False):\n            if admin.org not in self.orgs:\n                raise ValueError(f\"Admin {admin.name}'s org {admin.org} not defined in AuthPolicy\")\n            if self.disabled:\n                users[admin.name] = {\"org\": admin.org, \"roles\": [\"super\"]}\n            else:\n                for role in admin.props.get(\"roles\", {}):\n                    if role not in self.roles:\n                        raise ValueError(f\"Admin {admin.name}'s role {role} not defined in AuthPolicy\")\n                users[admin.name] = {\"org\": admin.org, \"roles\": admin.props.get(\"roles\")}\n        authz[\"users\"] = users\n        authz[\"orgs\"] = self.orgs\n        servers = project.get_participants_by_type(\"server\", first_only=False)\n        for server in servers:\n            if server.org not in self.orgs:\n                raise ValueError(f\"Server {server.name}'s org {server.org} not defined in AuthPolicy\")\n            sites = {\"server\": server.org}\n            for client in project.get_participants_by_type(\"client\", first_only=False):\n                if client.org not in self.orgs:\n                    raise ValueError(f\"client {client.name}'s org {client.org} not defined in AuthPolicy\")\n                sites[client.name] = client.org\n            authz[\"sites\"] = sites\n            authz.update(ctx.get(\"authz_def\"))\n            dest_dir = self.get_kit_dir(server, ctx)\n            with open(os.path.join(dest_dir, \"authorization.json\"), \"wt\") as f:\n                f.write(json.dumps(authz))\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that interacts with the OneViewClient from the hpeOneView library. The code should establish a connection to the client using a configuration file. It should then create a scope and a user with specific permissions. The code should also create multiple users with different permissions. After creating the users, the code should update the user's password, add a new role to an existing user, update the roles of a user, and remove certain roles from a user. The code should also retrieve a user by their username, retrieve all users, validate if a full name or username is already in use, get the roles associated with a user, and get users by their role. Finally, the code should delete a single user and multiple users.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 67, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that uses the pyaedt library to create a 2D Extractor CPWG (coplanar waveguide with ground) design and run a simulation. The code should import the necessary libraries, set the non-graphical mode, launch AEDT and 2D Extractor, define variables, create primitives, create a signal, create a coplanar ground, create a reference ground plane, create a dielectric, create a conformal coating, assign a conductor to the signal, create a reference ground, assign the Huray model on the signal, create the setup, analyze it, plot solution data, save the project and close AEDT.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def assign_net(self, objects, net_name=None, net_type=\"Signal\"):\n        \"\"\"Assign a net to a list of objects.\n\n        Parameters\n        ----------\n        objects : list, str\n            List of objects to assign the net to. It can be a single object.\n        net_name : str, optional\n            Name of the net. The default is ```None``, in which case the\n            default name is used.\n        net_type : str, bool\n            Type of net to create. Options are ``\"Signal\"``, ``\"Ground\"`` and ``\"Floating\"``.\n            The default is ``\"Signal\"``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Source object.\n\n        References\n        ----------\n\n        >>> oModule.AssignSignalNet\n        >>> oModule.AssignGroundNet\n        >>> oModule.AssignFloatingNet\n\n        Examples\n        --------\n        >>> from pyaedt import Q3d\n        >>> q3d = Q3d()\n        >>> box = q3d.modeler.create_box([30, 30, 30], [10, 10, 10], name=\"mybox\")\n        >>> net_name = \"my_net\"\n        >>> net = q3d.assign_net(box, net_name)\n        \"\"\"\n        objects = self.modeler.convert_to_selections(objects, True)\n        if not net_name:\n            net_name = generate_unique_name(\"Net\")\n        props = OrderedDict({\"Objects\": objects})\n        type_bound = \"SignalNet\"\n        if net_type.lower() == \"ground\":\n            type_bound = \"GroundNet\"\n        elif net_type.lower() == \"floating\":\n            type_bound = \"FloatingNet\"\n        bound = BoundaryObject(self, net_name, props, type_bound)\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n# --- Snippet Separator ---\n\nclass Emit(FieldAnalysisEmit, object):\n    \"\"\"Provides the Emit application interface.\n\n    .. note::\n       This object creates only a skeleton for an empty design.\n       It has very limited functionalities, and no methods\n       are implemented yet.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open.  The default is ``None``. If\n        ``None``, try to get an active project and, if no projects are\n        present, create an empty project.\n    designname : str, optional\n        Name of the design to select. The default is ``None``. If\n        ``None``, try to get an active design and, if no designs are\n        present, create an empty design.\n    solution_type : str, optional\n        Solution type to apply to the design. The default is ``None``, in which\n        case the default type is applied.\n    setup_name : str, optional\n       Name of the setup to use as the nominal. The default is\n       ``None``, in which case the active setup is used or nothing is\n       used.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``, in which case\n        the active setup is used or the latest installed version is\n        used.\n    non_graphical : bool, optional\n        Whether to launch AEDT in non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in graphical mode.\n        This parameter is ignored when a script is launched within AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine.  The default is ``True``.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit. The default is ``True``.\n    student_version : bool, optional\n        Whether to open the AEDT student version. The default is ``False``.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of Emit and connect to an existing Emit\n    design or create a new Emit design if one does not exist.\n\n    >>> from pyaedt import Emit\n    >>> app = Emit()\n\n    Create a instance of Emit and link to a project named\n    ``\"projectname\"``. If this project does not exist, create one with\n    this name.\n\n    >>> app = Emit(projectname)\n\n    Create an instance of Emit and link to a design named\n    ``\"designname\"`` in a project named ``\"projectname\"``.\n\n    >>> app = Emit(projectname,designame)\n\n    Create an instance of Emit and open the specified project,\n    which is named ``Myfile.aedt``.\n\n    >>> app = Emit(\"myfile.aedt\")\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        setup_name=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=True,\n        close_on_exit=True,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n        \"\"\"Constructor.\"\"\"\n        FieldAnalysisEmit.__init__(\n            self,\n            \"EMIT\",\n            projectname,\n            designname,\n            solution_type,\n            setup_name,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine=machine,\n            port=port,\n            aedt_process_id=aedt_process_id,\n        )\n\n    def __enter__(self):\n        return self\n\n# --- Snippet Separator ---\n\nclass Rmxprt(FieldAnalysisRMxprt):\n    \"\"\"Provides the RMxprt application interface.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open. The default is ``None``, in which\n        case an attempt is made to get an active project. If no\n        projects are present, an empty project is created.\n    designname : str, optional\n        Name of the design to select. The default is ``None``, in\n        which case an attempt is made to get an active design. If no\n        designs are present, an empty design is created.\n    solution_type : str, optional\n        Solution type to apply to the design. The default is\n        ``None``, in which ase the default type is applied.\n    model_units : str, optional\n        Model units.\n    setup_name : str, optional\n        Name of the setup to use as the nominal. The default is\n        ``None``, in which case the active setup is used or\n        nothing is used.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``, in which case\n        the active setup is used or the latest installed version is\n        used.\n    non_graphical : bool, optional\n        Whether to launch AEDT in non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in graphical mode.\n        This parameter is ignored when a script is launched within AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine.  The default is ``True``.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit. The default is ``True``.\n    student_version : bool, optional\n        Whether to open the AEDT student version. The default is ``False``.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of RMxprt and connect to an existing RMxprt\n    design or create a new RMxprt design if one does not exist.\n\n    >>> from pyaedt import Rmxprt\n    >>> app = Rmxprt()\n\n    Create an instance of Rmxprt and link to a project named\n    ``\"projectname\"``. If this project does not exist, create one with\n    this name.\n\n    >>> app = Rmxprt(projectname)\n\n    Create an instance of RMxprt and link to a design named\n    ``\"designname\"`` in a project named ``\"projectname\"``.\n\n    >>> app = Rmxprt(projectname,designame)\n\n    Create an instance of RMxprt and open the specified project,\n    which is ``\"myfile.aedt\"``.\n\n    >>> app = Rmxprt(\"myfile.aedt\")\n    \"\"\"\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        model_units=None,\n        setup_name=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=False,\n        close_on_exit=False,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n        FieldAnalysisRMxprt.__init__(\n            self,\n            \"RMxprtSolution\",\n            projectname,\n            designname,\n            solution_type,\n            setup_name,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine,\n            port,\n            aedt_process_id,\n        )\n        if not model_units or model_units == \"mm\":\n            model_units = \"mm\"\n        else:\n            assert model_units == \"in\", \"Invalid model units string {}\".format(model_units)\n        self.modeler.oeditor.SetMachineUnits([\"NAME:Units Parameter\", \"Units:=\", model_units, \"Rescale:=\", False])\n        self.stator = Stator(self.modeler.oeditor)\n        self.rotor = Rotor(self.modeler.oeditor)\n\n    def __enter__(self):\n        return self\n\n    @property\n    def design_type(self):\n        \"\"\"Machine design type.\"\"\"\n        return self.design_solutions.design_type\n\n    @design_type.setter\n    @pyaedt_function_handler()\n    def design_type(self, value):\n        self.design_solutions.design_type = value\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the pyaedt library to create a 2D Extractor CPWG (coplanar waveguide with ground) design and run a simulation. The code should import the necessary libraries, set the non-graphical mode, launch AEDT and 2D Extractor, define variables, create primitives, create a signal, create a coplanar ground, create a reference ground plane, create a dielectric, create a conformal coating, assign a conductor to the signal, create a reference ground, assign the Huray model on the signal, create the setup, analyze it, plot solution data, save the project and close AEDT.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 68, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that performs a restricted AGF2 calculation with density fitting using the PySCF library. The code should define a molecule with a specific atomic structure and basis, and then run a RHF calculation with a specified convergence tolerance and auxiliary basis. After running the AGF2 calculation with a specified convergence tolerance, the code should print the first three ionization potentials and electron affinities. Then, it should calculate the MO-basis density matrix and dipole moments, transforming dipole moment integrals into MO basis and adding the nuclear component. Finally, the code should print the calculated dipole moment.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def dip_moment(self, mol=None, dm=None, unit='Debye', verbose=logger.NOTE,\n                   picture_change=True, **kwargs):\n        r''' Dipole moment calculation with picture change correction\n\n        Args:\n             mol: an instance of :class:`Mole`\n             dm : a 2D ndarrays density matrices\n\n        Kwarg:\n            picture_change (bool) : Whether to compute the dipole moment with\n            picture change correction.\n\n        Return:\n            A list: the dipole moment on x, y and z component\n        '''\n        if mol is None: mol = self.mol\n        if dm is None: dm =self.make_rdm1()\n        log = logger.new_logger(mol, verbose)\n\n        if 'unit_symbol' in kwargs:  # pragma: no cover\n            log.warn('Kwarg \"unit_symbol\" was deprecated. It was replaced by kwarg '\n                     'unit since PySCF-1.5.')\n            unit = kwargs['unit_symbol']\n\n        if not (isinstance(dm, numpy.ndarray) and dm.ndim == 2):\n            # UHF denisty matrices\n            dm = dm[0] + dm[1]\n\n        with mol.with_common_orig((0,0,0)):\n            if picture_change:\n                ao_dip = self.with_x2c.picture_change(('int1e_r_spinor',\n                                                       'int1e_sprsp_spinor'))\n            else:\n                ao_dip = mol.intor_symmetric('int1e_r_spinor')\n\n        el_dip = numpy.einsum('xij,ji->x', ao_dip, dm).real\n\n        charges = mol.atom_charges()\n        coords  = mol.atom_coords()\n        nucl_dip = numpy.einsum('i,ix->x', charges, coords)\n        mol_dip = nucl_dip - el_dip\n\n        if unit.upper() == 'DEBYE':\n            mol_dip *= nist.AU2DEBYE\n            log.note('Dipole moment(X, Y, Z, Debye): %8.5f, %8.5f, %8.5f', *mol_dip)\n        else:\n            log.note('Dipole moment(X, Y, Z, A.U.): %8.5f, %8.5f, %8.5f', *mol_dip)\n        return mol_dip\n\n# --- Snippet Separator ---\n\ndef dip_moment(self, mol=None, dm=None, unit='Debye', verbose=logger.NOTE,\n                       picture_change=True, **kwargs):\n            r''' Dipole moment calculation with picture change correction\n\n            Args:\n                 mol: an instance of :class:`Mole`\n                 dm : a 2D ndarrays density matrices\n\n            Kwarg:\n                picture_chang (bool) : Whether to compute the dipole moment with\n                picture change correction.\n\n            Return:\n                A list: the dipole moment on x, y and z component\n            '''\n            if mol is None: mol = self.mol\n            if dm is None: dm =self.make_rdm1()\n            log = logger.new_logger(mol, verbose)\n\n            if 'unit_symbol' in kwargs:  # pragma: no cover\n                log.warn('Kwarg \"unit_symbol\" was deprecated. It was replaced by kwarg '\n                         'unit since PySCF-1.5.')\n                unit = kwargs['unit_symbol']\n\n            if not (isinstance(dm, numpy.ndarray) and dm.ndim == 2):\n                # UHF denisty matrices\n                dm = dm[0] + dm[1]\n\n            with mol.with_common_orig((0,0,0)):\n                if picture_change:\n                    xmol = self.with_x2c.get_xmol()[0]\n                    nao = xmol.nao\n                    prp = xmol.intor_symmetric('int1e_sprsp').reshape(3,4,nao,nao)[:,0]\n                    ao_dip = self.with_x2c.picture_change(('int1e_r', prp))\n                else:\n                    ao_dip = mol.intor_symmetric('int1e_r')\n\n            el_dip = numpy.einsum('xij,ji->x', ao_dip, dm).real\n\n            charges = mol.atom_charges()\n            coords  = mol.atom_coords()\n            nucl_dip = numpy.einsum('i,ix->x', charges, coords)\n            mol_dip = nucl_dip - el_dip\n\n            if unit.upper() == 'DEBYE':\n                mol_dip *= nist.AU2DEBYE\n                log.note('Dipole moment(X, Y, Z, Debye): %8.5f, %8.5f, %8.5f', *mol_dip)\n            else:\n                log.note('Dipole moment(X, Y, Z, A.U.): %8.5f, %8.5f, %8.5f', *mol_dip)\n            return mol_dip\n\n# --- Snippet Separator ---\n\ndef dip_moment(mol, dm, unit='Debye', verbose=logger.NOTE, **kwargs):\n    r''' Dipole moment calculation\n\n    .. math::\n\n        \\mu_x = -\\sum_{\\mu}\\sum_{\\nu} P_{\\mu\\nu}(\\nu|x|\\mu) + \\sum_A Q_A X_A\\\\\n        \\mu_y = -\\sum_{\\mu}\\sum_{\\nu} P_{\\mu\\nu}(\\nu|y|\\mu) + \\sum_A Q_A Y_A\\\\\n        \\mu_z = -\\sum_{\\mu}\\sum_{\\nu} P_{\\mu\\nu}(\\nu|z|\\mu) + \\sum_A Q_A Z_A\n\n    where :math:`\\mu_x, \\mu_y, \\mu_z` are the x, y and z components of dipole\n    moment\n\n    Args:\n         mol: an instance of :class:`Mole`\n         dm : a 2D ndarrays density matrices\n\n    Return:\n        A list: the dipole moment on x, y and z component\n    '''\n\n    log = logger.new_logger(mol, verbose)\n\n    if 'unit_symbol' in kwargs:  # pragma: no cover\n        log.warn('Kwarg \"unit_symbol\" was deprecated. It was replaced by kwarg '\n                 'unit since PySCF-1.5.')\n        unit = kwargs['unit_symbol']\n\n    if not (isinstance(dm, numpy.ndarray) and dm.ndim == 2):\n        # UHF denisty matrices\n        dm = dm[0] + dm[1]\n\n    with mol.with_common_orig((0,0,0)):\n        ao_dip = mol.intor_symmetric('int1e_r', comp=3)\n    el_dip = numpy.einsum('xij,ji->x', ao_dip, dm).real\n\n    charges = mol.atom_charges()\n    coords  = mol.atom_coords()\n    nucl_dip = numpy.einsum('i,ix->x', charges, coords)\n    mol_dip = nucl_dip - el_dip\n\n    if unit.upper() == 'DEBYE':\n        mol_dip *= nist.AU2DEBYE\n        log.note('Dipole moment(X, Y, Z, Debye): %8.5f, %8.5f, %8.5f', *mol_dip)\n    else:\n        log.note('Dipole moment(X, Y, Z, A.U.): %8.5f, %8.5f, %8.5f', *mol_dip)\n    return mol_dip\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a restricted AGF2 calculation with density fitting using the PySCF library. The code should define a molecule with a specific atomic structure and basis, and then run a RHF calculation with a specified convergence tolerance and auxiliary basis. After running the AGF2 calculation with a specified convergence tolerance, the code should print the first three ionization potentials and electron affinities. Then, it should calculate the MO-basis density matrix and dipole moments, transforming dipole moment integrals into MO basis and adding the nuclear component. Finally, the code should print the calculated dipole moment.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 69, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that imports necessary libraries and modules for solving a Poisson equation using the Virtual Element Method (VEM) on a polygonal mesh. The code should parse command line arguments for the degree of the VEM space, the maximum number of iterations for mesh refinement, and the adaptive parameter. It should initialize the problem with a specific PDE and domain, and set up an error matrix and a mesh.\n\nThe code should then enter a loop for the maximum number of iterations. In each iteration, it should set up the VEM space and the function, assemble the stiffness matrix and the right-hand side, apply Dirichlet boundary conditions, solve the linear system, and compute the error. It should also mark cells for refinement based on the residual estimate and adaptively refine the mesh. The loop should terminate if the number of degrees of freedom exceeds a certain threshold.\n\nAfter the loop, the code should display the error rates, save the number of degrees of freedom and the error matrix to text files, and plot the error. It should also plot the final mesh.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def uniform_refine(self, n=1, surface=None, interface=None, returnim=False):\n        \"\"\"\n        @brief Perform uniform refinement on the mesh.\n\n        @param[in] n The number of refinement iterations to perform, default: 1.\n        @param[in] returnim If True, returns a list of interpolation matrices for each refinement iteration, default: False.\n\n        @return A list of interpolation matrices if returnim is True, otherwise None.\n\n        @details This function performs n iterations of uniform refinement on the mesh.\n                 For each iteration, it updates the mesh extent, step size, number of cells, and number of nodes,\n                 as well as the data structure. If returnim is True, it also calculates and returns the\n                 interpolation matrices for each iteration.\n\n        \"\"\"\n        for i in range(n):\n            self.extent = [i*2 for i in self.extent]\n            self.h = [h/2.0 for h in self.h]\n            self.nx = self.extent[1] - self.extent[0]\n            self.ny = self.extent[3] - self.extent[2]\n\n            self.NC = self.nx * self.ny\n            self.NN = (self.nx + 1) * (self.ny + 1)\n            self.ds = StructureMesh2dDataStructure(self.nx, self.ny, itype=self.itype)\n\n# --- Snippet Separator ---\n\ndef uniform_refine(self, n=1, surface=None, interface=None, returnim=False):\n        \"\"\"\n        @brief Perform uniform refinement on the mesh.\n\n        @param[in] n The number of refinement iterations to perform, default: 1.\n        @param[in] returnim If True, returns a list of interpolation matrices for each refinement iteration, default: False.\n\n        @return A list of interpolation matrices if returnim is True, otherwise None.\n\n        @details This function performs n iterations of uniform refinement on the mesh.\n                 For each iteration, it updates the mesh extent, step size, number of cells, and number of nodes,\n                 as well as the data structure. If returnim is True, it also calculates and returns the\n                 interpolation matrices for each iteration.\n\n        \"\"\"\n        for i in range(n):\n            self.extent = [i * 2 for i in self.extent]\n            self.h = [h / 2.0 for h in self.h]\n            self.nx = self.extent[1] - self.extent[0]\n            self.ny = self.extent[3] - self.extent[2]\n\n            self.NC = self.nx * self.ny\n            self.NN = (self.nx + 1) * (self.ny + 1)\n            self.ds = StructureMesh2dDataStructure(self.nx, self.ny, itype=self.itype)\n\n# --- Snippet Separator ---\n\nclass Mpfa(Solver):\n\n    def __init__(self, physics='flow'):\n        self.physics = physics\n\n    def ndof(self, g):\n        \"\"\"\n        Return the number of degrees of freedom associated to the method.\n        In this case number of cells (pressure dof).\n\n        Parameter\n        ---------\n        g: grid, or a subclass.\n\n        Return\n        ------\n        dof: the number of degrees of freedom.\n\n        \"\"\"\n        return g.num_cells\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, g, data, discretize=True):\n        \"\"\"\n        Return the matrix and right-hand side for a discretization of a second\n        order elliptic equation using a FV method with a multi-point flux\n        approximation.\n\n        The name of data in the input dictionary (data) are:\n        k : second_order_tensor\n            Permeability defined cell-wise.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data. For details on necessary keywords,\n            see method discretize()\n        discretize (boolean, optional): Whether to discetize prior to matrix\n            assembly. If False, data should already contain discretization.\n            Defaults to True.\n\n        Return\n        ------\n        matrix: sparse csr (g_num_cells, g_num_cells)\n            Discretization matrix.\n        rhs: array (g_num_cells)\n            Right-hand side which contains the boundary conditions and the scalar\n            source term.\n\n        \"\"\"\n        if discretize:\n            self.discretize(g, data)\n\n        div = fvutils.scalar_divergence(g)\n        flux = data['flux']\n        M = div * flux\n\n        bound_flux = data['bound_flux']\n\n        param = data['param']\n\n        bc_val = param.get_bc_val(self)\n\n        return M, self.rhs(g, bound_flux, bc_val)\n\n#------------------------------------------------------------------------------#\n\n    def rhs(self, g, bound_flux, bc_val):\n        \"\"\"\n        Return the righ-hand side for a discretization of a second order elliptic\n        equation using the MPFA method. See self.matrix_rhs for a detaild\n        description.\n        \"\"\"\n        div = g.cell_faces.T\n\n        return -div * bound_flux * bc_val\n\n#------------------------------------------------------------------------------#\n\n    def discretize(self, g, data):\n        \"\"\"\n        The name of data in the input dictionary (data) are:\n        k : second_order_tensor\n            Permeability defined cell-wise. If not given a identity permeability\n            is assumed and a warning arised.\n        bc : boundary conditions (optional)\n        bc_val : dictionary (optional)\n            Values of the boundary conditions. The dictionary has at most the\n            following keys: 'dir' and 'neu', for Dirichlet and Neumann boundary\n            conditions, respectively.\n\n        Parameters\n        ----------\n        g : grid, or a subclass, with geometry fields computed.\n        data: dictionary to store the data.\n\n        \"\"\"\n        param = data['param']\n        k = param.get_tensor(self)\n        bnd = param.get_bc(self)\n        a = param.aperture\n\n        trm, bound_flux = mpfa(g, k, bnd, apertures=a)\n        data['flux'] = trm\n        data['bound_flux'] = bound_flux\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary libraries and modules for solving a Poisson equation using the Virtual Element Method (VEM) on a polygonal mesh. The code should parse command line arguments for the degree of the VEM space, the maximum number of iterations for mesh refinement, and the adaptive parameter. It should initialize the problem with a specific PDE and domain, and set up an error matrix and a mesh.\n\nThe code should then enter a loop for the maximum number of iterations. In each iteration, it should set up the VEM space and the function, assemble the stiffness matrix and the right-hand side, apply Dirichlet boundary conditions, solve the linear system, and compute the error. It should also mark cells for refinement based on the residual estimate and adaptively refine the mesh. The loop should terminate if the number of degrees of freedom exceeds a certain threshold.\n\nAfter the loop, the code should display the error rates, save the number of degrees of freedom and the error matrix to text files, and plot the error. It should also plot the final mesh.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 70, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that imports necessary modules from numpy, scipy, and porepy. The code should define four functions: add_data, write_network, main, and two test functions. \n\nThe add_data function should take a grid bucket, a domain, and a permeability factor as arguments. It should add parameters to the grid bucket such as permeability, source term, apertures, and boundary conditions. \n\nThe write_network function should take a file name as an argument and write a predefined network string to a file with that name.\n\nThe main function should take a permeability factor, a description, a boolean indicating whether the grid bucket should be coarsened, and a boolean indicating whether the results should be exported. It should define mesh parameters, a domain, and a file name. It should write a network to a file, import a 2D fracture network from the file, compute the geometry of the grid bucket, optionally coarsen the grid bucket, assign an ordering to the nodes of the grid bucket, and add data to the grid bucket. It should define solvers for flow and source, compute the right-hand side and the matrix of the linear systems, solve the linear systems, split the solution, extract the discharge and pressure from the solution, and project the discharge. If the results should be exported, it should export the pressure and the projected discharge to a vtk file.\n\nThe test_vem_blocking function should call the main function with a small permeability factor and the description \"blocking\".\n\nThe test_vem_permeable function should call the main function with a large permeability factor and the description \"permeable\".", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def generate_coarse_grid(g, subdiv):\n    \"\"\" Generate a coarse grid clustering the cells according to the flags\n    given by subdiv. Subdiv should be long as the number of cells in the\n    original grid, it contains integers (possibly not continuous) which\n    represent the cells in the final mesh. If a grid bucket is given the\n    coarsening is applied to the higher dimensional grid.\n\n    The values computed in \"compute_geometry\" are not preserved and they should\n    be computed out from this function.\n\n    Note: there is no check for disconnected cells in the final grid.\n\n    Parameters:\n        g: the grid or grid bucket\n        subdiv: a list of flags, one for each cell of the original grid\n\n    Return:\n        grid: if a grid is given as input, its coarser version is returned.\n        If a grid bucket is given as input, the grid is updated in place.\n\n    How to use:\n    subdiv = np.array([0,0,1,1,1,1,3,4,6,4,6,4])\n    g = generate_coarse_grid(g, subdiv)\n\n    or with a grid bucket:\n    subdiv = np.array([0,0,1,1,1,1,3,4,6,4,6,4])\n    generate_coarse_grid(gb, subdiv)\n\n    \"\"\"\n    if isinstance(g, grid.Grid):\n        generate_coarse_grid_single(g, subdiv, False)\n\n    if isinstance(g, grid_bucket.GridBucket):\n        generate_coarse_grid_gb(g, subdiv)\n\n# --- Snippet Separator ---\n\ndef write_vtk(self, data=None, time_step=None, grid=None):\n        \"\"\" Interface function to export in VTK the grid and additional data.\n\n        In 2d the cells are represented as polygon, while in 3d as polyhedra.\n        VTK module need to be installed.\n        In 3d the geometry of the mesh needs to be computed.\n\n        To work with python3, the package vtk should be installed in version 7\n        or higher.\n\n        Parameters:\n        data: if g is a single grid then data is a dictionary (see example)\n              if g is a grid bucket then list of names for optional data,\n              they are the keys in the grid bucket (see example).\n        time_step: (optional) in a time dependent problem defines the full name of\n            the file.\n        grid: (optional) in case of changing grid set a new one.\n\n        \"\"\"\n        if self.is_not_vtk:\n            return\n\n        if self.fixed_grid and grid is not None:\n            raise ValueError(\"Inconsistency in exporter setting\")\n        elif not self.fixed_grid and grid is not None:\n            self.gb = grid\n            self.is_GridBucket = isinstance(self.gb, grid_bucket.GridBucket)\n            self._update_gVTK()\n\n        if self.is_GridBucket:\n            self._export_vtk_gb(data, time_step)\n        else:\n            # No need of special naming, create the folder\n            name = self._make_folder(self.folder, self.name)\n            self._export_vtk_single(data, time_step, self.gb, name)\n\n# --- Snippet Separator ---\n\nclass EllipticModel():\n    '''\n    Class for solving an incompressible flow problem:\n    \\nabla K \\nabla p = q,\n    where K is the second order permeability tenser, p the fluid pressure\n    and q sinks and sources.\n\n    Parameters in Init:\n    gb: (Grid /GridBucket) a grid or grid bucket object. If gb = GridBucket\n        a Parameter class should be added to each grid bucket data node with\n        keyword 'param'.\n    data: (dictionary) Defaults to None. Only used if gb is a Grid. Should\n          contain a Parameter class with the keyword 'Param'\n    physics: (string): defaults to 'flow'\n\n    Functions:\n    solve(): Calls reassemble and solves the linear system.\n             Returns: the pressure p.\n             Sets attributes: self.x\n    step(): Same as solve, but without reassemble of the matrices\n    reassemble(): Assembles the lhs matrix and rhs array.\n            Returns: lhs, rhs.\n            Sets attributes: self.lhs, self.rhs\n    source_disc(): Defines the discretization of the source term.\n            Returns Source discretization object\n    flux_disc(): Defines the discretization of the flux term.\n            Returns Flux discretization object (E.g., Tpfa)\n    grid(): Returns: the Grid or GridBucket\n    data(): Returns: Data dictionary\n    split(name): Assignes the solution self.x to the data dictionary at each\n                 node in the GridBucket.\n                 Parameters:\n                    name: (string) The keyword assigned to the pressure\n    discharge(): Calls split('pressure'). Then calculate the discharges over each\n                 face in the grids and between edges in the GridBucket\n    save(): calls split('pressure'). Then export the pressure to a vtk file to the\n            folder kwargs['folder_name'] with file name\n            kwargs['file_name'], default values are 'results' for the folder and\n            physics for the file name.\n    '''\n\n    def __init__(self, gb, data=None, physics='flow', **kwargs):\n        self.physics = physics\n        self._gb = gb\n        self.is_GridBucket = isinstance(self._gb, GridBucket)\n        self._data = data\n\n        self.lhs = []\n        self.rhs = []\n        self.x = []\n\n        file_name = kwargs.get('file_name', physics)\n        folder_name = kwargs.get('folder_name', 'results')\n        mesh_kw = kwargs.get('mesh_kw', {})\n\n        tic = time.time()\n        logger.info('Create exporter')\n        self.exporter = Exporter(self._gb, file_name, folder_name, **mesh_kw)\n        logger.info('Elapsed time: ' + str(time.time() - tic))\n\n        self._flux_disc = self.flux_disc()\n        self._source_disc = self.source_disc()\n\n    def solve(self, max_direct=40000, callback=False, **kwargs):\n        \"\"\" Reassemble and solve linear system.\n\n        After the funtion has been called, the attributes lhs and rhs are\n        updated according to the parameter states. Also, the attribute x\n        gives the pressure given the current state.\n\n        TODO: Provide an option to save solver information if multiple\n        systems are to be solved with the same left hand side.\n\n        The function attempts to set up the best linear solver based on the\n        system size. The setup and parameter choices here are still\n        experimental.\n\n        Parameters:\n            max_direct (int): Maximum number of unknowns where a direct solver\n                is applied. If a direct solver can be applied this is usually\n                the most efficient option. However, if the system size is\n                too large compared to available memory, a direct solver becomes\n                extremely slow.\n            callback (boolean, optional): If True iteration information will be\n                output when an iterative solver is applied (system size larger\n                than max_direct)\n\n        Returns:\n            np.array: Pressure state.\n\n        \"\"\"\n        logger.error('Solve elliptic model')\n        # Discretize\n        tic = time.time()\n        logger.warning('Discretize')\n        self.lhs, self.rhs = self.reassemble()\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n\n        # Solve\n        tic = time.time()\n        ls = LSFactory()\n        if self.rhs.size < max_direct:\n            logger.warning('Solve linear system using direct solver')\n            self.x = ls.direct(self.lhs, self.rhs)\n        else:\n            logger.warning('Solve linear system using GMRES')\n            precond = self._setup_preconditioner()\n#            precond = ls.ilu(self.lhs)\n            slv = ls.gmres(self.lhs)\n            self.x, info = slv(self.rhs, M=precond, callback=callback,\n                               maxiter=10000, restart=1500, tol=1e-8)\n            if info == 0:\n                logger.warning('GMRES succeeded.')\n            else:\n                logger.warning('GMRES failed with status ' + str(info))\n\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n        return self.x\n\n    def step(self):\n        return self.solve()\n\n    def reassemble(self):\n        \"\"\"\n        reassemble matrices. This must be called between every time step to\n        update the rhs of the system.\n        \"\"\"\n        lhs_flux, rhs_flux = self._discretize(self._flux_disc)\n        lhs_source, rhs_source = self._discretize(self._source_disc)\n        assert lhs_source.nnz == 0, 'Source lhs different from zero!'\n        self.lhs = lhs_flux\n        self.rhs = rhs_flux + rhs_source\n        return self.lhs, self.rhs\n\n    def source_disc(self):\n        if self.is_GridBucket:\n            return source.IntegralMixedDim(physics=self.physics)\n        else:\n            return source.Integral(physics=self.physics)\n\n    def flux_disc(self):\n        if self.is_GridBucket:\n            return tpfa.TpfaMixedDim(physics=self.physics)\n        else:\n            return tpfa.Tpfa(physics=self.physics)\n\n    def _discretize(self, discr):\n        if self.is_GridBucket:\n            return discr.matrix_rhs(self.grid())\n        else:\n            return discr.matrix_rhs(self.grid(), self.data())\n\n    def grid(self):\n        return self._gb\n\n    def data(self):\n        return self._data\n\n    def split(self, x_name='solution'):\n        self.x_name = x_name\n        self._flux_disc.split(self.grid(), self.x_name, self.x)\n\n    def pressure(self, pressure_name='pressure'):\n        self.pressure_name = pressure_name\n        if self.is_GridBucket:\n            self.split(self.pressure_name)\n        else:\n            self._data[self.pressure_name] = self.x\n\n    def discharge(self, discharge_name='discharge'):\n        if self.is_GridBucket:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       p_name=self.pressure_name)\n        else:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       self.pressure_name,\n                                       self._data)\n\n    def permeability(self, perm_names=['kxx', 'kyy', 'kzz']):\n        \"\"\" Assign permeability to self._data, ready for export to vtk.\n\n        For the moment, we only dump the main diagonals of the permeabliity.\n        Extensions should be trivial if needed.\n\n        Parameters:\n            perm_names (list): Which components to export. Defaults to kxx,\n                kyy and xzz.\n\n        \"\"\"\n\n        def get_ind(n):\n            if n == 'kxx':\n                return 0\n            elif n == 'kyy':\n                return 1\n            elif n == 'kzz':\n                return 2\n            else:\n                raise ValueError('Unknown perm keyword ' + n)\n\n        for n in perm_names:\n            ind = get_ind(n)\n            if self.is_GridBucket:\n                for _, d in self.grid():\n                    d[n] = d['param'].get_permeability().perm[ind, ind, :]\n            else:\n                self._data[n] = self._data['param'].get_permeability()\\\n                    .perm[ind, ind, :]\n\n    def porosity(self, poro_name='porosity'):\n        if self.is_GridBucket:\n            for _, d in self.grid():\n                d[poro_name] = d['param'].get_porosity()\n        else:\n            self._data[poro_name] = self._data['param'].get_porosity()\n\n    def save(self, variables=None, save_every=None):\n        if variables is None:\n            self.exporter.write_vtk()\n        else:\n            if not self.is_GridBucket:\n                variables = {k: self._data[k] for k in variables\n                             if k in self._data}\n            self.exporter.write_vtk(variables)\n\n    # Helper functions for linear solve below\n    def _setup_preconditioner(self):\n        solvers, ind, not_ind = self._assign_solvers()\n\n        def precond(r):\n            x = np.zeros_like(r)\n            for s, i, ni in zip(solvers, ind, not_ind):\n                x[i] += s(r[i])\n            return x\n\n        def precond_mult(r):\n            x = np.zeros_like(r)\n            A = self.lhs\n            for s, i, ni in zip(solvers, ind, not_ind):\n                r_i = r[i] - A[i, :][:, ni] * x[ni]\n                x[i] += s(r_i)\n            return x\n\n        def M(r): return precond(r)\n        return spl.LinearOperator(self.lhs.shape, M)\n\n    def _assign_solvers(self):\n        mat, ind = self._obtain_submatrix()\n        all_ind = np.arange(self.rhs.size)\n        not_ind = [np.setdiff1d(all_ind, i) for i in ind]\n\n        factory = LSFactory()\n        num_mat = len(mat)\n        solvers = np.empty(num_mat, dtype=np.object)\n        for i, A in enumerate(mat):\n            sz = A.shape[0]\n            if sz < 5000:\n                solvers[i] = factory.direct(A)\n            else:\n                # amg solver is pyamg is installed, if not ilu\n                try:\n                    solvers[i] = factory.amg(A, as_precond=True)\n                except ImportError:\n                    solvers[i] = factory.ilu(A)\n\n        return solvers, ind, not_ind\n\n    def _obtain_submatrix(self):\n\n        if isinstance(self.grid(), GridBucket):\n            gb = self.grid()\n            fd = self.flux_disc()\n            mat = []\n            sub_ind = []\n            for g, _ in self.grid():\n                ind = fd.solver.dof_of_grid(gb, g)\n                A = self.lhs[ind, :][:, ind]\n                mat.append(A)\n                sub_ind.append(ind)\n            return mat, sub_ind\n        else:\n            return [self.lhs], [np.arange(self.grid().num_cells)]\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary modules from numpy, scipy, and porepy. The code should define four functions: add_data, write_network, main, and two test functions. \n\nThe add_data function should take a grid bucket, a domain, and a permeability factor as arguments. It should add parameters to the grid bucket such as permeability, source term, apertures, and boundary conditions. \n\nThe write_network function should take a file name as an argument and write a predefined network string to a file with that name.\n\nThe main function should take a permeability factor, a description, a boolean indicating whether the grid bucket should be coarsened, and a boolean indicating whether the results should be exported. It should define mesh parameters, a domain, and a file name. It should write a network to a file, import a 2D fracture network from the file, compute the geometry of the grid bucket, optionally coarsen the grid bucket, assign an ordering to the nodes of the grid bucket, and add data to the grid bucket. It should define solvers for flow and source, compute the right-hand side and the matrix of the linear systems, solve the linear systems, split the solution, extract the discharge and pressure from the solution, and project the discharge. If the results should be exported, it should export the pressure and the projected discharge to a vtk file.\n\nThe test_vem_blocking function should call the main function with a small permeability factor and the description \"blocking\".\n\nThe test_vem_permeable function should call the main function with a large permeability factor and the description \"permeable\".\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 71, "repo_full_name": "paddlepaddle__fastdeploy", "instruction": "Generate code that creates a command-line interface for parsing arguments related to model directory, tokenizer vocab path, inference device, runtime backend, batch size, sequence length, logging interval, and usage of FP16 mode and fast tokenizer. The code should also define a function to batchify text data. \n\nThen, create a class for sequence classification prediction using the Ernie model. This class should initialize the tokenizer and runtime, preprocess the input texts, perform inference, postprocess the inference data, and predict the output for given texts. \n\nIn the main function, the code should parse the arguments, instantiate the prediction class, batchify the text data, and predict the output for each batch of texts. The output should include the batch id, example id, input sentences, predicted label, and confidence score.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_dir\",\n        required=True,\n        help=\"The directory of model and tokenizer.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='gpu',\n        choices=['gpu', 'cpu'],\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default='pp',\n        choices=['ort', 'pp', 'trt', 'pp-trt'],\n        help=\"The inference runtime backend.\")\n    parser.add_argument(\n        \"--device_id\", type=int, default=0, help=\"device(gpu) id\")\n    parser.add_argument(\n        \"--batch_size\", type=int, default=32, help=\"The batch size of data.\")\n    parser.add_argument(\n        \"--max_length\",\n        type=int,\n        default=128,\n        help=\"The max length of sequence.\")\n    parser.add_argument(\n        \"--log_interval\",\n        type=int,\n        default=10,\n        help=\"The interval of logging.\")\n    parser.add_argument(\n        \"--cpu_num_threads\",\n        type=int,\n        default=1,\n        help=\"The number of threads when inferring on cpu.\")\n    parser.add_argument(\n        \"--use_fp16\",\n        type=distutils.util.strtobool,\n        default=False,\n        help=\"Use FP16 mode\")\n    parser.add_argument(\n        \"--use_fast\",\n        type=distutils.util.strtobool,\n        default=True,\n        help=\"Whether to use fast_tokenizer to accelarate the tokenization.\")\n    return parser.parse_args()\n\n# --- Snippet Separator ---\n\ndef parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--model_dir\",\n        required=True,\n        help=\"The directory of model and tokenizer.\")\n    parser.add_argument(\n        \"--data_path\", required=True, help=\"The path of uie data.\")\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        choices=['gpu', 'cpu'],\n        help=\"Type of inference device, support 'cpu' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default='paddle',\n        choices=['ort', 'paddle', 'trt', 'paddle_trt', 'ov'],\n        help=\"The inference runtime backend.\")\n    parser.add_argument(\n        \"--device_id\", type=int, default=0, help=\"device(gpu) id\")\n    parser.add_argument(\n        \"--batch_size\", type=int, default=1, help=\"The batch size of data.\")\n    parser.add_argument(\n        \"--max_length\",\n        type=int,\n        default=128,\n        help=\"The max length of sequence.\")\n    parser.add_argument(\n        \"--cpu_num_threads\",\n        type=int,\n        default=8,\n        help=\"The number of threads when inferring on cpu.\")\n    parser.add_argument(\n        \"--enable_trt_fp16\",\n        type=distutils.util.strtobool,\n        default=False,\n        help=\"whether enable fp16 in trt backend\")\n    parser.add_argument(\n        \"--epoch\", type=int, default=1, help=\"The epoch of test\")\n    parser.add_argument(\n        \"--enable_collect_memory_info\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"whether enable collect memory info\")\n    return parser.parse_args()\n\n# --- Snippet Separator ---\n\nclass Predict(Processor):\n    \"\"\"Perform input preprocessing, model prediction and output postprocessing.\n\n    # Arguments\n        model: Class with a ''predict'' method e.g. a Keras model.\n        preprocess: Function applied to given inputs.\n        postprocess: Function applied to outputted predictions from model.\n    \"\"\"\n    def __init__(self, model, preprocess=None, postprocess=None):\n        super(Predict, self).__init__()\n        self.model = model\n        self.preprocess = preprocess\n        self.postprocess = postprocess\n\n    def call(self, x):\n        return predict(x, self.model, self.preprocess, self.postprocess)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a command-line interface for parsing arguments related to model directory, tokenizer vocab path, inference device, runtime backend, batch size, sequence length, logging interval, and usage of FP16 mode and fast tokenizer. The code should also define a function to batchify text data. \n\nThen, create a class for sequence classification prediction using the Ernie model. This class should initialize the tokenizer and runtime, preprocess the input texts, perform inference, postprocess the inference data, and predict the output for given texts. \n\nIn the main function, the code should parse the arguments, instantiate the prediction class, batchify the text data, and predict the output for each batch of texts. The output should include the batch id, example id, input sentences, predicted label, and confidence score.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 72, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS) with AS numbers 150, 151, and 152. Each AS should have a host named 'web' and a router named 'router0'. The 'web' host in each AS should have a web service installed. Each AS should also have a network named 'net0' which both the 'web' host and 'router0' join. AS150 and AS152 should have a cross connection between their routers. An internet exchange with the number 100 should be created and AS150 and AS151 should be peers on this exchange. AS150 should also be a provider for AS152. The emulation should be rendered and compiled using Docker with self-managed network. The compiled emulation should be saved in the directory './cross-connect'.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int,\n    services: List[Service]):\n    \"\"\"!\n    @brief create a new stub AS.\n\n    @param emu reference to the Emulator object.\n    @param base reference to the base layer.\n    @param asn ASN for the newly created AS.\n    @param exchange IXP ID for new newly created AS to join.\n    @param list of instances of Service to install on hosts. One host will be\n    created for each.\n    \"\"\"\n\n    # Create AS and internal network\n    stub_as = base.createAutonomousSystem(asn)\n    stub_as.createNetwork('net0')\n\n    # Create a BGP router \n    # Attach the router to both the internal and external networks\n    router = stub_as.createRouter('router0')\n    router.joinNetwork('net0')\n    router.joinNetwork('ix{}'.format(exchange))\n\n    # Create a host node for each specified service\n    createHostsOnNetwork(emu, stub_as, 'net0', services)\n\n# --- Snippet Separator ---\n\ndef __init__(self, serviceNetworkPrefix: str = '192.168.66.0/24'):\n        \"\"\"!\n        @brief Construct a new emulation.\n\n        @param serviceNetworkPrefix (optional) service network prefix for this\n        emulator. A service network is a network that does not take part in the\n        emulation, and provide access between the emulation nodes and the host\n        node. Service network will not be created unless some layer/service/as\n        asks for it.\n        \"\"\"\n        self.__rendered = False\n        self.__dependencies_db = {}\n        self.__resolved_bindings = {}\n        self.__registry = Registry()\n        self.__layers = LayerDatabase()\n        self.__bindings = BindingDatabase()\n\n        self.__registry.register('seedemu', 'dict', 'layersdb', self.__layers)\n        self.__registry.register('seedemu', 'list', 'bindingdb', self.__bindings)\n\n        self.__service_net_prefix = '192.168.160.0/23'\n        self.__service_net = None\n\n# --- Snippet Separator ---\n\ndef __init__(\n        self,\n        onAsConflict: Callable[[AutonomousSystem, AutonomousSystem], AutonomousSystem] = lambda asA, asB: asA,\n        onIxConflict: Callable[[InternetExchange, InternetExchange], InternetExchange] = lambda ixA, ixB: ixA):\n        \"\"\"!\n        @brief DefaultBaseMerger constructor.\n        @param onAsConflict AS conflict handler. This will be called when the\n        same AS appears in both emulations. This parameter should be a function,\n        two AS objects will be passed in, and a new AS object should be\n        returned. This defaults to returning the AS object in the first\n        emulation.\n        @param onIxConflict IX conflict handler. This will be called when the\n        same IX appears in both emulations. This parameter should be a function,\n        two IX objects will be passed in, and a new IX object should be\n        returned. This defaults to returning the IX object in the first\n        emulation.\n        \"\"\"\n        super().__init__()\n        self.__asConflictHandler = onAsConflict\n        self.__ixConflictHandler = onIxConflict\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS) with AS numbers 150, 151, and 152. Each AS should have a host named 'web' and a router named 'router0'. The 'web' host in each AS should have a web service installed. Each AS should also have a network named 'net0' which both the 'web' host and 'router0' join. AS150 and AS152 should have a cross connection between their routers. An internet exchange with the number 100 should be created and AS150 and AS151 should be peers on this exchange. AS150 should also be a provider for AS152. The emulation should be rendered and compiled using Docker with self-managed network. The compiled emulation should be saved in the directory './cross-connect'.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 73, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that performs a listmode-to-sinograms conversion using the SIRF library. The code should accept command-line options for the path to data files, listmode file, output file prefix, raw data template, scanning time interval to convert, reconstruction engine, acquisition data storage scheme, and an option to disable interactive mode. The code should import the necessary modules, process the command-line options, and define a main function. \n\nIn the main function, it should set the acquisition data storage scheme, read the acquisition data template, create a listmode-to-sinograms converter object, set the input, output and template files, set the time interval, set some flags, set up the converter, perform the conversion, get access to the sinograms, copy the acquisition data into a Python array, print the acquisition data dimensions, and optionally show a 2D array of the acquisition data. \n\nThe code should also estimate randoms, convert the randoms to an array, and optionally show a 2D array of the randoms. The main function should be executed in a try-except block to handle any errors.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class ListmodeToSinograms(object):\n    \"\"\"\n    Class for listmode-to-sinogram converter.\n\n    This class reads list mode data and produces corresponding *sinograms*,\n    i.e. histogrammed data in the format of PETAcquisitionData.\n\n    It has two main functions:\n      - process() can be used to read prompts and/or delayed coincidences to\n        produce a single PETAcquisitionData.\n        Two conversion flags decide what is to be done with 3 possible cases:\n        - `store_prompts`=`true`, `store_delayeds`=`false`:\n        only prompts stored\n        - `store_prompts`=`false`, `store_delayeds`=`true`:\n        only delayeds stored\n        - `store_prompts`=`true`, `store_delayeds`=`true`:\n        prompts-delayeds stored\n        Clearly, enabling the `store_delayeds` option only makes sense if the\n        data was acquired accordingly.\n      - estimate_randoms() can be used to get a relatively noiseless estimate\n        of the random coincidences.\n\n    Currently, the randoms are estimated from the delayed coincidences using\n    the following strategy:\n       1. singles (one per detector) are estimated using a Maximum Likelihood\n          estimator\n       2. randoms-from-singles are computed per detector-pair via the usual\n          product formula. These are then added together for all detector pairs\n          in a certain histogram-bin in the data (accommodating for view\n          mashing and axial compression).\n\n    The actual algorithm is described in\n\n    D. Hogg, K. Thielemans, S. Mustafovic, and T. J. Spinks,\n    \"A study of bias for various iterative reconstruction methods in PET,\"\n    in 2002 IEEE Nuclear Science Symposium Conference Record, vol. 3. IEEE,\n    Nov. 2002, pp. 1519-1523 (http://dx.doi.org/10.1109/nssmic.2002.1239610).\n    \"\"\"\n\n    def __init__(self, file=None):\n        \"\"\"init.\"\"\"\n        self.handle = None\n        self.name = 'ListmodeToSinograms'\n        if file is None:\n            self.handle = pystir.cSTIR_newObject(self.name)\n        else:\n            self.handle = pystir.cSTIR_objectFromFile(self.name, file)\n        self.output = None\n\n    def __del__(self):\n        \"\"\"del.\"\"\"\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n    def set_input(self, lm_file):\n        \"\"\"Sets the listmode file name.\"\"\"\n        parms.set_char_par(self.handle, self.name, 'input', lm_file)\n\n    def set_output_prefix(self, sino_file):\n        \"\"\"Sets the sinograms file names prefix.\"\"\"\n        parms.set_char_par(self.handle, self.name, 'output', sino_file)\n\n    def set_template(self, templ):\n        \"\"\"Sets the sinograms template.\n\n        templ: either file name or AcquisitionData\n        \"\"\"\n        if type(templ) == type('a'):\n            parms.set_char_par(self.handle, self.name, 'template_file', templ)\n        else:\n            parms.set_parameter(self.handle, self.name, 'template', templ.handle)\n\n    def set_time_interval(self, start, stop):\n        \"\"\"Sets the time interval.\n\n        Only data scanned during this time interval will be converted.\n        \"\"\"\n        interval = numpy.ndarray((2,), dtype=numpy.float32)\n        interval[0] = start\n        interval[1] = stop\n        try_calling(pystir.cSTIR_setListmodeToSinogramsInterval(\n            self.handle, interval.ctypes.data))\n\n    def flag_on(self, flag):\n        \"\"\"Switches on (sets to 'true') a conversion flag.\n\n        (see conversion flags description above).\n        \"\"\"\n        try_calling(pystir.cSTIR_setListmodeToSinogramsFlag(\n            self.handle, flag, 1))\n\n    def flag_off(self, flag):\n        \"\"\"Switches off (sets to 'false') a conversion flag.\n\n        (see conversion flags description above).\n        \"\"\"\n        try_calling(pystir.cSTIR_setListmodeToSinogramsFlag(\n            self.handle, flag, 0))\n\n    def set_up(self):\n        \"\"\"Sets up the conversion.\"\"\"\n        try_calling(\n            pystir.cSTIR_setupListmodeToSinogramsConverter(self.handle))\n\n    def process(self):\n        \"\"\"Performs the conversion.\"\"\"\n        self.output = AcquisitionData()\n        self.output.handle = pystir.cSTIR_convertListmodeToSinograms(\n            self.handle)\n        check_status(self.output.handle)\n\n    def get_output(self):\n        \"\"\"Returns the sinograms as an AcquisitionData object.\"\"\"\n        if self.output is None:\n            raise error('Conversion to sinograms not done')\n        return self.output\n\n    def estimate_randoms(self):\n        \"\"\"Returns an estimate of the randoms as an AcquisitionData object.\"\"\"\n        randoms = AcquisitionData()\n        randoms.handle = pystir.cSTIR_computeRandoms(self.handle)\n        check_status(randoms.handle)\n        return randoms\n\n    def get_time_at_which_num_prompts_exceeds_threshold(self, threshold):\n        \"\"\"Returns the time at which the number of prompts exceeds <threshold>.\n\n        Returns -1 if no corresponding time is found.\n        \"\"\"\n        h = pystir.cSTIR_lm_num_prompts_exceeds_threshold(\n            self.handle, float(threshold))\n        check_status(h, inspect.stack()[1])\n        v = pyiutil.floatDataFromHandle(h)\n        pyiutil.deleteDataHandle(h)\n        return v\n\n# --- Snippet Separator ---\n\nclass AcquisitionData(DataContainer):\n    \"\"\"Class for PET acquisition data.\"\"\"\n\n    def __init__(self, src=None, span=1, max_ring_diff=-1, view_mash_factor=1, tof_mash_factor=1):\n        \"\"\"Creates new AcquisitionData.\n\n        Can create object from a file or another AcquisitionData object.\n        src:  file name (Python str) or AcquisitionData object or scanner name\n        \"\"\"\n        self.handle = None\n        self.name = 'AcquisitionData'\n        self.read_only = False\n        self.src = None\n        if src is None:\n            return\n        if isinstance(src, str):\n            i = src.find('.')\n            if i > -1:\n                # src is a file name\n                self.handle = pystir.cSTIR_objectFromFile(\n                    'AcquisitionData', src)\n                self.read_only = self.get_storage_scheme() == 'file'\n                self.src = 'file'\n            else:\n                # src is a scanner name\n                self.handle = pystir.cSTIR_acquisitionDataFromScannerInfo(\n                    src, span, max_ring_diff, view_mash_factor, tof_mash_factor)\n                if pyiutil.executionStatus(self.handle) != 0:\n                    msg = pyiutil.executionError(self.handle)\n                    if msg == 'Unknown scanner':\n                        raise error(\n                            'Unknown scanner ' + src +\n                            ' or missing raw data file extension')\n                self.src = 'scanner'\n        elif isinstance(src, AcquisitionData):\n            # src is AcquisitionData\n            if src.handle is None:\n                raise AssertionError()\n            self.handle = pystir.cSTIR_acquisitionDataFromTemplate(src.handle)\n            self.src = 'template'\n        else:\n            raise error('Wrong source in AcquisitionData constructor')\n        check_status(self.handle)\n\n    def __del__(self):\n        \"\"\"del.\"\"\"\n        # print('deleting AcquisitionData object originated from ', self.src)\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n    @staticmethod\n    def set_storage_scheme(scheme):\n        \"\"\"Sets acquisition data storage scheme.\n\n        scheme = 'file' (default):\n            all acquisition data generated from now on will be kept in\n            scratch files deleted after the user's script terminates\n        scheme = 'memory':\n            all acquisition data generated from now on will be kept in RAM\n            (avoid if data is very large)\n        \"\"\"\n        try_calling(pystir.cSTIR_setAcquisitionDataStorageScheme(scheme))\n\n    @staticmethod\n    def get_storage_scheme():\n        \"\"\"Returns acquisition data storage scheme.\"\"\"\n        handle = pystir.cSTIR_getAcquisitionDataStorageScheme()\n        check_status(handle)\n        scheme = pyiutil.charDataFromHandle(handle)\n        pyiutil.deleteDataHandle(handle)\n        return scheme\n\n    def same_object(self):\n        \"\"\"See DataContainer method.\"\"\"\n        return AcquisitionData()\n\n    def read_from_file(self, filename):  # 'read_from_file' is misleading\n        \"\"\"\n        Reads data from file.\n\n        Replaces the current content of the object.\n        \"\"\"\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n        self.handle = pystir.cSTIR_objectFromFile('AcquisitionData', filename)\n        check_status(self.handle)\n        self.read_only = True\n\n    def create_uniform_image(self, value=0, xy=None):\n        \"\"\"Crates uniform image.\n\n        Creates ImageData object containing PET image of z-dimension\n        and voxel sizes compatible with the scanner geometry stored\n        in this AcquisitionData object and assigns a given value\n        to all voxels;\n        value: a Python float.\n        xy   : x and y dimensions tuple (if None, set by STIR)\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        image = ImageData()\n        if xy is None:\n            image.handle = pystir.cSTIR_imageFromAcquisitionData(self.handle)\n        elif isinstance(xy, tuple):\n            image.handle = pystir.cSTIR_imageFromAcquisitionDataAndNxNy(\n                self.handle, xy[1], xy[0])\n        elif isinstance(xy, int):\n            image.handle = pystir.cSTIR_imageFromAcquisitionDataAndNxNy(\n                self.handle, xy, xy)\n        else:\n            raise error('Wrong second argument in create_uniform_image')\n        check_status(image.handle)\n        image.fill(value)\n        return image\n\n    def dimensions(self):\n        \"\"\"Returns a tuple of the data dimensions.\n\n        Contains:\n        - number of TOF bins\n        - number of sinograms\n        - number of views\n        - number of tangential positions.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        dim = numpy.ndarray((MAX_ACQ_DIMS,), dtype=cpp_int_dtype())\n        try_calling(pystir.cSTIR_getAcquisitionDataDimensions(\n            self.handle, dim.ctypes.data))\n        dim = dim[:4]\n        return tuple(dim[::-1])\n\n    def get_tof_mash_factor(self):\n        '''Returns TOF mashing factor.'''\n        return parms.int_par(self.handle, 'AcquisitionData', 'tof_mash_factor')\n\n    def as_array(self):\n        \"\"\"Returns bin values as ndarray.\n\n        Return a copy of acquisition data stored in this object as a\n        NumPy ndarray of 4 dimensions (in default C ordering of data):\n        - number of TOF bins\n        - number of sinograms\n        - number of views\n        - number of tangential positions.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        array = numpy.ndarray(self.dimensions(), dtype=numpy.float32)\n        try_calling(pystir.cSTIR_getAcquisitionData(\n            self.handle, array.ctypes.data))\n        return array\n\n    def fill(self, value):\n        \"\"\"Fills the object with values.\n\n        value:  either NumPy ndarray or another AcquisitionData object\n                or Python float.\n        \"\"\"\n        if self.handle is None:\n            raise AssertionError()\n        if self.read_only:\n            raise error(\n                'Cannot fill read-only object, consider filling a clone')\n        if isinstance(value, numpy.ndarray):\n            dims = self.dimensions()\n            shape = value.shape\n            if shape != dims:\n                msg = 'cannot fill AcquisitionData of size %s' \\\n                      + ' with data of size %s'\n                raise ValueError(msg % (repr(dims), repr(shape)))\n            if value.dtype is numpy.dtype('float32'):\n                # print('keeping dtype float32')\n                v = value\n            else:\n                # print('changing dtype to float32')\n                v = value.astype(numpy.float32)\n            if not v.flags['C_CONTIGUOUS']:\n                v = numpy.ascontiguousarray(v)\n            try_calling(pystir.cSTIR_setAcquisitionData(\n                self.handle, v.ctypes.data))\n        elif isinstance(value, AcquisitionData):\n            if value.handle is None:\n                raise AssertionError()\n            try_calling(pystir.cSTIR_fillAcquisitionDataFromAcquisitionData(\n                self.handle, value.handle))\n        elif isinstance(value, float):\n            try_calling(pystir.cSTIR_fillAcquisitionData(self.handle, value))\n        elif isinstance(value, (Integral,numpy.number)):\n            try_calling(pystir.cSTIR_fillAcquisitionData(\n                self.handle, float(value)))\n        else:\n            raise TypeError('Wrong fill value.' + \\\n                ' Should be numpy.ndarray, AcquisitionData, float or int, got {}'\\\n                .format(type(value)))\n        return self\n\n    def get_uniform_copy(self, value=0):\n        \"\"\"Returns a copy of this object filled with given value.\n\n        Returns a true copy of this object filled with a given value;\n        value:  a Python float.\n        \"\"\"\n        ad = AcquisitionData(self)\n        ad.fill(value)\n        ad.src = 'copy'\n        return ad\n\n    def rebin(self, num_segments_to_combine,\n              num_views_to_combine=1, num_tang_poss_to_trim=0,\n              do_normalisation=True, max_in_segment_num_to_process=-1,\n              num_tof_bins_to_combine=1):\n        \"\"\"Re-bins the data to lower resolution.\n\n        Keyword arguments:\n\t\tnum_segments_to_combine -- combines multiple oblique 'segments' together. If set to the\n\t\t    total number of segments, this corresponds to SSRB. Another example is if the input data\n\t\t\thas 'span=1', the output span will be equal to the \\c num_segments_to_combine.\n\t\tnum_views_to_combine -- combines neighbouring views. Needs to be a divisor of the total\n\t\t    number of views in the data.\n\t\tnum_tang_poss_to_trim -- removes a number of tangential positions (horizontal direction\n\t\t    in the sinogram) at each end\n\t\tdo_normalisation -- if True, averages the data, otherwise it adds the data. Often\n\t\t    the latter is required for emission data (as it preserves Poisson statistics),\n\t\t\twhile the former should be used for corrected data (or for attenuation correction factors).\n\t\tmax_in_segment_num_to_process -- by default all input data are used. If set to a non-negative\n\t\t    number, it will remove the most oblique segments.\n\t\tnum_tof_bins_to_combine -- number of TOF bins to combine.\n        \"\"\"\n        ad = AcquisitionData()\n        ad.handle = pystir.cSTIR_rebinnedAcquisitionData(\n            self.handle,\n            num_segments_to_combine, num_views_to_combine,\n            num_tang_poss_to_trim, do_normalisation,\n            max_in_segment_num_to_process, num_tof_bins_to_combine)\n        check_status(ad.handle)\n        return ad\n\n    def show(self, sino=None, tof=0, title=None):\n        '''Displays selected sinograms.'''\n        if self.handle is None:\n            raise AssertionError()\n        if not HAVE_PYLAB:\n            print('pylab not found')\n            return\n        data = self.as_array()\n        if tof <0 or tof >= data.shape[0]:\n            raise IndexError('TOF bin index out of range')\n        nz = data.shape[1]\n        if isinstance(sino, (Integral,numpy.integer)):\n            if sino < 0 or sino >= nz:\n                raise IndexError('Slice index out of range')\n            show_2D_array('sinogram %d' % sino, data[tof, sino, :, :])\n            return\n        elif sino is None:\n            ns = nz\n            sino = range(nz)\n        else:\n            try:\n                ns = len(sino)\n            except:\n                raise error('wrong sinograms list')\n        if title is None:\n            title = 'Selected sinograms'\n        if ns >= 16:\n            tiles = (4, 4)\n        else:\n            tiles = None\n        f = 0\n        while f < ns:\n            t = min(f + 16, ns)\n            show_3D_array(\n                data[0, :, :, :],\n                index=sino[f: t], tile_shape=tiles,\n                label='sinogram',\n                xlabel='tang.pos', ylabel='view',\n                suptitle=title, show=(t == ns))\n            f = t\n\n    def allocate(self, value=0, **kwargs):\n        \"\"\"Alias to get_uniform_copy.\n\n        CIL/SIRF compatibility\n        \"\"\"\n        if value in ['random', 'random_int']:\n            out = self.get_uniform_copy()\n            shape = out.as_array().shape\n            seed = kwargs.get('seed', None)\n            if seed is not None:\n                numpy.random.seed(seed)\n            if value == 'random':\n                out.fill(numpy.random.random_sample(shape))\n            elif value == 'random_int':\n                max_value = kwargs.get('max_value', 100)\n                out.fill(numpy.random.randint(max_value,size=shape))\n        elif value is None:\n            out = self.get_uniform_copy(0)\n        else:\n            out = self.get_uniform_copy(value)\n        return out\n\n    def get_info(self):\n        \"\"\"Returns the AcquisitionData's metadata as Python str.\"\"\"\n        handle = pystir.cSTIR_get_ProjDataInfo(self.handle)\n        check_status(handle)\n        info = pyiutil.charDataFromHandle(handle)\n        pyiutil.deleteDataHandle(handle)\n        return info\n\n    def get_subset(self, views):\n        \"\"\"Returns the subset of self data formed by specified views\n\n        views: array of views (will be converted to numpy ndarray)\n        \"\"\"\n        # Ensure the array passed to C++ is a contiguous array of C++ int's\n        v = cpp_int_array(views)\n        n = len(views)\n        subset = AcquisitionData()\n        subset.handle = pystir.cSTIR_get_subset(self.handle, n, v.ctypes.data)\n        check_status(subset.handle)\n        return subset\n\n    @property\n    def shape(self):\n        return self.dimensions()\n\n# --- Snippet Separator ---\n\ndef set_storage_scheme(scheme):\n        \"\"\"Sets acquisition data storage scheme.\n\n        scheme = 'file' (default):\n            all acquisition data generated from now on will be kept in\n            scratch files deleted after the user's script terminates\n        scheme = 'memory':\n            all acquisition data generated from now on will be kept in RAM\n            (avoid if data is very large)\n        \"\"\"\n        try_calling(pystir.cSTIR_setAcquisitionDataStorageScheme(scheme))\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a listmode-to-sinograms conversion using the SIRF library. The code should accept command-line options for the path to data files, listmode file, output file prefix, raw data template, scanning time interval to convert, reconstruction engine, acquisition data storage scheme, and an option to disable interactive mode. The code should import the necessary modules, process the command-line options, and define a main function. \n\nIn the main function, it should set the acquisition data storage scheme, read the acquisition data template, create a listmode-to-sinograms converter object, set the input, output and template files, set the time interval, set some flags, set up the converter, perform the conversion, get access to the sinograms, copy the acquisition data into a Python array, print the acquisition data dimensions, and optionally show a 2D array of the acquisition data. \n\nThe code should also estimate randoms, convert the randoms to an array, and optionally show a 2D array of the randoms. The main function should be executed in a try-except block to handle any errors.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 74, "repo_full_name": "deepmind__acme", "instruction": "Generate code that sets up and runs a Continuous Q-Learning (CQL) agent on a specified environment using the Acme library. The agent should be configured with various parameters such as batch size, evaluation period, number of demonstration episodes, random seed, learning rates, and CQL specific parameters. The environment should be created using the specified environment name and a demonstrations dataset should be obtained from a specified dataset name. The agent's networks should be created and optimized, and an evaluator network should be defined. The agent should then be run in an environment loop, where it learns and evaluates itself periodically.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class ActorLearnerBuilder(OfflineBuilder[Networks, Policy, Sample],\n                          Generic[Networks, Policy, Sample]):\n  \"\"\"Defines an interface for defining the components of an RL agent.\n\n  Implementations of this interface contain a complete specification of a\n  concrete RL agent. An instance of this class can be used to build an\n  RL agent which interacts with the environment either locally or in a\n  distributed setup.\n  \"\"\"\n\n  @abc.abstractmethod\n  def make_replay_tables(\n      self,\n      environment_spec: specs.EnvironmentSpec,\n      policy: Policy,\n  ) -> List[reverb.Table]:\n    \"\"\"Create tables to insert data into.\n\n    Args:\n      environment_spec: A container for all relevant environment specs.\n      policy: Agent's policy which can be used to extract the extras_spec.\n\n    Returns:\n      The replay tables used to store the experience the agent uses to train.\n    \"\"\"\n\n  @abc.abstractmethod\n  def make_dataset_iterator(\n      self,\n      replay_client: reverb.Client,\n  ) -> Iterator[Sample]:\n    \"\"\"Create a dataset iterator to use for learning/updating the agent.\"\"\"\n\n  @abc.abstractmethod\n  def make_adder(\n      self,\n      replay_client: reverb.Client,\n      environment_spec: Optional[specs.EnvironmentSpec],\n      policy: Optional[Policy],\n  ) -> Optional[adders.Adder]:\n    \"\"\"Create an adder which records data generated by the actor/environment.\n\n    Args:\n      replay_client: Reverb Client which points to the replay server.\n      environment_spec: specs of the environment.\n      policy: Agent's policy which can be used to extract the extras_spec.\n    \"\"\"\n    # TODO(sabela): make the parameters non-optional.\n\n  @abc.abstractmethod\n  def make_actor(\n      self,\n      random_key: networks_lib.PRNGKey,\n      policy: Policy,\n      environment_spec: specs.EnvironmentSpec,\n      variable_source: Optional[core.VariableSource] = None,\n      adder: Optional[adders.Adder] = None,\n  ) -> core.Actor:\n    \"\"\"Create an actor instance.\n\n    Args:\n      random_key: A key for random number generation.\n      policy: Instance of a policy expected by the algorithm corresponding to\n        this builder.\n      environment_spec: A container for all relevant environment specs.\n      variable_source: A source providing the necessary actor parameters.\n      adder: How data is recorded (e.g. added to replay).\n    \"\"\"\n\n  @abc.abstractmethod\n  def make_learner(\n      self,\n      random_key: networks_lib.PRNGKey,\n      networks: Networks,\n      dataset: Iterator[Sample],\n      logger_fn: loggers.LoggerFactory,\n      environment_spec: specs.EnvironmentSpec,\n      replay_client: Optional[reverb.Client] = None,\n      counter: Optional[counting.Counter] = None,\n  ) -> core.Learner:\n    \"\"\"Creates an instance of the learner.\n\n    Args:\n      random_key: A key for random number generation.\n      networks: struct describing the networks needed by the learner; this can\n        be specific to the learner in question.\n      dataset: iterator over samples from replay.\n      logger_fn: factory providing loggers used for logging progress.\n      environment_spec: A container for all relevant environment specs.\n      replay_client: client which allows communication with replay. Note that\n        this is only intended to be used for updating priorities. Samples should\n        be obtained from `dataset`.\n      counter: a Counter which allows for recording of counts (learner steps,\n        actor steps, etc.) distributed throughout the agent.\n    \"\"\"\n\n  def make_policy(self,\n                  networks: Networks,\n                  environment_spec: specs.EnvironmentSpec,\n                  evaluation: bool = False) -> Policy:\n    \"\"\"Creates the agent policy.\n\n       Creates the agent policy given the collection of network components and\n       environment spec. An optional boolean can be given to indicate if the\n       policy will be used for evaluation.\n\n    Args:\n      networks: struct describing the networks needed to generate the policy.\n      environment_spec: struct describing the specs of the environment.\n      evaluation: when true, a version of the policy to use for evaluation\n        should be returned. This is algorithm-specific so if an algorithm makes\n        no distinction between behavior and evaluation policies this boolean may\n        be ignored.\n\n    Returns:\n      Behavior policy or evaluation policy for the agent.\n    \"\"\"\n    # TODO(sabela): make abstract once all agents implement it.\n    del networks, environment_spec, evaluation\n    raise NotImplementedError\n\n# --- Snippet Separator ---\n\nclass OfflineExperimentConfig(Generic[builders.Networks, builders.Policy,\n                                      builders.Sample]):\n  \"\"\"Config which defines aspects of constructing an offline RL experiment.\n\n  This class is similar to the ExperimentConfig, but is tailored to offline RL\n  setting, so it excludes attributes related to training via interaction with\n  the environment (max_num_actor_steps, policy_network_factory) and instead\n  includes attributes specific to learning from demonstration.\n\n  Attributes:\n    builder: Builds components of an offline RL agent (Learner and Evaluator).\n    network_factory: Builds networks used by the agent.\n    demonstration_dataset_factory: Function that returns an iterator over\n      demonstrations.\n    environment_spec: Specification of the environment.\n    max_num_learner_steps: How many learner steps to perform.\n    seed: Seed used for agent initialization.\n    evaluator_factories: Factories of policy evaluators. When not specified the\n      default evaluators are constructed using eval_policy_network_factory. Set\n      to an empty list to disable evaluators.\n    eval_policy_factory: Policy factory used by evaluators. Should be specified\n      to use the default evaluators (when evaluator_factories is not provided).\n    environment_factory: Returns an instance of an environment to be used for\n      evaluation. Should be specified to use the default evaluators (when\n      evaluator_factories is not provided).\n    observers: Observers used for extending logs with custom information.\n    logger_factory: Loggers factory used to construct loggers for learner,\n      actors and evaluators.\n    checkpointing: Configuration options for checkpointing. If None,\n      checkpointing and snapshotting is disabled.\n  \"\"\"\n  # Below fields must be explicitly specified for any Agent.\n  builder: builders.OfflineBuilder[builders.Networks, builders.Policy,\n                                   builders.Sample]\n  network_factory: Callable[[specs.EnvironmentSpec], builders.Networks]\n  demonstration_dataset_factory: Callable[[types.PRNGKey],\n                                          Iterator[builders.Sample]]\n  environment_factory: types.EnvironmentFactory\n  max_num_learner_steps: int\n  seed: int\n  # Fields below are optional. If you just started with Acme do not worry about\n  # them. You might need them later when you want to customize your RL agent.\n  # TODO(stanczyk): Introduce a marker for the default value (instead of None).\n  evaluator_factories: Optional[Sequence[EvaluatorFactory]] = None\n  environment_spec: Optional[specs.EnvironmentSpec] = None\n  observers: Sequence[observers_lib.EnvLoopObserver] = ()\n  logger_factory: loggers.LoggerFactory = dataclasses.field(\n      default_factory=experiment_utils.create_experiment_logger_factory)\n  checkpointing: Optional[CheckpointingConfig] = CheckpointingConfig()\n\n  # TODO(stanczyk): Make get_evaluator_factories a standalone function.\n  def get_evaluator_factories(self):\n    \"\"\"Constructs the evaluator factories.\"\"\"\n    if self.evaluator_factories is not None:\n      return self.evaluator_factories\n    if self.environment_factory is None:\n      raise ValueError(\n          'You need to set `environment_factory` in `OfflineExperimentConfig` '\n          'when `evaluator_factories` are not specified. To disable evaluation '\n          'altogether just set `evaluator_factories = []`')\n\n    return [\n        default_evaluator_factory(\n            environment_factory=self.environment_factory,\n            network_factory=self.network_factory,\n            policy_factory=self.builder.make_policy,\n            logger_factory=self.logger_factory,\n            observers=self.observers)\n    ]\n\n# --- Snippet Separator ---\n\ndef test_train(self):\n    seed = 0\n    num_iterations = 6\n    batch_size = 64\n\n    # Create a fake environment to test with.\n    environment = fakes.ContinuousEnvironment(\n        episode_length=10, bounded=True, action_dim=6)\n    spec = specs.make_environment_spec(environment)\n\n    # Construct the agent.\n    networks = cql.make_networks(\n        spec, hidden_layer_sizes=(8, 8))\n    dataset = fakes.transition_iterator(environment)\n    key = jax.random.PRNGKey(seed)\n    learner = cql.CQLLearner(\n        batch_size,\n        networks,\n        key,\n        demonstrations=dataset(batch_size),\n        policy_optimizer=optax.adam(3e-5),\n        critic_optimizer=optax.adam(3e-4),\n        fixed_cql_coefficient=5.,\n        cql_lagrange_threshold=None,\n        target_entropy=0.1,\n        num_bc_iters=2,\n        num_sgd_steps_per_step=1)\n\n    # Train the agent\n    for _ in range(num_iterations):\n      learner.step()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up and runs a Continuous Q-Learning (CQL) agent on a specified environment using the Acme library. The agent should be configured with various parameters such as batch size, evaluation period, number of demonstration episodes, random seed, learning rates, and CQL specific parameters. The environment should be created using the specified environment name and a demonstrations dataset should be obtained from a specified dataset name. The agent's networks should be created and optimized, and an evaluator network should be defined. The agent should then be run in an environment loop, where it learns and evaluates itself periodically.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 75, "repo_full_name": "oarriaga__paz", "instruction": "Generate code that defines several classes for preprocessing, augmenting, and visualizing bounding boxes and images for object detection using the paz library. The classes should include:\n\n1. `PreprocessBoxes` that preprocesses bounding boxes by matching them, encoding them, and converting the box class to a one-hot vector.\n2. `PreprocessImage` that preprocesses an RGB image by resizing it and either subtracting a mean or normalizing it.\n3. `AugmentImage` that augments an RGB image by resizing it, blending it with a random cropped background, and applying random contrast, brightness, saturation, and hue adjustments.\n4. `AugmentBoxes` that augments bounding boxes by converting them to image box coordinates, expanding them, applying random sample cropping and random flipping.\n5. `DrawBoxData2D` that draws 2D box data on an image.\n6. `ShowBoxes` that shows the boxes on an image after resizing the image, decoding the boxes, denormalizing them, and drawing them on the image.\n7. `AugmentDetection` that augments boxes and images for object detection by loading an image, applying image and box augmentation if in training mode, preprocessing the image and boxes, and wrapping the sequence.\n\nFinally, the code should also include a main section that sets up GPU memory growth, loads a model and data, and applies the `AugmentDetection` and `ShowBoxes` processors to each sample in the dataset.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class AugmentDetection(SequentialProcessor):\n    \"\"\"Augment boxes and images for object detection.\n\n    # Arguments\n        prior_boxes: Numpy array of shape ``[num_boxes, 4]`` containing\n            prior/default bounding boxes.\n        split: Flag from `paz.processors.TRAIN`, ``paz.processors.VAL``\n            or ``paz.processors.TEST``. Certain transformations would take\n            place depending on the flag.\n        num_classes: Int.\n        size: Int. Image size.\n        mean: List of three elements indicating the per channel mean.\n        IOU: Float. Intersection over union used to match boxes.\n        variances: List of two floats indicating variances to be encoded\n            for encoding bounding boxes.\n    \"\"\"\n    def __init__(self, prior_boxes, split=pr.TRAIN, num_classes=21, size=300,\n                 mean=pr.BGR_IMAGENET_MEAN, IOU=.5,\n                 variances=[0.1, 0.1, 0.2, 0.2]):\n        super(AugmentDetection, self).__init__()\n        # image processors\n        self.augment_image = AugmentImage()\n        # self.augment_image.add(pr.ConvertColorSpace(pr.RGB2BGR))\n        self.preprocess_image = PreprocessImage((size, size), mean)\n        self.preprocess_image.insert(0, pr.ConvertColorSpace(pr.RGB2BGR))\n\n        # box processors\n        self.augment_boxes = AugmentBoxes()\n        args = (num_classes, prior_boxes, IOU, variances)\n        self.preprocess_boxes = PreprocessBoxes(*args)\n\n        # pipeline\n        self.add(pr.UnpackDictionary(['image', 'boxes']))\n        self.add(pr.ControlMap(pr.LoadImage(), [0], [0]))\n        if split == pr.TRAIN:\n            self.add(pr.ControlMap(self.augment_image, [0], [0]))\n            self.add(pr.ControlMap(self.augment_boxes, [0, 1], [0, 1]))\n        self.add(pr.ControlMap(self.preprocess_image, [0], [0]))\n        self.add(pr.ControlMap(self.preprocess_boxes, [1], [1]))\n        self.add(pr.SequenceWrapper(\n            {0: {'image': [size, size, 3]}},\n            {1: {'boxes': [len(prior_boxes), 4 + num_classes]}}))\n\n# --- Snippet Separator ---\n\ndef build_strides(branch_arg, image_shape, branches, num_scale_aspect):\n    \"\"\"Builds branch-wise EfficientNet anchor box strides.\n    The stride of an anchor box determines how densely the anchor boxes\n    are placed in the image. A smaller stride means that the anchor\n    boxes are more densely packed and cover a larger area of the image,\n    while a larger stride means that the anchor boxes are less densely\n    packed and cover a smaller area of the image.\n    In general, a smaller stride is more effective at detecting smaller\n    objects, while a larger stride is more effective at detecting larger\n    objects. The optimal stride for a particular object detection system\n    will depend on the sizes of the objects that it is trying to detect\n    and the resolution of the input images. The following shows an\n    example visualization of anchor box's centre marked by + each with\n    same octave and aspect ratio and scale but with different strides.\n\n            8.0                     16.0                     32.0\n    +-----------------+      +-----------------+     +-----------------+\n    | + + + + + + + + |      |                 |     |   +    +    +   |\n    | + + + + + + + + |      |  +  +  +  +  +  |     |                 |\n    | + + + + + + + + |      |                 |     |                 |\n    | + + + + + + + + |      |  +  +  +  +  +  |     |   +    +    +   |\n    | + + + + + + + + |      |                 |     |                 |\n    | + + + + + + + + |      |  +  +  +  +  +  |     |                 |\n    | + + + + + + + + |      |                 |     |   +    +    +   |\n    +-----------------+      +-----------------+     +-----------------+\n\n    # Arguments\n        branch_arg: Int, branch index.\n        image_shape: List, input image shape.\n        branches: List, EfficientNet branch tensors.\n        num_scale_aspect: Int, count of scale aspect ratio combinations.\n\n    # Returns\n        Tuple: Containing strides in y and x direction.\n    \"\"\"\n    H_image, W_image = image_shape\n    feature_H, feature_W = branches[branch_arg].shape[1:3]\n    features_H = np.repeat(feature_H, num_scale_aspect).astype('float32')\n    features_W = np.repeat(feature_W, num_scale_aspect).astype('float32')\n    strides_y = H_image / features_H\n    strides_x = W_image / features_W\n    return strides_y, strides_x\n\n# --- Snippet Separator ---\n\nclass RandomSampleCrop(Processor):\n    \"\"\"Crops image while adjusting the normalized corner form\n    bounding boxes.\n\n    # Arguments\n        probability: Float between ''[0, 1]''.\n    \"\"\"\n    def __init__(self, probability=0.50, max_trials=50):\n        self.probability = probability\n        self.max_trials = max_trials\n        self.jaccard_min_max = (\n            None,\n            (0.1, np.inf),\n            (0.3, np.inf),\n            (0.7, np.inf),\n            (0.9, np.inf),\n            (-np.inf, np.inf))\n\n    def call(self, image, boxes):\n\n        if self.probability < np.random.rand():\n            return image, boxes\n\n        labels = boxes[:, -1:]\n        boxes = boxes[:, :4]\n        H_original, W_original = image.shape[:2]\n\n        mode = np.random.randint(0, len(self.jaccard_min_max), 1)[0]\n        if self.jaccard_min_max[mode] is not None:\n            min_iou, max_iou = self.jaccard_min_max[mode]\n            for trial_arg in range(self.max_trials):\n                W = np.random.uniform(0.3 * W_original, W_original)\n                H = np.random.uniform(0.3 * H_original, H_original)\n                aspect_ratio = H / W\n                if (aspect_ratio < 0.5) or (aspect_ratio > 2):\n                    continue\n                x_min = np.random.uniform(W_original - W)\n                y_min = np.random.uniform(H_original - H)\n                x_max = int(x_min + W)\n                y_max = int(y_min + H)\n                x_min = int(x_min)\n                y_min = int(y_min)\n\n                image_crop_box = np.array([x_min, y_min, x_max, y_max])\n                overlap = compute_iou(image_crop_box, boxes)\n                if ((overlap.max() < min_iou) or (overlap.min() > max_iou)):\n                    continue\n\n                centers = (boxes[:, :2] + boxes[:, 2:]) / 2.0\n                centers_above_x_min = x_min < centers[:, 0]\n                centers_above_y_min = y_min < centers[:, 1]\n                centers_below_x_max = x_max > centers[:, 0]\n                centers_below_y_max = y_max > centers[:, 1]\n                mask = (centers_above_x_min * centers_above_y_min *\n                        centers_below_x_max * centers_below_y_max)\n                if not mask.any():\n                    continue\n\n                cropped_image = image[y_min:y_max, x_min:x_max, :].copy()\n                masked_boxes = boxes[mask, :].copy()\n                masked_labels = labels[mask].copy()\n                # should we use the box left and top corner or the crop's\n                masked_boxes[:, :2] = np.maximum(masked_boxes[:, :2],\n                                                 image_crop_box[:2])\n                # adjust to crop (by substracting crop's left,top)\n                masked_boxes[:, :2] -= image_crop_box[:2]\n                masked_boxes[:, 2:] = np.minimum(masked_boxes[:, 2:],\n                                                 image_crop_box[2:])\n                # adjust to crop (by substracting crop's left,top)\n                masked_boxes[:, 2:] -= image_crop_box[:2]\n                return cropped_image, np.hstack([masked_boxes, masked_labels])\n\n        boxes = np.hstack([boxes, labels])\n        return image, boxes\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that defines several classes for preprocessing, augmenting, and visualizing bounding boxes and images for object detection using the paz library. The classes should include:\n\n1. `PreprocessBoxes` that preprocesses bounding boxes by matching them, encoding them, and converting the box class to a one-hot vector.\n2. `PreprocessImage` that preprocesses an RGB image by resizing it and either subtracting a mean or normalizing it.\n3. `AugmentImage` that augments an RGB image by resizing it, blending it with a random cropped background, and applying random contrast, brightness, saturation, and hue adjustments.\n4. `AugmentBoxes` that augments bounding boxes by converting them to image box coordinates, expanding them, applying random sample cropping and random flipping.\n5. `DrawBoxData2D` that draws 2D box data on an image.\n6. `ShowBoxes` that shows the boxes on an image after resizing the image, decoding the boxes, denormalizing them, and drawing them on the image.\n7. `AugmentDetection` that augments boxes and images for object detection by loading an image, applying image and box augmentation if in training mode, preprocessing the image and boxes, and wrapping the sequence.\n\nFinally, the code should also include a main section that sets up GPU memory growth, loads a model and data, and applies the `AugmentDetection` and `ShowBoxes` processors to each sample in the dataset.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 76, "repo_full_name": "pybamm-team__pybamm", "instruction": "Generate code that creates a custom lithium-ion model using the pybamm library. The model should be named \"my li-ion model\" and should include submodels for the external circuit, current collector, thermal, porosity, electrolyte diffusion, electrolyte conductivity, SEI, SEI on cracks, and lithium plating. For both the negative and positive electrode domains, the model should include submodels for active material, electrode potential, particle, total particle concentration, open-circuit potential, interface, interface utilisation, interface current, surface potential difference, and particle mechanics. After defining the model, build it, create the geometry, process the model and geometry, set the mesh, discretise the model, and solve it. Finally, plot the solution dynamically.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class SPM(BaseModel):\n    \"\"\"Single Particle Model (SPM) of a lithium-ion battery, from [1]_.\n\n    Parameters\n    ----------\n    options : dict, optional\n        A dictionary of options to be passed to the model.\n    name : str, optional\n        The name of the model.\n    build :  bool, optional\n        Whether to build the model on instantiation. Default is True. Setting this\n        option to False allows users to change any number of the submodels before\n        building the complete model (submodels cannot be changed after the model is\n        built).\n\n    References\n    ----------\n    .. [1] SG Marquis, V Sulzer, R Timms, CP Please and SJ Chapman. “An asymptotic\n           derivation of a single particle model with electrolyte”. In: arXiv preprint\n           arXiv:1905.12553 (2019).\n\n    **Extends:** :class:`pybamm.lithium_ion.BaseModel`\n    \"\"\"\n\n    def __init__(self, options=None, name=\"Single Particle Model\", build=True):\n        super().__init__(options, name)\n\n        self.set_porosity_submodel()\n        self.set_convection_submodel()\n        self.set_interfacial_submodel()\n        self.set_particle_submodel()\n        self.set_negative_electrode_submodel()\n        self.set_electrolyte_submodel()\n        self.set_positive_electrode_submodel()\n        self.set_thermal_submodel()\n        self.set_current_collector_submodel()\n\n        if build:\n            self.build_model()\n\n    def set_porosity_submodel(self):\n\n        self.submodels[\"porosity\"] = pybamm.porosity.Constant(self.param)\n\n    def set_convection_submodel(self):\n\n        self.submodels[\"convection\"] = pybamm.convection.NoConvection(self.param)\n\n    def set_interfacial_submodel(self):\n\n        self.submodels[\n            \"negative interface\"\n        ] = pybamm.interface.lithium_ion.InverseButlerVolmer(self.param, \"Negative\")\n        self.submodels[\n            \"positive interface\"\n        ] = pybamm.interface.lithium_ion.InverseButlerVolmer(self.param, \"Positive\")\n\n    def set_particle_submodel(self):\n\n        if self.options[\"particle\"] == \"Fickian diffusion\":\n            self.submodels[\n                \"negative particle\"\n            ] = pybamm.particle.fickian.SingleParticle(self.param, \"Negative\")\n            self.submodels[\n                \"positive particle\"\n            ] = pybamm.particle.fickian.SingleParticle(self.param, \"Positive\")\n        elif self.options[\"particle\"] == \"fast diffusion\":\n            self.submodels[\"negative particle\"] = pybamm.particle.fast.SingleParticle(\n                self.param, \"Negative\"\n            )\n            self.submodels[\"positive particle\"] = pybamm.particle.fast.SingleParticle(\n                self.param, \"Positive\"\n            )\n\n    def set_negative_electrode_submodel(self):\n\n        self.submodels[\"negative electrode\"] = pybamm.electrode.ohm.LeadingOrder(\n            self.param, \"Negative\"\n        )\n\n    def set_positive_electrode_submodel(self):\n\n        self.submodels[\"positive electrode\"] = pybamm.electrode.ohm.LeadingOrder(\n            self.param, \"Positive\"\n        )\n\n    def set_electrolyte_submodel(self):\n\n        electrolyte = pybamm.electrolyte.stefan_maxwell\n\n        self.submodels[\n            \"electrolyte conductivity\"\n        ] = electrolyte.conductivity.LeadingOrder(self.param)\n        self.submodels[\n            \"electrolyte diffusion\"\n        ] = electrolyte.diffusion.ConstantConcentration(self.param)\n\n    @property\n    def default_geometry(self):\n        dimensionality = self.options[\"dimensionality\"]\n        if dimensionality == 0:\n            return pybamm.Geometry(\"1D macro\", \"1D micro\")\n        elif dimensionality == 1:\n            return pybamm.Geometry(\"1+1D macro\", \"(1+0)+1D micro\")\n        elif dimensionality == 2:\n            return pybamm.Geometry(\"2+1D macro\", \"(2+0)+1D micro\")\n\n    @property\n    def default_solver(self):\n        \"\"\"\n        Create and return the default solver for this model\n        \"\"\"\n        # Different solver depending on whether we solve ODEs or DAEs\n        dimensionality = self.options[\"dimensionality\"]\n        if dimensionality == 0:\n            return pybamm.ScipySolver()\n        else:\n            return pybamm.ScikitsDaeSolver()\n\n# --- Snippet Separator ---\n\nclass SPMe(BaseModel):\n    \"\"\"Single Particle Model with Electrolyte (SPMe) of a lithium-ion battery, from\n    [1]_.\n\n    Parameters\n    ----------\n    options : dict, optional\n        A dictionary of options to be passed to the model.\n    name : str, optional\n        The name of the model.\n    build :  bool, optional\n        Whether to build the model on instantiation. Default is True. Setting this\n        option to False allows users to change any number of the submodels before\n        building the complete model (submodels cannot be changed after the model is\n        built).\n\n    References\n    ----------\n    .. [1] SG Marquis, V Sulzer, R Timms, CP Please and SJ Chapman. “An asymptotic\n           derivation of a single particle model with electrolyte”. In: arXiv preprint\n           arXiv:1905.12553 (2019).\n\n    **Extends:** :class:`pybamm.lithium_ion.BaseModel`\n    \"\"\"\n\n    def __init__(\n        self, options=None, name=\"Single Particle Model with electrolyte\", build=True\n    ):\n        super().__init__(options, name)\n\n        self.set_reactions()\n        self.set_porosity_submodel()\n        self.set_convection_submodel()\n        self.set_interfacial_submodel()\n        self.set_particle_submodel()\n        self.set_negative_electrode_submodel()\n        self.set_electrolyte_submodel()\n        self.set_positive_electrode_submodel()\n        self.set_thermal_submodel()\n        self.set_current_collector_submodel()\n\n        if build:\n            self.build_model()\n\n    def set_porosity_submodel(self):\n\n        self.submodels[\"porosity\"] = pybamm.porosity.Constant(self.param)\n\n    def set_convection_submodel(self):\n\n        self.submodels[\"convection\"] = pybamm.convection.NoConvection(self.param)\n\n    def set_interfacial_submodel(self):\n\n        self.submodels[\n            \"negative interface\"\n        ] = pybamm.interface.lithium_ion.InverseButlerVolmer(self.param, \"Negative\")\n        self.submodels[\n            \"positive interface\"\n        ] = pybamm.interface.lithium_ion.InverseButlerVolmer(self.param, \"Positive\")\n\n    def set_particle_submodel(self):\n\n        if self.options[\"particle\"] == \"Fickian diffusion\":\n            self.submodels[\n                \"negative particle\"\n            ] = pybamm.particle.fickian.SingleParticle(self.param, \"Negative\")\n            self.submodels[\n                \"positive particle\"\n            ] = pybamm.particle.fickian.SingleParticle(self.param, \"Positive\")\n        elif self.options[\"particle\"] == \"fast diffusion\":\n            self.submodels[\"negative particle\"] = pybamm.particle.fast.SingleParticle(\n                self.param, \"Negative\"\n            )\n            self.submodels[\"positive particle\"] = pybamm.particle.fast.SingleParticle(\n                self.param, \"Positive\"\n            )\n\n    def set_negative_electrode_submodel(self):\n\n        self.submodels[\"negative electrode\"] = pybamm.electrode.ohm.Composite(\n            self.param, \"Negative\"\n        )\n\n    def set_positive_electrode_submodel(self):\n\n        self.submodels[\"positive electrode\"] = pybamm.electrode.ohm.Composite(\n            self.param, \"Positive\"\n        )\n\n    def set_electrolyte_submodel(self):\n\n        electrolyte = pybamm.electrolyte.stefan_maxwell\n\n        self.submodels[\"electrolyte conductivity\"] = electrolyte.conductivity.Composite(\n            self.param\n        )\n        self.submodels[\"electrolyte diffusion\"] = electrolyte.diffusion.Full(\n            self.param, self.reactions\n        )\n\n    @property\n    def default_geometry(self):\n        dimensionality = self.options[\"dimensionality\"]\n        if dimensionality == 0:\n            return pybamm.Geometry(\"1D macro\", \"1D micro\")\n        elif dimensionality == 1:\n            return pybamm.Geometry(\"1+1D macro\", \"(1+0)+1D micro\")\n        elif dimensionality == 2:\n            return pybamm.Geometry(\"2+1D macro\", \"(2+0)+1D micro\")\n\n    @property\n    def default_solver(self):\n        \"\"\"\n        Create and return the default solver for this model\n        \"\"\"\n        # Different solver depending on whether we solve ODEs or DAEs\n        dimensionality = self.options[\"dimensionality\"]\n        if dimensionality == 0:\n            return pybamm.ScipySolver()\n        else:\n            return pybamm.ScikitsDaeSolver()\n\n# --- Snippet Separator ---\n\nclass DFN(BaseModel):\n    \"\"\"Doyle-Fuller-Newman (DFN) model of a lithium-ion battery, from [1]_.\n\n    Parameters\n    ----------\n    options : dict, optional\n        A dictionary of options to be passed to the model.\n    name : str, optional\n        The name of the model.\n    build :  bool, optional\n        Whether to build the model on instantiation. Default is True. Setting this\n        option to False allows users to change any number of the submodels before\n        building the complete model (submodels cannot be changed after the model is\n        built).\n\n    References\n    ----------\n    .. [1] SG Marquis, V Sulzer, R Timms, CP Please and SJ Chapman. “An asymptotic\n           derivation of a single particle model with electrolyte”. In: arXiv preprint\n           arXiv:1905.12553 (2019).\n\n\n    **Extends:** :class:`pybamm.lithium_ion.BaseModel`\n    \"\"\"\n\n    def __init__(self, options=None, name=\"Doyle-Fuller-Newman model\", build=True):\n        super().__init__(options, name)\n\n        self.set_reactions()\n        self.set_porosity_submodel()\n        self.set_convection_submodel()\n        self.set_interfacial_submodel()\n        self.set_particle_submodel()\n        self.set_solid_submodel()\n        self.set_electrolyte_submodel()\n        self.set_thermal_submodel()\n        self.set_current_collector_submodel()\n\n        if build:\n            self.build_model()\n\n    def set_porosity_submodel(self):\n\n        self.submodels[\"porosity\"] = pybamm.porosity.Constant(self.param)\n\n    def set_convection_submodel(self):\n\n        self.submodels[\"convection\"] = pybamm.convection.NoConvection(self.param)\n\n    def set_interfacial_submodel(self):\n\n        self.submodels[\n            \"negative interface\"\n        ] = pybamm.interface.lithium_ion.ButlerVolmer(self.param, \"Negative\")\n        self.submodels[\n            \"positive interface\"\n        ] = pybamm.interface.lithium_ion.ButlerVolmer(self.param, \"Positive\")\n\n    def set_particle_submodel(self):\n\n        if self.options[\"particle\"] == \"Fickian diffusion\":\n            self.submodels[\"negative particle\"] = pybamm.particle.fickian.ManyParticles(\n                self.param, \"Negative\"\n            )\n            self.submodels[\"positive particle\"] = pybamm.particle.fickian.ManyParticles(\n                self.param, \"Positive\"\n            )\n        elif self.options[\"particle\"] == \"fast diffusion\":\n            self.submodels[\"negative particle\"] = pybamm.particle.fast.ManyParticles(\n                self.param, \"Negative\"\n            )\n            self.submodels[\"positive particle\"] = pybamm.particle.fast.ManyParticles(\n                self.param, \"Positive\"\n            )\n\n    def set_solid_submodel(self):\n\n        self.submodels[\"negative electrode\"] = pybamm.electrode.ohm.Full(\n            self.param, \"Negative\", self.reactions\n        )\n        self.submodels[\"positive electrode\"] = pybamm.electrode.ohm.Full(\n            self.param, \"Positive\", self.reactions\n        )\n\n    def set_electrolyte_submodel(self):\n\n        electrolyte = pybamm.electrolyte.stefan_maxwell\n\n        self.submodels[\"electrolyte conductivity\"] = electrolyte.conductivity.Full(\n            self.param, self.reactions\n        )\n        self.submodels[\"electrolyte diffusion\"] = electrolyte.diffusion.Full(\n            self.param, self.reactions\n        )\n\n    @property\n    def default_geometry(self):\n        dimensionality = self.options[\"dimensionality\"]\n        if dimensionality == 0:\n            return pybamm.Geometry(\"1D macro\", \"1+1D micro\")\n        elif dimensionality == 1:\n            return pybamm.Geometry(\"1+1D macro\", \"(1+1)+1D micro\")\n        elif dimensionality == 2:\n            return pybamm.Geometry(\"2+1D macro\", \"(2+1)+1D micro\")\n\n    @property\n    def default_solver(self):\n        \"\"\"\n        Create and return the default solver for this model\n        \"\"\"\n\n        # Default solver to DAE\n        return pybamm.ScikitsDaeSolver()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a custom lithium-ion model using the pybamm library. The model should be named \"my li-ion model\" and should include submodels for the external circuit, current collector, thermal, porosity, electrolyte diffusion, electrolyte conductivity, SEI, SEI on cracks, and lithium plating. For both the negative and positive electrode domains, the model should include submodels for active material, electrode potential, particle, total particle concentration, open-circuit potential, interface, interface utilisation, interface current, surface potential difference, and particle mechanics. After defining the model, build it, create the geometry, process the model and geometry, set the mesh, discretise the model, and solve it. Finally, plot the solution dynamically.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 77, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulator with 10 stub Autonomous Systems (AS) and hosts. Then, create an Ethereum service with saveState set to True and override set to True. Create two blockchains, one based on Proof of Work (POW) and the other on Proof of Authority (POA). For each blockchain, create four nodes and set the first two nodes of each blockchain as bootnodes and start mining on them. For the third node of each blockchain, create accounts with a certain balance. For the fourth node of each blockchain, set custom geth command options. Enable HTTP and WebSocket connections on certain nodes and set custom geth binary file on one of the nodes. Customize the display names of the nodes for visualization purposes. Bind the virtual nodes to physical nodes using filters. Add the Ethereum layer to the emulator and save the component to a file. Finally, compile the emulator with Docker and save the output to a directory.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class EthereumService(Service):\n    \"\"\"!\n    @brief The Ethereum network service.\n    This service allows one to run a private Ethereum network in the emulator.\n    \"\"\"\n\n    __blockchains: Dict[str, Blockchain]\n\n    __save_state: bool\n    __save_path: str\n    __override: bool\n    __blockchain_id: int\n    __serial: int\n\n    def __init__(self, saveState: bool = False, savePath: str = './eth-states', override:bool=False):\n        \"\"\"!\n        @brief The EthereumService class initializer.\n\n        @param saveState (optional) If true, the service will try to save state\n        of the block chain by saving the datadir of every node. Default to\n        false.\n        @param savePath (optional) The path to save containers' datadirs on the\n        host. Default to \"./eth-states\". \n        @param override (optional) If true, override the output folder if it already\n        exist. False by default.\n\n        @returns An instance of the EthereumService class.\n        \"\"\"\n        super().__init__()\n\n        self.__serial = 0\n        self.__save_state = saveState\n        self.__save_path = savePath\n        self.__override = override\n        self.__blockchains = {}\n        self.__blockchain_id = 1337\n\n    def getName(self):\n        return 'EthereumService'\n\n    def isSave(self):\n        return self.__save_state\n\n    def getSavePath(self):\n        return self.__save_path\n\n    def _doConfigure(self, node: Node, server: EthereumServer):\n        blockchain = server.getBlockchain()\n        blockchain._doConfigure(node, server)\n\n    def configure(self, emulator: Emulator):\n        if self.__save_state:\n            self._createSharedFolder()\n        super().configure(emulator)\n        for blockchain in self.__blockchains.values():\n            blockchain.configure(emulator)\n\n    def _createSharedFolder(self):\n        if path.exists(self.__save_path):\n            if self.__override:\n                self._log('eth_state folder \"{}\" already exist, overriding.'.format(self.__save_path))\n                i = 1\n                while True:\n                    rename_save_path = \"{}-{}\".format(self.__save_path, i)\n                    if not path.exists(rename_save_path):\n                        rename(self.__save_path, rename_save_path)\n                        break\n                    else:\n                        i = i+1\n            else:\n                self._log('eth_state folder \"{}\" already exist. Set \"override = True\" when calling compile() to override.'.format(self.__save_path))\n                exit(1)\n        mkdir(self.__save_path)\n\n    def _doInstall(self, node: Node, server: EthereumServer):\n        self._log('installing eth on as{}/{}...'.format(node.getAsn(), node.getName()))\n\n        server.install(node, self)\n\n    def _createServer(self, blockchain: Blockchain = None) -> Server:\n        self.__serial += 1\n        assert blockchain != None, 'EthereumService::_createServer(): create server using Blockchain::createNode() not EthereumService::install()'.format()\n        consensus = blockchain.getConsensusMechanism()\n        if consensus == ConsensusMechanism.POA:\n            return PoAServer(self.__serial, blockchain)\n        if consensus == ConsensusMechanism.POW:\n            return PoWServer(self.__serial, blockchain)\n        if consensus == ConsensusMechanism.POS:\n            return PoSServer(self.__serial, blockchain)\n\n    def installByBlockchain(self, vnode: str, blockchain: Blockchain) -> EthereumServer:\n        \"\"\"!\n        @brief Install the service on a node identified by given name. \n                This API is called by Blockchain Class. \n\n        @param vnode The name of the virtual node. \n        @param blockchain The blockchain that the created node is belongs to.\n\n        @returns EthereumServer.\n        \"\"\"\n        if vnode in self._pending_targets.keys(): return self._pending_targets[vnode]\n\n        s = self._createServer(blockchain)\n        self._pending_targets[vnode] = s\n\n        return self._pending_targets[vnode]\n\n    def createBlockchain(self, chainName:str, consensus: ConsensusMechanism, chainId: int = -1):\n        \"\"\"!\n        @brief Create an instance of Blockchain class which is a sub-layer of the EthereumService.\n\n        @param chainName The name of the Blockchain.\n        @param consensus The consensus mechanism of the blockchain.\n        @param chainId The chain id of the Blockchain.\n\n        @returns an instance of Blockchain class.\n        \"\"\"\n\n        if chainId < 0 : \n            chainId = self.__blockchain_id\n            self.__blockchain_id += 1\n        blockchain = Blockchain(self, chainName, chainId, consensus)\n        self.__blockchains[chainName] = blockchain\n        return blockchain\n\n    def print(self, indent: int) -> str:\n        out = ' ' * indent\n        out += 'EthereumService:\\n'\n\n        indent += 4\n\n        out += ' ' * indent\n        out += 'Boot Nodes:\\n'\n\n        indent += 4\n\n        for node in self.getBootNodes(ConsensusMechanism.POW):\n            out += ' ' * indent\n            out += 'POW-{}\\n'.format(node)\n\n        for node in self.getBootNodes(ConsensusMechanism.POA):\n            out += ' ' * indent\n            out += 'POA-{}\\n'.format(node)\n\n        return out\n\n# --- Snippet Separator ---\n\nclass Blockchain:\n    \"\"\"!\n    @brief The individual blockchain in EthereumService.\n    This Blockchain class allows to maintain multiple blockchains inside EthereumService.\n    \"\"\"\n    __consensus: ConsensusMechanism\n    __genesis: Genesis\n    __eth_service: EthereumService\n    __boot_node_addresses: Dict[ConsensusMechanism, List[str]]\n    __joined_accounts: List[AccountStructure]\n    __joined_signer_accounts: List[AccountStructure]\n    __validator_ids: List[str]\n    __beacon_setup_node_address: str\n    __chain_id:int\n    __pending_targets:list\n    __chain_name:str\n    __emu_mnemonic:str\n    __total_accounts_per_node: int\n    __emu_account_balance: int\n    __local_mnemonic:str\n    __local_accounts_total:int\n    __local_account_balance:int\n    __terminal_total_difficulty:int\n    __target_aggregater_per_committee:int\n    __target_committee_size:int\n\n    def __init__(self, service:EthereumService, chainName: str, chainId: int, consensus:ConsensusMechanism):\n        \"\"\"!\n        @brief The Blockchain class initializer.\n\n        @param service The EthereumService that creates the Blockchain class instance.\n        @param chainName The name of the Blockchain to create.\n        @param chainid The chain id of the Blockchain to create.\n        @param consensus The consensus of the Blockchain to create (supports POA, POS, POW).\n\n        @returns An instance of The Blockchain class.\n        \"\"\"\n        self.__eth_service = service\n        self.__consensus = consensus\n        self.__chain_name = chainName\n        self.__genesis = Genesis(ConsensusMechanism.POA) if self.__consensus == ConsensusMechanism.POS else Genesis(self.__consensus)\n        self.__boot_node_addresses = []\n        self.__miner_node_address = []\n        self.__joined_accounts = []\n        self.__joined_signer_accounts = []\n        self.__validator_ids = []\n        self.__beacon_setup_node_address = ''\n        self.__pending_targets = []\n        self.__emu_mnemonic = \"great awesome fun seed security lab protect system network prevent attack future\"\n        self.__total_accounts_per_node = 1\n        self.__emu_account_balance = 32 * EthUnit.ETHER.value\n        self.__local_mnemonic = \"great amazing fun seed lab protect network system security prevent attack future\"\n        self.__local_accounts_total = 5\n        self.__local_account_balance = 10 * EthUnit.ETHER.value\n        self.__chain_id = chainId\n        self.__terminal_total_difficulty = 20\n        self.__target_aggregater_per_committee = 2\n        self.__target_committee_size = 3\n\n    def _doConfigure(self, node:Node, server:EthereumServer):\n        self._log('configuring as{}/{} as an eth node...'.format(node.getAsn(), node.getName()))\n\n        ifaces = node.getInterfaces()\n        assert len(ifaces) > 0, 'EthereumService::_doConfigure(): node as{}/{} has not interfaces'.format()\n        addr = '{}:{}'.format(str(ifaces[0].getAddress()), server.getBootNodeHttpPort())\n\n        if server.isBootNode():\n            self._log('adding as{}/{} as consensus-{} bootnode...'.format(node.getAsn(), node.getName(), self.__consensus.value))\n            self.__boot_node_addresses.append(addr)\n\n        if self.__consensus == ConsensusMechanism.POS:\n            if server.isStartMiner():\n                self._log('adding as{}/{} as consensus-{} miner...'.format(node.getAsn(), node.getName(), self.__consensus.value))\n                self.__miner_node_address.append(str(ifaces[0].getAddress())) \n            if server.isBeaconSetupNode():\n                self.__beacon_setup_node_address = '{}:{}'.format(ifaces[0].getAddress(), server.getBeaconSetupHttpPort())\n\n        server._createAccounts(self)\n\n        accounts = server._getAccounts()\n        if len(accounts) > 0:\n            if self.__consensus == ConsensusMechanism.POS and server.isValidatorAtRunning():\n                accounts[0].balance = 33 * EthUnit.ETHER.value\n            self.__joined_accounts.extend(accounts)\n            if self.__consensus in [ConsensusMechanism.POA, ConsensusMechanism.POS] and server.isStartMiner():\n                self.__joined_signer_accounts.append(accounts[0])\n\n        if self.__consensus == ConsensusMechanism.POS and server.isValidatorAtGenesis():\n            self.__validator_ids.append(str(server.getId()))\n\n        server._generateGethStartCommand()\n\n        if self.__eth_service.isSave():\n            save_path = self.__eth_service.getSavePath()\n            node.addSharedFolder('/root/.ethereum', '../{}/{}/{}/ethereum'.format(save_path, self.__chain_name, server.getId()))\n            node.addSharedFolder('/root/.ethash', '../{}/{}/{}/ethash'.format(save_path, self.__chain_name, server.getId()))\n            makedirs('{}/{}/{}/ethereum'.format(save_path, self.__chain_name, server.getId()))\n            makedirs('{}/{}/{}/ethash'.format(save_path, self.__chain_name, server.getId()))\n\n    def configure(self, emulator:Emulator):\n        pending_targets = self.__eth_service.getPendingTargets()\n        localAccounts = EthAccount.createLocalAccountsFromMnemonic(mnemonic=self.__local_mnemonic, balance=self.__local_account_balance, total=self.__local_accounts_total)\n        self.__genesis.addAccounts(localAccounts)\n        self.__genesis.setChainId(self.__chain_id)\n        for vnode in self.__pending_targets:\n            node = emulator.getBindingFor(vnode)\n            server = pending_targets[vnode]\n            if self.__consensus == ConsensusMechanism.POS and server.isStartMiner():\n                ifaces = node.getInterfaces()\n                assert len(ifaces) > 0, 'EthereumService::_doConfigure(): node as{}/{} has not interfaces'.format()\n                addr = str(ifaces[0].getAddress())\n                miner_ip = self.__miner_node_address[0]\n                if addr == miner_ip:\n                    validator_count = len(self.getValidatorIds())\n                    index = self.__joined_accounts.index(server._getAccounts()[0])\n                    self.__joined_accounts[index].balance = 32*pow(10,18)*(validator_count+2)\n\n        self.__genesis.addAccounts(self.getAllAccounts())\n\n        if self.__consensus in [ConsensusMechanism.POA, ConsensusMechanism.POS] :\n            self.__genesis.setSigner(self.getAllSignerAccounts())\n\n    def getBootNodes(self) -> List[str]:\n        \"\"\"!\n        @brief Get bootnode IPs.\n\n        @returns List of bootnodes IP addresses.\n        \"\"\"\n        return self.__boot_node_addresses\n\n    def getMinerNodes(self) -> List[str]:\n        \"\"\"!\n        @brief Get miner node IPs.\n\n        @returns List of miner nodes IP addresses.\n        \"\"\"\n        return self.__miner_node_address\n\n    def getAllAccounts(self) -> List[AccountStructure]:\n        \"\"\"!\n        @brief Get a joined list of all the created accounts on all nodes in the blockchain.\n\n        @returns List of accounts.\n        \"\"\"\n        return self.__joined_accounts\n\n    def getAllSignerAccounts(self) -> List[AccountStructure]:\n        \"\"\"!\n        @brief Get a list of all signer accounts on all nodes in the blockchain.\n\n        returns List of signer accounts.\n        \"\"\"\n        return self.__joined_signer_accounts\n\n    def getValidatorIds(self) -> List[str]:\n        \"\"\"!\n        @brief Get a list of all validators ids on all nodes in the blockchain.\n\n        @returns List of all validators ids.\n        \"\"\"\n        return self.__validator_ids\n\n    def getBeaconSetupNodeIp(self) -> str:\n        \"\"\"!\n        @brief Get the IP of a beacon setup node.\n\n        @returns The IP address.\n        \"\"\"\n        return self.__beacon_setup_node_address\n\n    def setGenesis(self, genesis:str) -> EthereumServer:\n        \"\"\"!\n        @brief Set the custom genesis.\n\n        @param genesis The genesis file contents to set. \n\n        @returns Self, for chaining API calls.\n        \"\"\"\n        self.__genesis.setGenesis(genesis)\n\n        return self\n\n    def getGenesis(self) -> Genesis:\n        \"\"\"!\n        @brief Get the genesis file content.\n\n        @returns Genesis. \n        \"\"\"\n        return self.__genesis\n\n    def setConsensusMechanism(self, consensus:ConsensusMechanism) -> EthereumServer:\n        \"\"\"!\n        @brief Set consensus mechanism of this blockchain.\n\n        @param consensusMechanism Consensus mechanism to set (supports POW, POA and POS).\n\n        @returns Self, for chaining API calls. \n        \"\"\"\n        self.__consensus = consensus\n        self.__genesis = Genesis(self.__consensus)\n\n        return self\n\n    def getConsensusMechanism(self) -> ConsensusMechanism:\n        \"\"\"!\n        @brief Get the consensus mechanism of this blockchain.\n\n        @returns ConsensusMechanism\n        \"\"\"\n        return self.__consensus\n\n    def setTerminalTotalDifficulty(self, ttd:int):\n        \"\"\"!\n        @brief Set the terminal total difficulty, which is the value to designate\n                when the Merge is happen. In POA, difficulty is tend to increase by 2\n                for every one block. For example, if the terminal_total_difficulty is \n                set to 20, the Ethereum blockchain will keep POA consensus for approximately\n                150 sec (20/2*15) and then stop signing the block until the Merge happens.\n                Default to 20. \n\n        @param ttd The terminal total difficulty to set.\n\n        @returns Self, for chaining API calls.\n        \"\"\"\n        self.__terminal_total_difficulty = ttd\n\n        return self\n\n    def getTerminalTotalDifficulty(self) -> int:\n        \"\"\"!\n        @brief Get the value of the terminal total difficulty.\n\n        @returns terminal_total_difficulty.\n        \"\"\"\n\n        return self.__terminal_total_difficulty\n\n    def setGasLimitPerBlock(self, gasLimit:int):\n        \"\"\"!\n        @brief Set GasLimit at Genesis (the limit of gas cost per block).\n\n        @param gasLimit The gas limit per block.\n\n        @returns Self, for chaining API calls.\n        \"\"\"\n        self.__genesis.setGasLimit(gasLimit)\n        return self\n\n    def setChainId(self, chainId:int):\n        \"\"\"!\n        @brief Set chain Id at Genesis.\n\n        @param chainId The chain Id to set.\n\n        @returns Self, for chaining API calls\n        \"\"\"\n\n        self.__chain_id = chainId\n        return self\n\n    def createNode(self, vnode: str) -> EthereumServer:\n        \"\"\"!\n        @brief Create a node belongs to this blockchain.\n\n        @param vnode The name of vnode.\n\n        @returns EthereumServer\n        \"\"\"\n        eth = self.__eth_service\n        self.__pending_targets.append(vnode)\n        return eth.installByBlockchain(vnode, self)\n\n    def addLocalAccount(self, address: str, balance: int, unit:EthUnit=EthUnit.ETHER) -> Blockchain:\n        \"\"\"!\n        @brief Allocate balance to an external account by setting alloc field of genesis file.\n\n        @param address The External account's address.\n        @param balance The balance to allocate.\n        @param unit The unit of Ethereum.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        balance = balance * unit.value\n        self.__genesis.addLocalAccount(address, balance)\n\n        return self\n\n    def addLocalAccountsFromMnemonic(self, mnemonic:str, total:int, balance:int, unit:EthUnit=EthUnit.ETHER) -> Blockchain:\n        \"\"\"!\n        @brief Add local account from the given Mnemonic in addition to default local accounts.\n\n        @param mnemonic The mnemonic phrase to generate accounts from.\n        @param total The total number of accounts to generate.\n        @param balance The balance to allocate to the generated accounts.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        balance = balance * unit.value\n        mnemonic_account = EthAccount.createLocalAccountsFromMnemonic(mnemonic = mnemonic, balance=balance, total=total)\n        self.__genesis.addAccounts(mnemonic_account)\n\n    def getChainName(self) -> str:\n        \"\"\"!\n        @brief Get the name of the blockchain.\n\n        @returns The name of this blockchain.\n        \"\"\"\n        return self.__chain_name\n\n    def getChainId(self) -> int:\n        \"\"\"!\n        @brief Get the chain Id of the blockchain.\n\n        @returns The chain Id of this blockchain.\n        \"\"\"\n        return self.__chain_id\n\n    def setEmuAccountParameters(self, mnemonic:str, balance:int, total_per_node:int, unit:EthUnit=EthUnit.ETHER):\n        \"\"\"!\n        @brief Set mnemonic, balance, and total_per_node value to customize the account generation in this blockchain.\n\n        @param mnemonic The mnemonic phrase to generate the accounts per a node in this blockchain.\n        @param balance The balance to allocate to the generated accounts.\n        @param total_per_node The total number of the accounts to generate per a node in this blockchain.\n        @param unit The unit of Ethereum.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        self.__emu_mnemonic = mnemonic\n        self.__emu_account_balance = balance * unit.value\n        self.__total_accounts_per_node = total_per_node\n        return self\n\n    def getEmuAccountParameters(self):\n        \"\"\"!\n        @brief Get values of mnemonic, balance, and total_per_node value used for the account generation.\n\n        returns The value of mnemonic, balance, and total_per_node.\n        \"\"\"\n        return self.__emu_mnemonic, self.__emu_account_balance, self.__total_accounts_per_node\n\n    def setLocalAccountParameters(self, mnemonic:str, balance:int, total:int, unit:EthUnit=EthUnit.ETHER):\n        \"\"\"!\n        @brief Set mnemonic, balance, and total_per_node value to customize the local account generation.\n\n        @param mnemonic The mnemonic phrase to generate the local accounts.\n        @param balance The balance to allocate to the generated accounts.\n        @param total The total number of the local accounts.\n        @param unit The unit of Ethereum.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        self.__local_mnemonic = mnemonic\n        self.__local_account_balance = balance * unit.value\n        self.__local_accounts_total = total\n        return self\n\n    def setTargetAggregatorPerCommittee(self, target_aggregator_per_committee:int):\n        \"\"\"!\n        @brief Set target aggregator per committee for Beacon chain.\n\n        @param target_aggregator_per_committee The target value of the number of aggregator per committee to set.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        self.__target_aggregater_per_committee = target_aggregator_per_committee\n        return self\n\n    def getTargetAggregatorPerCommittee(self):\n        \"\"\"!\n        @brief Get the value of target aggregator per committee for Beacon chain.\n\n        @returns The value of target_aggregator_per_committee.\n        \"\"\"\n        return self.__target_aggregater_per_committee\n\n    def setTargetCommitteeSize(self, target_committee_size:int):\n        \"\"\"!\n        @brief Set target committee size for Beacon chain.\n\n        @param target_committee_size The target value of committee size to set.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        self.__target_committee_size = target_committee_size\n        return self\n\n    def getTargetCommitteeSize(self):\n        \"\"\"!\n        @brief Get the value of target committee size for Beacon Chain.\n\n        @returns The value of target_committee_size.\n        \"\"\"\n        return self.__target_committee_size\n\n    def _log(self, message: str) -> None:\n        \"\"\"!\n        @brief Log to stderr.\n\n        @returns None.\n        \"\"\"\n        print(\"==== Blockchain Sub Layer: {}\".format(message), file=stderr)\n\n# --- Snippet Separator ---\n\nclass EthereumServer(Server):\n    \"\"\"!\n    @brief The Ethereum Server\n    \"\"\"\n\n    _id: int\n    _blockchain: Blockchain\n    _is_bootnode: bool\n    _bootnode_http_port: int\n    _smart_contract: SmartContract\n    _accounts: List[AccountStructure]\n    _mnemonic_accounts: List[AccountStructure]\n    _consensus_mechanism: ConsensusMechanism\n\n    _custom_geth_binary_path: str\n    _custom_geth_command_option: str\n    _geth_options: dict\n\n    _data_dir: str\n    _syncmode: Syncmode\n    _snapshot: bool\n    _no_discover: bool \n    _enable_http: bool\n    _geth_http_port: int\n    _enable_ws: bool\n    _geth_ws_port: int\n    _unlock_accounts: bool\n    _start_mine: bool\n    _miner_thread: int\n    _coinbase: str\n\n    _geth_start_command: str\n\n    _role: list\n\n    def __init__(self, id: int, blockchain:Blockchain):\n        \"\"\"!\n        @brief create new eth server.\n        @param id serial number of this server.\n        \"\"\"\n\n        super().__init__()\n\n        self._id = id\n        self._blockchain = blockchain\n        self._is_bootnode = False\n        self._bootnode_http_port = 8088\n        self._smart_contract = None\n        self._accounts = []\n        self._mnemonic, self._account_base_balance, self._account_total = self._blockchain.getEmuAccountParameters()\n        self._mnemonic_accounts = EthAccount.createEmulatorAccountsFromMnemonic(self._id, mnemonic=self._mnemonic, balance=self._account_base_balance, total=self._account_total, password=\"admin\")\n        self._consensus_mechanism = blockchain.getConsensusMechanism()\n\n        self._custom_geth_binary_path = None\n        self._custom_geth_command_option = None\n        self._geth_options = {\"finding_peers\": \"\", \"http\":\"\", \"ws\":\"\", \"pos\":\"\", \"custom\":\"\", \"unlock\":\"\", \"mine\":\"\"}\n\n        self._data_dir = \"/root/.ethereum\"\n        self._syncmode = Syncmode.FULL\n        self._snapshot = False\n        self._no_discover = False\n        self._enable_ws = False\n        self._enable_http = False\n        self._geth_http_port = 8545\n        self._geth_ws_port = 8546\n        self._unlock_accounts = True\n        self._start_mine = False\n        self._miner_thread = 1\n        self._coinbase = None\n        self._geth_start_command = \"\"\n\n        self._base_system = BaseSystem.SEEDEMU_ETHEREUM\n\n        self._role = []\n\n\n    def _generateGethStartCommand(self):\n        \"\"\"!\n        @brief generate geth start commands from the properties. \n\n        @returns geth command. \n        \"\"\"\n        if self._no_discover:\n            self._geth_options['finding_peers'] = GethCommandTemplates['nodiscover']\n        else:\n            self._geth_options['finding_peers'] = GethCommandTemplates['bootnodes']\n        if self._enable_http:\n            self._geth_options['http'] = GethCommandTemplates['http'].format(gethHttpPort=self._geth_http_port)\n        if self._enable_ws:\n            self._geth_options['ws'] = GethCommandTemplates['ws'].format(gethWsPort=self._geth_ws_port)\n        if self._custom_geth_command_option:\n            self._geth_options['custom'] = self._custom_geth_command_option\n        if self._unlock_accounts:\n            accounts = []\n            for account in self._accounts:\n                accounts.append(account.address)\n            self._geth_options['unlock'] = GethCommandTemplates['unlock'].format(accounts=', '.join(accounts))\n        self._geth_start_command = GethCommandTemplates['base'].format(node_id=self._id, chain_id=self._blockchain.getChainId(), datadir=self._data_dir, syncmode=self._syncmode.value, snapshot=self._snapshot, option=self._geth_options)\n\n    def install(self, node: Node, eth: EthereumService):\n        \"\"\"!\n        @brief ETH server installation step.\n\n        @param node node object\n        @param eth reference to the eth service.\n        @param allBootnode all-bootnode mode: all nodes are boot node.\n\n        \"\"\"\n\n        node.appendClassName('EthereumService')\n        node.setLabel(ETH_LABEL_META.format(key='node_id'), self.getId())\n        node.setLabel(ETH_LABEL_META.format(key='consensus'), self._consensus_mechanism.value)\n        node.setLabel(ETH_LABEL_META.format(key='chain_name'), self._blockchain.getChainName())\n        node.setLabel(ETH_LABEL_META.format(key='chain_id'), self._blockchain.getChainId())\n\n        if self.isBootNode(): self._role.append(\"bootnode\")\n        if self.isStartMiner(): self._role.append(\"miner\")\n        node.setLabel(ETH_LABEL_META.format(key='role'), json.dumps(self._role).replace(\"\\\"\", \"\\\\\\\"\"))\n\n        ifaces = node.getInterfaces()\n        assert len(ifaces) > 0, 'EthereumServer::install: node as{}/{} has no interfaces'.format(node.getAsn(), node.getName())\n        addr = str(ifaces[0].getAddress())\n\n        self.__genesis = self._blockchain.getGenesis()\n\n        node.setFile('/tmp/eth-genesis.json', self.__genesis.getGenesis())\n\n        # set account passwords to /tmp/eth-password\n        account_passwords = []\n\n        for account in self._accounts:\n            node.setFile(\"/tmp/keystore/\"+account.keystore_filename, account.keystore_content)\n            account_passwords.append(account.password)\n\n        node.setFile('/tmp/eth-password', '\\n'.join(account_passwords))\n\n        # install required software\n        # node.addSoftware('software-properties-common')\n        # tap the eth repo\n        # node.addBuildCommand('add-apt-repository ppa:ethereum/ethereum')\n\n        # install geth and bootnode\n        if self._custom_geth_binary_path : \n            #node.addBuildCommand('apt-get update && apt-get install --yes bootnode')\n            node.importFile(\"../../\"+self._custom_geth_binary_path, '/usr/bin/geth')\n            node.appendStartCommand(\"chmod +x /usr/bin/geth\")\n        # else:\n        #     node.addBuildCommand('apt-get update && apt-get install --yes geth bootnode')\n\n        # genesis\n        node.appendStartCommand('[ ! -e \"/root/.ethereum/geth/nodekey\" ] && geth --datadir {} init /tmp/eth-genesis.json'.format(self._data_dir))\n\n        # copy keystore to the proper folder\n        for account in self._accounts:\n            node.appendStartCommand(\"cp /tmp/keystore/{} /root/.ethereum/keystore/\".format(account.keystore_filename))\n\n        if self._is_bootnode:\n            # generate enode url. other nodes will access this to bootstrap the network.\n            node.appendStartCommand('[ ! -e \"/root/.ethereum/geth/bootkey\" ] && bootnode -genkey /root/.ethereum/geth/bootkey')\n            node.appendStartCommand('echo \"enode://$(bootnode -nodekey /root/.ethereum/geth/bootkey -writeaddress)@{}:30301\" > /tmp/eth-enode-url'.format(addr))\n\n            # Default port is 30301, use -addr :<port> to specify a custom port\n            node.appendStartCommand('bootnode -nodekey /root/.ethereum/geth/bootkey -verbosity 9 -addr {}:30301 2> /tmp/bootnode-logs &'.format(addr))          \n            node.appendStartCommand('python3 -m http.server {} -d /tmp'.format(self._bootnode_http_port), True)\n\n        # get other nodes IP for the bootstrapper.\n        bootnodes = self._blockchain.getBootNodes()[:]\n        if len(bootnodes) > 0 :\n            node.setFile('/tmp/eth-nodes', '\\n'.join(bootnodes))\n\n            node.setFile('/tmp/eth-bootstrapper', EthServerFileTemplates['bootstrapper'])\n\n            # load enode urls from other nodes\n            node.appendStartCommand('chmod +x /tmp/eth-bootstrapper')\n            node.appendStartCommand('/tmp/eth-bootstrapper')\n\n        # launch Ethereum process.\n        node.appendStartCommand(self._geth_start_command, True) \n\n\n        # Rarely used and tentatively not supported. \n        # if self.__smart_contract != None :\n        #     smartContractCommand = self.__smart_contract.generateSmartContractCommand()\n        #     node.appendStartCommand('(\\n {})&'.format(smartContractCommand))\n\n    def setCustomGeth(self, customGethBinaryPath:str) -> EthereumServer:\n        \"\"\"\n        @brief set custom geth binary file\n\n        @param customGethBinaryPath set absolute path of geth binary to move to the service.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        assert path.exists(customGethBinaryPath), \"EthereumServer::setCustomGeth: custom geth binary file does not exist. path : {}\".format(customGethBinaryPath)\n\n        self._custom_geth_binary_path = customGethBinaryPath\n\n        return self\n\n    def setCustomGethCommandOption(self, customOptions:str) -> EthereumServer:\n        \"\"\"\n        @brief set custom geth start command option\n\n        @param customOptions options to set\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        assert customOptions.startswith(\"--\"), \"option should start with '--'\"\n        assert \";\" not in customOptions, \"letter ';' cannot contain in the options\"\n        assert \"&\" not in customOptions, \"letter '|' cannot contain in the options\"\n        assert \"|\" not in customOptions, \"letter '|' cannot contain in the options\"\n\n        self._custom_geth_command_option = customOptions\n        return self\n\n    def setSyncmode(self, syncmode:Syncmode) -> EthereumServer:\n        \"\"\"\n        @brief setting geth syncmode (default: snap)\n\n        @param syncmode use Syncmode enum options.\n                Syncmode.SNAP, Syncmode.FULL, Syncmode.LIGHT\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self._syncmode = syncmode\n        return self\n\n    def setNoDiscover(self, noDiscover:bool = True) -> EthereumServer:\n        \"\"\"\n        @brief setting the automatic peer discovery to true/false\n        \"\"\"\n        self._no_discover = noDiscover\n        return self\n\n    def setSnapshot(self, snapshot:bool = True) -> EthereumServer:\n        \"\"\"!\n        @brief set geth snapshot \n\n        @param snapshot bool\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self._snapshot = snapshot\n        return self\n\n    def getId(self) -> int:\n        \"\"\"!\n        @brief get ID of this node.\n        @returns ID.\n        \"\"\"\n        return self._id\n\n    def setBootNode(self, isBootNode: bool) -> EthereumServer:\n        \"\"\"!\n        @brief set bootnode status of this node.\n        Note: if no nodes are configured as boot nodes, all nodes will be each\n        other's boot nodes.\n        @param isBootNode True to set this node as a bootnode, False otherwise.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self._is_bootnode = isBootNode\n\n        return self\n\n    def isBootNode(self) -> bool:\n        \"\"\"!\n        @brief get bootnode status of this node.\n        @returns True if this node is a boot node. False otherwise.\n        \"\"\"\n        return self._is_bootnode\n\n    def setBootNodeHttpPort(self, port: int) -> EthereumServer:\n        \"\"\"!\n        @brief set the http server port number hosting the enode url file.\n        @param port port\n        @returns self, for chaining API calls.\n        \"\"\"\n\n        self._bootnode_http_port = port\n\n        return self\n\n\n    def getBootNodeHttpPort(self) -> int:\n        \"\"\"!\n        @brief get the http server port number hosting the enode url file.\n        @returns port\n        \"\"\"\n\n        return self._bootnode_http_port\n\n    def setGethHttpPort(self, port: int) -> EthereumServer:\n        \"\"\"!\n        @brief set the http server port number for normal ethereum nodes\n        @param port port\n        @returns self, for chaining API calls\n        \"\"\"\n\n        self._geth_http_port = port\n\n        return self\n\n    def getGethHttpPort(self) -> int:\n        \"\"\"!\n        @brief get the http server port number for normal ethereum nodes\n        @returns int\n        \"\"\"\n\n        return self._geth_http_port\n\n    def setGethWsPort(self, port: int) -> EthereumServer:\n        \"\"\"!\n        @brief set the ws server port number for normal ethereum nodes\n\n        @param port port\n\n        @returns self, for chaining API calls\n        \"\"\"\n\n        self._geth_ws_port = port\n\n        return self\n\n    def getGethWsPort(self) -> int:\n        \"\"\"!\n        @brief get the ws server port number for normal ethereum nodes\n\n        @returns int\n        \"\"\"\n\n        return self._geth_ws_port\n\n    def enableGethHttp(self) -> EthereumServer:\n        \"\"\"!\n        @brief setting a geth to enable http connection \n        \"\"\"\n        self._enable_http = True\n\n        return self\n\n    def isGethHttpEnabled(self) -> bool:\n        \"\"\"!\n        @brief returns whether a geth enabled http connection or not\n        \"\"\"\n        return self._enable_http\n\n    def enableGethWs(self) -> EthereumServer:\n        \"\"\"!\n        @brief setting a geth to enable ws connection\n        \"\"\"\n        self._enable_ws = True\n\n        return self\n\n    def isGethWsEnabled(self) -> bool:\n        \"\"\"!\n        @brief returns whether a geth enabled ws connection or not\n        \"\"\"\n\n        return self._enable_ws\n\n    def createAccount(self, balance:int, unit:EthUnit=EthUnit.ETHER, password=\"admin\") -> EthereumServer:\n        \"\"\"\n        @brief call this api to create new accounts\n\n        @param balance the balance to be allocated to the account.\n        @param unit EthUnit (Default: EthUnit.Ether)\n\n        @returns self, for chaining API calls.\n\n        \"\"\"\n\n        balance = balance * unit.value\n        self._mnemonic_accounts.append(EthAccount.createEmulatorAccountFromMnemonic(self._id, mnemonic=self._mnemonic, balance=balance, index=self._account_total, password=password))\n        self._account_total += 1\n        return self\n\n    # it should depend on createAccount() method\n    def createAccounts(self, total:int, balance:int, unit:EthUnit=EthUnit.ETHER, password=\"admin\") -> EthereumServer:\n        \"\"\"\n        @brief Call this api to create new accounts.\n\n        @param total The total number of account need to create.\n        @param balance The balance to allocate to the accounts.\n        @param unit The unit of Ethereum. EthUnit (Default: EthUnit.Ether).\n\n        @returns self, for chaining API calls.\n        \"\"\"\n\n        for i in range(total):\n            self.createAccount(balance, unit, password)\n\n        return self\n\n    def _createAccounts(self, eth:EthereumService) -> EthereumServer:\n        \"\"\"\n        @brief Call this api to create new accounts from account_info\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self._accounts.extend(self._mnemonic_accounts)\n\n        return self    \n\n    def importAccount(self, keyfilePath:str, password:str = \"admin\", balance: int = 0, unit:EthUnit=EthUnit.ETHER) -> EthereumServer:\n        \"\"\"\n        @brief Call this api to import an account.\n\n        @param keyfilePath The keyfile path to import.\n        @param password The password to decrypt the keyfile.\n        @param balance The balance to allocate to the account.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n\n        assert path.exists(keyfilePath), \"EthereumServer::importAccount: keyFile does not exist. path : {}\".format(keyfilePath)\n        account = EthAccount.importAccount(balance=balance,password=password, keyfilePath=keyfilePath)\n        self._accounts.append(account)\n        return self\n\n\n    def _getAccounts(self) -> List[AccountStructure]:\n        \"\"\"\n        @brief Call this api to get the accounts for this node\n\n        @returns accounts\n        \"\"\"\n\n        return self._accounts\n\n\n    def unlockAccounts(self) -> EthereumServer:\n        \"\"\"!\n        @brief This is mainly used to unlock the accounts in the remix node to make it directly possible for transactions to be \n        executed through Remix without the need to access the geth account in the docker container and unlocking manually\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self._unlock_accounts = True\n\n        return self\n\n    def startMiner(self) -> EthereumServer:\n        \"\"\"!\n        @brief Call this api to start Miner in the node.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self._start_mine = True\n        self._syncmode = Syncmode.FULL\n\n        return self\n\n    def isStartMiner(self) -> bool:\n        \"\"\"!\n        @brief Call this api to get startMiner status in the node.\n\n        @returns __start_mine status.\n        \"\"\"\n        return self._start_mine\n\n    def deploySmartContract(self, smart_contract: SmartContract) -> EthereumServer:\n        \"\"\"!\n        @brief Call this api to deploy smartContract on the node.\n        @returns self, for chaining API calls.\n        \"\"\"\n        self._smart_contract = smart_contract\n\n        return self\n\n    def getBlockchain(self):\n        return self._blockchain\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulator with 10 stub Autonomous Systems (AS) and hosts. Then, create an Ethereum service with saveState set to True and override set to True. Create two blockchains, one based on Proof of Work (POW) and the other on Proof of Authority (POA). For each blockchain, create four nodes and set the first two nodes of each blockchain as bootnodes and start mining on them. For the third node of each blockchain, create accounts with a certain balance. For the fourth node of each blockchain, set custom geth command options. Enable HTTP and WebSocket connections on certain nodes and set custom geth binary file on one of the nodes. Customize the display names of the nodes for visualization purposes. Bind the virtual nodes to physical nodes using filters. Add the Ethereum layer to the emulator and save the component to a file. Finally, compile the emulator with Docker and save the output to a directory.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 78, "repo_full_name": "federatedai__fate", "instruction": "Generate code that creates a pipeline for a federated learning task using the FATE library. The pipeline should include components for reading data, transforming data, sampling, feature binning, one-hot encoding, logistic regression, local baseline model, secure boosting, and evaluation. The pipeline should be set up for a guest and a host party, with data read from specified tables for each party. The pipeline should be compiled and fitted, and the summary of the evaluation components should be printed. The main function should accept a configuration file as an argument.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def main():\n    # parties config\n    guest = 9999\n    host = 10000\n    arbiter = 10000\n\n    # specify input data name & namespace in database\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    guest_eval_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_eval_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    # initialize pipeline\n    pipeline = PipeLine()\n    # set job initiator\n    pipeline.set_initiator(role=\"guest\", party_id=guest)\n    # set participants information\n    pipeline.set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    # define Reader components to read in data\n    reader_0 = Reader(name=\"reader_0\")\n    # configure Reader for guest\n    reader_0.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_train_data)\n    # configure Reader for host\n    reader_0.get_party_instance(role=\"host\", party_id=host).component_param(table=host_train_data)\n\n    reader_1 = Reader(name=\"reader_1\")\n    reader_1.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_1.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n\n    # define DataIO components\n    dataio_0 = DataIO(name=\"dataio_0\")\n    dataio_1 = DataIO(name=\"dataio_1\")\n\n    # get DataIO party instance of guest\n    dataio_0_guest_party_instance = dataio_0.get_party_instance(role=\"guest\", party_id=guest)\n    # configure DataIO for guest\n    dataio_0_guest_party_instance.component_param(with_label=True, output_format=\"dense\")\n    # get and configure DataIO party instance of host\n    dataio_0.get_party_instance(role=\"host\", party_id=host).component_param(with_label=False)\n\n    # define Intersection components\n    intersection_0 = Intersection(name=\"intersection_0\")\n    intersection_1 = Intersection(name=\"intersection_1\")\n\n    # define HeteroLR component\n    hetero_lr_0 = HeteroLR(name=\"hetero_lr_0\", early_stop=\"weight_diff\", learning_rate=0.15, optimizer=\"rmsprop\",\n                           max_iter=10, early_stopping_rounds=2, validation_freqs=1)\n\n    # add components to pipeline, in order of task execution\n    pipeline.add_component(reader_0)\n    pipeline.add_component(reader_1)\n    pipeline.add_component(dataio_0, data=Data(data=reader_0.output.data))\n    # set dataio_1 to replicate model from dataio_0\n    pipeline.add_component(dataio_1, data=Data(data=reader_1.output.data), model=Model(dataio_0.output.model))\n    # set data input sources of intersection components\n    pipeline.add_component(intersection_0, data=Data(data=dataio_0.output.data))\n    pipeline.add_component(intersection_1, data=Data(data=dataio_1.output.data))\n    # set train & validate data of hetero_lr_0 component\n    pipeline.add_component(\n        hetero_lr_0,\n        data=Data(\n            train_data=intersection_0.output.data,\n            validate_data=intersection_1.output.data))\n\n    # compile pipeline once finished adding modules, this step will form conf and dsl files for running job\n    pipeline.compile()\n\n    # fit model\n    pipeline.fit()\n    # query component summary\n    import json\n    print(json.dumps(pipeline.get_component(\"hetero_lr_0\").get_summary(), indent=4))\n\n    # predict\n    # deploy required components\n    pipeline.deploy_component([dataio_0, intersection_0, hetero_lr_0])\n\n    # initiate predict pipeline\n    predict_pipeline = PipeLine()\n\n    reader_2 = Reader(name=\"reader_2\")\n    reader_2.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_2.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n    # add data reader onto predict pipeline\n    predict_pipeline.add_component(reader_2)\n    # add selected components from train pipeline onto predict pipeline\n    # specify data source\n    predict_pipeline.add_component(pipeline,\n                                   data=Data(predict_input={pipeline.dataio_0.input.data: reader_2.output.data}))\n    # run predict model\n    predict_pipeline.predict()\n\n# --- Snippet Separator ---\n\ndef maxabs_scale(X, *, axis=0, copy=True):\n    \"\"\"Scale each feature to the [-1, 1] range without breaking the sparsity.\n\n    This estimator scales each feature individually such\n    that the maximal absolute value of each feature in the\n    training set will be 1.0.\n\n    This scaler can also be applied to sparse CSR or CSC matrices.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        The data.\n\n    axis : {0, 1}, default=0\n        Axis used to scale along. If 0, independently scale each feature,\n        otherwise (if 1) scale each sample.\n\n    copy : bool, default=True\n        Set to False to perform inplace scaling and avoid a copy (if the input\n        is already a numpy array).\n\n    Returns\n    -------\n    X_tr : {ndarray, sparse matrix} of shape (n_samples, n_features)\n        The transformed data.\n\n    .. warning:: Risk of data leak\n\n        Do not use :func:`~sklearn.preprocessing.maxabs_scale` unless you know\n        what you are doing. A common mistake is to apply it to the entire data\n        *before* splitting into training and test sets. This will bias the\n        model evaluation because information would have leaked from the test\n        set to the training set.\n        In general, we recommend using\n        :class:`~sklearn.preprocessing.MaxAbsScaler` within a\n        :ref:`Pipeline <pipeline>` in order to prevent most risks of data\n        leaking: `pipe = make_pipeline(MaxAbsScaler(), LogisticRegression())`.\n\n    See Also\n    --------\n    MaxAbsScaler : Performs scaling to the [-1, 1] range using\n        the Transformer API (e.g. as part of a preprocessing\n        :class:`~sklearn.pipeline.Pipeline`).\n\n    Notes\n    -----\n    NaNs are treated as missing values: disregarded to compute the statistics,\n    and maintained during the data transformation.\n\n    For a comparison of the different scalers, transformers, and normalizers,\n    see: :ref:`sphx_glr_auto_examples_preprocessing_plot_all_scaling.py`.\n    \"\"\"\n    # Unlike the scaler object, this function allows 1d input.\n\n    # If copy is required, it will be done inside the scaler object.\n    X = check_array(\n        X,\n        accept_sparse=(\"csr\", \"csc\"),\n        copy=False,\n        ensure_2d=False,\n        dtype=FLOAT_DTYPES,\n        force_all_finite=\"allow-nan\",\n    )\n    original_ndim = X.ndim\n\n    if original_ndim == 1:\n        X = X.reshape(X.shape[0], 1)\n\n    s = MaxAbsScaler(copy=copy)\n    if axis == 0:\n        X = s.fit_transform(X)\n    else:\n        X = s.fit_transform(X.T).T\n\n    if original_ndim == 1:\n        X = X.ravel()\n\n    return X\n\n# --- Snippet Separator ---\n\ndef add_component(self, component, data=None, model=None, cache=None):\n        if isinstance(component, PipeLine):\n            if component.is_deploy() is False:\n                raise ValueError(\"To use a training pipeline object as predict component, should deploy model first\")\n\n            if model:\n                raise ValueError(\"pipeline should not have model as input!\")\n\n            if not data:\n                raise ValueError(\"To use pipeline as a component, please set data input\")\n\n            self._stage = \"predict\"\n            self._predict_pipeline.append({\"pipeline\": component, \"data\": data.predict_input})\n\n            meta = component.get_predict_meta()\n            self.restore_roles(meta.get(\"initiator\"), meta.get(\"roles\"))\n\n            return self\n\n        if not isinstance(component, Component):\n            raise ValueError(\n                \"To add a component to pipeline, component {} should be a Component object\".format(component))\n\n        if component.name in self._components:\n            raise Warning(\"component {} is added before\".format(component.name))\n\n        self._components[component.name] = component\n\n        if data is not None:\n            if not isinstance(data, Data):\n                raise ValueError(\"data input of component {} should be passed by data object\".format(component.name))\n\n            attrs_dict = vars(data)\n            self._components_input[component.name] = {\"data\": {}}\n            for attr, val in attrs_dict.items():\n                if not attr.endswith(\"data\"):\n                    continue\n\n                if val is None:\n                    continue\n\n                data_key = attr.strip(\"_\")\n\n                if isinstance(val, list):\n                    self._components_input[component.name][\"data\"][data_key] = val\n                else:\n                    self._components_input[component.name][\"data\"][data_key] = [val]\n\n        if model is not None:\n            if not isinstance(model, Model):\n                raise ValueError(\"model input of component {} should be passed by model object\".format(component.name))\n\n            attrs_dict = vars(model)\n            for attr, val in attrs_dict.items():\n                if not attr.endswith(\"model\"):\n                    continue\n\n                if val is None:\n                    continue\n\n                if isinstance(val, list):\n                    self._components_input[component.name][attr.strip(\"_\")] = val\n                else:\n                    self._components_input[component.name][attr.strip(\"_\")] = [val]\n\n        if cache is not None:\n            if not isinstance(cache, Cache):\n                raise ValueError(\"cache input of component {} should be passed by cache object\".format(component.name))\n\n            attr = cache.cache\n            if not isinstance(attr, list):\n                attr = [attr]\n\n            self._components_input[component.name][\"cache\"] = attr\n\n        return self\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a pipeline for a federated learning task using the FATE library. The pipeline should include components for reading data, transforming data, sampling, feature binning, one-hot encoding, logistic regression, local baseline model, secure boosting, and evaluation. The pipeline should be set up for a guest and a host party, with data read from specified tables for each party. The pipeline should be compiled and fitted, and the summary of the evaluation components should be printed. The main function should accept a configuration file as an argument.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 79, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a dd4hep simulation using Python configuration. The code should define a function that initializes a DDG4 kernel, loads a geometry from an XML file located in the 'OpticalSurfaces/compact' directory of the 'DD4hepExamplesINSTALL' environment variable, and imports constants from the kernel's detector description. \n\nThe function should also configure a Geant4 instance with a tracking field, event actions, detector construction, and a particle gun. The Geant4 instance should be set up with a UI, which uses a macro if provided as a command line argument. The particle gun should be set up with a gamma particle, an energy of 5 keV, and a multiplicity of 1. \n\nThe function should also set up a tracker named 'MaterialTester' and a physics list named 'QGSP_BERT'. After all configurations, the function should execute the Geant4 instance. \n\nFinally, the code should call this function if it is the main module being run.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def run(self):\n    \"\"\"setup the geometry and dd4hep and geant4 and do what was asked to be done\"\"\"\n    import ROOT\n    ROOT.PyConfig.IgnoreCommandLineOptions = True\n\n    import DDG4\n    import dd4hep\n\n    self.printLevel = getOutputLevel(self.printLevel)\n\n    kernel = DDG4.Kernel()\n    dd4hep.setPrintLevel(self.printLevel)\n\n    for compactFile in self.compactFile:\n      kernel.loadGeometry(str(\"file:\" + os.path.abspath(compactFile)))\n    detectorDescription = kernel.detectorDescription()\n\n    DDG4.importConstants(detectorDescription)\n\n  # ----------------------------------------------------------------------------------\n\n    # simple = DDG4.Geant4( kernel, tracker='Geant4TrackerAction',calo='Geant4CalorimeterAction')\n    # geant4 = DDG4.Geant4( kernel, tracker='Geant4TrackerCombineAction',calo='Geant4ScintillatorCalorimeterAction')\n    geant4 = DDG4.Geant4(kernel, tracker=self.action.tracker, calo=self.action.calo)\n\n    geant4.printDetectors()\n\n    if self.runType == \"vis\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=True, macro=self.macroFile)\n    elif self.runType == \"qt\":\n      uiaction = geant4.setupUI(typ=\"qt\", vis=True, macro=self.macroFile)\n    elif self.runType == \"run\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=self.macroFile, ui=False)\n    elif self.runType == \"shell\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=True)\n    elif self.runType == \"batch\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=False)\n    else:\n      logger.error(\"unknown runType\")\n      exit(1)\n\n    # User Configuration for the Geant4Phases\n    uiaction.ConfigureCommands = self.ui._commandsConfigure\n    uiaction.InitializeCommands = self.ui._commandsInitialize\n    uiaction.PostRunCommands = self.ui._commandsPostRun\n    uiaction.PreRunCommands = self.ui._commandsPreRun\n    uiaction.TerminateCommands = self.ui._commandsTerminate\n\n    kernel.NumEvents = self.numberOfEvents\n\n    # -----------------------------------------------------------------------------------\n    # setup the magnetic field:\n    self.__setMagneticFieldOptions(geant4)\n\n    # configure geometry creation\n    self.geometry.constructGeometry(kernel, geant4, self.output.geometry)\n\n    # ----------------------------------------------------------------------------------\n    # Configure Run actions\n    run1 = DDG4.RunAction(kernel, 'Geant4TestRunAction/RunInit')\n    kernel.registerGlobalAction(run1)\n    kernel.runAction().add(run1)\n\n    # Configure the random seed, do it before the I/O because we might change the seed!\n    self.random.initialize(DDG4, kernel, self.output.random)\n\n    # Configure the output file format and plugin\n    self.outputConfig.initialize(dd4hepsimulation=self, geant4=geant4)\n\n    actionList = []\n\n    if self.enableGun:\n      gun = DDG4.GeneratorAction(kernel, \"Geant4ParticleGun/\" + \"Gun\")\n      self.gun.setOptions(gun)\n      gun.Standalone = False\n      gun.Mask = 1\n      actionList.append(gun)\n      self.__applyBoostOrSmear(kernel, actionList, 1)\n      logger.info(\"++++ Adding DD4hep Particle Gun ++++\")\n\n    if self.enableG4Gun:\n      # GPS Create something\n      self._g4gun = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/Gun\")\n      self._g4gun.Uses = 'G4ParticleGun'\n      self._g4gun.Mask = 2\n      logger.info(\"++++ Adding Geant4 Particle Gun ++++\")\n      actionList.append(self._g4gun)\n\n    if self.enableG4GPS:\n      # GPS Create something\n      self._g4gps = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/GPS\")\n      self._g4gps.Uses = 'G4GeneralParticleSource'\n      self._g4gps.Mask = 3\n      logger.info(\"++++ Adding Geant4 General Particle Source ++++\")\n      actionList.append(self._g4gps)\n\n    start = 4\n    for index, plugin in enumerate(self.inputConfig.userInputPlugin, start=start):\n      gen = plugin(self)\n      gen.Mask = index\n      start = index + 1\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n      logger.info(\"++++ Adding User Plugin %s ++++\", gen.Name)\n\n    for index, inputFile in enumerate(self.inputFiles, start=start):\n      if inputFile.endswith(\".slcio\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/LCIO%d\" % index)\n        gen.Parameters = self.lcio.getParameters()\n        gen.Input = \"LCIOFileReader|\" + inputFile\n      elif inputFile.endswith(\".stdhep\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/STDHEP%d\" % index)\n        gen.Input = \"LCIOStdHepReader|\" + inputFile\n      elif inputFile.endswith(\".HEPEvt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/HEPEvt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtShort|\" + inputFile\n      elif inputFile.endswith(\".hepevt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepevt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtLong|\" + inputFile\n      elif inputFile.endswith(tuple([\".hepmc\"] + HEPMC3_SUPPORTED_EXTENSIONS)):\n        if self.hepmc3.useHepMC3:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Parameters = self.hepmc3.getParameters()\n          gen.Input = \"HEPMC3FileReader|\" + inputFile\n        else:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Input = \"Geant4EventReaderHepMC|\" + inputFile\n      elif inputFile.endswith(\".pairs\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/GuineaPig%d\" % index)\n        gen.Input = \"Geant4EventReaderGuineaPig|\" + inputFile\n        gen.Parameters = self.guineapig.getParameters()\n      else:\n        # this should never happen because we already check at the top, but in case of some LogicError...\n        raise RuntimeError(\"Unknown input file type: %s\" % inputFile)\n      gen.Sync = self.skipNEvents\n      gen.Mask = index\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n\n    if actionList:\n      self._buildInputStage(geant4, actionList, output_level=self.output.inputStage,\n                            have_mctruth=self._enablePrimaryHandler())\n\n    # ================================================================================================\n\n    # And handle the simulation particles.\n    part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n    kernel.generatorAction().adopt(part)\n    # part.SaveProcesses = ['conv','Decay']\n    part.SaveProcesses = self.part.saveProcesses\n    part.MinimalKineticEnergy = self.part.minimalKineticEnergy\n    part.KeepAllParticles = self.part.keepAllParticles\n    part.PrintEndTracking = self.part.printEndTracking\n    part.PrintStartTracking = self.part.printStartTracking\n    part.MinDistToParentVertex = self.part.minDistToParentVertex\n    part.OutputLevel = self.output.part\n    part.enableUI()\n\n    if self.part.enableDetailedHitsAndParticleInfo:\n      self.part.setDumpDetailedParticleInfo(kernel, DDG4)\n\n    self.part.setupUserParticleHandler(part, kernel, DDG4)\n\n    # =================================================================================\n\n    # Setup global filters for use in sensitive detectors\n    try:\n      self.filter.setupFilters(kernel)\n    except RuntimeError as e:\n      logger.error(\"%s\", e)\n      exit(1)\n\n    # =================================================================================\n    # get lists of trackers and calorimeters in detectorDescription\n\n    trk, cal, unk = self.getDetectorLists(detectorDescription)\n\n    for detectors, function, defFilter, abort in [(trk, geant4.setupTracker, self.filter.tracker, False),\n                                                  (cal, geant4.setupCalorimeter, self.filter.calo, False),\n                                                  (unk, geant4.setupDetector, None, True),\n                                                  ]:\n      try:\n        self.__setupSensitiveDetectors(detectors, function, defFilter, abort)\n      except Exception as e:\n        logger.error(\"Failed setting up sensitive detector %s\", e)\n        raise\n\n  # =================================================================================\n    # Now build the physics list:\n    _phys = self.physics.setupPhysics(kernel, name=self.physicsList)\n\n    # add the G4StepLimiterPhysics to activate the max step limits in volumes\n    ph = DDG4.PhysicsList(kernel, 'Geant4PhysicsList/Myphysics')\n    ph.addPhysicsConstructor(str('G4StepLimiterPhysics'))\n    _phys.add(ph)\n\n    dd4hep.setPrintLevel(self.printLevel)\n\n    kernel.configure()\n    kernel.initialize()\n\n    # GPS\n    if self._g4gun is not None:\n      self._g4gun.generator()\n    if self._g4gps is not None:\n      self._g4gps.generator()\n\n    startUpTime, _sysTime, _cuTime, _csTime, _elapsedTime = os.times()\n\n    kernel.run()\n    kernel.terminate()\n\n    totalTimeUser, totalTimeSys, _cuTime, _csTime, _elapsedTime = os.times()\n    if self.printLevel <= 3:\n      logger.info(\"DDSim            INFO  Total Time:   %3.2f s (User), %3.2f s (System)\" %\n                  (totalTimeUser, totalTimeSys))\n      if self.numberOfEvents != 0:\n        eventTime = totalTimeUser - startUpTime\n        perEventTime = eventTime / self.numberOfEvents\n        logger.info(\"DDSim            INFO  StartUp Time: %3.2f s, Event Processing: %3.2f s (%3.2f s/Event) \"\n                    % (startUpTime, eventTime, perEventTime))\n\n# --- Snippet Separator ---\n\ndef materialScan(opts):\n  kernel = DDG4.Kernel()\n  kernel.loadGeometry(str(opts.compact))\n  DDG4.Core.setPrintFormat(str(\"%-32s %6s %s\"))\n  geant4 = DDG4.Geant4(kernel)\n  # Configure UI\n  geant4.setupCshUI(ui=None)\n  for i in geant4.description.detectors():\n    o = DDG4.DetElement(i.second.ptr())\n    sd = geant4.description.sensitiveDetector(o.name())\n    if sd.isValid():\n      typ = sd.type()\n      if typ in geant4.sensitive_types:\n        geant4.setupDetector(o.name(), geant4.sensitive_types[typ])\n      else:\n        logger.error('+++  %-32s type:%-12s  --> Unknown Sensitive type: %s', o.name(), typ, typ)\n        sys.exit(errno.EINVAL)\n\n  geant4.setupGun(\"Gun\",\n                  Standalone=True,\n                  particle='geantino',\n                  energy=20 * g4units.GeV,\n                  position=opts.position,\n                  direction=opts.direction,\n                  multiplicity=1,\n                  isotrop=False)\n  scan = DDG4.SteppingAction(kernel, 'Geant4GeometryScanner/GeometryScan')\n  kernel.steppingAction().adopt(scan)\n\n  # Now build the physics list:\n  geant4.setupPhysics('QGSP_BERT')\n\n  kernel.configure()\n  kernel.initialize()\n  kernel.NumEvents = 1\n  kernel.run()\n  kernel.terminate()\n  return 0\n\n# --- Snippet Separator ---\n\ndef materialScan(opts):\n  kernel = DDG4.Kernel()\n  kernel.loadGeometry(str(opts.compact))\n  DDG4.Core.setPrintFormat(str(\"%-32s %6s %s\"))\n  geant4 = DDG4.Geant4(kernel)\n  # Configure UI\n  geant4.setupCshUI(ui=None)\n  for i in geant4.description.detectors():\n    o = DDG4.DetElement(i.second.ptr())\n    sd = geant4.description.sensitiveDetector(o.name())\n    if sd.isValid():\n      typ = sd.type()\n      if typ in geant4.sensitive_types:\n        geant4.setupDetector(o.name(), geant4.sensitive_types[typ])\n      else:\n        logger.error('+++  %-32s type:%-12s  --> Unknown Sensitive type: %s', o.name(), typ, typ)\n        sys.exit(errno.EINVAL)\n\n  geant4.setupGun(\"Gun\",\n                  Standalone=True,\n                  particle='geantino',\n                  energy=20 * g4units.GeV,\n                  position=opts.position,\n                  direction=opts.direction,\n                  multiplicity=1,\n                  isotrop=False)\n  scan = DDG4.SteppingAction(kernel, 'Geant4MaterialScanner/MaterialScan')\n  kernel.steppingAction().adopt(scan)\n\n  # Now build the physics list:\n  geant4.setupPhysics('QGSP_BERT')\n\n  kernel.configure()\n  kernel.initialize()\n  kernel.NumEvents = 1\n  kernel.run()\n  kernel.terminate()\n  return 0\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a dd4hep simulation using Python configuration. The code should define a function that initializes a DDG4 kernel, loads a geometry from an XML file located in the 'OpticalSurfaces/compact' directory of the 'DD4hepExamplesINSTALL' environment variable, and imports constants from the kernel's detector description. \n\nThe function should also configure a Geant4 instance with a tracking field, event actions, detector construction, and a particle gun. The Geant4 instance should be set up with a UI, which uses a macro if provided as a command line argument. The particle gun should be set up with a gamma particle, an energy of 5 keV, and a multiplicity of 1. \n\nThe function should also set up a tracker named 'MaterialTester' and a physics list named 'QGSP_BERT'. After all configurations, the function should execute the Geant4 instance. \n\nFinally, the code should call this function if it is the main module being run.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 80, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that adapts the Avalanche library to use Huggingface models and datasets for a machine translation task. The code should include a custom data collator class that pads labels and prepares decoder input ids. It should also include a class that modifies the Avalanche Naive training strategy to handle Huggingface minibatches and adapt the forward and criterion methods for machine translation tasks. The main function should load a tokenizer and a dataset, preprocess the dataset, create a sequence-to-sequence model, and set up a continual learning scenario with the Avalanche library. The model should be trained and evaluated on the continual learning benchmark.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class DynamicModule(Module):\n    \"\"\"\n        Dynamic Modules are Avalanche modules that can be incrementally\n        expanded to allow architectural modifications (multi-head\n        classifiers, progressive networks, ...).\n\n        Compared to pytoch Modules, they provide an additional method,\n        `model_adaptation`, which adapts the model given data from the\n        current experience.\n    \"\"\"\n\n    def adaptation(self, dataset: AvalancheDataset = None):\n        \"\"\" Adapt the module (freeze units, add units...) using the current\n        data. Optimizers must be updated after the model adaptation.\n\n        Avalanche strategies call this method to adapt the architecture\n        *before* processing each experience. Strategies also update the\n        optimizer automatically.\n\n        .. warning::\n            As a general rule, you should NOT use this method to train the\n            model. The dataset should be used only to check conditions which\n            require the model's adaptation, such as the discovery of new\n            classes or tasks.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        if self.training:\n            self.train_adaptation(dataset)\n        else:\n            self.eval_adaptation(dataset)\n\n    def train_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at training time.\n\n        Avalanche strategies automatically call this method *before* training\n        on each experience.\n        \"\"\"\n        pass\n\n    def eval_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at evaluation time.\n\n        Avalanche strategies automatically call this method *before* evaluating\n        on each experience.\n\n        .. warning::\n            This method receives the experience's data at evaluation time\n            because some dynamic models need it for adaptation. For example,\n            an incremental classifier needs to be expanded even at evaluation\n            time if new classes are available. However, you should **never**\n            use this data to **train** the module's parameters.\n        \"\"\"\n        pass\n\n# --- Snippet Separator ---\n\ndef __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger):\n        \"\"\"\n        JointTraining performs joint training (also called offline training) on\n        the entire stream of data. This means that it is not a continual\n        learning strategy but it can be used as an \"offline\" upper bound for\n        them.\n\n        .. warnings also::\n            Currently :py:class:`JointTraining` adapts its own dataset.\n            Please check that the plugins you are using do not implement\n            :py:meth:`adapt_trainin_dataset`. Otherwise, they are incompatible\n            with :py:class:`JointTraining`.\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device to run the model.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        \"\"\"\n        super().__init__(model, optimizer, criterion, train_mb_size,\n                         train_epochs, eval_mb_size, device, plugins, evaluator)\n        # JointTraining can be trained only once.\n        self._is_fitted = False\n\n# --- Snippet Separator ---\n\ndef dataset_scenario(\n        train_dataset_list: Sequence[SupportedDataset],\n        test_dataset_list: Sequence[SupportedDataset],\n        task_labels: Sequence[int],\n        *,\n        complete_test_set_only: bool = False,\n        dataset_type: AvalancheDatasetType = AvalancheDatasetType.UNDEFINED) \\\n        -> GenericCLScenario:\n    \"\"\"\n    This helper function is DEPRECATED in favor of `dataset_benchmark`.\n\n    Creates a generic scenario given a list of datasets and the respective task\n    labels. Each training dataset will be considered as a separate training\n    experience. Contents of the datasets will not be changed, including the\n    targets.\n\n    When loading the datasets from a set of fixed file lists, consider using\n    the :func:`filelist_scenario` helper method instead. Also, loading from\n    a list of paths is supported through the :func:`paths_scenario` helper.\n\n    In its base form, this function accepts a list of test datasets that must\n    contain the same amount of datasets of the training list.\n    Those pairs are then used to create the \"past\", \"cumulative\"\n    (a.k.a. growing) and \"future\" test sets. However, in certain Continual\n    Learning scenarios only the concept of \"complete\" test set makes sense. In\n    that case, the ``complete_test_set_only`` parameter should be set to True\n    (see the parameter description for more info).\n\n    Beware that pattern transformations must already be included in the\n    datasets (when needed).\n\n    :param train_dataset_list: A list of training datasets.\n    :param test_dataset_list: A list of test datasets.\n    :param task_labels: A list of task labels. Must contain the same amount of\n        elements of the ``train_dataset_list`` parameter. For\n        Single-Incremental-Task (a.k.a. Task-Free) scenarios, this is usually\n        a list of zeros. For Multi Task scenario, this is usually a list of\n        ascending task labels (starting from 0).\n    :param complete_test_set_only: If True, only the complete test set will\n        be returned by the scenario. This means that the ``test_dataset_list``\n        parameter must be list with a single element (the complete test set).\n        Defaults to False, which means that ``train_dataset_list`` and\n        ``test_dataset_list`` must contain the same amount of datasets.\n    :param dataset_type: The type of the dataset. Defaults to None, which\n        means that the type will be obtained from the input datasets. If input\n        datasets are not instances of :class:`AvalancheDataset`, the type\n        UNDEFINED will be used.\n\n    :returns: A properly initialized :class:`GenericCLScenario` instance.\n    \"\"\"\n\n    warnings.warn('dataset_scenario is deprecated in favor of '\n                  'dataset_benchmark.', DeprecationWarning)\n\n    return create_multi_dataset_generic_scenario(\n        train_dataset_list=train_dataset_list,\n        test_dataset_list=test_dataset_list,\n        task_labels=task_labels,\n        complete_test_set_only=complete_test_set_only,\n        dataset_type=dataset_type)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that adapts the Avalanche library to use Huggingface models and datasets for a machine translation task. The code should include a custom data collator class that pads labels and prepares decoder input ids. It should also include a class that modifies the Avalanche Naive training strategy to handle Huggingface minibatches and adapt the forward and criterion methods for machine translation tasks. The main function should load a tokenizer and a dataset, preprocess the dataset, create a sequence-to-sequence model, and set up a continual learning scenario with the Avalanche library. The model should be trained and evaluated on the continual learning benchmark.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 81, "repo_full_name": "unidata__metpy", "instruction": "Generate code that performs a number of calculations using sounding data from the MetPy library. The code should define a function to determine the effective inflow layer for a convective sounding, using the default values of Thompison et al. (2004) for CAPE (100 J/kg) and CIN (-250 J/kg). The code should also read in sample data, isolate needed variables and attach units, compute wind components, compute common sounding index parameters, compute the parcel profile for a surface-based parcel, compute corresponding LI, CAPE, CIN values for a surface parcel, determine the LCL, LFC, and EL for the surface parcel, compute characteristics of a mean layer parcel and the most unstable parcel, compute the Bunkers Storm Motion vector and use it to calculate the critical angle, compute the characteristics needed to compute the significant tornado parameter, compute the supercell composite parameter if possible, and finally print out the important sounding parameters.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def cape_cin(pressure, temperature, dewpoint, parcel_profile, which_lfc='bottom',\n             which_el='top'):\n    r\"\"\"Calculate CAPE and CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile and parcel path. CIN is integrated between the surface and\n    LFC, CAPE is integrated between the LFC and EL (or top of sounding). Intersection points\n    of the measured temperature profile and parcel profile are logarithmically interpolated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure level(s) of interest, in order from highest to\n        lowest pressure\n\n    temperature : `pint.Quantity`\n        Atmospheric temperature corresponding to pressure\n\n    dewpoint : `pint.Quantity`\n        Atmospheric dewpoint corresponding to pressure\n\n    parcel_profile : `pint.Quantity`\n        Temperature profile of the parcel\n\n    which_lfc : str\n        Choose which LFC to integrate from. Valid options are 'top', 'bottom', 'wide',\n        and 'most_cape'. Default is 'bottom'.\n\n    which_el : str\n        Choose which EL to integrate to. Valid options are 'top', 'bottom', 'wide',\n        and 'most_cape'. Default is 'top'.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Convective Available Potential Energy (CAPE)\n\n    `pint.Quantity`\n        Convective Inhibition (CIN)\n\n    Examples\n    --------\n    >>> from metpy.calc import cape_cin, dewpoint_from_relative_humidity, parcel_profile\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # compture parcel temperature\n    >>> prof = parcel_profile(p, T[0], Td[0]).to('degC')\n    >>> # calculate surface based CAPE/CIN\n    >>> cape_cin(p, T, Td, prof)\n    (<Quantity(4703.77306, 'joule / kilogram')>, <Quantity(0, 'joule / kilogram')>)\n\n    See Also\n    --------\n    lfc, el\n\n    Notes\n    -----\n    Formula adopted from [Hobbs1977]_.\n\n    .. math:: \\text{CAPE} = -R_d \\int_{LFC}^{EL}\n            (T_{{v}_{parcel}} - T_{{v}_{env}}) d\\text{ln}(p)\n\n    .. math:: \\text{CIN} = -R_d \\int_{SFC}^{LFC}\n            (T_{{v}_{parcel}} - T_{{v}_{env}}) d\\text{ln}(p)\n\n\n    * :math:`CAPE` is convective available potential energy\n    * :math:`CIN` is convective inhibition\n    * :math:`LFC` is pressure of the level of free convection\n    * :math:`EL` is pressure of the equilibrium level\n    * :math:`SFC` is the level of the surface or beginning of parcel path\n    * :math:`R_d` is the gas constant\n    * :math:`g` is gravitational acceleration\n    * :math:`T_{{v}_{parcel}}` is the parcel virtual temperature\n    * :math:`T_{{v}_{env}}` is environment virtual temperature\n    * :math:`p` is atmospheric pressure\n\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    .. versionchanged:: 1.0\n       Renamed ``dewpt`` parameter to ``dewpoint``\n\n    \"\"\"\n    pressure, temperature, dewpoint, parcel_profile = _remove_nans(pressure, temperature,\n                                                                   dewpoint, parcel_profile)\n\n    pressure_lcl, _ = lcl(pressure[0], temperature[0], dewpoint[0])\n    below_lcl = pressure > pressure_lcl\n\n    # The mixing ratio of the parcel comes from the dewpoint below the LCL, is saturated\n    # based on the temperature above the LCL\n    parcel_mixing_ratio = np.where(below_lcl, saturation_mixing_ratio(pressure, dewpoint),\n                                   saturation_mixing_ratio(pressure, temperature))\n\n    # Convert the temperature/parcel profile to virtual temperature\n    temperature = virtual_temperature_from_dewpoint(pressure, temperature, dewpoint)\n    parcel_profile = virtual_temperature(parcel_profile, parcel_mixing_ratio)\n\n    # Calculate LFC limit of integration\n    lfc_pressure, _ = lfc(pressure, temperature, dewpoint,\n                          parcel_temperature_profile=parcel_profile, which=which_lfc)\n\n    # If there is no LFC, no need to proceed.\n    if np.isnan(lfc_pressure):\n        return units.Quantity(0, 'J/kg'), units.Quantity(0, 'J/kg')\n    else:\n        lfc_pressure = lfc_pressure.magnitude\n\n    # Calculate the EL limit of integration\n    el_pressure, _ = el(pressure, temperature, dewpoint,\n                        parcel_temperature_profile=parcel_profile, which=which_el)\n\n    # No EL and we use the top reading of the sounding.\n    if np.isnan(el_pressure):\n        el_pressure = pressure[-1].magnitude\n    else:\n        el_pressure = el_pressure.magnitude\n\n    # Difference between the parcel path and measured temperature profiles\n    y = (parcel_profile - temperature).to(units.degK)\n\n    # Estimate zero crossings\n    x, y = _find_append_zero_crossings(np.copy(pressure), y)\n\n    # CAPE\n    # Only use data between the LFC and EL for calculation\n    p_mask = _less_or_close(x.m, lfc_pressure) & _greater_or_close(x.m, el_pressure)\n    x_clipped = x[p_mask].magnitude\n    y_clipped = y[p_mask].magnitude\n    cape = (mpconsts.Rd\n            * units.Quantity(np.trapz(y_clipped, np.log(x_clipped)), 'K')).to(units('J/kg'))\n\n    # CIN\n    # Only use data between the surface and LFC for calculation\n    p_mask = _greater_or_close(x.m, lfc_pressure)\n    x_clipped = x[p_mask].magnitude\n    y_clipped = y[p_mask].magnitude\n    cin = (mpconsts.Rd\n           * units.Quantity(np.trapz(y_clipped, np.log(x_clipped)), 'K')).to(units('J/kg'))\n\n    # Set CIN to 0 if it's returned as a positive value (#1190)\n    if cin > units.Quantity(0, 'J/kg'):\n        cin = units.Quantity(0, 'J/kg')\n    return cape, cin\n\n# --- Snippet Separator ---\n\ndef surface_based_cape_cin(pressure, temperature, dewpoint):\n    r\"\"\"Calculate surface-based CAPE and CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile for a surface-based parcel. CIN is integrated\n    between the surface and LFC, CAPE is integrated between the LFC and EL (or top of\n    sounding). Intersection points of the measured temperature profile and parcel profile are\n    logarithmically interpolated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile. The first entry should be the starting\n        (surface) observation, with the array going from high to low pressure.\n\n    temperature : `pint.Quantity`\n        Temperature profile corresponding to the `pressure` profile\n\n    dewpoint : `pint.Quantity`\n        Dewpoint profile corresponding to the `pressure` profile\n\n    Returns\n    -------\n    `pint.Quantity`\n        Surface based Convective Available Potential Energy (CAPE)\n\n    `pint.Quantity`\n        Surface based Convective Inhibition (CIN)\n\n    See Also\n    --------\n    cape_cin, parcel_profile\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    \"\"\"\n    pressure, temperature, dewpoint = _remove_nans(pressure, temperature, dewpoint)\n    p, t, td, profile = parcel_profile_with_lcl(pressure, temperature, dewpoint)\n    return cape_cin(p, t, td, profile)\n\n# --- Snippet Separator ---\n\ndef most_unstable_cape_cin(pressure, temperature, dewpoint, **kwargs):\n    r\"\"\"Calculate most unstable CAPE/CIN.\n\n    Calculate the convective available potential energy (CAPE) and convective inhibition (CIN)\n    of a given upper air profile and most unstable parcel path. CIN is integrated between the\n    surface and LFC, CAPE is integrated between the LFC and EL (or top of sounding).\n    Intersection points of the measured temperature profile and parcel profile are\n    logarithmically interpolated.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Pressure profile\n\n    temperature : `pint.Quantity`\n        Temperature profile\n\n    dewpoint : `pint.Quantity`\n        Dew point profile\n\n    kwargs\n        Additional keyword arguments to pass to `most_unstable_parcel`\n\n    Returns\n    -------\n    `pint.Quantity`\n        Most unstable Convective Available Potential Energy (CAPE)\n\n    `pint.Quantity`\n        Most unstable Convective Inhibition (CIN)\n\n    Examples\n    --------\n    >>> from metpy.calc import dewpoint_from_relative_humidity, most_unstable_cape_cin\n    >>> from metpy.units import units\n    >>> # pressure\n    >>> p = [1008., 1000., 950., 900., 850., 800., 750., 700., 650., 600.,\n    ...      550., 500., 450., 400., 350., 300., 250., 200.,\n    ...      175., 150., 125., 100., 80., 70., 60., 50.,\n    ...      40., 30., 25., 20.] * units.hPa\n    >>> # temperature\n    >>> T = [29.3, 28.1, 23.5, 20.9, 18.4, 15.9, 13.1, 10.1, 6.7, 3.1,\n    ...      -0.5, -4.5, -9.0, -14.8, -21.5, -29.7, -40.0, -52.4,\n    ...      -59.2, -66.5, -74.1, -78.5, -76.0, -71.6, -66.7, -61.3,\n    ...      -56.3, -51.7, -50.7, -47.5] * units.degC\n    >>> # relative humidity\n    >>> rh = [.85, .65, .36, .39, .82, .72, .75, .86, .65, .22, .52,\n    ...       .66, .64, .20, .05, .75, .76, .45, .25, .48, .76, .88,\n    ...       .56, .88, .39, .67, .15, .04, .94, .35] * units.dimensionless\n    >>> # calculate dewpoint\n    >>> Td = dewpoint_from_relative_humidity(T, rh)\n    >>> # calculate most unstbale CAPE/CIN\n    >>> most_unstable_cape_cin(p, T, Td)\n    (<Quantity(4703.77306, 'joule / kilogram')>, <Quantity(0, 'joule / kilogram')>)\n\n    See Also\n    --------\n    cape_cin, most_unstable_parcel, parcel_profile\n\n    Notes\n    -----\n    Only functions on 1D profiles (not higher-dimension vertical cross sections or grids).\n    Since this function returns scalar values when given a profile, this will return Pint\n    Quantities even when given xarray DataArray profiles.\n\n    \"\"\"\n    pressure, temperature, dewpoint = _remove_nans(pressure, temperature, dewpoint)\n    _, _, _, parcel_idx = most_unstable_parcel(pressure, temperature, dewpoint, **kwargs)\n    p, t, td, mu_profile = parcel_profile_with_lcl(pressure[parcel_idx:],\n                                                   temperature[parcel_idx:],\n                                                   dewpoint[parcel_idx:])\n    return cape_cin(p, t, td, mu_profile)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a number of calculations using sounding data from the MetPy library. The code should define a function to determine the effective inflow layer for a convective sounding, using the default values of Thompison et al. (2004) for CAPE (100 J/kg) and CIN (-250 J/kg). The code should also read in sample data, isolate needed variables and attach units, compute wind components, compute common sounding index parameters, compute the parcel profile for a surface-based parcel, compute corresponding LI, CAPE, CIN values for a surface parcel, determine the LCL, LFC, and EL for the surface parcel, compute characteristics of a mean layer parcel and the most unstable parcel, compute the Bunkers Storm Motion vector and use it to calculate the critical angle, compute the characteristics needed to compute the significant tornado parameter, compute the supercell composite parameter if possible, and finally print out the important sounding parameters.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 82, "repo_full_name": "nvidia__nvflare", "instruction": "Generate code that defines a class named `SupervisedMonaiProstateDittoLearner` which inherits from `SupervisedMonaiProstateLearner`. This class should be used for a prostate segmentation task and should include methods for training configuration and training. The class should initialize with parameters for the training configuration filename, the number of training epochs of the global model for a round, the number of training epochs of the personalized model for a round, and the name of the task to train the model. The training configuration method should initialize a `SupervisedPTDittoHelper` with a `UNet` model and an `SGD` optimizer. The training method should include a pipeline for Ditto training, which includes getting global model weights, preparing for fedprox loss, loading Ditto personalized model info, local training of the reference model and personalized model, and returning updated weights of the reference model. The training method should also handle abort signals and exceptions.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class PTDittoHelper(object):\n    def __init__(\n        self, criterion, model, optimizer, device, app_dir: str, ditto_lambda: float = 0.1, model_epochs: int = 1\n    ):\n        \"\"\"Helper to be used with Ditto components.\n        Implements the functions used for the algorithm proposed in\n        Li et al. \"Ditto: Fair and Robust Federated Learning Through Personalization\"\n        (https://arxiv.org/abs/2012.04221) using PyTorch.\n\n        Args:\n            criterion: base loss criterion\n            model: the personalized model of Ditto method\n            optimizer: training optimizer for personalized model\n            device: device for personalized model training\n            app_dir: needed for local personalized model saving\n            ditto_lambda: lambda weight for Ditto prox loss term when combining with the base loss, defaults to 0.1\n            model_epochs: training epoch for personalized model, defaults to 1\n\n        Returns:\n            None\n        \"\"\"\n\n        self.criterion = criterion\n        self.model = model\n        self.optimizer = optimizer\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = device\n        self.model_epochs = model_epochs\n        # initialize Ditto criterion\n        self.prox_criterion = PTFedProxLoss(mu=ditto_lambda)\n        # check criterion, model, and optimizer type\n        if not isinstance(self.criterion, torch.nn.modules.loss._Loss):\n            raise ValueError(f\"criterion component must be torch loss. \" f\"But got: {type(self.criterion)}\")\n        if not isinstance(self.model, torch.nn.Module):\n            raise ValueError(f\"model component must be torch model. \" f\"But got: {type(self.model)}\")\n        if not isinstance(self.optimizer, torch.optim.Optimizer):\n            raise ValueError(f\"optimizer component must be torch optimizer. \" f\"But got: {type(self.optimizer)}\")\n        if not isinstance(self.device, torch.device):\n            raise ValueError(f\"device component must be torch device. \" f\"But got: {type(self.device)}\")\n\n        # initialize other recording related parameters\n        self.epoch_global = 0\n        self.epoch_of_start_time = 0\n        self.best_metric: int = 0\n        self.model_file_path = os.path.join(app_dir, \"personalized_model.pt\")\n        self.best_model_file_path = os.path.join(app_dir, \"best_personalized_model.pt\")\n\n    def load_model(self, global_weights):\n        # load local model from last round's record if model exist,\n        # otherwise initialize from global model weights for the first round.\n        if os.path.exists(self.model_file_path):\n            model_data = torch.load(self.model_file_path)\n            self.model.load_state_dict(model_data[\"model\"])\n            self.epoch_of_start_time = model_data[\"epoch\"]\n        else:\n            self.model.load_state_dict(global_weights)\n            self.epoch_of_start_time = 0\n        if os.path.exists(self.best_model_file_path):\n            model_data = torch.load(self.best_model_file_path)\n            self.best_metric = model_data[\"best_metric\"]\n\n    def save_model(self, is_best=False):\n        # save personalized model locally\n        model_weights = self.model.state_dict()\n        save_dict = {\"model\": model_weights, \"epoch\": self.epoch_global}\n        if is_best:\n            save_dict.update({\"best_metric\": self.best_metric})\n            torch.save(save_dict, self.best_model_file_path)\n        else:\n            torch.save(save_dict, self.model_file_path)\n\n    def update_metric_save_model(self, metric):\n        self.save_model(is_best=False)\n        if metric > self.best_metric:\n            self.best_metric = metric\n            self.save_model(is_best=True)\n\n    @abstractmethod\n    def local_train(self, train_loader, model_global, abort_signal: Signal, writer):\n        # Train personal model for self.model_epochs, and keep track of curves\n        # This part is task dependent, need customization\n        # Basic idea is to train personalized model with prox term as compare to model_global\n        raise NotImplementedError\n\n# --- Snippet Separator ---\n\ndef __init__(\n        self, criterion, model, optimizer, device, app_dir: str, ditto_lambda: float = 0.1, model_epochs: int = 1\n    ):\n        \"\"\"Helper to be used with Ditto components.\n        Implements the functions used for the algorithm proposed in\n        Li et al. \"Ditto: Fair and Robust Federated Learning Through Personalization\"\n        (https://arxiv.org/abs/2012.04221) using PyTorch.\n\n        Args:\n            criterion: base loss criterion\n            model: the personalized model of Ditto method\n            optimizer: training optimizer for personalized model\n            device: device for personalized model training\n            app_dir: needed for local personalized model saving\n            ditto_lambda: lambda weight for Ditto prox loss term when combining with the base loss, defaults to 0.1\n            model_epochs: training epoch for personalized model, defaults to 1\n\n        Returns:\n            None\n        \"\"\"\n\n        self.criterion = criterion\n        self.model = model\n        self.optimizer = optimizer\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = device\n        self.model_epochs = model_epochs\n        # initialize Ditto criterion\n        self.prox_criterion = PTFedProxLoss(mu=ditto_lambda)\n        # check criterion, model, and optimizer type\n        if not isinstance(self.criterion, torch.nn.modules.loss._Loss):\n            raise ValueError(f\"criterion component must be torch loss. \" f\"But got: {type(self.criterion)}\")\n        if not isinstance(self.model, torch.nn.Module):\n            raise ValueError(f\"model component must be torch model. \" f\"But got: {type(self.model)}\")\n        if not isinstance(self.optimizer, torch.optim.Optimizer):\n            raise ValueError(f\"optimizer component must be torch optimizer. \" f\"But got: {type(self.optimizer)}\")\n        if not isinstance(self.device, torch.device):\n            raise ValueError(f\"device component must be torch device. \" f\"But got: {type(self.device)}\")\n\n        # initialize other recording related parameters\n        self.epoch_global = 0\n        self.epoch_of_start_time = 0\n        self.best_metric: int = 0\n        self.model_file_path = os.path.join(app_dir, \"personalized_model.pt\")\n        self.best_model_file_path = os.path.join(app_dir, \"best_personalized_model.pt\")\n\n# --- Snippet Separator ---\n\ndef __init__(self, model: Module, optimizer: Optimizer,\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        BaseStrategy is the super class of all task-based continual learning\n        strategies. It implements a basic training loop and callback system\n        that allows to execute code at each experience of the training loop.\n        Plugins can be used to implement callbacks to augment the training\n        loop with additional behavior (e.g. a memory buffer for replay).\n\n        **Scenarios**\n        This strategy supports several continual learning scenarios:\n\n        * class-incremental scenarios (no task labels)\n        * multi-task scenarios, where task labels are provided)\n        * multi-incremental scenarios, where the same task may be revisited\n\n        The exact scenario depends on the data stream and whether it provides\n        the task labels.\n\n        **Training loop**\n        The training loop is organized as follows::\n            train\n                train_exp  # for each experience\n                    adapt_train_dataset\n                    train_dataset_adaptation\n                    make_train_dataloader\n                    train_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        **Evaluation loop**\n        The evaluation loop is organized as follows::\n            eval\n                eval_exp  # for each experience\n                    adapt_eval_dataset\n                    eval_dataset_adaptation\n                    make_eval_dataloader\n                    eval_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device where the model will be allocated.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience and before training on the first experience.\n                if >0: calls `eval` every `eval_every` epochs, at the end\n                    of all the epochs for a single experience and before\n                    training on the first experience.\n        \"\"\"\n        self._criterion = criterion\n\n        self.model: Module = model\n        \"\"\" PyTorch model. \"\"\"\n\n        self.optimizer = optimizer\n        \"\"\" PyTorch optimizer. \"\"\"\n\n        self.train_epochs: int = train_epochs\n        \"\"\" Number of training epochs. \"\"\"\n\n        self.train_mb_size: int = train_mb_size\n        \"\"\" Training mini-batch size. \"\"\"\n\n        self.eval_mb_size: int = train_mb_size if eval_mb_size is None \\\n            else eval_mb_size\n        \"\"\" Eval mini-batch size. \"\"\"\n\n        self.device = device\n        \"\"\" PyTorch device where the model will be allocated. \"\"\"\n\n        self.plugins = [] if plugins is None else plugins\n        \"\"\" List of `StrategyPlugin`s. \"\"\"\n\n        if evaluator is None:\n            evaluator = EvaluationPlugin()\n        self.plugins.append(evaluator)\n        self.evaluator = evaluator\n        \"\"\" EvaluationPlugin used for logging and metric computations. \"\"\"\n\n        self.eval_every = eval_every\n        \"\"\" Frequency of the evaluation during training. \"\"\"\n\n        ###################################################################\n        # State variables. These are updated during the train/eval loops. #\n        ###################################################################\n        self.training_exp_counter = 0\n        \"\"\" Counts the number of training steps. +1 at the end of each \n        experience. \"\"\"\n\n        self.epoch: Optional[int] = None\n        \"\"\" Epoch counter. \"\"\"\n\n        self.experience = None\n        \"\"\" Current experience. \"\"\"\n\n        self.adapted_dataset = None\n        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n        append data to it (e.g. for replay). \n\n        .. note:: \n            This dataset may contain samples from different experiences. If you \n            want the original data for the current experience  \n            use :attr:`.BaseStrategy.experience`.\n        \"\"\"\n\n        self.dataloader = None\n        \"\"\" Dataloader. \"\"\"\n\n        self.mb_it = None\n        \"\"\" Iteration counter. Reset at the start of a new epoch. \"\"\"\n\n        self.mbatch = None\n        \"\"\" Current mini-batch. \"\"\"\n\n        self.mb_output = None\n        \"\"\" Model's output computed on the current mini-batch. \"\"\"\n\n        self.loss = None\n        \"\"\" Loss of the current mini-batch. \"\"\"\n\n        self.is_training: bool = False\n        \"\"\" True if the strategy is in training mode. \"\"\"\n\n        self.current_eval_stream = None\n        \"\"\"User-provided evaluation stream on `eval` call\"\"\"\n\n        self._stop_training = False\n\n        self._warn_for_disabled_plugins_callbacks()\n        self._warn_for_disabled_metrics_callbacks()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that defines a class named `SupervisedMonaiProstateDittoLearner` which inherits from `SupervisedMonaiProstateLearner`. This class should be used for a prostate segmentation task and should include methods for training configuration and training. The class should initialize with parameters for the training configuration filename, the number of training epochs of the global model for a round, the number of training epochs of the personalized model for a round, and the name of the task to train the model. The training configuration method should initialize a `SupervisedPTDittoHelper` with a `UNet` model and an `SGD` optimizer. The training method should include a pipeline for Ditto training, which includes getting global model weights, preparing for fedprox loss, loading Ditto personalized model info, local training of the reference model and personalized model, and returning updated weights of the reference model. The training method should also handle abort signals and exceptions.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 83, "repo_full_name": "oarriaga__paz", "instruction": "Generate code that downloads an image and performs various image and box augmentations using the paz library. The code should include the creation of sequential pipelines for image and box augmentation, as well as preprocessing. The image augmentation pipeline should include random contrast, brightness, saturation, and hue adjustments. The box augmentation pipeline should include conversion to image box coordinates, expansion, random sample cropping, and random flipping of boxes left and right. The code should also include a pipeline for drawing boxes and a pipeline for preprocessing boxes, which includes matching boxes to a set of default boxes, encoding them, and expanding the class label to a one-hot vector. Finally, the code should put everything together in a single processor and demonstrate the image and box augmentations. The code should also include a sequence generator for processing batches of data.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class ToImageBoxCoordinates(Processor):\n    \"\"\"Convert normalized box coordinates to image-size box coordinates.\n    \"\"\"\n    def __init__(self):\n        super(ToImageBoxCoordinates, self).__init__()\n\n    def call(self, image, boxes):\n        boxes = to_image_coordinates(boxes, image)\n        return image, boxes\n\n# --- Snippet Separator ---\n\nclass ToNormalizedBoxCoordinates(Processor):\n    \"\"\"Convert image-size box coordinates to normalized box coordinates.\n    \"\"\"\n    def __init__(self):\n        super(ToNormalizedBoxCoordinates, self).__init__()\n\n    def call(self, image, boxes):\n        boxes = to_normalized_coordinates(boxes, image)\n        return image, boxes\n\n# --- Snippet Separator ---\n\nclass VideoPlayer(object):\n    \"\"\"Performs visualization inferences in a real-time video.\n\n    # Properties\n        image_size: List of two integers. Output size of the displayed image.\n        pipeline: Function. Should take RGB image as input and it should\n            output a dictionary with key 'image' containing a visualization\n            of the inferences. Built-in pipelines can be found in\n            ``paz/processing/pipelines``.\n\n    # Methods\n        run()\n        record()\n    \"\"\"\n\n    def __init__(self, image_size, pipeline, camera, topic='image'):\n        self.image_size = image_size\n        self.pipeline = pipeline\n        self.camera = camera\n        self.topic = topic\n\n    def step(self):\n        \"\"\" Runs the pipeline process once\n\n        # Returns\n            Inferences from ``pipeline``.\n        \"\"\"\n        if self.camera.is_open() is False:\n            raise ValueError('Camera has not started. Call ``start`` method.')\n\n        frame = self.camera.read()\n        if frame is None:\n            print('Frame: None')\n            return None\n        # all pipelines start with an RGB image\n        frame = convert_color_space(frame, BGR2RGB)\n        return self.pipeline(frame)\n\n    def run(self):\n        \"\"\"Opens camera and starts continuous inference using ``pipeline``,\n        until the user presses ``q`` inside the opened window.\n        \"\"\"\n        self.camera.start()\n        while True:\n            output = self.step()\n            if output is None:\n                continue\n            image = resize_image(output[self.topic], tuple(self.image_size))\n            show_image(image, 'inference', wait=False)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n        self.camera.stop()\n        cv2.destroyAllWindows()\n\n    def record(self, name='video.avi', fps=20, fourCC='XVID'):\n        \"\"\"Opens camera and records continuous inference using ``pipeline``.\n\n        # Arguments\n            name: String. Video name. Must include the postfix .avi.\n            fps: Int. Frames per second.\n            fourCC: String. Indicates the four character code of the video.\n            e.g. XVID, MJPG, X264.\n        \"\"\"\n        self.camera.start()\n        fourCC = cv2.VideoWriter_fourcc(*fourCC)\n        writer = cv2.VideoWriter(name, fourCC, fps, self.image_size)\n        while True:\n            output = self.step()\n            if output is None:\n                continue\n            image = resize_image(output['image'], tuple(self.image_size))\n            show_image(image, 'inference', wait=False)\n            writer.write(image)\n            if cv2.waitKey(1) & 0xFF == ord('q'):\n                break\n\n        self.camera.stop()\n        writer.release()\n        cv2.destroyAllWindows()\n\n    def record_from_file(self, video_file_path, name='video.avi',\n                         fps=20, fourCC='XVID'):\n        \"\"\"Load video and records continuous inference using ``pipeline``.\n\n        # Arguments\n            video_file_path: String. Path to the video file.\n            name: String. Output video name. Must include the postfix .avi.\n            fps: Int. Frames per second.\n            fourCC: String. Indicates the four character code of the video.\n            e.g. XVID, MJPG, X264.\n        \"\"\"\n\n        fourCC = cv2.VideoWriter_fourcc(*fourCC)\n        writer = cv2.VideoWriter(name, fourCC, fps, self.image_size)\n\n        video = cv2.VideoCapture(video_file_path)\n        if (video.isOpened() is False):\n            print(\"Error opening video  file\")\n\n        while video.isOpened():\n            is_frame_received, frame = video.read()\n            if not is_frame_received:\n                print(\"Frame not received. Exiting ...\")\n                break\n            if is_frame_received is True:\n                output = self.pipeline(frame)\n                if output is None:\n                    continue\n                image = resize_image(output['image'], tuple(self.image_size))\n                show_image(image, 'inference', wait=False)\n                writer.write(image)\n                if cv2.waitKey(1) & 0xFF == ord('q'):\n                    break\n\n        writer.release()\n        cv2.destroyAllWindows()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that downloads an image and performs various image and box augmentations using the paz library. The code should include the creation of sequential pipelines for image and box augmentation, as well as preprocessing. The image augmentation pipeline should include random contrast, brightness, saturation, and hue adjustments. The box augmentation pipeline should include conversion to image box coordinates, expansion, random sample cropping, and random flipping of boxes left and right. The code should also include a pipeline for drawing boxes and a pipeline for preprocessing boxes, which includes matching boxes to a set of default boxes, encoding them, and expanding the class label to a one-hot vector. Finally, the code should put everything together in a single processor and demonstrate the image and box augmentations. The code should also include a sequence generator for processing batches of data.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 84, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that constructs Maximally Localized Wannier Functions (MLWFs) using the pywannier90 tool from the pyscf library. The code should define a unit cell, perform a PBE calculation, save and load the kks object, construct MLWFs, export the MLWFs in xsf format for plotting, export certain matrices and run a wannier90 using these, interpolate the Fock or band structure using the Slater-Koster scheme, print the difference in the eigenvalues interpolated by scf.get_bands function and by pywannier90, and plot the band structure using mcu.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def interpolate_band(self, frac_kpts, ham_kpts=None, use_ws_distance=True,\n                         ws_search_size=[2,2,2], ws_distance_tol=1e-6):\n        ''' Interpolate the band structure using the Slater-Koster scheme\n            Return:\n                eigenvalues and eigenvectors at the desired kpts\n        '''\n\n        assert self.U_matrix is not None, (\n            \"You must wannierize first, then you can run this function\")\n        inter_hamiltonian_kpts = self.interpolate_ham_kpts(\n            frac_kpts, ham_kpts, use_ws_distance, ws_search_size, ws_distance_tol)\n        # Diagonalize H(kpts) to get eigenvalues and eigenvector\n        nkpts = frac_kpts.shape[0]\n        eigvals, eigvecs = np.linalg.eigh(inter_hamiltonian_kpts)\n        idx_kpts = eigvals.argsort()\n        eigvals = np.asarray([eigvals[kpt][idx_kpts[kpt]] for kpt in range(nkpts)])\n        eigvecs = np.asarray([eigvecs[kpt][:,idx_kpts[kpt]] for kpt in range(nkpts)])\n\n        return eigvals, eigvecs\n\n# --- Snippet Separator ---\n\ndef export_AME(self, grid=[50,50,50]):\n        r'''\n        Export A_{m,n}^{\\mathbf{k}} and M_{m,n}^{(\\mathbf{k,b})} and \\epsilon_{n}^(\\mathbf{k})\n        '''\n\n        if self.A_matrix_loc is None:\n            self.make_win()\n            self.setup()\n            self.M_matrix_loc = self.get_M_mat()\n            self.A_matrix_loc = self.get_A_mat()\n            self.eigenvalues_loc = self.get_epsilon_mat()\n            self.export_unk(self, grid = grid)\n\n        with open('wannier90.mmn', 'w') as f:\n            f.write('Generated by the pyWannier90. Date: %s\\n' % (time.ctime()))\n            f.write('    %d    %d    %d\\n' % (self.num_bands_loc, self.num_kpts_loc, self.nntot_loc))\n\n            for k_id in range(self.num_kpts_loc):\n                for nn in range(self.nntot_loc):\n                    k_id1 = k_id + 1\n                    k_id2 = self.nn_list[nn, k_id, 0]\n                    nnn, nnm, nnl = self.nn_list[nn, k_id, 1:4]\n                    f.write('    %d  %d    %d  %d  %d\\n' % (k_id1, k_id2, nnn, nnm, nnl))\n                    for m in range(self.num_bands_loc):\n                        for n in range(self.num_bands_loc):\n                            f.write('    %22.18e  %22.18e\\n' % (self.M_matrix_loc[k_id, nn,m,n].real,\n                                                                self.M_matrix_loc[k_id, nn,m,n].imag))\n\n        with open('wannier90.amn', 'w') as f:\n            f.write('Generated by the pyWannier90. Date: %s\\n' % (time.ctime()))\n            f.write('    %d    %d    %d\\n' % (self.num_bands_loc, self.num_kpts_loc, self.num_wann_loc))\n\n            for k_id in range(self.num_kpts_loc):\n                for ith_wann in range(self.num_wann_loc):\n                    for band in range(self.num_bands_loc):\n                        f.write('    %d    %d    %d    %22.18e    %22.18e\\n' %\n                                (band+1, ith_wann+1, k_id+1,\n                                 self.A_matrix_loc[k_id,ith_wann,band].real,\n                                 self.A_matrix_loc[k_id,ith_wann,band].imag))\n\n        with open('wannier90.eig', 'w') as f:\n            for k_id in range(self.num_kpts_loc):\n                for band in range(self.num_bands_loc):\n                    f.write('    %d    %d    %22.18e\\n' % (band+1, k_id+1, self.eigenvalues_loc[k_id,band]))\n\n# --- Snippet Separator ---\n\ndef interpolate_ham_kpts(self, frac_kpts, ham_kpts=None,\n                             use_ws_distance=True, ws_search_size=[2,2,2],\n                             ws_distance_tol=1e-6):\n        ''' Interpolate the band structure using the Slater-Koster scheme\n            Return:\n                eigenvalues and eigenvectors at the desired kpts\n        '''\n\n        assert self.U_matrix is not None, \"You must wannierize first, then you can run this function\"\n        ndegen, Rs, center = self.get_wigner_seitz_supercell(ws_search_size, ws_distance_tol)\n        hamiltonian_R0 = self.get_hamiltonian_Rs(Rs, ham_kpts)\n\n        # Interpolate H(kpts) at the desired k-pts\n        if use_ws_distance:\n            wdist_ndeg, wdist_ndeg_, irdist_ws, crdist_ws = self.ws_translate_dist(Rs)\n            temp = lib.einsum('iRstx,kx->iRstk', irdist_ws, frac_kpts)\n            phase = lib.einsum('iRstk,iRst->Rstk', np.exp(1j* 2*np.pi * temp), wdist_ndeg_)\n            inter_hamiltonian_kpts = \\\n                    lib.einsum('R,Rst,Rts,Rstk->kst', 1/ndegen, 1/wdist_ndeg, hamiltonian_R0, phase)\n        else:\n            phase = np.exp(1j* 2*np.pi * np.dot(Rs, frac_kpts.T))\n            inter_hamiltonian_kpts = \\\n                    lib.einsum('R,Rst,Rk->kst', 1/ndegen, hamiltonian_R0, phase)\n\n        return inter_hamiltonian_kpts\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that constructs Maximally Localized Wannier Functions (MLWFs) using the pywannier90 tool from the pyscf library. The code should define a unit cell, perform a PBE calculation, save and load the kks object, construct MLWFs, export the MLWFs in xsf format for plotting, export certain matrices and run a wannier90 using these, interpolate the Fock or band structure using the Slater-Koster scheme, print the difference in the eigenvalues interpolated by scf.get_bands function and by pywannier90, and plot the band structure using mcu.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 85, "repo_full_name": "labsn__expyfun", "instruction": "Generate code that uses the expyfun library to prepare and run an experiment using the CRM corpus. The experiment should prepare two talkers with different genders and the same talker number at a sampling rate of 40000 Hz. It should then print the valid callsigns and read a sentence from the hard drive. The code should also preload all the talkers and get a second sentence from memory. The two sentences should be padded and aligned at the start. The experiment should be run with a specific name, window size, participant, session, and version. It should display a text prompt on the screen, load the padded sentences into the buffer, start the stimulus, and wait for a specific duration. The code should also include a response menu and display a prompt based on the response. Finally, it should plot a screenshot of the experiment.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def run(self, *args, **kwargs):\n        \"\"\"This method should be the meat of the experiment (needs overriden).\n\n        This is where your experiment code goes.  Note that you should use\n        `self.wait_or_stop()` to pause your experiment between readings, to\n        allow the background thread to be stopped if necessary.\n\n        If you set `self.latest_data`, this may be used to display your\n        results in real time.  You can also use `self.log()` to output text\n        describing the experiment's progress; this may be picked up and \n        displayed graphically or in the console.\n\n        The arguments are passed through from start() to here, so you should\n        either use or ignore them as appropriate.  These are the same args\n        as are passed to run(), so if one of the two functions requires an\n        argument you should make sure the other won't fail if the same\n        argument is passed to it (simple rule: accept *args, **kwargs in\n        both, in addition to any arguments you might have).\n        \"\"\"\n        NotImplementedError(\"The run() method of an Experiment must be overridden!\")\n\n# --- Snippet Separator ---\n\nclass Experiment(Instrument):\n    \"\"\"A class representing an experimental protocol.\n\n    This base class is a subclass of Instrument, so it provides all the GUI\n    code and data management that instruments have.  It's also got an\n    improved logging mechanism, designed for use as a status display, and some\n    template methods for running a long experiment in the background.\n    \"\"\"\n\n    latest_data = DumbNotifiedProperty(doc=\"The last dataset/group we acquired\")\n    log_messages = DumbNotifiedProperty(doc=\"Log messages from the latest run\")\n    log_to_console = False\n    experiment_can_be_safely_aborted = False # set to true if you want to suppress warnings about ExperimentStopped\n\n    def __init__(self):\n        \"\"\"Create an instance of the Experiment class\"\"\"\n        super(Experiment, self).__init__()\n        self._stop_event = threading.Event()\n        self._finished_event = threading.Event()\n        self._experiment_thread = None\n        self.log_messages = \"\"\n\n    def prepare_to_run(self, *args, **kwargs):\n        \"\"\"This method is always run in the foreground thread before run()\n\n        Use this method if you might need to pop up a GUI, for example.  The\n        most common use of this would be to create a data group or to ensure\n        the current data file exists - doing that in run() could give rise\n        to nasty threading problems.  By default, it does nothing.\n\n        The arguments are passed through from start() to here, so you should\n        either use or ignore them as appropriate.  These are the same args\n        as are passed to run(), so if one of the two functions requires an\n        argument you should make sure the other won't fail if the same\n        argument is passed to it (simple rule: accept *args, **kwargs in\n        both, in addition to any arguments you might have).\n        \"\"\"\n        pass\n\n    def run(self, *args, **kwargs):\n        \"\"\"This method should be the meat of the experiment (needs overriden).\n\n        This is where your experiment code goes.  Note that you should use\n        `self.wait_or_stop()` to pause your experiment between readings, to\n        allow the background thread to be stopped if necessary.\n\n        If you set `self.latest_data`, this may be used to display your\n        results in real time.  You can also use `self.log()` to output text\n        describing the experiment's progress; this may be picked up and \n        displayed graphically or in the console.\n\n        The arguments are passed through from start() to here, so you should\n        either use or ignore them as appropriate.  These are the same args\n        as are passed to run(), so if one of the two functions requires an\n        argument you should make sure the other won't fail if the same\n        argument is passed to it (simple rule: accept *args, **kwargs in\n        both, in addition to any arguments you might have).\n        \"\"\"\n        NotImplementedError(\"The run() method of an Experiment must be overridden!\")\n\n    def wait_or_stop(self, timeout, raise_exception=True):\n        \"\"\"Wait for the specified time in seconds.  Stop if requested.\n\n        This waits for a given time, unless the experiment has been manually \n        stopped, in which case it will terminate the thread by raising an\n        ExperimentStopped exception.  You should call this whenever your\n        experiment is in a state that would be OK to stop, such as between\n        readings.\n\n        If raise_exception is False, it will simply return False when the\n        experiment should stop.  This is appropriate if you want to use it in a\n        while loop, e.g. ``while self.wait_or_stop(10,raise_exception=False):``\n\n        You may want to explicitly handle the ExperimentStopped exception to\n        close down cleanly.\n        \"\"\"\n        if self._stop_event.wait(timeout):\n            if raise_exception:\n                raise ExperimentStopped()\n        return True\n\n    @background_action\n    @locked_action\n    def run_in_background(self, *args, **kwargs):\n        \"\"\"Run the experiment in a background thread.\n\n        This is important in order to keep the GUI responsive.\n        \"\"\"\n        self.log_messages = \"\"\n        self._stop_event.clear()\n        self._finished_event.clear()\n        self.run(*args, **kwargs)\n        self._finished_event.set()\n\n    def start(self, *args, **kwargs):\n        \"\"\"Start the experiment running in a background thread.  See run_in_background.\"\"\"\n        assert self.running == False, \"Can't start the experiment when it is already running!\"\n        self.prepare_to_run(*args, **kwargs)\n        self._experiment_thread = self.run_in_background(*args, **kwargs)\n\n    def stop(self, join=False):\n        \"\"\"Stop the experiment running, if supported.  May take a little while.\"\"\"\n        self._stop_event.set()\n        if join:\n            try:\n                self._experiment_thread.join()\n            except ExperimentStopped as e:\n                if not self.experiment_can_be_safely_aborted:\n                    raise e\n\n    @property\n    def running(self):\n        \"\"\"Whether the experiment is currently running in the background.\"\"\"\n        return background_actions_running(self)\n\n    def log(self, message):\n        \"\"\"Log a message to the current HDF5 file and to the experiment's history\"\"\"\n        self.log_messages += message + \"\\n\"\n        if self.log_to_console:\n            print(message)\n        super(Experiment, self).log(message)\n\n# --- Snippet Separator ---\n\nclass QlibRecorder:\n    \"\"\"\n    A global system that helps to manage the experiments.\n    \"\"\"\n\n    def __init__(self, exp_manager):\n        self.exp_manager = exp_manager\n\n    def __repr__(self):\n        return \"{name}(manager={manager})\".format(name=self.__class__.__name__, manager=self.exp_manager)\n\n    @contextmanager\n    def start(\n        self,\n        experiment_name: Optional[Text] = None,\n        recorder_name: Optional[Text] = None,\n        uri: Optional[Text] = None,\n        resume: bool = False,\n    ):\n        \"\"\"\n        Method to start an experiment. This method can only be called within a Python's `with` statement. Here is the example code:\n\n        .. code-block:: Python\n\n            # start new experiment and recorder\n            with R.start('test', 'recorder_1'):\n                model.fit(dataset)\n                R.log...\n                ... # further operations\n\n            # resume previous experiment and recorder\n            with R.start('test', 'recorder_1', resume=True): # if users want to resume recorder, they have to specify the exact same name for experiment and recorder.\n                ... # further operations\n\n        Parameters\n        ----------\n        experiment_name : str\n            name of the experiment one wants to start.\n        recorder_name : str\n            name of the recorder under the experiment one wants to start.\n        uri : str\n            The tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.\n            The default uri is set in the qlib.config. Note that this uri argument will not change the one defined in the config file.\n            Therefore, the next time when users call this function in the same experiment,\n            they have to also specify this argument with the same value. Otherwise, inconsistent uri may occur.\n        resume : bool\n            whether to resume the specific recorder with given name under the given experiment.\n        \"\"\"\n        run = self.start_exp(experiment_name, recorder_name, uri, resume)\n        try:\n            yield run\n        except Exception as e:\n            self.end_exp(Recorder.STATUS_FA)  # end the experiment if something went wrong\n            raise e\n        self.end_exp(Recorder.STATUS_FI)\n\n    def start_exp(self, experiment_name=None, recorder_name=None, uri=None, resume=False):\n        \"\"\"\n        Lower level method for starting an experiment. When use this method, one should end the experiment manually\n        and the status of the recorder may not be handled properly. Here is the example code:\n\n        .. code-block:: Python\n\n            R.start_exp(experiment_name='test', recorder_name='recorder_1')\n            ... # further operations\n            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)\n\n\n        Parameters\n        ----------\n        experiment_name : str\n            the name of the experiment to be started\n        recorder_name : str\n            name of the recorder under the experiment one wants to start.\n        uri : str\n            the tracking uri of the experiment, where all the artifacts/metrics etc. will be stored.\n            The default uri are set in the qlib.config.\n        resume : bool\n            whether to resume the specific recorder with given name under the given experiment.\n\n        Returns\n        -------\n        An experiment instance being started.\n        \"\"\"\n        return self.exp_manager.start_exp(experiment_name, recorder_name, uri, resume)\n\n    def end_exp(self, recorder_status=Recorder.STATUS_FI):\n        \"\"\"\n        Method for ending an experiment manually. It will end the current active experiment, as well as its\n        active recorder with the specified `status` type. Here is the example code of the method:\n\n        .. code-block:: Python\n\n            R.start_exp(experiment_name='test')\n            ... # further operations\n            R.end_exp('FINISHED') or R.end_exp(Recorder.STATUS_S)\n\n        Parameters\n        ----------\n        status : str\n            The status of a recorder, which can be SCHEDULED, RUNNING, FINISHED, FAILED.\n        \"\"\"\n        self.exp_manager.end_exp(recorder_status)\n\n    def search_records(self, experiment_ids, **kwargs):\n        \"\"\"\n        Get a pandas DataFrame of records that fit the search criteria.\n\n        The arguments of this function are not set to be rigid, and they will be different with different implementation of\n        ``ExpManager`` in ``Qlib``. ``Qlib`` now provides an implementation of ``ExpManager`` with mlflow, and here is the\n        example code of the this method with the ``MLflowExpManager``:\n\n        .. code-block:: Python\n\n            R.log_metrics(m=2.50, step=0)\n            records = R.search_runs([experiment_id], order_by=[\"metrics.m DESC\"])\n\n        Parameters\n        ----------\n        experiment_ids : list\n            list of experiment IDs.\n        filter_string : str\n            filter query string, defaults to searching all runs.\n        run_view_type : int\n            one of enum values ACTIVE_ONLY, DELETED_ONLY, or ALL (e.g. in mlflow.entities.ViewType).\n        max_results  : int\n            the maximum number of runs to put in the dataframe.\n        order_by : list\n            list of columns to order by (e.g., “metrics.rmse”).\n\n        Returns\n        -------\n        A pandas.DataFrame of records, where each metric, parameter, and tag\n        are expanded into their own columns named metrics.*, params.*, and tags.*\n        respectively. For records that don't have a particular metric, parameter, or tag, their\n        value will be (NumPy) Nan, None, or None respectively.\n        \"\"\"\n        return self.exp_manager.search_records(experiment_ids, **kwargs)\n\n    def list_experiments(self):\n        \"\"\"\n        Method for listing all the existing experiments (except for those being deleted.)\n\n        .. code-block:: Python\n\n            exps = R.list_experiments()\n\n        Returns\n        -------\n        A dictionary (name -> experiment) of experiments information that being stored.\n        \"\"\"\n        return self.exp_manager.list_experiments()\n\n    def list_recorders(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for listing all the recorders of experiment with given id or name.\n\n        If user doesn't provide the id or name of the experiment, this method will try to retrieve the default experiment and\n        list all the recorders of the default experiment. If the default experiment doesn't exist, the method will first\n        create the default experiment, and then create a new recorder under it. (More information about the default experiment\n        can be found `here <../component/recorder.html#qlib.workflow.exp.Experiment>`_).\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            recorders = R.list_recorders(experiment_name='test')\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n\n        Returns\n        -------\n        A dictionary (id -> recorder) of recorder information that being stored.\n        \"\"\"\n        return self.get_exp(experiment_id, experiment_name).list_recorders()\n\n    def get_exp(self, experiment_id=None, experiment_name=None, create: bool = True) -> Experiment:\n        \"\"\"\n        Method for retrieving an experiment with given id or name. Once the `create` argument is set to\n        True, if no valid experiment is found, this method will create one for you. Otherwise, it will\n        only retrieve a specific experiment or raise an Error.\n\n        - If '`create`' is True:\n\n            - If `active experiment` exists:\n\n                - no id or name specified, return the active experiment.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given id or name, and the experiment is set to be active.\n\n            - If `active experiment` not exists:\n\n                - no id or name specified, create a default experiment, and the experiment is set to be active.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, create a new experiment with given name or the default experiment, and the experiment is set to be active.\n\n        - Else If '`create`' is False:\n\n            - If ``active experiment` exists:\n\n                - no id or name specified, return the active experiment.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n            - If `active experiment` not exists:\n\n                - no id or name specified. If the default experiment exists, return it, otherwise, raise Error.\n\n                - if id or name is specified, return the specified experiment. If no such exp found, raise Error.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                exp = R.get_exp()\n                recorders = exp.list_recorders()\n\n            # Case 2\n            with R.start('test'):\n                exp = R.get_exp('test1')\n\n            # Case 3\n            exp = R.get_exp() -> a default experiment.\n\n            # Case 4\n            exp = R.get_exp(experiment_name='test')\n\n            # Case 5\n            exp = R.get_exp(create=False) -> the default experiment if exists.\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n        create : boolean\n            an argument determines whether the method will automatically create a new experiment\n            according to user's specification if the experiment hasn't been created before.\n\n        Returns\n        -------\n        An experiment instance with given id or name.\n        \"\"\"\n        return self.exp_manager.get_exp(experiment_id, experiment_name, create)\n\n    def delete_exp(self, experiment_id=None, experiment_name=None):\n        \"\"\"\n        Method for deleting the experiment with given id or name. At least one of id or name must be given,\n        otherwise, error will occur.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            R.delete_exp(experiment_name='test')\n\n        Parameters\n        ----------\n        experiment_id : str\n            id of the experiment.\n        experiment_name : str\n            name of the experiment.\n        \"\"\"\n        self.exp_manager.delete_exp(experiment_id, experiment_name)\n\n    def get_uri(self):\n        \"\"\"\n        Method for retrieving the uri of current experiment manager.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            uri = R.get_uri()\n\n        Returns\n        -------\n        The uri of current experiment manager.\n        \"\"\"\n        return self.exp_manager.uri\n\n    def set_uri(self, uri: Optional[Text]):\n        \"\"\"\n        Method to reset the current uri of current experiment manager.\n        \"\"\"\n        self.exp_manager.set_uri(uri)\n\n    def get_recorder(self, recorder_id=None, recorder_name=None, experiment_name=None):\n        \"\"\"\n        Method for retrieving a recorder.\n\n        - If `active recorder` exists:\n\n            - no id or name specified, return the active recorder.\n\n            - if id or name is specified, return the specified recorder.\n\n        - If `active recorder` not exists:\n\n            - no id or name specified, raise Error.\n\n            - if id or name is specified, and the corresponding experiment_name must be given, return the specified recorder. Otherwise, raise Error.\n\n        The recorder can be used for further process such as `save_object`, `load_object`, `log_params`,\n        `log_metrics`, etc.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                recorder = R.get_recorder()\n\n            # Case 2\n            with R.start('test'):\n                recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')\n\n            # Case 3\n            recorder = R.get_recorder() -> Error\n\n            # Case 4\n            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d') -> Error\n\n            # Case 5\n            recorder = R.get_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d', experiment_name='test')\n\n        Parameters\n        ----------\n        recorder_id : str\n            id of the recorder.\n        recorder_name : str\n            name of the recorder.\n        experiment_name : str\n            name of the experiment.\n\n        Returns\n        -------\n        A recorder instance.\n        \"\"\"\n        return self.get_exp(experiment_name=experiment_name, create=False).get_recorder(\n            recorder_id, recorder_name, create=False\n        )\n\n    def delete_recorder(self, recorder_id=None, recorder_name=None):\n        \"\"\"\n        Method for deleting the recorders with given id or name. At least one of id or name must be given,\n        otherwise, error will occur.\n\n        Here is the example code:\n\n        .. code-block:: Python\n\n            R.delete_recorder(recorder_id='2e7a4efd66574fa49039e00ffaefa99d')\n\n        Parameters\n        ----------\n        recorder_id : str\n            id of the experiment.\n        recorder_name : str\n            name of the experiment.\n        \"\"\"\n        self.get_exp().delete_recorder(recorder_id, recorder_name)\n\n    def save_objects(self, local_path=None, artifact_path=None, **kwargs):\n        \"\"\"\n        Method for saving objects as artifacts in the experiment to the uri. It supports either saving\n        from a local file/directory, or directly saving objects. User can use valid python's keywords arguments\n        to specify the object to be saved as well as its name (name: value).\n\n        - If `active recorder` exists: it will save the objects through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment, and a new recorder and save objects under it.\n\n        .. note::\n\n            If one wants to save objects with a specific recorder. It is recommended to first get the specific recorder through `get_recorder` API and use the recorder the save objects. The supported arguments are the same as this method.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                pred = model.predict(dataset)\n                R.save_objects(**{\"pred.pkl\": pred}, artifact_path='prediction')\n\n            # Case 2\n            with R.start('test'):\n                R.save_objects(local_path='results/pred.pkl')\n\n        Parameters\n        ----------\n        local_path : str\n            if provided, them save the file or directory to the artifact URI.\n        artifact_path : str\n            the relative path for the artifact to be stored in the URI.\n        \"\"\"\n        self.get_exp().get_recorder().save_objects(local_path, artifact_path, **kwargs)\n\n    def log_params(self, **kwargs):\n        \"\"\"\n        Method for logging parameters during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will log parameters through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log parameters under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.log_params(learning_rate=0.01)\n\n            # Case 2\n            R.log_params(learning_rate=0.01)\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().log_params(**kwargs)\n\n    def log_metrics(self, step=None, **kwargs):\n        \"\"\"\n        Method for logging metrics during an experiment. In addition to using ``R``, one can also log to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will log metrics through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and log metrics under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.log_metrics(train_loss=0.33, step=1)\n\n            # Case 2\n            R.log_metrics(train_loss=0.33, step=1)\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().log_metrics(step, **kwargs)\n\n    def set_tags(self, **kwargs):\n        \"\"\"\n        Method for setting tags for a recorder. In addition to using ``R``, one can also set the tag to a specific recorder after getting it with `get_recorder` API.\n\n        - If `active recorder` exists: it will set tags through the active recorder.\n        - If `active recorder` not exists: the system will create a default experiment as well as a new recorder, and set the tags under it.\n\n        Here are some use cases:\n\n        .. code-block:: Python\n\n            # Case 1\n            with R.start('test'):\n                R.set_tags(release_version=\"2.2.0\")\n\n            # Case 2\n            R.set_tags(release_version=\"2.2.0\")\n\n        Parameters\n        ----------\n        keyword argument:\n            name1=value1, name2=value2, ...\n        \"\"\"\n        self.get_exp().get_recorder().set_tags(**kwargs)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the expyfun library to prepare and run an experiment using the CRM corpus. The experiment should prepare two talkers with different genders and the same talker number at a sampling rate of 40000 Hz. It should then print the valid callsigns and read a sentence from the hard drive. The code should also preload all the talkers and get a second sentence from memory. The two sentences should be padded and aligned at the start. The experiment should be run with a specific name, window size, participant, session, and version. It should display a text prompt on the screen, load the padded sentences into the buffer, start the stimulus, and wait for a specific duration. The code should also include a response menu and display a prompt based on the response. Finally, it should plot a screenshot of the experiment.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 86, "repo_full_name": "pyqtgraph__pyqtgraph", "instruction": "Generate code that creates a scatter plot using the pyqtgraph library. The scatter plot should demonstrate a variety of features. The code should create a main window and a graphics layout widget. Four plots should be added to the widget, each demonstrating a different way of drawing scatter plots. \n\nThe first plot should have all spots identical and transform-invariant. The second plot should have spots that are transform-invariant, but not identical. The third plot should have spots that are not transform-invariant and not identical. The fourth plot should test the performance of large scatterplots. \n\nAll plots should be clickable, and the clicked points should be highlighted. The code should also generate random data for the plots. The application should be executed if the script is run as the main program.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))\n\n# --- Snippet Separator ---\n\ndef clear(self, plot_type=None):\n        \"\"\"Remove all plots of the specified type from this chart.\n\n        Parameters\n        ----------\n        plot_type : str, optional\n            The type of the plots to remove. Allowed types are\n            ``\"scatter\"``, ``\"line\"``, ``\"area\"``, ``\"bar\"``\n            and ``\"stack\"``.\n\n            If no type is provided (``None``), all plots are removed,\n            regardless of their type.\n\n        Examples\n        --------\n        Create a 2D chart with multiple line and scatter plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.plot([0, 1, 2], [2, 1, 3], \"o-b\")\n        >>> _ = chart.plot([-2, -1, 0], [3, 1, 2], \"d-r\")\n        >>> chart.show()\n\n        Remove all scatter plots from the chart.\n\n        >>> chart.clear(\"scatter\")\n        >>> chart.show()\n\n        \"\"\"\n        plot_types = self.PLOT_TYPES.keys() if plot_type is None else [plot_type]\n        for plot_type in plot_types:\n            # Make a copy, as this list will be modified by remove_plot\n            plots = [*self._plots[plot_type]]\n            for plot in plots:\n                self.remove_plot(plot)\n\n# --- Snippet Separator ---\n\nclass StationPlotLayout(dict):\n    r\"\"\"Make a layout to encapsulate plotting using `StationPlot`.\n\n    This class keeps a collection of offsets, plot formats, etc. for a parameter based\n    on its name. This then allows a dictionary of data (or any object that allows looking\n    up of arrays based on a name) to be passed to `plot()` to plot the data all at once.\n\n    See Also\n    --------\n    StationPlot\n\n    \"\"\"\n\n    class PlotTypes(Enum):\n        r\"\"\"Different plotting types for the layout.\n\n        Controls how items are displayed (e.g. converting values to symbols).\n        \"\"\"\n\n        value = 1\n        symbol = 2\n        text = 3\n        barb = 4\n\n    def add_value(self, location, name, fmt='.0f', units=None, **kwargs):\n        r\"\"\"Add a numeric value to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. The conversion of the data values to\n        a string is controlled by `fmt`. The units required for plotting can also\n        be passed in using `units`, which will cause the data to be converted before\n        plotting.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        fmt : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_text\n\n        \"\"\"\n        self[location] = (self.PlotTypes.value, name, (fmt, units, kwargs))\n\n    def add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))\n\n    def add_text(self, location, name, **kwargs):\n        r\"\"\"Add a text field to the  station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted directly as text with no conversion\n        applied.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple(float, float)\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.text, name, kwargs)\n\n    def add_barb(self, u_name, v_name, units=None, **kwargs):\n        r\"\"\"Add a wind barb to the center of the station layout.\n\n        This specifies that u- and v-component data should be pulled from the data\n        container using the keys `u_name` and `v_name`, respectively, and plotted as\n        a wind barb at the center of the station plot. If `units` are given, both\n        components will be converted to these units.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or line width.\n\n        Parameters\n        ----------\n        u_name : str\n            The name of the parameter for the u-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        v_name : str\n            The name of the parameter for the v-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's\n            :meth:`~matplotlib.axes.Axes.barbs` function.\n\n        See Also\n        --------\n        add_symbol, add_text, add_value\n\n        \"\"\"\n        # Not sure if putting the v_name as a plot-specific option is appropriate,\n        # but it seems simpler than making name code in plot handle tuples\n        self['barb'] = (self.PlotTypes.barb, (u_name, v_name), (units, kwargs))\n\n    def names(self):\n        \"\"\"Get the list of names used by the layout.\n\n        Returns\n        -------\n        list[str]\n            the list of names of variables used by the layout\n\n        \"\"\"\n        ret = []\n        for item in self.values():\n            if item[0] == self.PlotTypes.barb:\n                ret.extend(item[1])\n            else:\n                ret.append(item[1])\n        return ret\n\n    def plot(self, plotter, data_dict):\n        \"\"\"Plot a collection of data using this layout for a station plot.\n\n        This function iterates through the entire specified layout, pulling the fields named\n        in the layout from `data_dict` and plotting them using `plotter` as specified\n        in the layout. Fields present in the layout, but not in `data_dict`, are ignored.\n\n        Parameters\n        ----------\n        plotter : StationPlot\n            :class:`StationPlot` to use to plot the data. This controls the axes,\n            spacing, station locations, etc.\n        data_dict : dict[str, array-like]\n            Data container that maps a name to an array of data. Data from this object\n            will be used to fill out the station plot.\n\n        \"\"\"\n        def coerce_data(dat, u):\n            try:\n                return dat.to(u).magnitude\n            except AttributeError:\n                return dat\n\n        for loc, info in self.items():\n            typ, name, args = info\n            if typ == self.PlotTypes.barb:\n                # Try getting the data\n                u_name, v_name = name\n                u_data = data_dict.get(u_name)\n                v_data = data_dict.get(v_name)\n\n                # Plot if we have the data\n                if not (v_data is None or u_data is None):\n                    units, kwargs = args\n                    plotter.plot_barb(coerce_data(u_data, units), coerce_data(v_data, units),\n                                      **kwargs)\n            else:\n                # Check that we have the data for this location\n                data = data_dict.get(name)\n                if data is not None:\n                    # If we have it, hand it to the appropriate method\n                    if typ == self.PlotTypes.value:\n                        fmt, units, kwargs = args\n                        plotter.plot_parameter(loc, coerce_data(data, units), fmt, **kwargs)\n                    elif typ == self.PlotTypes.symbol:\n                        mapper, kwargs = args\n                        plotter.plot_symbol(loc, data, mapper, **kwargs)\n                    elif typ == self.PlotTypes.text:\n                        plotter.plot_text(loc, data, **args)\n\n    def __repr__(self):\n        \"\"\"Return string representation of layout.\"\"\"\n        return ('{'\n                + ', '.join(f'{loc}: ({info[0].name}, {info[1]}, ...)'\n                            for loc, info in sorted(self.items()))\n                + '}')\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a scatter plot using the pyqtgraph library. The scatter plot should demonstrate a variety of features. The code should create a main window and a graphics layout widget. Four plots should be added to the widget, each demonstrating a different way of drawing scatter plots. \n\nThe first plot should have all spots identical and transform-invariant. The second plot should have spots that are transform-invariant, but not identical. The third plot should have spots that are not transform-invariant and not identical. The fourth plot should test the performance of large scatterplots. \n\nAll plots should be clickable, and the clicked points should be highlighted. The code should also generate random data for the plots. The application should be executed if the script is run as the main program.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 87, "repo_full_name": "ansys__pymapdl", "instruction": "Generate code that uses the pymapdl library to create contact elements for general contact. The code should first launch MAPDL, enter the pre-processor, create a block and mesh it with tetrahedral elements. Then, it should create a second volume block above the existing one and mesh it with quadratic hexahedral elements, ensuring that the blocks do not touch. The code should then select all the elements at the intersection between the two blocks and generate contact elements. Finally, it should plot the contact element pairs as a wire-frame to show that the contact pairs overlap, and then stop MAPDL.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def Keyopt(self, itype=\"\", knum=\"\", value=\"\", **kwargs):\n        \"\"\"\n        APDL Command: KEYOPT\n\n        Sets element key options.\n\n        Parameters\n        ----------\n        itype\n            Element type number as defined on the ET command. The label GCN is\n            also valid input for general contact elements (see Notes).\n\n        knum\n            Number of the KEYOPT to be defined (KEYOPT(KNUM)).\n\n        value\n            Value of this KEYOPT.\n\n        Notes\n        -----\n        Alternative to inputting KEYOPT values on ET command.  Must be used if\n        KEYOPT(7) or greater values are to be input. ITYPE must first be\n        defined with the ET command.\n\n        Specify ITYPE = GCN to set element key options for all contact elements\n        types used in any existing general contact definitions (that is,\n        contact elements having a real constant set number = 0).\n\n        \"\"\"\n        command = \"KEYOPT,%s,%s,%s\" % (str(itype), str(knum), str(value))\n        return self.Run(command, **kwargs)\n\n# --- Snippet Separator ---\n\ndef Edpc(self, min=\"\", max=\"\", inc=\"\", **kwargs):\n        \"\"\"\n        APDL Command: EDPC\n\n        Selects and plots explicit dynamic contact entities.\n\n        Parameters\n        ----------\n        min\n             Minimum contact entity number to be selected and plotted (default\n            = 1).\n\n        max\n            Maximum contact entity number to be selected and plotted (default =\n            MIN).\n\n        inc\n            Contact entity number increment (default = 1).\n\n        Notes\n        -----\n        EDPC invokes an ANSYS macro which selects and plots explicit dynamic\n        contact entities. The plot will consist of nodes or elements, depending\n        on the method (node components or parts) that was used to define the\n        contact surfaces (see the EDCGEN command). For single surface contact\n        definitions, all external surfaces within the model are plotted.\n\n        Note:: : EDPC changes the selected set of nodes and elements. After\n        plotting contact entities, you must reselect all nodes and elements\n        (NSEL and ESEL) required for subsequent operations, such as SOLVE\n\n        Use the EDCLIST command to list the contact entity numbers for all\n        defined contact.\n\n        This command is also valid in SOLUTION.\n\n        Distributed ANSYS Restriction: This command is not supported in\n        Distributed ANSYS.\n\n        \"\"\"\n        command = \"EDPC,%s,%s,%s\" % (str(min), str(max), str(inc))\n        return self.Run(command, **kwargs)\n\n# --- Snippet Separator ---\n\ndef Nladaptive(self, component=\"\", action=\"\", criterion=\"\", option=\"\",\n                   val1=\"\", val2=\"\", val3=\"\", **kwargs):\n        \"\"\"\n        APDL Command: NLADAPTIVE\n\n        Defines the criteria under which the mesh is refined or modified during\n        a nonlinear solution.\n\n        Parameters\n        ----------\n        component\n            Specifies the element component upon which this command should act:\n\n            ALL  - All selected components, or all selected elements if no component is selected\n                   (default).\n\n            Name - Component name.\n\n        action\n            The action to perform on the selected component(s):\n\n            ADD  - Add a criterion to the database.\n\n            LIST  - List the criteria defined for the specified component(s).\n\n            DELETE  - Delete the criteria defined for the specified component(s).\n\n            ON - Enable the defined criteria for the specified component(s) and specify how\n                 frequently and when to check them (via ON,,,VAL1,VAL2,VAL3):\n\n            VAL1 -- Checking frequency. If > 0, check criteria at every VAL1 substeps. If < 0, check criteria at each of the VAL1 points (approximately equally spaced) between VAL2 and VAL3. (Default = -1.) - VAL2 -- Checking start time, where VAL2 < VAL3. (Default is the start time of\n                              the load step.)\n\n            VAL3 -- Checking end time, where VAL3 > VAL2. (Default is the end time of the load step.)  - OFF\n\n        criterion\n            The type of criterion to apply to the selected component(s):\n\n            CONTACT  - Contact-based. (Valid only for Action = ADD, Action = LIST, or Action =\n                       DELETE.)\n\n            ENERGY  - Energy-based. (Valid only for Action = ADD, Action = LIST, or Action = DELETE.)\n\n            BOX  - A position-based criterion, defined by a box. (Valid only for Action = ADD,\n                   Action = LIST, or Action = DELETE.)\n\n            MESH  - A mesh-quality-based criterion. (Valid only for Action = LIST, or Action =\n                    DELETE.)\n\n            ALL  - All criteria and options. (Valid only for Action = LIST or Action = DELETE.\n                   Option and all subsequent arguments are ignored.)\n\n        option\n            Criterion option to apply to the selected component(s):\n\n            NUMELEM  - For target elements only, define the minimum number of contact elements to\n                       contact with each target element. If this criterion is\n                       not satisfied, the program refines the contact elements\n                       and the associated solid elements. For this option, VAL1\n                       must be a positive integer. (Valid only for Action =\n                       ADD, Action = LIST, or Action = DELETE. )\n\n            MEAN  - Check the strain energy of any element that is part of the defined component\n                    for the condition Ee ≥ c1 * Etotal / NUME (where c1 = VAL1,\n                    Etotal is the total strain energy of the component, and\n                    NUME is the number of elements of the component). If this\n                    criterion is satisfied at an element, the program refines\n                    the element. For this option, VAL1 must be non-negative and\n                    defaults to 1. (Valid only for Action = ADD, Action = LIST,\n                    or Action = DELETE.)\n\n            XYZRANGE  - Define the location box in which all elements within are to be split or\n                        refined. Up to six values following the Option argument\n                        (representing the x1, x2, y1, y2, z1, and z2\n                        coordinates) are allowed. An unspecified coordinate is\n                        not checked. (Valid only for Action = ADD, Action =\n                        LIST, or Action = DELETE.)\n\n            SKEWNESS - Mesh-quality control threshold for element SOLID285. Valid values (VAL1) are\n                       0.0 through 1.0. Default = 0.9. (Valid only for Action =\n                       ADD, Action = LIST, or Action = DELETE.)\n\n            WEAR - This option is valid only for contact elements having surface wear specified\n                   (TB,WEAR). Define VAL1 as a critical ratio of magnitude of\n                   wear to the average depth of the solid element underlying\n                   the contact element. Once this critical ratio is reached for\n                   any element, the program morphs the mesh to improve the\n                   quality of the elements. VAL1 must be a positive integer.\n                   (Valid only for Action = ADD, Action = LIST, or Action =\n                   DELETE.) The WEAR criterion cannot be combined with any\n                   other criterion.\n\n            ALL  - All options. (Valid only for Action = LIST or Action = DELETE. All subsequent\n                   arguments are ignored.)\n\n        Notes\n        -----\n        If a specified component (Component) is an assembly, the defined\n        criterion applies to all element components included in the assembly.\n\n        All components must be defined and selected before the first solve\n        (SOLVE), although their nonlinear adaptivity criteria can be modified\n        from load step to load step, and upon restart. For nonlinear adaptivity\n        to work properly, ensure that all components are selected before each\n        solve.\n\n        After using this command to define a new criterion, the new criterion\n        becomes active and is checked one time during each load step, roughly\n        in mid-loading (unless this behavior is changed via Action = ON).\n\n        When a criterion is defined, it overwrites a previously defined\n        criterion (if one exists) through the same component, or through the\n        component assembly that includes the specified component.\n\n        During solution, the same criteria defined for an element through\n        different components are combined, and the tightest criteria and action\n        control (Action,ON,,,VAL1) are used. If an ON action is defined by a\n        positive VAL1 value through one component and a negative VAL1 value\n        through another, the program uses the positive value.\n\n        For Action = ON, if VAL2 (start time) and/or VAL3 (end time) are\n        unspecified or invalid, the program uses the start and/or end time\n        (respectively) of the load step. If VAL1 < 0, the program checks VAL1\n        points between VAL2 and VAL3. The time interval between each check\n        points is determined by (VAL3 - VAL2) / (VAL1 + 1), with the first\n        check point as close to VAL2 + (VAL3 - VAL2) / (VAL1 + 1) as possible.\n        Fewer check points can be used if the number of substeps during\n        solution is insufficient (as the program can only check at the end of a\n        substep).\n\n        Option = SKEWNESS applies to linear tetrahedral element SOLID285 only.\n        When the skewness of a SOLID285 element is >= the defined value, the\n        element is used as the core (seed) element of the remeshed region(s).\n        If this criterion is used together with any other criteria for the same\n        component, the other criteria defined for the component are ignored.\n        The most desirable skewness value is 0, applicable when the element is\n        a standard tetrahedral element; the highest value is 1, applicable when\n        the element becomes flat with zero volume. For more information about\n        skewness and remeshing, see Mesh Nonlinear Adaptivity in the Advanced\n        Analysis Guide.\n\n        For more granular control of the source mesh geometry, see NLMESH.\n\n        \"\"\"\n        command = \"NLADAPTIVE,%s,%s,%s,%s,%s,%s,%s\" % (str(component), str(action), str(criterion), str(option), str(val1), str(val2), str(val3))\n        return self.Run(command, **kwargs)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the pymapdl library to create contact elements for general contact. The code should first launch MAPDL, enter the pre-processor, create a block and mesh it with tetrahedral elements. Then, it should create a second volume block above the existing one and mesh it with quadratic hexahedral elements, ensuring that the blocks do not touch. The code should then select all the elements at the intersection between the two blocks and generate contact elements. Finally, it should plot the contact element pairs as a wire-frame to show that the contact pairs overlap, and then stop MAPDL.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 88, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulator base with 10 Stub AS and 3 hosts per stub AS using the seedemu library. Then, create an Ethereum service layer and a sub-layer of it, a blockchain with the name \"pos\" and consensus mechanism set to POS. Set the terminal total difficulty of the blockchain to 30. \n\nFor each host in the AS, create a blockchain virtual node, a Docker container label, and enable Geth to communicate with the geth node via http. Set specific hosts as BeaconSetupNode, BootNode, and validator nodes with different conditions. Also, customize the display names of the nodes and bind the virtual node to the physical node. \n\nFinally, add the Ethereum layer to the emulator, render it, and compile it with Docker with internetMap and etherView enabled. The output should be saved in the './output' directory and existing files should be overridden.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Emulator:\n    \"\"\"!\n    @brief The Emulator class.\n\n    Emulator class is the entry point for emulations. \n    \"\"\"\n\n    __registry: Registry\n    __layers: LayerDatabase\n    __dependencies_db: Dict[str, Set[Tuple[str, bool]]]\n    __rendered: bool\n    __bindings: BindingDatabase\n    __resolved_bindings: Dict[str, core.Node]\n\n    __service_net: Network\n    __service_net_prefix: str\n\n    def __init__(self, serviceNetworkPrefix: str = '192.168.66.0/24'):\n        \"\"\"!\n        @brief Construct a new emulation.\n\n        @param serviceNetworkPrefix (optional) service network prefix for this\n        emulator. A service network is a network that does not take part in the\n        emulation, and provide access between the emulation nodes and the host\n        node. Service network will not be created unless some layer/service/as\n        asks for it.\n        \"\"\"\n        self.__rendered = False\n        self.__dependencies_db = {}\n        self.__resolved_bindings = {}\n        self.__registry = Registry()\n        self.__layers = LayerDatabase()\n        self.__bindings = BindingDatabase()\n\n        self.__registry.register('seedemu', 'dict', 'layersdb', self.__layers)\n        self.__registry.register('seedemu', 'list', 'bindingdb', self.__bindings)\n\n        self.__service_net_prefix = '192.168.160.0/23'\n        self.__service_net = None\n\n    def __render(self, layerName, optional: bool, configure: bool):\n        \"\"\"!\n        @brief Render a layer.\n\n        @param layerName name of layer.\n        @throws AssertionError if dependencies unmet \n        \"\"\"\n        verb = 'configure' if configure else 'render'\n\n        self.__log('requesting {}: {}'.format(verb, layerName))\n\n        if optional and layerName not in self.__layers.db:\n            self.__log('{}: not found but is optional, skipping'.format(layerName))\n            return\n\n        assert layerName in self.__layers.db, 'Layer {} required but missing'.format(layerName)\n\n        (layer, done) = self.__layers.db[layerName]\n        if done:\n            self.__log('{}: already done, skipping'.format(layerName))\n            return\n\n        if layerName in self.__dependencies_db:\n            for (dep, opt) in self.__dependencies_db[layerName]:\n                self.__log('{}: requesting dependency render: {}'.format(layerName, dep))\n                self.__render(dep, opt, configure)\n\n        self.__log('entering {}...'.format(layerName))\n\n        hooks: List[core.Hook] = []\n        for hook in self.__registry.getByType('seedemu', 'hook'):\n            if hook.getTargetLayer() == layerName: hooks.append(hook)\n\n        if configure:\n            self.__log('invoking pre-configure hooks for {}...'.format(layerName))\n            for hook in hooks: hook.preconfigure(self)\n            self.__log('configuring {}...'.format(layerName))\n            layer.configure(self)\n            self.__log('invoking post-configure hooks for {}...'.format(layerName))\n            for hook in hooks: hook.postconfigure(self)\n        else:\n            self.__log('invoking pre-render hooks for {}...'.format(layerName))\n            for hook in hooks: hook.prerender(self)\n            self.__log('rendering {}...'.format(layerName))\n            layer.render(self)\n            self.__log('invoking post-render hooks for {}...'.format(layerName))\n            for hook in hooks: hook.postrender(self)\n\n        self.__log('done: {}'.format(layerName))\n        self.__layers.db[layerName] = (layer, True)\n\n    def __loadDependencies(self, deps: Dict[str, Set[Tuple[str, bool]]]):\n        \"\"\"!\n        @brief Load dependencies list.\n\n        @param deps dependencies list.\n        \"\"\"\n        for (layer, deps) in deps.items():\n            if not layer in self.__dependencies_db:\n                self.__dependencies_db[layer] = deps\n                continue\n\n            self.__dependencies_db[layer] |= deps\n\n    def __log(self, message: str):\n        \"\"\"!\n        @brief log to stderr.\n\n        @param message message.\n        \"\"\"\n        print('== Emulator: {}'.format(message), file=stderr)\n\n    def rendered(self) -> bool:\n        \"\"\"!\n        @brief test if the emulator is rendered.\n\n        @returns True if rendered\n        \"\"\"\n        return self.__rendered\n\n    def addHook(self, hook: core.Hook) -> Emulator:\n        \"\"\"!\n        @brief Add a hook.\n\n        @param hook Hook.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self.__registry.register('seedemu', 'hook', hook.getName(), hook)\n\n        return self\n\n    def addBinding(self, binding: core.Binding) -> Emulator:\n        \"\"\"!\n        @brief Add a binding.\n\n        @param binding binding.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self.__bindings.db.append(binding)\n\n        return self\n\n    def getBindings(self) -> List[core.Binding]:\n        \"\"\"!\n        @brief Get all bindings.\n\n        @returns list of bindings.\n        \"\"\"\n        return self.__bindings.db\n\n    def addLayer(self, layer: core.Layer) -> Emulator:\n        \"\"\"!\n        @brief Add a layer.\n\n        @param layer layer to add.\n        @throws AssertionError if layer already exist.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n\n        lname = layer.getName()\n        assert lname not in self.__layers.db, 'layer {} already added.'.format(lname)\n        self.__registry.register('seedemu', 'layer', lname, layer)\n        self.__layers.db[lname] = (layer, False)\n\n        return self\n\n    def getLayer(self, layerName: str) -> core.Layer:\n        \"\"\"!\n        @brief Get a layer.\n\n        @param layerName of the layer.\n        @returns layer.\n        \"\"\"\n        return self.__registry.get('seedemu', 'layer', layerName)\n\n    def getLayers(self) -> List[core.Layer]:\n        \"\"\"!\n        @brief Get all layers.\n\n        @returns list of layers.\n        \"\"\"\n        return self.__registry.getByType('seedemu', 'layer')\n\n    def getServerByVirtualNodeName(self, vnodeName: str) -> core.Server:\n        \"\"\"!\n        @brief Get server by virtual node name. \n\n        Note that vnodeName is created and mapped with server in service layer.\n\n        @param vnodeName name of vnode.\n        @returns server.\n        \"\"\"\n        for (layer, _) in self.__layers.db.values():\n            if not isinstance(layer, core.Service): continue\n            for (vnode, server) in layer.getPendingTargets().items():\n                if vnode == vnodeName:\n                    return server\n        return None\n\n    def resolvVnode(self, vnode: str) -> core.Node:\n        \"\"\"!\n        @brief resolve physical node for the given virtual node.\n\n        @param vnode virtual node name.\n\n        @returns physical node.\n        \"\"\"\n        if vnode in self.__resolved_bindings: return self.__resolved_bindings[vnode]\n        for binding in self.getBindings():\n            pnode = binding.getCandidate(vnode, self, True)\n            if pnode == None: continue\n            return pnode\n        assert False, 'cannot resolve vnode {}'.format(vnode)\n\n    def getBindingFor(self, vnode: str) -> core.Node:\n        \"\"\"!\n        @brief get physical node for the given virtual node from the\n        pre-populated vnode-pnode mappings.\n\n        Note that the bindings are processed in the early render stage, meaning\n        calls to this function will always fail before render, and only virtual\n        node names that have been used in service will be available to be\n        \"resolve\" to the physical node using this function.\n\n        This is meant to be used by services to find the physical node to\n        install their servers on and should not be used for any other purpose. \n        if you try to resolve some arbitrary vnode names to physical node,\n        use the resolveVnode function instead.\n\n        tl;dr: don't use this, use resolvVnode, unless you know what you are\n        doing.\n\n        @param vnode virtual node.\n\n        @returns physical node.\n        \"\"\"\n        assert vnode in self.__resolved_bindings, 'failed to find binding for vnode {}.'.format(vnode)\n        return self.__resolved_bindings[vnode]\n\n    def getServiceNetwork(self) -> Network:\n        \"\"\"!\n        @brief get the for-service network of this emulation. If one does not\n        exist, a new one will be created.\n\n        A for-service network is a network that does not take part in the\n        emulation, and provide access between the emulation nodes and the host\n        node.\n\n        @returns service network.\n        \"\"\"\n        if self.__service_net == None:\n            self.__service_net = self.__registry.register('seedemu', 'net', '000_svc', Network('000_svc', NetworkType.Bridge, IPv4Network(self.__service_net_prefix), direct = False))\n\n        return self.__service_net\n\n    def render(self) -> Emulator:\n        \"\"\"!\n        @brief Render to emulation.\n\n        @throws AssertionError if dependencies unmet \n\n        @returns self, for chaining API calls.\n        \"\"\"\n        assert not self.__rendered, 'already rendered.'\n\n        for (layer, _) in self.__layers.db.values():\n            self.__loadDependencies(layer.getDependencies())\n\n        # render base first\n        self.__render('Base', False, True)\n\n        # collect all pending vnode names\n        self.__log('collecting virtual node names in the emulation...')\n        vnodes: List[str] = []\n        for (layer, _) in self.__layers.db.values():\n            if not isinstance(layer, core.Service): continue\n            for (vnode, _) in layer.getPendingTargets().items():\n                assert vnode not in vnodes, 'duplicated vnode: {}'.format(vnode)\n                vnodes.append(vnode)\n        self.__log('found {} virtual nodes.'.format(len(vnodes)))\n\n        # resolv bindings for all vnodes\n        self.__log('resolving binding for all virtual nodes...')\n        for binding in self.getBindings():\n            for vnode in vnodes:\n                if vnode in self.__resolved_bindings: continue\n                pnode = binding.getCandidate(vnode, self)\n                if pnode == None: continue\n                self.__log('vnode {} bound to as{}/{}'.format(vnode, pnode.getAsn(), pnode.getName()))\n                self.__resolved_bindings[vnode] = pnode\n\n        self.__log('applying changes made to virtual physical nodes to real physical nodes...')\n        vpnodes = self.__bindings.vpnodes\n        for (vnode, pnode) in self.__resolved_bindings.items():\n            if not vnode in vpnodes: continue\n            vpnode = vpnodes[vnode]\n\n            self.__log('applying changes made on vnode {} to pnode as{}/{}...'.format(vnode, pnode.getAsn(), pnode.getName()))\n            pnode.copySettings(vpnode)\n\n        for layerName in self.__layers.db.keys():\n            self.__render(layerName, False, True)\n\n        # FIXME\n        for (name, (layer, _)) in self.__layers.db.items():\n            self.__layers.db[name] = (layer, False)\n\n        for layerName in self.__layers.db.keys():\n            self.__render(layerName, False, False)\n\n        self.__rendered = True\n\n        return self\n\n    def compile(self, compiler: core.Compiler, output: str, override: bool = False) -> Emulator:\n        \"\"\"!\n        @brief Compile the simulation.\n\n        @param compiler to use.\n        @param output output directory path.\n        @param override (optional) override the output folder if it already\n        exist. False by default.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        compiler.compile(self, output, override)\n\n        return self\n\n    def updateOutputDirectory(self, compiler: core.Compiler, callbacks: list) -> Emulator:\n        \"\"\"!\n        @brief update the output directory in a flexible way. Each service might need to update it in a different way\n        @param compiler to use\n        @param callbacks which is a list of custom functions that will be executed to update the output directory\n        \"\"\"\n\n        for func in callbacks:\n            func(compiler)\n\n    def getRegistry(self) -> Registry: \n        \"\"\"!\n        @brief Get the Registry.\n\n        @returns Registry.\n        \"\"\"\n        return self.__registry\n\n    def getVirtualNode(self, vnode_name: str) -> core.Node:\n        \"\"\"!\n        @brief get a virtual \"physical\" node.\n\n        This API allows you to create a \"virtual\" physical node for a virtual\n        node. A real \"Node\" instance will be returned, you can make any changes\n        to it, and those changes will be copied to the real physical node the\n        virtual node has bound to during render.\n\n        Note that all the APIs that require the node to be in an AS will not\n        work. Like `getAsn`, `joinNetwork`, etc. You will get an error if you\n        use them.\n\n        @param vnode_name virtual node name.\n\n        @returns node\n        \"\"\"\n        if vnode_name not in self.__bindings.vpnodes:\n            self.__bindings.vpnodes[vnode_name] = core.Node(vnode_name, NodeRole.Host, 0)\n\n        return self.__bindings.vpnodes[vnode_name]\n\n    def setVirtualNode(self, vnode_name: str, node: core.Node) -> Emulator:\n        \"\"\"!\n        @brief set a virtual node.\n\n        This API allows you to overwrite an existing, or create new virtual node\n        with the given node object.\n\n        You should use the getVirtualNode API instead, unless you know what you\n        are doing.\n\n        @param vnode_name virtual node name.\n        @param node virtual physical node.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        assert node.getAsn() == 0, 'vpnode asn must be 0.'\n        self.__bindings.vpnodes[vnode_name] = node\n\n        return self\n\n    def getVirtualNodes(self) -> Dict[str, core.Node]:\n        \"\"\"!\n        @brief get dict of virtual \"physical\" nodes.\n\n        @return dict of nodes where key is virtual node name.\n        \"\"\"\n        return self.__bindings.vpnodes\n\n    def merge(self, other: Emulator, mergers: List[Merger] = [], vnodePrefix: str = '') -> Emulator:\n        \"\"\"!\n        @brief merge two emulators.\n\n        @param other the other emulator.\n        @param mergers list of merge handlers.\n        @param vnodePrefix prefix to add to the vnodes from the other emulator.\n\n        @returns new emulator.\n        \"\"\"\n\n        new_layers: Dict[Mergeable] = {}\n        other_layers: Dict[Mergeable] = {}\n\n        for l in self.getLayers(): new_layers[l.getTypeName()] = l\n        for l in other.getLayers(): other_layers[l.getTypeName()] = l\n\n        for l in other_layers.values():\n            typename = l.getTypeName()\n\n            if isinstance(l, core.Service):\n                l.addPrefix(vnodePrefix)\n\n            if typename not in new_layers.keys():\n                new_layers[typename] = l\n                continue\n\n            merged = False\n\n            for merger in mergers:\n                if merger.getTargetType() != typename: continue\n                new_layers[typename] = merger.doMerge(new_layers[typename], l)\n                merged = True\n\n            assert merged, 'abort: no merger found for {}'.format(typename)\n\n        new_sim = Emulator()\n        for l in new_layers.values(): new_sim.addLayer(l)\n\n        for binding in self.getBindings(): new_sim.addBinding(binding)\n        for binding in other.getBindings(): new_sim.addBinding(binding)\n\n        for hook in self.getRegistry().getByType('seedemu', 'hook'): new_sim.addHook(hook)\n        for hook in other.getRegistry().getByType('seedemu', 'hook'): new_sim.addHook(hook)\n\n        for (v, n) in other.getVirtualNodes().items(): new_sim.setVirtualNode(v, n)\n        for (v, n) in self.getVirtualNodes().items(): new_sim.setVirtualNode(v, n)\n\n        return new_sim\n\n    def dump(self, fileName: str) -> Emulator:\n        \"\"\"!\n        @brief dump the emulation to file.\n\n        @param fileName output path.\n        @throws AssertionError if the emulation is already rendered.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n\n        assert not self.__rendered, 'cannot dump emulation after render.'\n        with open(fileName, 'wb') as f:\n            pickle.dump(self.__registry, f)\n\n        return self\n\n    def load(self, fileName: str) -> Emulator:\n        \"\"\"!\n        @brief load emulation from file.\n\n        @param fileName path to the dumped emulation.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n\n        with open(fileName, 'rb') as f:\n            self.__rendered = False\n            self.__dependencies_db = {}\n            self.__registry = pickle.load(f)\n            self.__layers = self.__registry.get('seedemu', 'dict', 'layersdb')\n            self.__bindings = self.__registry.get('seedemu', 'list', 'bindingdb')\n\n        return self\n\n# --- Snippet Separator ---\n\nclass EthereumService(Service):\n    \"\"\"!\n    @brief The Ethereum network service.\n    This service allows one to run a private Ethereum network in the emulator.\n    \"\"\"\n\n    __blockchains: Dict[str, Blockchain]\n\n    __save_state: bool\n    __save_path: str\n    __override: bool\n    __blockchain_id: int\n    __serial: int\n\n    def __init__(self, saveState: bool = False, savePath: str = './eth-states', override:bool=False):\n        \"\"\"!\n        @brief The EthereumService class initializer.\n\n        @param saveState (optional) If true, the service will try to save state\n        of the block chain by saving the datadir of every node. Default to\n        false.\n        @param savePath (optional) The path to save containers' datadirs on the\n        host. Default to \"./eth-states\". \n        @param override (optional) If true, override the output folder if it already\n        exist. False by default.\n\n        @returns An instance of the EthereumService class.\n        \"\"\"\n        super().__init__()\n\n        self.__serial = 0\n        self.__save_state = saveState\n        self.__save_path = savePath\n        self.__override = override\n        self.__blockchains = {}\n        self.__blockchain_id = 1337\n\n    def getName(self):\n        return 'EthereumService'\n\n    def isSave(self):\n        return self.__save_state\n\n    def getSavePath(self):\n        return self.__save_path\n\n    def _doConfigure(self, node: Node, server: EthereumServer):\n        blockchain = server.getBlockchain()\n        blockchain._doConfigure(node, server)\n\n    def configure(self, emulator: Emulator):\n        if self.__save_state:\n            self._createSharedFolder()\n        super().configure(emulator)\n        for blockchain in self.__blockchains.values():\n            blockchain.configure(emulator)\n\n    def _createSharedFolder(self):\n        if path.exists(self.__save_path):\n            if self.__override:\n                self._log('eth_state folder \"{}\" already exist, overriding.'.format(self.__save_path))\n                i = 1\n                while True:\n                    rename_save_path = \"{}-{}\".format(self.__save_path, i)\n                    if not path.exists(rename_save_path):\n                        rename(self.__save_path, rename_save_path)\n                        break\n                    else:\n                        i = i+1\n            else:\n                self._log('eth_state folder \"{}\" already exist. Set \"override = True\" when calling compile() to override.'.format(self.__save_path))\n                exit(1)\n        mkdir(self.__save_path)\n\n    def _doInstall(self, node: Node, server: EthereumServer):\n        self._log('installing eth on as{}/{}...'.format(node.getAsn(), node.getName()))\n\n        server.install(node, self)\n\n    def _createServer(self, blockchain: Blockchain = None) -> Server:\n        self.__serial += 1\n        assert blockchain != None, 'EthereumService::_createServer(): create server using Blockchain::createNode() not EthereumService::install()'.format()\n        consensus = blockchain.getConsensusMechanism()\n        if consensus == ConsensusMechanism.POA:\n            return PoAServer(self.__serial, blockchain)\n        if consensus == ConsensusMechanism.POW:\n            return PoWServer(self.__serial, blockchain)\n        if consensus == ConsensusMechanism.POS:\n            return PoSServer(self.__serial, blockchain)\n\n    def installByBlockchain(self, vnode: str, blockchain: Blockchain) -> EthereumServer:\n        \"\"\"!\n        @brief Install the service on a node identified by given name. \n                This API is called by Blockchain Class. \n\n        @param vnode The name of the virtual node. \n        @param blockchain The blockchain that the created node is belongs to.\n\n        @returns EthereumServer.\n        \"\"\"\n        if vnode in self._pending_targets.keys(): return self._pending_targets[vnode]\n\n        s = self._createServer(blockchain)\n        self._pending_targets[vnode] = s\n\n        return self._pending_targets[vnode]\n\n    def createBlockchain(self, chainName:str, consensus: ConsensusMechanism, chainId: int = -1):\n        \"\"\"!\n        @brief Create an instance of Blockchain class which is a sub-layer of the EthereumService.\n\n        @param chainName The name of the Blockchain.\n        @param consensus The consensus mechanism of the blockchain.\n        @param chainId The chain id of the Blockchain.\n\n        @returns an instance of Blockchain class.\n        \"\"\"\n\n        if chainId < 0 : \n            chainId = self.__blockchain_id\n            self.__blockchain_id += 1\n        blockchain = Blockchain(self, chainName, chainId, consensus)\n        self.__blockchains[chainName] = blockchain\n        return blockchain\n\n    def print(self, indent: int) -> str:\n        out = ' ' * indent\n        out += 'EthereumService:\\n'\n\n        indent += 4\n\n        out += ' ' * indent\n        out += 'Boot Nodes:\\n'\n\n        indent += 4\n\n        for node in self.getBootNodes(ConsensusMechanism.POW):\n            out += ' ' * indent\n            out += 'POW-{}\\n'.format(node)\n\n        for node in self.getBootNodes(ConsensusMechanism.POA):\n            out += ' ' * indent\n            out += 'POA-{}\\n'.format(node)\n\n        return out\n\n# --- Snippet Separator ---\n\nclass Blockchain:\n    \"\"\"!\n    @brief The individual blockchain in EthereumService.\n    This Blockchain class allows to maintain multiple blockchains inside EthereumService.\n    \"\"\"\n    __consensus: ConsensusMechanism\n    __genesis: Genesis\n    __eth_service: EthereumService\n    __boot_node_addresses: Dict[ConsensusMechanism, List[str]]\n    __joined_accounts: List[AccountStructure]\n    __joined_signer_accounts: List[AccountStructure]\n    __validator_ids: List[str]\n    __beacon_setup_node_address: str\n    __chain_id:int\n    __pending_targets:list\n    __chain_name:str\n    __emu_mnemonic:str\n    __total_accounts_per_node: int\n    __emu_account_balance: int\n    __local_mnemonic:str\n    __local_accounts_total:int\n    __local_account_balance:int\n    __terminal_total_difficulty:int\n    __target_aggregater_per_committee:int\n    __target_committee_size:int\n\n    def __init__(self, service:EthereumService, chainName: str, chainId: int, consensus:ConsensusMechanism):\n        \"\"\"!\n        @brief The Blockchain class initializer.\n\n        @param service The EthereumService that creates the Blockchain class instance.\n        @param chainName The name of the Blockchain to create.\n        @param chainid The chain id of the Blockchain to create.\n        @param consensus The consensus of the Blockchain to create (supports POA, POS, POW).\n\n        @returns An instance of The Blockchain class.\n        \"\"\"\n        self.__eth_service = service\n        self.__consensus = consensus\n        self.__chain_name = chainName\n        self.__genesis = Genesis(ConsensusMechanism.POA) if self.__consensus == ConsensusMechanism.POS else Genesis(self.__consensus)\n        self.__boot_node_addresses = []\n        self.__miner_node_address = []\n        self.__joined_accounts = []\n        self.__joined_signer_accounts = []\n        self.__validator_ids = []\n        self.__beacon_setup_node_address = ''\n        self.__pending_targets = []\n        self.__emu_mnemonic = \"great awesome fun seed security lab protect system network prevent attack future\"\n        self.__total_accounts_per_node = 1\n        self.__emu_account_balance = 32 * EthUnit.ETHER.value\n        self.__local_mnemonic = \"great amazing fun seed lab protect network system security prevent attack future\"\n        self.__local_accounts_total = 5\n        self.__local_account_balance = 10 * EthUnit.ETHER.value\n        self.__chain_id = chainId\n        self.__terminal_total_difficulty = 20\n        self.__target_aggregater_per_committee = 2\n        self.__target_committee_size = 3\n\n    def _doConfigure(self, node:Node, server:EthereumServer):\n        self._log('configuring as{}/{} as an eth node...'.format(node.getAsn(), node.getName()))\n\n        ifaces = node.getInterfaces()\n        assert len(ifaces) > 0, 'EthereumService::_doConfigure(): node as{}/{} has not interfaces'.format()\n        addr = '{}:{}'.format(str(ifaces[0].getAddress()), server.getBootNodeHttpPort())\n\n        if server.isBootNode():\n            self._log('adding as{}/{} as consensus-{} bootnode...'.format(node.getAsn(), node.getName(), self.__consensus.value))\n            self.__boot_node_addresses.append(addr)\n\n        if self.__consensus == ConsensusMechanism.POS:\n            if server.isStartMiner():\n                self._log('adding as{}/{} as consensus-{} miner...'.format(node.getAsn(), node.getName(), self.__consensus.value))\n                self.__miner_node_address.append(str(ifaces[0].getAddress())) \n            if server.isBeaconSetupNode():\n                self.__beacon_setup_node_address = '{}:{}'.format(ifaces[0].getAddress(), server.getBeaconSetupHttpPort())\n\n        server._createAccounts(self)\n\n        accounts = server._getAccounts()\n        if len(accounts) > 0:\n            if self.__consensus == ConsensusMechanism.POS and server.isValidatorAtRunning():\n                accounts[0].balance = 33 * EthUnit.ETHER.value\n            self.__joined_accounts.extend(accounts)\n            if self.__consensus in [ConsensusMechanism.POA, ConsensusMechanism.POS] and server.isStartMiner():\n                self.__joined_signer_accounts.append(accounts[0])\n\n        if self.__consensus == ConsensusMechanism.POS and server.isValidatorAtGenesis():\n            self.__validator_ids.append(str(server.getId()))\n\n        server._generateGethStartCommand()\n\n        if self.__eth_service.isSave():\n            save_path = self.__eth_service.getSavePath()\n            node.addSharedFolder('/root/.ethereum', '../{}/{}/{}/ethereum'.format(save_path, self.__chain_name, server.getId()))\n            node.addSharedFolder('/root/.ethash', '../{}/{}/{}/ethash'.format(save_path, self.__chain_name, server.getId()))\n            makedirs('{}/{}/{}/ethereum'.format(save_path, self.__chain_name, server.getId()))\n            makedirs('{}/{}/{}/ethash'.format(save_path, self.__chain_name, server.getId()))\n\n    def configure(self, emulator:Emulator):\n        pending_targets = self.__eth_service.getPendingTargets()\n        localAccounts = EthAccount.createLocalAccountsFromMnemonic(mnemonic=self.__local_mnemonic, balance=self.__local_account_balance, total=self.__local_accounts_total)\n        self.__genesis.addAccounts(localAccounts)\n        self.__genesis.setChainId(self.__chain_id)\n        for vnode in self.__pending_targets:\n            node = emulator.getBindingFor(vnode)\n            server = pending_targets[vnode]\n            if self.__consensus == ConsensusMechanism.POS and server.isStartMiner():\n                ifaces = node.getInterfaces()\n                assert len(ifaces) > 0, 'EthereumService::_doConfigure(): node as{}/{} has not interfaces'.format()\n                addr = str(ifaces[0].getAddress())\n                miner_ip = self.__miner_node_address[0]\n                if addr == miner_ip:\n                    validator_count = len(self.getValidatorIds())\n                    index = self.__joined_accounts.index(server._getAccounts()[0])\n                    self.__joined_accounts[index].balance = 32*pow(10,18)*(validator_count+2)\n\n        self.__genesis.addAccounts(self.getAllAccounts())\n\n        if self.__consensus in [ConsensusMechanism.POA, ConsensusMechanism.POS] :\n            self.__genesis.setSigner(self.getAllSignerAccounts())\n\n    def getBootNodes(self) -> List[str]:\n        \"\"\"!\n        @brief Get bootnode IPs.\n\n        @returns List of bootnodes IP addresses.\n        \"\"\"\n        return self.__boot_node_addresses\n\n    def getMinerNodes(self) -> List[str]:\n        \"\"\"!\n        @brief Get miner node IPs.\n\n        @returns List of miner nodes IP addresses.\n        \"\"\"\n        return self.__miner_node_address\n\n    def getAllAccounts(self) -> List[AccountStructure]:\n        \"\"\"!\n        @brief Get a joined list of all the created accounts on all nodes in the blockchain.\n\n        @returns List of accounts.\n        \"\"\"\n        return self.__joined_accounts\n\n    def getAllSignerAccounts(self) -> List[AccountStructure]:\n        \"\"\"!\n        @brief Get a list of all signer accounts on all nodes in the blockchain.\n\n        returns List of signer accounts.\n        \"\"\"\n        return self.__joined_signer_accounts\n\n    def getValidatorIds(self) -> List[str]:\n        \"\"\"!\n        @brief Get a list of all validators ids on all nodes in the blockchain.\n\n        @returns List of all validators ids.\n        \"\"\"\n        return self.__validator_ids\n\n    def getBeaconSetupNodeIp(self) -> str:\n        \"\"\"!\n        @brief Get the IP of a beacon setup node.\n\n        @returns The IP address.\n        \"\"\"\n        return self.__beacon_setup_node_address\n\n    def setGenesis(self, genesis:str) -> EthereumServer:\n        \"\"\"!\n        @brief Set the custom genesis.\n\n        @param genesis The genesis file contents to set. \n\n        @returns Self, for chaining API calls.\n        \"\"\"\n        self.__genesis.setGenesis(genesis)\n\n        return self\n\n    def getGenesis(self) -> Genesis:\n        \"\"\"!\n        @brief Get the genesis file content.\n\n        @returns Genesis. \n        \"\"\"\n        return self.__genesis\n\n    def setConsensusMechanism(self, consensus:ConsensusMechanism) -> EthereumServer:\n        \"\"\"!\n        @brief Set consensus mechanism of this blockchain.\n\n        @param consensusMechanism Consensus mechanism to set (supports POW, POA and POS).\n\n        @returns Self, for chaining API calls. \n        \"\"\"\n        self.__consensus = consensus\n        self.__genesis = Genesis(self.__consensus)\n\n        return self\n\n    def getConsensusMechanism(self) -> ConsensusMechanism:\n        \"\"\"!\n        @brief Get the consensus mechanism of this blockchain.\n\n        @returns ConsensusMechanism\n        \"\"\"\n        return self.__consensus\n\n    def setTerminalTotalDifficulty(self, ttd:int):\n        \"\"\"!\n        @brief Set the terminal total difficulty, which is the value to designate\n                when the Merge is happen. In POA, difficulty is tend to increase by 2\n                for every one block. For example, if the terminal_total_difficulty is \n                set to 20, the Ethereum blockchain will keep POA consensus for approximately\n                150 sec (20/2*15) and then stop signing the block until the Merge happens.\n                Default to 20. \n\n        @param ttd The terminal total difficulty to set.\n\n        @returns Self, for chaining API calls.\n        \"\"\"\n        self.__terminal_total_difficulty = ttd\n\n        return self\n\n    def getTerminalTotalDifficulty(self) -> int:\n        \"\"\"!\n        @brief Get the value of the terminal total difficulty.\n\n        @returns terminal_total_difficulty.\n        \"\"\"\n\n        return self.__terminal_total_difficulty\n\n    def setGasLimitPerBlock(self, gasLimit:int):\n        \"\"\"!\n        @brief Set GasLimit at Genesis (the limit of gas cost per block).\n\n        @param gasLimit The gas limit per block.\n\n        @returns Self, for chaining API calls.\n        \"\"\"\n        self.__genesis.setGasLimit(gasLimit)\n        return self\n\n    def setChainId(self, chainId:int):\n        \"\"\"!\n        @brief Set chain Id at Genesis.\n\n        @param chainId The chain Id to set.\n\n        @returns Self, for chaining API calls\n        \"\"\"\n\n        self.__chain_id = chainId\n        return self\n\n    def createNode(self, vnode: str) -> EthereumServer:\n        \"\"\"!\n        @brief Create a node belongs to this blockchain.\n\n        @param vnode The name of vnode.\n\n        @returns EthereumServer\n        \"\"\"\n        eth = self.__eth_service\n        self.__pending_targets.append(vnode)\n        return eth.installByBlockchain(vnode, self)\n\n    def addLocalAccount(self, address: str, balance: int, unit:EthUnit=EthUnit.ETHER) -> Blockchain:\n        \"\"\"!\n        @brief Allocate balance to an external account by setting alloc field of genesis file.\n\n        @param address The External account's address.\n        @param balance The balance to allocate.\n        @param unit The unit of Ethereum.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        balance = balance * unit.value\n        self.__genesis.addLocalAccount(address, balance)\n\n        return self\n\n    def addLocalAccountsFromMnemonic(self, mnemonic:str, total:int, balance:int, unit:EthUnit=EthUnit.ETHER) -> Blockchain:\n        \"\"\"!\n        @brief Add local account from the given Mnemonic in addition to default local accounts.\n\n        @param mnemonic The mnemonic phrase to generate accounts from.\n        @param total The total number of accounts to generate.\n        @param balance The balance to allocate to the generated accounts.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        balance = balance * unit.value\n        mnemonic_account = EthAccount.createLocalAccountsFromMnemonic(mnemonic = mnemonic, balance=balance, total=total)\n        self.__genesis.addAccounts(mnemonic_account)\n\n    def getChainName(self) -> str:\n        \"\"\"!\n        @brief Get the name of the blockchain.\n\n        @returns The name of this blockchain.\n        \"\"\"\n        return self.__chain_name\n\n    def getChainId(self) -> int:\n        \"\"\"!\n        @brief Get the chain Id of the blockchain.\n\n        @returns The chain Id of this blockchain.\n        \"\"\"\n        return self.__chain_id\n\n    def setEmuAccountParameters(self, mnemonic:str, balance:int, total_per_node:int, unit:EthUnit=EthUnit.ETHER):\n        \"\"\"!\n        @brief Set mnemonic, balance, and total_per_node value to customize the account generation in this blockchain.\n\n        @param mnemonic The mnemonic phrase to generate the accounts per a node in this blockchain.\n        @param balance The balance to allocate to the generated accounts.\n        @param total_per_node The total number of the accounts to generate per a node in this blockchain.\n        @param unit The unit of Ethereum.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        self.__emu_mnemonic = mnemonic\n        self.__emu_account_balance = balance * unit.value\n        self.__total_accounts_per_node = total_per_node\n        return self\n\n    def getEmuAccountParameters(self):\n        \"\"\"!\n        @brief Get values of mnemonic, balance, and total_per_node value used for the account generation.\n\n        returns The value of mnemonic, balance, and total_per_node.\n        \"\"\"\n        return self.__emu_mnemonic, self.__emu_account_balance, self.__total_accounts_per_node\n\n    def setLocalAccountParameters(self, mnemonic:str, balance:int, total:int, unit:EthUnit=EthUnit.ETHER):\n        \"\"\"!\n        @brief Set mnemonic, balance, and total_per_node value to customize the local account generation.\n\n        @param mnemonic The mnemonic phrase to generate the local accounts.\n        @param balance The balance to allocate to the generated accounts.\n        @param total The total number of the local accounts.\n        @param unit The unit of Ethereum.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        self.__local_mnemonic = mnemonic\n        self.__local_account_balance = balance * unit.value\n        self.__local_accounts_total = total\n        return self\n\n    def setTargetAggregatorPerCommittee(self, target_aggregator_per_committee:int):\n        \"\"\"!\n        @brief Set target aggregator per committee for Beacon chain.\n\n        @param target_aggregator_per_committee The target value of the number of aggregator per committee to set.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        self.__target_aggregater_per_committee = target_aggregator_per_committee\n        return self\n\n    def getTargetAggregatorPerCommittee(self):\n        \"\"\"!\n        @brief Get the value of target aggregator per committee for Beacon chain.\n\n        @returns The value of target_aggregator_per_committee.\n        \"\"\"\n        return self.__target_aggregater_per_committee\n\n    def setTargetCommitteeSize(self, target_committee_size:int):\n        \"\"\"!\n        @brief Set target committee size for Beacon chain.\n\n        @param target_committee_size The target value of committee size to set.\n\n        @returns Self, for chaining calls.\n        \"\"\"\n        self.__target_committee_size = target_committee_size\n        return self\n\n    def getTargetCommitteeSize(self):\n        \"\"\"!\n        @brief Get the value of target committee size for Beacon Chain.\n\n        @returns The value of target_committee_size.\n        \"\"\"\n        return self.__target_committee_size\n\n    def _log(self, message: str) -> None:\n        \"\"\"!\n        @brief Log to stderr.\n\n        @returns None.\n        \"\"\"\n        print(\"==== Blockchain Sub Layer: {}\".format(message), file=stderr)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulator base with 10 Stub AS and 3 hosts per stub AS using the seedemu library. Then, create an Ethereum service layer and a sub-layer of it, a blockchain with the name \"pos\" and consensus mechanism set to POS. Set the terminal total difficulty of the blockchain to 30. \n\nFor each host in the AS, create a blockchain virtual node, a Docker container label, and enable Geth to communicate with the geth node via http. Set specific hosts as BeaconSetupNode, BootNode, and validator nodes with different conditions. Also, customize the display names of the nodes and bind the virtual node to the physical node. \n\nFinally, add the Ethereum layer to the emulator, render it, and compile it with Docker with internetMap and etherView enabled. The output should be saved in the './output' directory and existing files should be overridden.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 89, "repo_full_name": "federatedai__fate", "instruction": "Generate code that creates a pipeline for a machine learning task using the FATE library. The pipeline should include the following steps: reading data, transforming data, scaling features, training a logistic regression model, and evaluating the model. The pipeline should be set up to handle a multi-party computation scenario with a guest, a host, and an arbiter. The data for the guest and host should be read from specified tables. The logistic regression model should be configured with specific parameters including penalty, optimizer, tolerance, alpha, maximum iterations, early stopping criteria, batch size, learning rate, decay, initialization method, and cross-validation parameters. After the pipeline is compiled and fitted, selected components should be deployed. A prediction pipeline should be created by adding the data reader and selected components from the training pipeline. The prediction pipeline should be compiled and used to make predictions. The DSL and configuration of the prediction pipeline should be saved as JSON files. Finally, the summaries of the logistic regression and evaluation components should be printed. The main function should accept a configuration file as an argument.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def main():\n    # parties config\n    guest = 9999\n    host = 10000\n    arbiter = 10000\n\n    # specify input data name & namespace in database\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    guest_eval_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_eval_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    # initialize pipeline\n    pipeline = PipeLine()\n    # set job initiator\n    pipeline.set_initiator(role=\"guest\", party_id=guest)\n    # set participants information\n    pipeline.set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    # define Reader components to read in data\n    reader_0 = Reader(name=\"reader_0\")\n    # configure Reader for guest\n    reader_0.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_train_data)\n    # configure Reader for host\n    reader_0.get_party_instance(role=\"host\", party_id=host).component_param(table=host_train_data)\n\n    reader_1 = Reader(name=\"reader_1\")\n    reader_1.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_1.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n\n    # define DataIO components\n    dataio_0 = DataIO(name=\"dataio_0\")\n    dataio_1 = DataIO(name=\"dataio_1\")\n\n    # get DataIO party instance of guest\n    dataio_0_guest_party_instance = dataio_0.get_party_instance(role=\"guest\", party_id=guest)\n    # configure DataIO for guest\n    dataio_0_guest_party_instance.component_param(with_label=True, output_format=\"dense\")\n    # get and configure DataIO party instance of host\n    dataio_0.get_party_instance(role=\"host\", party_id=host).component_param(with_label=False)\n\n    # define Intersection components\n    intersection_0 = Intersection(name=\"intersection_0\")\n    intersection_1 = Intersection(name=\"intersection_1\")\n\n    # define HeteroLR component\n    hetero_lr_0 = HeteroLR(name=\"hetero_lr_0\", early_stop=\"weight_diff\", learning_rate=0.15, optimizer=\"rmsprop\",\n                           max_iter=10, early_stopping_rounds=2, validation_freqs=1)\n\n    # add components to pipeline, in order of task execution\n    pipeline.add_component(reader_0)\n    pipeline.add_component(reader_1)\n    pipeline.add_component(dataio_0, data=Data(data=reader_0.output.data))\n    # set dataio_1 to replicate model from dataio_0\n    pipeline.add_component(dataio_1, data=Data(data=reader_1.output.data), model=Model(dataio_0.output.model))\n    # set data input sources of intersection components\n    pipeline.add_component(intersection_0, data=Data(data=dataio_0.output.data))\n    pipeline.add_component(intersection_1, data=Data(data=dataio_1.output.data))\n    # set train & validate data of hetero_lr_0 component\n    pipeline.add_component(\n        hetero_lr_0,\n        data=Data(\n            train_data=intersection_0.output.data,\n            validate_data=intersection_1.output.data))\n\n    # compile pipeline once finished adding modules, this step will form conf and dsl files for running job\n    pipeline.compile()\n\n    # fit model\n    pipeline.fit()\n    # query component summary\n    import json\n    print(json.dumps(pipeline.get_component(\"hetero_lr_0\").get_summary(), indent=4))\n\n    # predict\n    # deploy required components\n    pipeline.deploy_component([dataio_0, intersection_0, hetero_lr_0])\n\n    # initiate predict pipeline\n    predict_pipeline = PipeLine()\n\n    reader_2 = Reader(name=\"reader_2\")\n    reader_2.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_2.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n    # add data reader onto predict pipeline\n    predict_pipeline.add_component(reader_2)\n    # add selected components from train pipeline onto predict pipeline\n    # specify data source\n    predict_pipeline.add_component(pipeline,\n                                   data=Data(predict_input={pipeline.dataio_0.input.data: reader_2.output.data}))\n    # run predict model\n    predict_pipeline.predict()\n\n# --- Snippet Separator ---\n\ndef _deprecate_normalize(normalize, estimator_name):\n    \"\"\"Normalize is to be deprecated from linear models and a use of\n    a pipeline with a StandardScaler is to be recommended instead.\n    Here the appropriate message is selected to be displayed to the user\n    depending on the default normalize value (as it varies between the linear\n    models and normalize value selected by the user).\n\n    Parameters\n    ----------\n    normalize : bool,\n        normalize value passed by the user\n\n    estimator_name : str\n        name of the linear estimator which calls this function.\n        The name will be used for writing the deprecation warnings\n\n    Returns\n    -------\n    normalize : bool,\n        normalize value which should further be used by the estimator at this\n        stage of the depreciation process\n\n    Notes\n    -----\n    This function should be completely removed in 1.4.\n    \"\"\"\n\n    if normalize not in [True, False, \"deprecated\"]:\n        raise ValueError(\n            \"Leave 'normalize' to its default value or set it to True or False\"\n        )\n\n    if normalize == \"deprecated\":\n        _normalize = False\n    else:\n        _normalize = normalize\n\n    pipeline_msg = (\n        \"If you wish to scale the data, use Pipeline with a StandardScaler \"\n        \"in a preprocessing stage. To reproduce the previous behavior:\\n\\n\"\n        \"from sklearn.pipeline import make_pipeline\\n\\n\"\n        \"model = make_pipeline(StandardScaler(with_mean=False), \"\n        f\"{estimator_name}())\\n\\n\"\n        \"If you wish to pass a sample_weight parameter, you need to pass it \"\n        \"as a fit parameter to each step of the pipeline as follows:\\n\\n\"\n        \"kwargs = {s[0] + '__sample_weight': sample_weight for s \"\n        \"in model.steps}\\n\"\n        \"model.fit(X, y, **kwargs)\\n\\n\"\n    )\n\n    alpha_msg = \"\"\n    if \"LassoLars\" in estimator_name:\n        alpha_msg = \"Set parameter alpha to: original_alpha * np.sqrt(n_samples). \"\n\n    if normalize != \"deprecated\" and normalize:\n        warnings.warn(\n            \"'normalize' was deprecated in version 1.2 and will be removed in 1.4.\\n\"\n            + pipeline_msg\n            + alpha_msg,\n            FutureWarning,\n        )\n    elif not normalize:\n        warnings.warn(\n            (\n                \"'normalize' was deprecated in version 1.2 and will be \"\n                \"removed in 1.4. \"\n                \"Please leave the normalize parameter to its default value to \"\n                \"silence this warning. The default behavior of this estimator \"\n                \"is to not do any normalization. If normalization is needed \"\n                \"please use sklearn.preprocessing.StandardScaler instead.\"\n            ),\n            FutureWarning,\n        )\n\n    return _normalize\n\n# --- Snippet Separator ---\n\ndef fit(self, X, y=None, **params):\n        \"\"\"Fit the model.\n\n        Fit all the transformers one after the other and transform the\n        data. Finally, fit the transformed data using the final estimator.\n\n        Parameters\n        ----------\n        X : iterable\n            Training data. Must fulfill input requirements of first step of the\n            pipeline.\n\n        y : iterable, default=None\n            Training targets. Must fulfill label requirements for all steps of\n            the pipeline.\n\n        **params : dict of str -> object\n            - If `enable_metadata_routing=False` (default):\n\n                Parameters passed to the ``fit`` method of each step, where\n                each parameter name is prefixed such that parameter ``p`` for step\n                ``s`` has key ``s__p``.\n\n            - If `enable_metadata_routing=True`:\n\n                Parameters requested and accepted by steps. Each step must have\n                requested certain metadata for these parameters to be forwarded to\n                them.\n\n            .. versionchanged:: 1.4\n                Parameters are now passed to the ``transform`` method of the\n                intermediate steps as well, if requested, and if\n                `enable_metadata_routing=True` is set via\n                :func:`~sklearn.set_config`.\n\n            See :ref:`Metadata Routing User Guide <metadata_routing>` for more\n            details.\n\n        Returns\n        -------\n        self : object\n            Pipeline with fitted steps.\n        \"\"\"\n        routed_params = self._check_method_params(method=\"fit\", props=params)\n        Xt = self._fit(X, y, routed_params)\n        with _print_elapsed_time(\"Pipeline\", self._log_message(len(self.steps) - 1)):\n            if self._final_estimator != \"passthrough\":\n                last_step_params = routed_params[self.steps[-1][0]]\n                self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n\n        return self\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a pipeline for a machine learning task using the FATE library. The pipeline should include the following steps: reading data, transforming data, scaling features, training a logistic regression model, and evaluating the model. The pipeline should be set up to handle a multi-party computation scenario with a guest, a host, and an arbiter. The data for the guest and host should be read from specified tables. The logistic regression model should be configured with specific parameters including penalty, optimizer, tolerance, alpha, maximum iterations, early stopping criteria, batch size, learning rate, decay, initialization method, and cross-validation parameters. After the pipeline is compiled and fitted, selected components should be deployed. A prediction pipeline should be created by adding the data reader and selected components from the training pipeline. The prediction pipeline should be compiled and used to make predictions. The DSL and configuration of the prediction pipeline should be saved as JSON files. Finally, the summaries of the logistic regression and evaluation components should be printed. The main function should accept a configuration file as an argument.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 90, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that sets up a self-consistent fluid DREAM run, where no kinetic equations are solved, but the electric field and temperature are evolved self-consistently. The code should import necessary modules and classes from the DREAM library, set up the simulation parameters, set up the radial grid, set the time stepper, add ions, set the electric field and cold electron temperature, set up the hot tail grid, disable the runaway grid, set the solver type and its parameters, and include other necessary settings. The code should then save these settings to an HDF5 file and run the simulation. After the initial run, the code should restart the simulation twice, each time loading the settings from the output of the previous run, adjusting the time stepper, and saving the new settings to an HDF5 file before running the simulation again.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def load(self, filename, path=\"\", lazy=True, loadsettings=True):\n        \"\"\"\n        Loads DREAM output from the specified file. If 'path' is\n        given, this indicates which group path in the file to load\n        the output from.\n\n        :param str filename:      Name of file to load output from.\n        :param str path:          Path to subsect of HDF5 file containing DREAM output.\n        :param bool lazy:         If ``True``, allows the file to be read lazily (on-demand) by return h5py DataSet objects instead of the actual data (wrapped in a DREAM.DataObject).  This can greatly reduce load times, but may complicate typing slightly. Note also that the HDF5 file will be locked for as long as the Python interpreter is running.\n        :param bool loadsettings: If ``True``, load the settings stored in the output object.\n        \"\"\"\n        self.filename = filename\n\n        od, self.h5handle, self.filesize = DREAMIO.LoadHDF5AsDict(filename, path=path, returnhandle=True, returnsize=True, lazy=lazy)\n\n        if 'grid' in od:\n            self.grid = Grid(od['grid'])\n        else:\n            print(\"WARNING: No grid found in '{}'.\".format(filename))\n\n        if 'ionmeta' in od:\n            self.ionmeta = IonMetaData(od['ionmeta'])\n        else:\n            print(\"WARNING: No ion meta data found in '{}'.\".format(filename))\n\n        # Equation system should be loaded last, because it\n        # may depend on previously loaded sections\n        if 'eqsys' in od:\n            self.eqsys = EquationSystem(od['eqsys'], grid=self.grid, output=self)\n        else:\n            print(\"WARNING: No equation system found in '{}'.\".format(filename))\n\n\n        # Load \"other\" quantities (i.e. quantities which are not part of\n        # the equation system, but may still be interesting to know the\n        # evolution of; this include collision frequencies, bounce averages\n        # and more)\n        if 'other' in od:\n            self.other = OtherQuantityHandler(od['other'], grid=self.grid, output=self)\n\n        # Load settings for the run\n        if 'settings' in od and loadsettings:\n            s = od['settings']\n            if lazy:\n                # Actually read all settings\n                # (yes, we explicitly ignore the lazy loading request since\n                # much of the settings interface has been created without lazy\n                # loading in mind, and since settings are typically *very* small\n                # compared to full-blown DREAM output)\n                s = DREAMIO.unlazy(s)\n\n            self.settings = DREAMSettings(s)\n\n        # Solver statistics\n        if 'solver' in od:\n            self.solver = self._getsolver(od['solver'], output=self)\n\n        # Timing information\n        if 'timings' in od:\n            self.timings = Timings(od['timings'], output=self)\n\n        return od\n\n# --- Snippet Separator ---\n\ndef Mfrc(self, freq=\"\", maxfiles=\"\", **kwargs):\n        \"\"\"\n        APDL Command: MFRC\n\n        Controls file writing for multiframe restarts for the ANSYS Multi-field\n        solver.\n\n        Parameters\n        ----------\n        freq\n            Frequency at which the .Xnnn files are written.\n\n            0 - Do not write any .Xnnn files for this simulation\n\n            LAST - Write the .Xnnn files for the last multifield time step (default).\n\n            N - If N is a positive number, write the .Xnnn file every Nth multifield time step.\n\n        maxfiles\n            Maximum number of .Xnnn files to save for a multifield analysis.\n\n            0 - Do not overwrite existing .Xnnn files (default). The total maximum number of\n                .Xnnn files for one run is 999. If this number is reached\n                before the analysis is complete, the analysis will continue,\n                but will no longer write .Xnnn files.\n\n            N - The maximum number of .Xnnn file to keep for this multifield simulation. When\n                this number of .Rnnn have been written, ANSYS will overwrite\n                the first .Xnnn file of the subsequent multifield time step.\n\n        Notes\n        -----\n        This command sets up the restart parameters for a multiframe restart,\n        allowing you to restart an analysis from any multifield time step for\n        which there is a .Rnnn file. You can perform a multiframe restart only\n        for nonlinear static and full transient structural, thermal and\n        thermal- structural (see RESCONTROL for details).\n\n        If you have many multifield time steps, and are writing .Xnnn files\n        frequently, use MAXFILES to limit the number of .Xnnn files saved,\n        since these files can fill up your disk quickly.\n\n        For a CFX analysis, you must also configure the MFOUTPUT and MFRSTART\n        settings to ensure consistent time points for postprocessing and\n        restart simulation.\n\n        For MFX simulation, the RESCONTROL command will be ignored.\n\n        Default Behavior\n\n        In nonlinear static and full transient analyses, the default behavior\n        is multiframe restart. (MFRC,LAST). By default, the .Rnnn file is\n        written at the last multifield time step . An .Rnnn file and\n        corresponding load set of .ldhi files is also written at the multifield\n        time step prior to the abort point of the run if an abort file was\n        used, or if the job terminated because of a failure to converge or some\n        other solution error. No information at the aborted multifield time\n        step is saved in either the .Rnnn file or the .ldhi file.\n\n        This command cannot be issued after restarting a multifield analysis.\n\n        Distributed ANSYS Restriction: This command is not supported in\n        Distributed ANSYS.\n\n        \"\"\"\n        command = \"MFRC,%s,%s\" % (str(freq), str(maxfiles))\n        return self.Run(command, **kwargs)\n\n# --- Snippet Separator ---\n\nclass DREAMSettings:\n\n\n    def __init__(self, filename=None, path=\"\", chain=True, keepignore=False):\n        \"\"\"\n        Construct a new DREAMSettings object. If 'filename' is given,\n        the object is read from the (HDF5) file with that name.\n        If 'path' is also given, this is used to locate the group\n        in the file which contains the settings. \n\n        :param str filename:    Name of the file to load settings from.\n        :param str path:        Path to group in HDF5 file containing the settings.\n        :param bool chain:      If ``True``, sets the newly created ``DREAMSettings`` object \n                                to take the output of the simulation defined by 'filename' \n                                as input (i.e. calls :py:method:`fromOutput`).\n        :param bool keepignore: If ``True``, keeps the list of unknown quantities to ignore\n                                when initializing from a previous simulation (and ``chain=True``).\n        \"\"\"\n\n        # Defaults\n        self.settings = {}\n        self.init = {}\n\n        self.addSetting('collisions', CollisionHandler())\n        self.addSetting('hottailgrid', MomentumGrid('hottailgrid'))\n        self.addSetting('other', OtherQuantities())\n        self.addSetting('output', Output())\n        self.addSetting('radialgrid', RadialGrid())\n        self.addSetting('runawaygrid', MomentumGrid('runawaygrid'))\n        self.addSetting('solver', Solver())\n        self.addSetting('timestep', TimeStepper())\n        self.addSetting('atomic', Atomics())\n\n        # Should be defined last as it may need access to the\n        # objects created above...\n        self.addSetting('eqsys', EquationSystem(settings=self))\n\n        if filename is not None:\n            if type(filename) == str:\n                self.load(filename, path=path, lazy=False)\n            elif type(filename) == dict:\n                # We first generate an empty settings object so that we\n                # get all default values in a dict...\n                self.hottailgrid.setEnabled(False)\n                self.runawaygrid.setEnabled(False)\n                dct = self.todict(verify=False)\n                s = merge_dicts(dct, filename)\n\n                self.fromdict(s)\n\n                if chain:\n                    if 'output' in s and 'filename' in s['output']:\n                        self.fromOutput(s['output']['filename'])\n                        self.output.setFilename('output.h5')\n\n                        if not keepignore:\n                            self.clearIgnore()\n            elif type(filename) == DREAMSettings:\n                self.fromdict(filename.todict())\n\n                if chain:\n                    self.fromOutput(filename.output.filename)\n                    self.output.setFilename('output.h5')\n\n                    if not keepignore:\n                        self.clearIgnore()\n\n\n    def __contains__(self, item):\n        \"\"\"\n        Overriding the Python 'in' keyword.\n        \"\"\"\n        return (item in self.settings)\n\n\n    def __getitem__(self, index):\n        \"\"\"\n        Retrieves a parameter by name.\n        \"\"\"\n        return self.settings[index]\n\n\n    def addSetting(self, name, obj):\n        \"\"\"\n        Add a setting to this object. This adds the setting to\n        the 'settings' list, in addition to making it accessible\n        through usual \"dot\" notation (i.e. you can access it either\n        as \"self.mySetting\" or \"self.settings['mySetting']\")\n\n        name: Name of settings object to add.\n        obj:  Settings object to add.\n        \"\"\"\n        setattr(self, name, obj)\n        self.settings[name] = obj\n\n\n    def fromdict(self, data, filename='<dictionary>'):\n        sets  = list(self.settings.keys())\n        other = ['init']\n\n        # Settings to remove first\n        special = ['hottailgrid', 'runawaygrid']\n\n        datakeys = list(data.keys())\n        for k in special:\n            datakeys.remove(k)\n\n        for key in special+datakeys:\n            # Warn about unrecognized settings\n            if key in sets:\n                # Remove from list of not-found settings\n                sets.remove(key)\n                # Set settings\n                if type(self.settings[key]) == MomentumGrid:\n                    self.settings[key].fromdict(key, data[key])\n                else:\n                    self.settings[key].fromdict(data[key])\n            elif key in other:\n                # Remove from list of not found\n                other.remove(key)\n\n                # Set settings\n                setattr(self, key, data[key])\n            else:\n                print(\"WARNING: Unrecognized setting '{}' found in '{}'.\".format(key, filename))\n                continue\n\n        # Convert 'eqsysignore' to a list of strings\n        if 'eqsysignore' in self.init:\n            self.init['eqsysignore'] = self.init['eqsysignore'].split(';')\n\n        # Warn about missing settings\n        missing = sets+other\n        if len(missing) > 0:\n            for s in missing:\n                print(\"WARNING: Setting '{}' not specified in '{}'.\".format(s, filename))\n\n\n    def clearIgnore(self):\n        \"\"\"\n        Clear the list of quantities to ignore when initializing from previous\n        simulation.\n        \"\"\"\n        self.init['eqsysignore'] = []\n\n\n    def fromOutput(self, filename, relpath=False, ignore=list(), timeindex=-1):\n        \"\"\"\n        Specify that the simulation should be initialized from the\n        DREAM output stored in the named file. Some unknown quantities\n        can be ignored and initialized conventionally by adding them\n        to the 'ignore' list.\n\n        filename:  Name of file to load output from.\n        relpath:   If 'True', forces the path to 'filename' to be encoded\n                   as a relative path. Otherwise, the absolute path to the\n                   named file will be calculated and given to the settings\n                   object. (default: False)\n        ignore:    List of unknown quantities to initialize as usual, and\n                   thus NOT from the specified output file.\n        timeindex: Index of time point to use for initializing the simulation.\n        \"\"\"\n        # Input as relative or absolute path?\n        if relpath:\n            fname = filename\n        else:\n            fname = os.path.abspath(filename)\n\n        if type(ignore) == str:\n            ignore = [ignore]\n        elif type(ignore) != list:\n            raise DREAMException(\"Unrecognized type of argument 'ignore'. Expected list of strings.\")\n\n        self.init['fromfile']   = fname\n        self.init['eqsysignore'] = ignore\n        self.init['timeindex']  = timeindex\n\n\n    def load(self, filename, path=\"\", lazy=False):\n        \"\"\"\n        Load a DREAMSettings object from the named HDF5 file.\n        'path' specifies the path within the HDF5 file where\n        the DREAMSettings object is stored.\n        \"\"\"\n        data = DREAMIO.LoadHDF5AsDict(filename, path=path, lazy=lazy)\n        self.fromdict(data, filename=filename)\n\n\n    def save(self, filename):\n        \"\"\"\n        Save this settings object to the specified file.\n\n        filename: Name of file to save settings to (the file will\n                  be overwritten if it exists).\n        \"\"\"\n        DREAMIO.SaveDictAsHDF5(filename, self.todict())\n\n\n    def todict(self, verify=True):\n        \"\"\"\n        Returns the settings in this object as a Python dictionary.\n        \"\"\"\n        if verify:\n            self.verifySettings()\n\n        data = {}\n        for key, setting in self.settings.items():\n            data[key] = setting.todict(verify=False)\n\n        data['init'] = {}\n\n        if ('eqsysignore' in self.init) and (len(self.init['eqsysignore']) > 0):\n            data['init']['eqsysignore'] = ';'.join(self.init['eqsysignore'])\n\n        if 'timeindex' in self.init:\n            data['init']['filetimeindex'] = self.init['timeindex']\n        if 'fromfile' in self.init:\n            data['init']['fromfile'] = self.init['fromfile']\n\n        return data\n\n\n    def verifySettings(self):\n        \"\"\"\n        Verify that the DREAM run has been correctly configured\n        and that all settings are consistent.\n        \"\"\"\n        for _, setting in self.settings.items():\n            setting.verifySettings()\n\n        if ('fromfile' in self.init) and (self.init['fromfile'] != ''):\n            # Verify that the file exists...\n            if not os.path.exists(self.init['fromfile']):\n                print(\"WARNING: The output file from which to initialize '{}' does not exist.\".format(self.init['fromfile']))\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a self-consistent fluid DREAM run, where no kinetic equations are solved, but the electric field and temperature are evolved self-consistently. The code should import necessary modules and classes from the DREAM library, set up the simulation parameters, set up the radial grid, set the time stepper, add ions, set the electric field and cold electron temperature, set up the hot tail grid, disable the runaway grid, set the solver type and its parameters, and include other necessary settings. The code should then save these settings to an HDF5 file and run the simulation. After the initial run, the code should restart the simulation twice, each time loading the settings from the output of the previous run, adjusting the time stepper, and saving the new settings to an HDF5 file before running the simulation again.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 91, "repo_full_name": "rstudio__py-shiny", "instruction": "Generate code that creates a CPU usage monitoring application using the py-shiny library. The application should be able to run both in a standard Python environment and in a Pyodide environment. It should use the psutil library to get CPU usage data, but if running in Pyodide, it should use a fake version of psutil. The application should display the CPU usage data in a graphical format using matplotlib and in a tabular format using pandas. The user interface should allow the user to select the colormap for the graphs, clear the history of CPU usage data, freeze the output, and specify the number of samples per graph and the number of rows to display in the table. The application should also include a function to hide ticks on a graph's axis.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def table(\n    _fn: TableTransformer.ValueFn | None = None,\n    *,\n    index: bool = False,\n    classes: str = \"table shiny-table w-auto\",\n    border: int = 0,\n    **kwargs: object,\n) -> TableTransformer.OutputRenderer | TableTransformer.OutputRendererDecorator:\n    \"\"\"\n    Reactively render a Pandas data frame object (or similar) as a basic HTML table.\n\n    Consider using ~shiny.render.data_frame instead of this renderer, as it provides\n    high performance virtual scrolling, built-in filtering and sorting, and a better\n    default appearance. This renderer may still be helpful if you use pandas styling\n    features that are not currently supported by ~shiny.render.data_frame.\n\n    Parameters\n    ----------\n    index\n        Whether to print index (row) labels. (Ignored for pandas :class:`Styler`\n        objects; call ``style.hide(axis=\"index\")`` from user code instead.)\n    classes\n        CSS classes (space separated) to apply to the resulting table. By default, we\n        use `table shiny-table w-auto` which is designed to look reasonable with Bootstrap 5.\n        (Ignored for pandas :class:`Styler` objects; call\n        ``style.set_table_attributes('class=\"dataframe table shiny-table w-auto\"')``\n        from user code instead.)\n    **kwargs\n        Additional keyword arguments passed to ``pandas.DataFrame.to_html()`` or\n        ``pandas.io.formats.style.Styler.to_html()``.\n\n    Returns\n    -------\n    :\n        A decorator for a function that returns any of the following:\n\n        1. A pandas :class:`DataFrame` object.\n        2. A pandas :class:`Styler` object.\n        3. Any object that has a `.to_pandas()` method (e.g., a Polars data frame or\n           Arrow table).\n\n    Tip\n    ----\n    This decorator should be applied **before** the ``@output`` decorator. Also, the\n    name of the decorated function (or ``@output(id=...)``) should match the ``id`` of\n    a :func:`~shiny.ui.output_table` container (see :func:`~shiny.ui.output_table` for\n    example usage).\n\n    See Also\n    --------\n    ~shiny.ui.output_table for the corresponding UI component to this render function.\n    \"\"\"\n    return TableTransformer(\n        _fn,\n        TableTransformer.params(\n            index=index,\n            classes=classes,\n            border=border,\n            **kwargs,\n        ),\n    )\n\n# --- Snippet Separator ---\n\ndef data_frame(\n    _fn: DataFrameTransformer.ValueFn | None = None,\n) -> DataFrameTransformer.OutputRenderer | DataFrameTransformer.OutputRendererDecorator:\n    \"\"\"\n    Reactively render a Pandas data frame object (or similar) as an interactive table or\n    grid. Features fast virtualized scrolling, sorting, filtering, and row selection\n    (single or multiple).\n\n    Returns\n    -------\n    :\n        A decorator for a function that returns any of the following:\n\n        1. A :class:`~shiny.render.DataGrid` or :class:`~shiny.render.DataTable` object,\n           which can be used to customize the appearance and behavior of the data frame\n           output.\n        2. A pandas :class:`DataFrame` object. (Equivalent to\n           `shiny.render.DataGrid(df)`.)\n        3. Any object that has a `.to_pandas()` method (e.g., a Polars data frame or\n           Arrow table). (Equivalent to `shiny.render.DataGrid(df.to_pandas())`.)\n\n    Row selection\n    -------------\n    When using the row selection feature, you can access the selected rows by using the\n    `input.<id>_selected_rows()` function, where `<id>` is the `id` of the\n    :func:`~shiny.ui.output_data_frame`. The value returned will be `None` if no rows\n    are selected, or a tuple of integers representing the indices of the selected rows.\n    To filter a pandas data frame down to the selected rows, use\n    `df.iloc[list(input.<id>_selected_rows())]`.\n\n    Tip\n    ----\n    This decorator should be applied **before** the ``@output`` decorator. Also, the\n    name of the decorated function (or ``@output(id=...)``) should match the ``id`` of a\n    :func:`~shiny.ui.output_table` container (see :func:`~shiny.ui.output_table` for\n    example usage).\n\n    See Also\n    --------\n    * :func:`~shiny.ui.output_data_frame`\n    * :class:`~shiny.render.DataGrid` and :class:`~shiny.render.DataTable` are the\n      objects you can return from the rendering function to specify options.\n    \"\"\"\n    return DataFrameTransformer(_fn)\n\n# --- Snippet Separator ---\n\nclass CPUUsage(Metric[float]):\n    \"\"\"\n    The standalone CPU usage metric.\n\n    Instances of this metric compute the average CPU usage as a float value.\n    The metric starts tracking the CPU usage when the `update` method is called\n    for the first time. That is, the tracking does not start at the time the\n    constructor is invoked.\n\n    Calling the `update` method more than twice will update the metric to the\n    average usage between the first and the last call to `update`.\n\n    The result, obtained using the `result` method, is the usage computed\n    as stated above.\n\n    The reset method will bring the metric to its initial state. By default\n    this metric in its initial state will return an usage value of 0.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Creates an instance of the standalone CPU usage metric.\n\n        By default this metric in its initial state will return a CPU usage\n        value of 0. The metric can be updated by using the `update` method\n        while the average CPU usage can be retrieved using the `result` method.\n        \"\"\"\n\n        self._mean_usage = Mean()\n        \"\"\"\n        The mean utility that will be used to store the average usage.\n        \"\"\"\n\n        self._process_handle: Optional[Process] = None\n        \"\"\"\n        The process handle, lazily initialized.\n        \"\"\"\n\n        self._first_update = True\n        \"\"\"\n        An internal flag to keep track of the first call to the `update` method.\n        \"\"\"\n\n    def update(self) -> None:\n        \"\"\"\n        Update the running CPU usage.\n\n        For more info on how to set the starting moment see the class\n        description.\n\n        :return: None.\n        \"\"\"\n        if self._first_update:\n            self._process_handle = Process(os.getpid())\n\n        last_time = getattr(\n            self._process_handle, '_last_sys_cpu_times', None)\n        utilization = self._process_handle.cpu_percent()\n        current_time = getattr(\n            self._process_handle, '_last_sys_cpu_times', None)\n\n        if self._first_update:\n            self._first_update = False\n        else:\n            if current_time is None or last_time is None:\n                warnings.warn('CPUUsage can\\'t detect the elapsed time. It is '\n                              'recommended to update avalanche to the latest '\n                              'version.')\n                # Fallback, shouldn't happen\n                current_time = 1.0\n                last_time = 0.0\n            self._mean_usage.update(utilization, current_time - last_time)\n\n    def result(self) -> float:\n        \"\"\"\n        Retrieves the average CPU usage.\n\n        Calling this method will not change the internal state of the metric.\n\n        :return: The average CPU usage, as a float value.\n        \"\"\"\n        return self._mean_usage.result()\n\n    def reset(self) -> None:\n        \"\"\"\n        Resets the metric.\n\n        :return: None.\n        \"\"\"\n        self._mean_usage.reset()\n        self._process_handle = None\n        self._first_update = True\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a CPU usage monitoring application using the py-shiny library. The application should be able to run both in a standard Python environment and in a Pyodide environment. It should use the psutil library to get CPU usage data, but if running in Pyodide, it should use a fake version of psutil. The application should display the CPU usage data in a graphical format using matplotlib and in a tabular format using pandas. The user interface should allow the user to select the colormap for the graphs, clear the history of CPU usage data, freeze the output, and specify the number of samples per graph and the number of rows to display in the table. The application should also include a function to hide ticks on a graph's axis.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 92, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that uses the PyAEDT library to create an antenna array in HFSS. The code should perform the following tasks:\n\n1. Import necessary modules.\n2. Set the non-graphical mode.\n3. Download a 3D component needed for the example.\n4. Launch HFSS and save the project with a unique name.\n5. Read the array definition from a JSON file and load a 3D component into the dictionary from a specified path.\n6. Set up a simulation and analyze it.\n7. Get far field data after the simulation completes.\n8. Generate a contour plot.\n9. Generate 2D cutout plots.\n10. Generate 3D polar plots in Matplotlib.\n11. Generate 3D plots in PyVista.\n12. Release AEDT at the end.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def create_report(\n        self,\n        expressions,\n        setup_sweep_name=None,\n        domain=\"Sweep\",\n        variations=None,\n        primary_sweep_variable=None,\n        secondary_sweep_variable=None,\n        report_category=None,\n        plot_type=\"Rectangular Plot\",\n        context=None,\n        subdesign_id=None,\n        polyline_points=1001,\n        plotname=None,\n    ):\n        \"\"\"Create a report in AEDT. It can be a 2D plot, 3D plot, polar plots or data tables.\n\n        Parameters\n        ----------\n        expressions : str or list, optional\n            One or more formulas to add to the report. Example is value = ``\"dB(S(1,1))\"``.\n        setup_sweep_name : str, optional\n            Setup name with the sweep. The default is ``\"\"``.\n        domain : str, optional\n            Plot Domain. Options are \"Sweep\" and \"Time\".\n        variations : dict, optional\n            Dictionary of all families including the primary sweep. The default is ``{\"Freq\": [\"All\"]}``.\n        primary_sweep_variable : str, optional\n            Name of the primary sweep. The default is ``\"Freq\"``.\n        secondary_sweep_variable : str, optional\n            Name of the secondary sweep variable in 3D Plots.\n        report_category : str, optional\n            Category of the Report to be created. If `None` default data Report will be used.\n            The Report Category can be one of the types available for creating a report depend on the simulation setup.\n            For example for a Far Field Plot in HFSS the UI shows the report category as \"Create Far Fields Report\".\n            The report category will be in this case \"Far Fields\".\n            Depending on the setup different categories are available.\n            If `None` default category will be used (the first item in the Results drop down menu in AEDT).\n        plot_type : str, optional\n            The format of Data Visualization. Default is ``Rectangular Plot``.\n        context : str, optional\n            The default is ``None``. It can be `None`, `\"Differential Pairs\"` or\n            Reduce Matrix Name for Q2d/Q3d solution or Infinite Sphere name for Far Fields Plot.\n        plotname : str, optional\n            Name of the plot. The default is ``None``.\n        polyline_points : int, optional,\n            Number of points on which create the report for plots on polylines.\n        subdesign_id : int, optional\n            Specify a subdesign ID to export a Touchstone file of this subdesign. Valid for Circuit Only.\n            The default value is ``None``.\n        context : str, optional\n\n        Returns\n        -------\n        :class:`pyaedt.modules.report_templates.Standard`\n            ``True`` when successful, ``False`` when failed.\n\n\n        References\n        ----------\n\n        >>> oModule.CreateReport\n\n        Examples\n        --------\n        >>> from pyaedt import Hfss\n        >>> aedtapp = Hfss()\n        >>> aedtapp.post.create_report(\"dB(S(1,1))\")\n\n        >>> variations = aedtapp.available_variations.nominal_w_values_dict\n        >>> variations[\"Theta\"] = [\"All\"]\n        >>> variations[\"Phi\"] = [\"All\"]\n        >>> variations[\"Freq\"] = [\"30GHz\"]\n        >>> aedtapp.post.create_report(\n        ...    \"db(GainTotal)\",\n        ...    aedtapp.nominal_adaptive,\n        ...    variations=variations,\n        ...    primary_sweep_variable=\"Phi\",\n        ...    secondary_sweep_variable=\"Theta\",\n        ...    plot_type=\"3D Polar Plot\",\n        ...    context=\"3D\",\n        ...    report_category=\"Far Fields\",\n        ...)\n\n        >>> aedtapp.post.create_report(\n        ...    \"S(1,1)\",\n        ...    aedtapp.nominal_sweep,\n        ...    variations=variations,\n        ...    plot_type=\"Smith Chart\",\n        ...)\n\n        >>> from pyaedt import Maxwell2d\n        >>> maxwell_2d = Maxwell2d()\n        >>> maxwell_2d.post.create_report(\n        ...     \"InputCurrent(PHA)\", domain=\"Time\", primary_sweep_variable=\"Time\", plotname=\"Winding Plot 1\"\n        ... )\n        \"\"\"\n        if domain in [\"Spectral\", \"Spectrum\"]:\n            report_category = \"Spectrum\"\n        elif not report_category and not self._app.design_solutions.report_type:\n            self.logger.error(\"Solution not supported\")\n            return False\n        elif not report_category:\n            report_category = self._app.design_solutions.report_type\n        if report_category in TEMPLATES_BY_NAME:\n            report_class = TEMPLATES_BY_NAME[report_category]\n        elif \"Fields\" in report_category:\n            report_class = TEMPLATES_BY_NAME[\"Fields\"]\n        else:\n            report_class = TEMPLATES_BY_NAME[\"Standard\"]\n        if not setup_sweep_name:\n            setup_sweep_name = self._app.nominal_sweep\n        report = report_class(self, report_category, setup_sweep_name)\n        report.expressions = expressions\n        report.domain = domain\n        if primary_sweep_variable:\n            report.primary_sweep = primary_sweep_variable\n        if secondary_sweep_variable:\n            report.secondary_sweep = secondary_sweep_variable\n        if variations:\n            report.variations = variations\n        report.report_type = plot_type\n        report.sub_design_id = subdesign_id\n        report.point_number = polyline_points\n        if context == \"Differential Pairs\":\n            report.differential_pairs = True\n        elif self._app.design_type in [\"Q3D Extractor\", \"2D Extractor\"] and context:\n            report.matrix = context\n        elif report_category == \"Far Fields\":\n            if not context and self._app._field_setups:\n                report.far_field_sphere = self._app.field_setups[0].name\n            else:\n                report.far_field_sphere = context\n        elif report_category == \"Near Fields\":\n            report.near_field = context\n        elif context:\n            if context in self.modeler.line_names:\n                report.polyline = context\n\n        result = report.create(plotname)\n        if result:\n            return report\n        return False\n\n# --- Snippet Separator ---\n\ndef create_sbr_file_based_antenna(\n        self,\n        ffd_full_path,\n        antenna_size=\"1mm\",\n        antenna_impedance=\"50ohm\",\n        representation_type=\"Far Field\",\n        target_cs=None,\n        model_units=None,\n        antenna_name=None,\n    ):\n        \"\"\"Create a linked antenna.\n\n        Parameters\n        ----------\n        ffd_full_path : str\n            Full path to the FFD file.\n        antenna_size : str, optional\n            Antenna size with units. The default is ``\"1mm\"``.\n        antenna_impedance : str, optional\n            Antenna impedance with units. The default is ``\"50ohm\"``.\n        representation_type : str, optional\n            Type of the antenna. Options are ``\"Far Field\"`` and ``\"Near Field\"``.\n            The default is ``\"Far Field\"``.\n        target_cs : str, optional\n            Target coordinate system. The default is ``None``, in which case the\n            active coordinate system is used.\n        model_units : str, optional\n            Model units to apply to the object. The default is\n            ``None``, in which case the active modeler units are applied.\n        antenna_name : str, optional\n            Name of the 3D component. The default is ``None``, in which case\n            the name is auto-generated based on the antenna type.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.NativeComponentObject`\n\n        References\n        ----------\n\n        >>> oEditor.InsertNativeComponent\n\n        Examples\n        --------\n        >>> from pyaedt import Hfss\n        >>> hfss = Hfss(solution_type=\"SBR+\")  # doctest: +SKIP\n        >>> ffd_file = \"full_path/to/ffdfile.ffd\"\n        >>> par_beam = hfss.create_sbr_file_based_antenna(ffd_file)  # doctest: +SKIP\n\n        \"\"\"\n        if self.solution_type != \"SBR+\":\n            self.logger.error(\"This native component only applies to a SBR+ solution.\")\n            return False\n        if target_cs is None:\n            target_cs = self.modeler.oeditor.GetActiveCoordinateSystem()\n\n        par_dicts = OrderedDict(\n            {\n                \"Size\": antenna_size,\n                \"MatchedPortImpedance\": antenna_impedance,\n                \"Representation\": representation_type,\n                \"ExternalFile\": ffd_full_path,\n            }\n        )\n        if not antenna_name:\n            antenna_name = generate_unique_name(os.path.basename(ffd_full_path).split(\".\")[0])\n\n        return self._create_native_component(\"File Based Antenna\", target_cs, model_units, par_dicts, antenna_name)\n\n# --- Snippet Separator ---\n\nclass Maxwell3d(Maxwell, FieldAnalysis3D, object):\n    \"\"\"Provides the Maxwell 3D application interface.\n\n    This class allows you to connect to an existing Maxwell 3D design or create a\n    new Maxwell 3D design if one does not exist.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open. The default is ``None``, in which\n        case an attempt is made to get an active project. If no\n        projects are present, an empty project is created.\n    designname : str, optional\n        Name of the design to select. The default is ``None``, in\n        which case an attempt is made to get an active design. If no\n        designs are present, an empty design is created.\n    solution_type : str, optional\n        Solution type to apply to the design. The default is\n        ``None``, in which case the default type is applied.\n    setup_name : str, optional\n        Name of the setup to use as the nominal. The default is\n        ``None``, in which case the active setup is used or\n        nothing is used.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``, in which case\n        the active version or latest installed version is used. This\n        parameter is ignored when Script is launched within AEDT.\n    non_graphical : bool, optional\n        Whether to launch AEDT in non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in graphical\n        mode. This parameter is ignored when a script is launched within\n        AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine. The default is ``True``. This parameter is ignored\n        when Script is launched within AEDT.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit. The default is ``False``.\n    student_version : bool, optional\n        Whether to open the AEDT student version. The default is\n        ``False``. This parameter is ignored when Script is launched\n        within AEDT.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of Maxwell 3D and open the specified\n    project, which is named ``mymaxwell.aedt``.\n\n    >>> from pyaedt import Maxwell3d\n    >>> aedtapp = Maxwell3d(\"mymaxwell.aedt\")\n    pyaedt info: Added design ...\n\n    Create an instance of Maxwell 3D using the 2021 R1 release and open\n    the specified project, which is named ``mymaxwell2.aedt``.\n\n    >>> aedtapp = Maxwell3d(specified_version=\"2021.2\", projectname=\"mymaxwell2.aedt\")\n    pyaedt info: Added design ...\n\n    \"\"\"\n\n    @property  # for legacy purposes\n    def dim(self):\n        \"\"\"Dimensions.\"\"\"\n        return \"3D\"\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        setup_name=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=False,\n        close_on_exit=False,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n        \"\"\"\n        Initialize the ``Maxwell`` class.\n        \"\"\"\n        self.is3d = True\n        FieldAnalysis3D.__init__(\n            self,\n            \"Maxwell 3D\",\n            projectname,\n            designname,\n            solution_type,\n            setup_name,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine,\n            port,\n            aedt_process_id,\n        )\n        Maxwell.__init__(self)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the PyAEDT library to create an antenna array in HFSS. The code should perform the following tasks:\n\n1. Import necessary modules.\n2. Set the non-graphical mode.\n3. Download a 3D component needed for the example.\n4. Launch HFSS and save the project with a unique name.\n5. Read the array definition from a JSON file and load a 3D component into the dictionary from a specified path.\n6. Set up a simulation and analyze it.\n7. Get far field data after the simulation completes.\n8. Generate a contour plot.\n9. Generate 2D cutout plots.\n10. Generate 3D polar plots in Matplotlib.\n11. Generate 3D plots in PyVista.\n12. Release AEDT at the end.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 93, "repo_full_name": "burnysc2__python-sc2", "instruction": "Generate code that creates a StarCraft II bot using the python-sc2 library. The bot should be designed to play as the Protoss race and follow a strategy of building three bases and three stargates. The bot should manage resources, build structures, train units, and engage in combat. It should use chrono boost on active nexuses, attack with all workers if no nexuses are left, and attack the enemy with void rays when there are more than five. The bot should also manage worker distribution, build pylons when supply is low, train probes on undersaturated nexuses, expand when there are less than three nexuses, build a gateway and a cybernetics core, build gas near completed nexuses, and build stargates when there are less than three but at least three nexuses. The bot should also train void rays at idle stargates when there are at least three townhalls. The bot should be run on the \"(2)CatalystLE\" map against an easy difficulty Protoss computer opponent.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class BotAI(DistanceCalculation):\n    \"\"\"Base class for bots.\"\"\"\n\n    EXPANSION_GAP_THRESHOLD = 15\n\n    def _initialize_variables(self):\n        \"\"\" Called from main.py internally \"\"\"\n        DistanceCalculation.__init__(self)\n        # Specific opponent bot ID used in sc2ai ladder games http://sc2ai.net/ and on ai arena https://aiarena.net\n        # The bot ID will stay the same each game so your bot can \"adapt\" to the opponent\n        if not hasattr(self, \"opponent_id\"):\n            # Prevent overwriting the opponent_id which is set here https://github.com/Hannessa/python-sc2-ladderbot/blob/master/__init__.py#L40\n            # otherwise set it to None\n            self.opponent_id: str = None\n        # Select distance calculation method, see distances.py: _distances_override_functions function\n        if not hasattr(self, \"distance_calculation_method\"):\n            self.distance_calculation_method: int = 2\n        # Select if the Unit.command should return UnitCommand objects. Set this to True if your bot uses 'self.do(unit(ability, target))'\n        if not hasattr(self, \"unit_command_uses_self_do\"):\n            self.unit_command_uses_self_do: bool = False\n        # This value will be set to True by main.py in self._prepare_start if game is played in realtime (if true, the bot will have limited time per step)\n        self.realtime: bool = False\n        self.base_build: int = -1\n        self.all_units: Units = Units([], self)\n        self.units: Units = Units([], self)\n        self.workers: Units = Units([], self)\n        self.larva: Units = Units([], self)\n        self.structures: Units = Units([], self)\n        self.townhalls: Units = Units([], self)\n        self.gas_buildings: Units = Units([], self)\n        self.all_own_units: Units = Units([], self)\n        self.enemy_units: Units = Units([], self)\n        self.enemy_structures: Units = Units([], self)\n        self.all_enemy_units: Units = Units([], self)\n        self.resources: Units = Units([], self)\n        self.destructables: Units = Units([], self)\n        self.watchtowers: Units = Units([], self)\n        self.mineral_field: Units = Units([], self)\n        self.vespene_geyser: Units = Units([], self)\n        self.placeholders: Units = Units([], self)\n        self.techlab_tags: Set[int] = set()\n        self.reactor_tags: Set[int] = set()\n        self.minerals: int = 50\n        self.vespene: int = 0\n        self.supply_army: float = 0\n        self.supply_workers: float = 12  # Doesn't include workers in production\n        self.supply_cap: float = 15\n        self.supply_used: float = 12\n        self.supply_left: float = 3\n        self.idle_worker_count: int = 0\n        self.army_count: int = 0\n        self.warp_gate_count: int = 0\n        self.actions: List[UnitCommand] = []\n        self.blips: Set[Blip] = set()\n        self.race: Race = None\n        self.enemy_race: Race = None\n        self._units_created: Counter = Counter()\n        self._unit_tags_seen_this_game: Set[int] = set()\n        self._units_previous_map: Dict[int, Unit] = {}\n        self._structures_previous_map: Dict[int, Unit] = {}\n        self._enemy_units_previous_map: Dict[int, Unit] = {}\n        self._enemy_structures_previous_map: Dict[int, Unit] = {}\n        self._all_units_previous_map: Dict[int, Unit] = {}\n        self._previous_upgrades: Set[UpgradeId] = set()\n        self._expansion_positions_list: List[Point2] = []\n        self._resource_location_to_expansion_position_dict: Dict[Point2, Point2] = {}\n        self._time_before_step: float = None\n        self._time_after_step: float = None\n        self._min_step_time: float = math.inf\n        self._max_step_time: float = 0\n        self._last_step_step_time: float = 0\n        self._total_time_in_on_step: float = 0\n        self._total_steps_iterations: int = 0\n        # Internally used to keep track which units received an action in this frame, so that self.train() function does not give the same larva two orders - cleared every frame\n        self.unit_tags_received_action: Set[int] = set()\n\n    @property\n    def time(self) -> float:\n        \"\"\" Returns time in seconds, assumes the game is played on 'faster' \"\"\"\n        return self.state.game_loop / 22.4  # / (1/1.4) * (1/16)\n\n    @property\n    def time_formatted(self) -> str:\n        \"\"\" Returns time as string in min:sec format \"\"\"\n        t = self.time\n        return f\"{int(t // 60):02}:{int(t % 60):02}\"\n\n    @property\n    def step_time(self) -> Tuple[float, float, float, float]:\n        \"\"\"Returns a tuple of step duration in milliseconds.\n        First value is the minimum step duration - the shortest the bot ever took\n        Second value is the average step duration\n        Third value is the maximum step duration - the longest the bot ever took (including on_start())\n        Fourth value is the step duration the bot took last iteration\n        If called in the first iteration, it returns (inf, 0, 0, 0)\"\"\"\n        avg_step_duration = (\n            (self._total_time_in_on_step / self._total_steps_iterations) if self._total_steps_iterations else 0\n        )\n        return (\n            self._min_step_time * 1000,\n            avg_step_duration * 1000,\n            self._max_step_time * 1000,\n            self._last_step_step_time * 1000,\n        )\n\n    @property\n    def game_info(self) -> GameInfo:\n        \"\"\" See game_info.py \"\"\"\n        return self._game_info\n\n    @property\n    def game_data(self) -> GameData:\n        \"\"\" See game_data.py \"\"\"\n        return self._game_data\n\n    @property\n    def client(self) -> Client:\n        \"\"\" See client.py \"\"\"\n        return self._client\n\n    @property\n    def larva_count(self):\n        \"\"\" Replacement for self.state.common.larva_count https://github.com/Blizzard/s2client-proto/blob/d3d18392f9d7c646067d447df0c936a8ca57d587/s2clientprotocol/sc2api.proto#L614 \"\"\"\n        warnings.warn(\n            \"self.larva_count will be removed soon, please use len(self.larva) or self.larva.amount instead\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return len(self.larva)\n\n    def alert(self, alert_code: Alert) -> bool:\n        \"\"\"\n        Check if alert is triggered in the current step.\n        Possible alerts are listed here https://github.com/Blizzard/s2client-proto/blob/e38efed74c03bec90f74b330ea1adda9215e655f/s2clientprotocol/sc2api.proto#L679-L702\n\n        Example use::\n\n            from sc2.data import Alert\n            if self.alert(Alert.AddOnComplete):\n                print(\"Addon Complete\")\n\n        Alert codes::\n\n            AlertError\n            AddOnComplete\n            BuildingComplete\n            BuildingUnderAttack\n            LarvaHatched\n            MergeComplete\n            MineralsExhausted\n            MorphComplete\n            MothershipComplete\n            MULEExpired\n            NuclearLaunchDetected\n            NukeComplete\n            NydusWormDetected\n            ResearchComplete\n            TrainError\n            TrainUnitComplete\n            TrainWorkerComplete\n            TransformationComplete\n            UnitUnderAttack\n            UpgradeComplete\n            VespeneExhausted\n            WarpInComplete\n\n        :param alert_code:\n        \"\"\"\n        assert isinstance(alert_code, Alert), f\"alert_code {alert_code} is no Alert\"\n        return alert_code.value in self.state.alerts\n\n    @property\n    def start_location(self) -> Point2:\n        \"\"\"\n        Returns the spawn location of the bot, using the position of the first created townhall.\n        This will be None if the bot is run on an arcade or custom map that does not feature townhalls at game start.\n        \"\"\"\n        return self._game_info.player_start_location\n\n    @property\n    def enemy_start_locations(self) -> List[Point2]:\n        \"\"\"Possible start locations for enemies.\"\"\"\n        return self._game_info.start_locations\n\n    @property\n    def main_base_ramp(self) -> Ramp:\n        \"\"\"Returns the Ramp instance of the closest main-ramp to start location.\n        Look in game_info.py for more information about the Ramp class\n\n        Example: See terran ramp wall bot\n        \"\"\"\n        if hasattr(self, \"cached_main_base_ramp\"):\n            return self.cached_main_base_ramp\n        # The reason for len(ramp.upper) in {2, 5} is:\n        # ParaSite map has 5 upper points, and most other maps have 2 upper points at the main ramp.\n        # The map Acolyte has 4 upper points at the wrong ramp (which is closest to the start position).\n        try:\n            self.cached_main_base_ramp = min(\n                (ramp for ramp in self.game_info.map_ramps if len(ramp.upper) in {2, 5}),\n                key=lambda r: self.start_location.distance_to(r.top_center),\n            )\n        except ValueError:\n            # Hardcoded hotfix for Honorgrounds LE map, as that map has a large main base ramp with inbase natural\n            self.cached_main_base_ramp = min(\n                (ramp for ramp in self.game_info.map_ramps if len(ramp.upper) in {4, 9}),\n                key=lambda r: self.start_location.distance_to(r.top_center),\n            )\n        return self.cached_main_base_ramp\n\n    @property_cache_once_per_frame\n    def expansion_locations_list(self) -> List[Point2]:\n        \"\"\" Returns a list of expansion positions, not sorted in any way. \"\"\"\n        assert (\n            self._expansion_positions_list\n        ), f\"self._find_expansion_locations() has not been run yet, so accessing the list of expansion locations is pointless.\"\n        return self._expansion_positions_list\n\n    @property_cache_once_per_frame\n    def expansion_locations_dict(self) -> Dict[Point2, Units]:\n        \"\"\"\n        Returns dict with the correct expansion position Point2 object as key,\n        resources as Units (mineral fields and vespene geysers) as value.\n\n        Caution: This function is slow. If you only need the expansion locations, use the property above.\n        \"\"\"\n        assert (\n            self._expansion_positions_list\n        ), f\"self._find_expansion_locations() has not been run yet, so accessing the list of expansion locations is pointless.\"\n        expansion_locations: Dict[Point2, Units] = {pos: Units([], self) for pos in self._expansion_positions_list}\n        for resource in self.resources:\n            # It may be that some resources are not mapped to an expansion location\n            exp_position: Point2 = self._resource_location_to_expansion_position_dict.get(resource.position, None)\n            if exp_position:\n                assert exp_position in expansion_locations\n                expansion_locations[exp_position].append(resource)\n        return expansion_locations\n\n    # Deprecated\n    @property_cache_once_per_frame\n    def expansion_locations(self) -> Dict[Point2, Units]:\n        \"\"\" Same as the function above. \"\"\"\n        assert (\n            self._expansion_positions_list\n        ), f\"self._find_expansion_locations() has not been run yet, so accessing the list of expansion locations is pointless.\"\n        warnings.warn(\n            f\"You are using 'self.expansion_locations', please use 'self.expansion_locations_list' (fast) or 'self.expansion_locations_dict' (slow) instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        return self.expansion_locations_dict\n\n    def _find_expansion_locations(self):\n        \"\"\" Ran once at the start of the game to calculate expansion locations. \"\"\"\n        # Idea: create a group for every resource, then merge these groups if\n        # any resource in a group is closer than a threshold to any resource of another group\n\n        # Distance we group resources by\n        resource_spread_threshold: float = 8.5\n        geysers: Units = self.vespene_geyser\n        # Create a group for every resource\n        resource_groups: List[List[Unit]] = [\n            [resource] for resource in self.resources\n            if resource.name != \"MineralField450\"  # dont use low mineral count patches\n        ]\n        # Loop the merging process as long as we change something\n        merged_group = True\n        while merged_group:\n            merged_group = False\n            # Check every combination of two groups\n            for group_a, group_b in itertools.combinations(resource_groups, 2):\n                # Check if any pair of resource of these groups is closer than threshold together\n                if any(\n                    resource_a.distance_to(resource_b) <= resource_spread_threshold\n                    for resource_a, resource_b in itertools.product(group_a, group_b)\n                ):\n                    # Remove the single groups and add the merged group\n                    resource_groups.remove(group_a)\n                    resource_groups.remove(group_b)\n                    resource_groups.append(group_a + group_b)\n                    merged_group = True\n                    break\n        # Distance offsets we apply to center of each resource group to find expansion position\n        offset_range = 7\n        offsets = [\n            (x, y) for x, y in itertools.product(range(-offset_range, offset_range + 1), repeat=2)\n            if 4 < math.hypot(x, y) <= 8\n        ]\n        # Dict we want to return\n        centers = {}\n        # For every resource group:\n        for resources in resource_groups:\n            # Possible expansion points\n            amount = len(resources)\n            # Calculate center, round and add 0.5 because expansion location will have (x.5, y.5)\n            # coordinates because bases have size 5.\n            center_x = int(sum(resource.position.x for resource in resources) / amount) + 0.5\n            center_y = int(sum(resource.position.y for resource in resources) / amount) + 0.5\n            possible_points = (Point2((offset[0] + center_x, offset[1] + center_y)) for offset in offsets)\n            # Filter out points that are too near\n            possible_points = (\n                point for point in possible_points\n                # Check if point can be built on\n                if self._game_info.placement_grid[point.rounded] == 1\n                # Check if all resources have enough space to point\n                and all(\n                    point.distance_to(resource) >= (7 if resource._proto.unit_type in geyser_ids else 6)\n                    for resource in resources\n                )\n            )\n            # Choose best fitting point\n            result: Point2 = min(\n                possible_points, key=lambda point: sum(point.distance_to(resource) for resource in resources)\n            )\n            centers[result] = resources\n            # Put all expansion locations in a list\n            self._expansion_positions_list.append(result)\n            # Maps all resource positions to the expansion position\n            for resource in resources:\n                self._resource_location_to_expansion_position_dict[resource.position] = result\n\n    @property\n    def units_created(self) -> Counter:\n        \"\"\"Returns a Counter for all your units and buildings you have created so far.\n\n        This may be used for statistics (at the end of the game) or for strategic decision making.\n\n        CAUTION: This does not properly work at the moment for morphing units and structures. Please use the 'on_unit_type_changed' event to add these morphing unit types manually to 'self._units_created'.\n        Issues would arrise in e.g. siege tank morphing to sieged tank, and then morphing back (suddenly the counter counts 2 tanks have been created).\n\n        Examples::\n\n            # Give attack command to enemy base every time 10 marines have been trained\n            async def on_unit_created(self, unit: Unit):\n                if unit.type_id == UnitTypeId.MARINE:\n                    if self.units_created[MARINE] % 10 == 0:\n                        for marine in self.units(UnitTypeId.MARINE):\n                            marine.attack(self.enemy_start_locations[0])\n        \"\"\"\n        return self._units_created\n\n    def _correct_zerg_supply(self):\n        \"\"\"The client incorrectly rounds zerg supply down instead of up (see\n        https://github.com/Blizzard/s2client-proto/issues/123), so self.supply_used\n        and friends return the wrong value when there are an odd number of zerglings\n        and banelings. This function corrects the bad values.\"\"\"\n        # TODO: remove when Blizzard/sc2client-proto#123 gets fixed.\n        half_supply_units = {\n            UnitTypeId.ZERGLING,\n            UnitTypeId.ZERGLINGBURROWED,\n            UnitTypeId.BANELING,\n            UnitTypeId.BANELINGBURROWED,\n            UnitTypeId.BANELINGCOCOON,\n        }\n        correction = self.units(half_supply_units).amount % 2\n        self.supply_used += correction\n        self.supply_army += correction\n        self.supply_left -= correction\n\n    async def get_available_abilities(\n        self, units: Union[List[Unit], Units], ignore_resource_requirements: bool = False\n    ) -> List[List[AbilityId]]:\n        \"\"\"Returns available abilities of one or more units. Right now only checks cooldown, energy cost, and whether the ability has been researched.\n\n        Examples::\n\n            units_abilities = await self.get_available_abilities(self.units)\n\n        or::\n\n            units_abilities = await self.get_available_abilities([self.units.random])\n\n        :param units:\n        :param ignore_resource_requirements:\"\"\"\n        return await self._client.query_available_abilities(units, ignore_resource_requirements)\n\n    async def expand_now(\n        self, building: UnitTypeId = None, max_distance: float = 10, location: Optional[Point2] = None\n    ):\n        \"\"\"Finds the next possible expansion via 'self.get_next_expansion()'. If the target expansion is blocked (e.g. an enemy unit), it will misplace the expansion.\n\n        :param building:\n        :param max_distance:\n        :param location:\"\"\"\n\n        if not building:\n            # self.race is never Race.Random\n            start_townhall_type = {\n                Race.Protoss: UnitTypeId.NEXUS,\n                Race.Terran: UnitTypeId.COMMANDCENTER,\n                Race.Zerg: UnitTypeId.HATCHERY,\n            }\n            building = start_townhall_type[self.race]\n\n        assert isinstance(building, UnitTypeId), f\"{building} is no UnitTypeId\"\n\n        if not location:\n            location = await self.get_next_expansion()\n        if not location:\n            # All expansions are used up or mined out\n            logger.warning(\"Trying to expand_now() but bot is out of locations to expand to\")\n            return\n        await self.build(building, near=location, max_distance=max_distance, random_alternative=False, placement_step=1)\n\n    async def get_next_expansion(self) -> Optional[Point2]:\n        \"\"\"Find next expansion location.\"\"\"\n\n        closest = None\n        distance = math.inf\n        for el in self.expansion_locations_list:\n\n            def is_near_to_expansion(t):\n                return t.distance_to(el) < self.EXPANSION_GAP_THRESHOLD\n\n            if any(map(is_near_to_expansion, self.townhalls)):\n                # already taken\n                continue\n\n            startp = self._game_info.player_start_location\n            d = await self._client.query_pathing(startp, el)\n            if d is None:\n                continue\n\n            if d < distance:\n                distance = d\n                closest = el\n\n        return closest\n\n    async def distribute_workers(self, resource_ratio: float = 2):\n        \"\"\"\n        Distributes workers across all the bases taken.\n        Keyword `resource_ratio` takes a float. If the current minerals to gas\n        ratio is bigger than `resource_ratio`, this function prefer filling gas_buildings\n        first, if it is lower, it will prefer sending workers to minerals first.\n\n        NOTE: This function is far from optimal, if you really want to have\n        refined worker control, you should write your own distribution function.\n        For example long distance mining control and moving workers if a base was killed\n        are not being handled.\n\n        WARNING: This is quite slow when there are lots of workers or multiple bases.\n\n        :param resource_ratio:\"\"\"\n        if not self.mineral_field or not self.workers or not self.townhalls.ready:\n            return\n        worker_pool = [worker for worker in self.workers.idle]\n        bases = self.townhalls.ready\n        gas_buildings = self.gas_buildings.ready\n\n        # list of places that need more workers\n        deficit_mining_places = []\n\n        for mining_place in bases | gas_buildings:\n            difference = mining_place.surplus_harvesters\n            # perfect amount of workers, skip mining place\n            if not difference:\n                continue\n            if mining_place.has_vespene:\n                # get all workers that target the gas extraction site\n                # or are on their way back from it\n                local_workers = self.workers.filter(\n                    lambda unit: unit.order_target == mining_place.tag or\n                    (unit.is_carrying_vespene and unit.order_target == bases.closest_to(mining_place).tag)\n                )\n            else:\n                # get tags of minerals around expansion\n                local_minerals_tags = {\n                    mineral.tag\n                    for mineral in self.mineral_field if mineral.distance_to(mining_place) <= 8\n                }\n                # get all target tags a worker can have\n                # tags of the minerals he could mine at that base\n                # get workers that work at that gather site\n                local_workers = self.workers.filter(\n                    lambda unit: unit.order_target in local_minerals_tags or\n                    (unit.is_carrying_minerals and unit.order_target == mining_place.tag)\n                )\n            # too many workers\n            if difference > 0:\n                for worker in local_workers[:difference]:\n                    worker_pool.append(worker)\n            # too few workers\n            # add mining place to deficit bases for every missing worker\n            else:\n                deficit_mining_places += [mining_place for _ in range(-difference)]\n\n        # prepare all minerals near a base if we have too many workers\n        # and need to send them to the closest patch\n        if len(worker_pool) > len(deficit_mining_places):\n            all_minerals_near_base = [\n                mineral for mineral in self.mineral_field\n                if any(mineral.distance_to(base) <= 8 for base in self.townhalls.ready)\n            ]\n        # distribute every worker in the pool\n        for worker in worker_pool:\n            # as long as have workers and mining places\n            if deficit_mining_places:\n                # choose only mineral fields first if current mineral to gas ratio is less than target ratio\n                if self.vespene and self.minerals / self.vespene < resource_ratio:\n                    possible_mining_places = [place for place in deficit_mining_places if not place.vespene_contents]\n                # else prefer gas\n                else:\n                    possible_mining_places = [place for place in deficit_mining_places if place.vespene_contents]\n                # if preferred type is not available any more, get all other places\n                if not possible_mining_places:\n                    possible_mining_places = deficit_mining_places\n                # find closest mining place\n                current_place = min(deficit_mining_places, key=lambda place: place.distance_to(worker))\n                # remove it from the list\n                deficit_mining_places.remove(current_place)\n                # if current place is a gas extraction site, go there\n                if current_place.vespene_contents:\n                    worker.gather(current_place)\n                # if current place is a gas extraction site,\n                # go to the mineral field that is near and has the most minerals left\n                else:\n                    local_minerals = (\n                        mineral for mineral in self.mineral_field if mineral.distance_to(current_place) <= 8\n                    )\n                    # local_minerals can be empty if townhall is misplaced\n                    target_mineral = max(local_minerals, key=lambda mineral: mineral.mineral_contents, default=None)\n                    if target_mineral:\n                        worker.gather(target_mineral)\n            # more workers to distribute than free mining spots\n            # send to closest if worker is doing nothing\n            elif worker.is_idle and all_minerals_near_base:\n                target_mineral = min(all_minerals_near_base, key=lambda mineral: mineral.distance_to(worker))\n                worker.gather(target_mineral)\n            else:\n                # there are no deficit mining places and worker is not idle\n                # so dont move him\n                pass\n\n    @property\n    def owned_expansions(self) -> Dict[Point2, Unit]:\n        \"\"\"List of expansions owned by the player.\"\"\"\n        owned = {}\n        for el in self.expansion_locations_list:\n\n            def is_near_to_expansion(t):\n                return t.distance_to(el) < self.EXPANSION_GAP_THRESHOLD\n\n            th = next((x for x in self.townhalls if is_near_to_expansion(x)), None)\n            if th:\n                owned[el] = th\n        return owned\n\n    def calculate_supply_cost(self, unit_type: UnitTypeId) -> float:\n        \"\"\"\n        This function calculates the required supply to train or morph a unit.\n        The total supply of a baneling is 0.5, but a zergling already uses up 0.5 supply, so the morph supply cost is 0.\n        The total supply of a ravager is 3, but a roach already uses up 2 supply, so the morph supply cost is 1.\n        The required supply to build zerglings is 1 because they pop in pairs, so this function returns 1 because the larva morph command requires 1 free supply.\n\n        Example::\n\n            roach_supply_cost = self.calculate_supply_cost(UnitTypeId.ROACH) # Is 2\n            ravager_supply_cost = self.calculate_supply_cost(UnitTypeId.RAVAGER) # Is 1\n            baneling_supply_cost = self.calculate_supply_cost(UnitTypeId.BANELING) # Is 0\n\n        :param unit_type:\"\"\"\n        if unit_type in {UnitTypeId.ZERGLING}:\n            return 1\n        unit_supply_cost = self._game_data.units[unit_type.value]._proto.food_required\n        if unit_supply_cost > 0 and unit_type in UNIT_TRAINED_FROM and len(UNIT_TRAINED_FROM[unit_type]) == 1:\n            producer: UnitTypeId\n            for producer in UNIT_TRAINED_FROM[unit_type]:\n                producer_unit_data = self.game_data.units[producer.value]\n                if producer_unit_data._proto.food_required <= unit_supply_cost:\n                    producer_supply_cost = producer_unit_data._proto.food_required\n                    unit_supply_cost -= producer_supply_cost\n        return unit_supply_cost\n\n    def can_feed(self, unit_type: UnitTypeId) -> bool:\n        \"\"\"Checks if you have enough free supply to build the unit\n\n        Example::\n\n            cc = self.townhalls.idle.random_or(None)\n            # self.townhalls can be empty or there are no idle townhalls\n            if cc and self.can_feed(UnitTypeId.SCV):\n                cc.train(UnitTypeId.SCV)\n\n        :param unit_type:\"\"\"\n        required = self.calculate_supply_cost(unit_type)\n        # \"required <= 0\" in case self.supply_left is negative\n        return required <= 0 or self.supply_left >= required\n\n    def calculate_unit_value(self, unit_type: UnitTypeId) -> Cost:\n        \"\"\"\n        Unlike the function below, this function returns the value of a unit given by the API (e.g. the resources lost value on kill).\n\n        Examples::\n\n            self.calculate_value(UnitTypeId.ORBITALCOMMAND) == Cost(550, 0)\n            self.calculate_value(UnitTypeId.RAVAGER) == Cost(100, 100)\n            self.calculate_value(UnitTypeId.ARCHON) == Cost(175, 275)\n\n        :param unit_type:\n        \"\"\"\n        unit_data = self.game_data.units[unit_type.value]\n        return Cost(unit_data._proto.mineral_cost, unit_data._proto.vespene_cost)\n\n    def calculate_cost(self, item_id: Union[UnitTypeId, UpgradeId, AbilityId]) -> Cost:\n        \"\"\"\n        Calculate the required build, train or morph cost of a unit. It is recommended to use the UnitTypeId instead of the ability to create the unit.\n        The total cost to create a ravager is 100/100, but the actual morph cost from roach to ravager is only 25/75, so this function returns 25/75.\n\n        It is adviced to use the UnitTypeId instead of the AbilityId. Instead of::\n\n            self.calculate_cost(AbilityId.UPGRADETOORBITAL_ORBITALCOMMAND)\n\n        use::\n\n            self.calculate_cost(UnitTypeId.ORBITALCOMMAND)\n\n        More examples::\n\n            from sc2.game_data import Cost\n\n            self.calculate_cost(UnitTypeId.BROODLORD) == Cost(150, 150)\n            self.calculate_cost(UnitTypeId.RAVAGER) == Cost(25, 75)\n            self.calculate_cost(UnitTypeId.BANELING) == Cost(25, 25)\n            self.calculate_cost(UnitTypeId.ORBITALCOMMAND) == Cost(150, 0)\n            self.calculate_cost(UnitTypeId.REACTOR) == Cost(50, 50)\n            self.calculate_cost(UnitTypeId.TECHLAB) == Cost(50, 25)\n            self.calculate_cost(UnitTypeId.QUEEN) == Cost(150, 0)\n            self.calculate_cost(UnitTypeId.HATCHERY) == Cost(300, 0)\n            self.calculate_cost(UnitTypeId.LAIR) == Cost(150, 100)\n            self.calculate_cost(UnitTypeId.HIVE) == Cost(200, 150)\n\n        :param item_id:\n        \"\"\"\n        if isinstance(item_id, UnitTypeId):\n            # Fix cost for reactor and techlab where the API returns 0 for both\n            if item_id in {UnitTypeId.REACTOR, UnitTypeId.TECHLAB, UnitTypeId.ARCHON}:\n                if item_id == UnitTypeId.REACTOR:\n                    return Cost(50, 50)\n                elif item_id == UnitTypeId.TECHLAB:\n                    return Cost(50, 25)\n                elif item_id == UnitTypeId.ARCHON:\n                    return self.calculate_unit_value(UnitTypeId.ARCHON)\n            unit_data = self._game_data.units[item_id.value]\n            # Cost of morphs is automatically correctly calculated by 'calculate_ability_cost'\n            return self._game_data.calculate_ability_cost(unit_data.creation_ability)\n\n        elif isinstance(item_id, UpgradeId):\n            cost = self._game_data.upgrades[item_id.value].cost\n        else:\n            # Is already AbilityId\n            cost = self._game_data.calculate_ability_cost(item_id)\n        return cost\n\n    def can_afford(self, item_id: Union[UnitTypeId, UpgradeId, AbilityId], check_supply_cost: bool = True) -> bool:\n        \"\"\"Tests if the player has enough resources to build a unit or structure.\n\n        Example::\n\n            cc = self.townhalls.idle.random_or(None)\n            # self.townhalls can be empty or there are no idle townhalls\n            if cc and self.can_afford(UnitTypeId.SCV):\n                cc.train(UnitTypeId.SCV)\n\n        Example::\n\n            # Current state: we have 150 minerals and one command center and a barracks\n            can_afford_morph = self.can_afford(UnitTypeId.ORBITALCOMMAND, check_supply_cost=False)\n            # Will be 'True' although the API reports that an orbital is worth 550 minerals, but the morph cost is only 150 minerals\n\n        :param item_id:\n        :param check_supply_cost:\"\"\"\n        cost = self.calculate_cost(item_id)\n        if cost.minerals > self.minerals or cost.vespene > self.vespene:\n            return False\n        if check_supply_cost and isinstance(item_id, UnitTypeId):\n            supply_cost = self.calculate_supply_cost(item_id)\n            if supply_cost and supply_cost > self.supply_left:\n                return False\n        return True\n\n    async def can_cast(\n        self,\n        unit: Unit,\n        ability_id: AbilityId,\n        target: Optional[Union[Unit, Point2]] = None,\n        only_check_energy_and_cooldown: bool = False,\n        cached_abilities_of_unit: List[AbilityId] = None,\n    ) -> bool:\n        \"\"\"Tests if a unit has an ability available and enough energy to cast it.\n\n        Example::\n\n            stalkers = self.units(UnitTypeId.STALKER)\n            stalkers_that_can_blink = stalkers.filter(lambda unit: unit.type_id == UnitTypeId.STALKER and (await self.can_cast(unit, AbilityId.EFFECT_BLINK_STALKER, only_check_energy_and_cooldown=True)))\n\n        See data_pb2.py (line 161) for the numbers 1-5 to make sense\n\n        :param unit:\n        :param ability_id:\n        :param target:\n        :param only_check_energy_and_cooldown:\n        :param cached_abilities_of_unit:\"\"\"\n        assert isinstance(unit, Unit), f\"{unit} is no Unit object\"\n        assert isinstance(ability_id, AbilityId), f\"{ability_id} is no AbilityId\"\n        assert isinstance(target, (type(None), Unit, Point2))\n        # check if unit has enough energy to cast or if ability is on cooldown\n        if cached_abilities_of_unit:\n            abilities = cached_abilities_of_unit\n        else:\n            abilities = (await self.get_available_abilities([unit], ignore_resource_requirements=False))[0]\n\n        if ability_id in abilities:\n            if only_check_energy_and_cooldown:\n                return True\n            cast_range = self._game_data.abilities[ability_id.value]._proto.cast_range\n            ability_target = self._game_data.abilities[ability_id.value]._proto.target\n            # Check if target is in range (or is a self cast like stimpack)\n            if (\n                ability_target == 1 or ability_target == Target.PointOrNone.value and isinstance(target, Point2)\n                and unit.distance_to(target) <= unit.radius + target.radius + cast_range\n            ):  # cant replace 1 with \"Target.None.value\" because \".None\" doesnt seem to be a valid enum name\n                return True\n            # Check if able to use ability on a unit\n            elif (\n                ability_target in {Target.Unit.value, Target.PointOrUnit.value} and isinstance(target, Unit)\n                and unit.distance_to(target) <= unit.radius + target.radius + cast_range\n            ):\n                return True\n            # Check if able to use ability on a position\n            elif (\n                ability_target in {Target.Point.value, Target.PointOrUnit.value} and isinstance(target, Point2)\n                and unit.distance_to(target) <= unit.radius + cast_range\n            ):\n                return True\n        return False\n\n    def select_build_worker(self, pos: Union[Unit, Point2], force: bool = False) -> Optional[Unit]:\n        \"\"\"Select a worker to build a building with.\n\n        Example::\n\n            barracks_placement_position = self.main_base_ramp.barracks_correct_placement\n            worker = self.select_build_worker(barracks_placement_position)\n            # Can return None\n            if worker:\n                worker.build(UnitTypeId.BARRACKS, barracks_placement_position)\n\n        :param pos:\n        :param force:\"\"\"\n        workers = (\n            self.workers.filter(lambda w: (w.is_gathering or w.is_idle) and w.distance_to(pos) < 20) or self.workers\n        )\n        if workers:\n            for worker in workers.sorted_by_distance_to(pos).prefer_idle:\n                if (\n                    worker not in self.unit_tags_received_action and not worker.orders or len(worker.orders) == 1\n                    and worker.orders[0].ability.id in {AbilityId.MOVE, AbilityId.HARVEST_GATHER}\n                ):\n                    return worker\n\n            return workers.random if force else None\n\n    async def can_place_single(self, building: Union[AbilityId, UnitTypeId], position: Point2) -> bool:\n        \"\"\" Checks the placement for only one position. \"\"\"\n        if isinstance(building, UnitTypeId):\n            creation_ability = self._game_data.units[building.value].creation_ability.id\n            return (await self._client._query_building_placement_fast(creation_ability, [position]))[0]\n        return (await self._client._query_building_placement_fast(building, [position]))[0]\n\n    async def can_place(self, building: Union[AbilityData, AbilityId, UnitTypeId],\n                        positions: List[Point2]) -> List[bool]:\n        \"\"\"Tests if a building can be placed in the given locations.\n\n        Example::\n\n            barracks_placement_position = self.main_base_ramp.barracks_correct_placement\n            worker = self.select_build_worker(barracks_placement_position)\n            # Can return None\n            if worker and (await self.can_place(UnitTypeId.BARRACKS, [barracks_placement_position])[0]:\n                worker.build(UnitTypeId.BARRACKS, barracks_placement_position)\n\n        :param building:\n        :param position:\"\"\"\n        building_type = type(building)\n        assert type(building) in {AbilityData, AbilityId, UnitTypeId}, f\"{building}, {building_type}\"\n        if building_type == UnitTypeId:\n            building = self._game_data.units[building.value].creation_ability.id\n        elif building_type == AbilityData:\n            warnings.warn(\n                \"Using AbilityData is deprecated and may be removed soon. Please use AbilityId or UnitTypeId instead.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            building = building_type.id\n\n        if isinstance(positions, (Point2, tuple)):\n            warnings.warn(\n                \"The support for querying single entries will be removed soon. Please use either 'await self.can_place_single(building, position)' or 'await (self.can_place(building, [position]))[0]\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            return await self.can_place_single(building, positions)\n        else:\n            assert isinstance(positions, list), f\"Expected an iterable (list, tuple), but was: {positions}\"\n            assert isinstance(\n                positions[0], Point2\n            ), f\"List is expected to have Point2, but instead had: {positions[0]} {type(positions[0])}\"\n\n        return await self._client._query_building_placement_fast(building, positions)\n\n    async def find_placement(\n        self,\n        building: Union[UnitTypeId, AbilityId],\n        near: Point2,\n        max_distance: int = 20,\n        random_alternative: bool = True,\n        placement_step: int = 2,\n        addon_place: bool = False,\n    ) -> Optional[Point2]:\n        \"\"\"Finds a placement location for building.\n\n        Example::\n\n            if self.townhalls:\n                cc = self.townhalls[0]\n                depot_position = await self.find_placement(UnitTypeId.SUPPLYDEPOT, near=cc)\n\n        :param building:\n        :param near:\n        :param max_distance:\n        :param random_alternative:\n        :param placement_step:\n        :param addon_place:\"\"\"\n\n        assert isinstance(building, (AbilityId, UnitTypeId))\n        assert isinstance(near, Point2), f\"{near} is no Point2 object\"\n\n        if isinstance(building, UnitTypeId):\n            building = self._game_data.units[building.value].creation_ability.id\n\n        if await self.can_place_single(\n            building, near\n        ) and (not addon_place or await self.can_place_single(UnitTypeId.SUPPLYDEPOT, near.offset((2.5, -0.5)))):\n            return near\n\n        if max_distance == 0:\n            return None\n\n        for distance in range(placement_step, max_distance, placement_step):\n            possible_positions = [\n                Point2(p).offset(near).to2 for p in (\n                    [(dx, -distance) for dx in range(-distance, distance + 1, placement_step)] +\n                    [(dx, distance) for dx in range(-distance, distance + 1, placement_step)] +\n                    [(-distance, dy) for dy in range(-distance, distance + 1, placement_step)] +\n                    [(distance, dy) for dy in range(-distance, distance + 1, placement_step)]\n                )\n            ]\n            res = await self._client._query_building_placement_fast(building, possible_positions)\n            # Filter all positions if building can be placed\n            possible = [p for r, p in zip(res, possible_positions) if r]\n\n            if addon_place:\n                # Filter remaining positions if addon can be placed\n                res = await self._client._query_building_placement_fast(\n                    AbilityId.TERRANBUILDDROP_SUPPLYDEPOTDROP,\n                    [p.offset((2.5, -0.5)) for p in possible],\n                )\n                possible = [p for r, p in zip(res, possible) if r]\n\n            if not possible:\n                continue\n\n            if random_alternative:\n                return random.choice(possible)\n            else:\n                return min(possible, key=lambda p: p.distance_to_point2(near))\n        return None\n\n    # TODO: improve using cache per frame\n    def already_pending_upgrade(self, upgrade_type: UpgradeId) -> float:\n        \"\"\"Check if an upgrade is being researched\n\n        Returns values are::\n\n            0 # not started\n            0 < x < 1 # researching\n            1 # completed\n\n        Example::\n\n            stim_completion_percentage = self.already_pending_upgrade(UpgradeId.STIMPACK)\n\n        :param upgrade_type:\n        \"\"\"\n        assert isinstance(upgrade_type, UpgradeId), f\"{upgrade_type} is no UpgradeId\"\n        if upgrade_type in self.state.upgrades:\n            return 1\n        creationAbilityID = self._game_data.upgrades[upgrade_type.value].research_ability.exact_id\n        for structure in self.structures.filter(lambda unit: unit.is_ready):\n            for order in structure.orders:\n                if order.ability.exact_id == creationAbilityID:\n                    return order.progress\n        return 0\n\n    @property_cache_once_per_frame_no_copy\n    def _abilities_all_units(self) -> Tuple[Counter, Dict[UnitTypeId, float]]:\n        \"\"\"Cache for the already_pending function, includes protoss units warping in,\n        all units in production and all structures, and all morphs\"\"\"\n        abilities_amount = Counter()\n        max_build_progress: Dict[UnitTypeId, float] = {}\n        unit: Unit\n        for unit in self.units + self.structures:\n            for order in unit.orders:\n                abilities_amount[order.ability] += 1\n            if not unit.is_ready:\n                if self.race != Race.Terran or not unit.is_structure:\n                    # If an SCV is constructing a building, already_pending would count this structure twice\n                    # (once from the SCV order, and once from \"not structure.is_ready\")\n                    creation_ability: AbilityData = self._game_data.units[unit.type_id.value].creation_ability\n                    abilities_amount[creation_ability] += 1\n                    max_build_progress[creation_ability] = max(\n                        max_build_progress.get(creation_ability, 0), unit.build_progress\n                    )\n\n        return abilities_amount, max_build_progress\n\n    def structure_type_build_progress(self, structure_type: Union[UnitTypeId, int]) -> float:\n        \"\"\"\n        Returns the build progress of a structure type.\n\n        Return range: 0 <= x <= 1 where\n            0: no such structure exists\n            0 < x < 1: at least one structure is under construction, returns the progress of the one with the highest progress\n            1: we have at least one such structure complete\n\n        Example::\n\n            # Assuming you have one barracks building at 0.5 build progress:\n            progress = self.structure_type_build_progress(UnitTypeId.BARRACKS)\n            print(progress)\n            # This prints out 0.5\n\n            # If you want to save up money for mutalisks, you can now save up once the spire is nearly completed:\n            spire_almost_completed: bool = self.structure_type_build_progress(UnitTypeId.SPIRE) > 0.75\n\n            # If you have a Hive completed but no lair, this function returns 1.0 for the following:\n            self.structure_type_build_progress(UnitTypeId.LAIR)\n\n            # Assume you have 2 command centers in production, one has 0.5 build_progress and the other 0.2, the following returns 0.5\n            highest_progress_of_command_center: float = self.structure_type_build_progress(UnitTypeId.COMMANDCENTER)\n\n        :param structure_type:\n        \"\"\"\n        assert isinstance(\n            structure_type, (int, UnitTypeId)\n        ), f\"Needs to be int or UnitTypeId, but was: {type(structure_type)}\"\n        if isinstance(structure_type, int):\n            structure_type_value: int = structure_type\n            structure_type = UnitTypeId(structure_type_value)\n        else:\n            structure_type_value = structure_type.value\n        assert structure_type_value, f\"structure_type can not be 0 or NOTAUNIT, but was: {structure_type_value}\"\n        equiv_values: Set[int] = {structure_type_value} | {\n            s_type.value\n            for s_type in EQUIVALENTS_FOR_TECH_PROGRESS.get(structure_type, set())\n        }\n        # SUPPLYDEPOTDROP is not in self._game_data.units, so bot_ai should not check the build progress via creation ability (worker abilities)\n        if structure_type_value not in self._game_data.units:\n            return max([s.build_progress for s in self.structures if s._proto.unit_type in equiv_values], default=0)\n        creation_ability: AbilityData = self._game_data.units[structure_type_value].creation_ability\n        max_value = max(\n            [s.build_progress for s in self.structures if s._proto.unit_type in equiv_values] +\n            [self._abilities_all_units[1].get(creation_ability, 0)],\n            default=0,\n        )\n        return max_value\n\n    def tech_requirement_progress(self, structure_type: UnitTypeId) -> float:\n        \"\"\"Returns the tech requirement progress for a specific building\n\n        Example::\n\n            # Current state: supply depot is at 50% completion\n            tech_requirement = self.tech_requirement_progress(UnitTypeId.BARRACKS)\n            print(tech_requirement) # Prints 0.5 because supply depot is half way done\n\n        Example::\n\n            # Current state: your bot has one hive, no lair\n            tech_requirement = self.tech_requirement_progress(UnitTypeId.HYDRALISKDEN)\n            print(tech_requirement) # Prints 1 because a hive exists even though only a lair is required\n\n        Example::\n\n            # Current state: One factory is flying and one is half way done\n            tech_requirement = self.tech_requirement_progress(UnitTypeId.STARPORT)\n            print(tech_requirement) # Prints 1 because even though the type id of the flying factory is different, it still has build progress of 1 and thus tech requirement is completed\n\n        :param structure_type:\"\"\"\n        race_dict = {\n            Race.Protoss: PROTOSS_TECH_REQUIREMENT,\n            Race.Terran: TERRAN_TECH_REQUIREMENT,\n            Race.Zerg: ZERG_TECH_REQUIREMENT,\n        }\n        unit_info_id = race_dict[self.race][structure_type]\n        unit_info_id_value = unit_info_id.value\n        # The following commented out line is unreliable for ghost / thor as they return 0 which is incorrect\n        # unit_info_id_value = self._game_data.units[structure_type.value]._proto.tech_requirement\n        if not unit_info_id_value:  # Equivalent to \"if unit_info_id_value == 0:\"\n            return 1\n        progresses: List[float] = [self.structure_type_build_progress(unit_info_id_value)]\n        for equiv_structure in EQUIVALENTS_FOR_TECH_PROGRESS.get(unit_info_id, []):\n            progresses.append(self.structure_type_build_progress(equiv_structure.value))\n        return max(progresses)\n\n    def already_pending(self, unit_type: Union[UpgradeId, UnitTypeId]) -> float:\n        \"\"\"\n        Returns a number of buildings or units already in progress, or if a\n        worker is en route to build it. This also includes queued orders for\n        workers and build queues of buildings.\n\n        Example::\n\n            amount_of_scv_in_production: int = self.already_pending(UnitTypeId.SCV)\n            amount_of_CCs_in_queue_and_production: int = self.already_pending(UnitTypeId.COMMANDCENTER)\n            amount_of_lairs_morphing: int = self.already_pending(UnitTypeId.LAIR)\n\n\n        :param unit_type:\n        \"\"\"\n        if isinstance(unit_type, UpgradeId):\n            return self.already_pending_upgrade(unit_type)\n        ability = self._game_data.units[unit_type.value].creation_ability\n        return self._abilities_all_units[0][ability]\n\n    @property_cache_once_per_frame_no_copy\n    def _worker_orders(self) -> Counter:\n        \"\"\" This function is used internally, do not use! It is to store all worker abilities. \"\"\"\n        abilities_amount = Counter()\n        structures_in_production: Set[Union[Point2, int]] = set()\n        for structure in self.structures:\n            if structure.type_id in TERRAN_STRUCTURES_REQUIRE_SCV:\n                structures_in_production.add(structure.position)\n                structures_in_production.add(structure.tag)\n        for worker in self.workers:\n            for order in worker.orders:\n                # Skip if the SCV is constructing (not isinstance(order.target, int))\n                # or resuming construction (isinstance(order.target, int))\n                is_int = isinstance(order.target, int)\n                if (\n                    is_int and order.target in structures_in_production\n                    or not is_int and Point2.from_proto(order.target) in structures_in_production\n                ):\n                    continue\n                abilities_amount[order.ability] += 1\n        return abilities_amount\n\n    def worker_en_route_to_build(self, unit_type: UnitTypeId) -> float:\n        \"\"\"This function counts how many workers are on the way to start the construction a building.\n        Warning: this function may change its name in the future!\n        New function. Please report any bugs!\n\n        :param unit_type:\"\"\"\n        ability = self._game_data.units[unit_type.value].creation_ability\n        return self._worker_orders[ability]\n\n    @property_cache_once_per_frame\n    def structures_without_construction_SCVs(self) -> Units:\n        \"\"\"Returns all structures that do not have an SCV constructing it.\n        Warning: this function may move to become a Units filter.\n        New function. Please report any bugs!\"\"\"\n        worker_targets: Set[Union[int, Point2]] = set()\n        for worker in self.workers:\n            # Ignore repairing workers\n            if not worker.is_constructing_scv:\n                continue\n            for order in worker.orders:\n                # When a construction is resumed, the worker.orders[0].target is the tag of the structure, else it is a Point2\n                target = order.target\n                if isinstance(target, int):\n                    worker_targets.add(target)\n                else:\n                    worker_targets.add(Point2.from_proto(target))\n        return self.structures.filter(\n            lambda structure: structure.build_progress < 1\n            # Redundant check?\n            and structure.type_id in TERRAN_STRUCTURES_REQUIRE_SCV and structure.position not in worker_targets and\n            structure.tag not in worker_targets and structure.tag in self._structures_previous_map and self.\n            _structures_previous_map[structure.tag].build_progress == structure.build_progress\n        )\n\n    async def build(\n        self,\n        building: UnitTypeId,\n        near: Union[Unit, Point2],\n        max_distance: int = 20,\n        build_worker: Optional[Unit] = None,\n        random_alternative: bool = True,\n        placement_step: int = 2,\n    ) -> bool:\n        \"\"\"Not recommended as this function checks many positions if it \"can place\" on them until it found a valid\n        position. Also if the given position is not placeable, this function tries to find a nearby position to place\n        the structure. Then uses 'self.do' to give the worker the order to start the construction.\n\n        :param building:\n        :param near:\n        :param max_distance:\n        :param build_worker:\n        :param random_alternative:\n        :param placement_step:\"\"\"\n\n        assert isinstance(near, (Unit, Point2))\n        if not self.can_afford(building):\n            return False\n        p = None\n        gas_buildings = {UnitTypeId.EXTRACTOR, UnitTypeId.ASSIMILATOR, UnitTypeId.REFINERY}\n        if isinstance(near, Unit) and building not in gas_buildings:\n            near = near.position\n        if isinstance(near, Point2):\n            near = near.to2\n        if isinstance(near, Point2):\n            p = await self.find_placement(building, near, max_distance, random_alternative, placement_step)\n            if p is None:\n                return False\n        builder = build_worker or self.select_build_worker(near)\n        if builder is None:\n            return False\n        if building in gas_buildings:\n            builder.build_gas(near)\n            return True\n        self.do(builder.build(building, p), subtract_cost=True, ignore_warning=True)\n        return True\n\n    def train(\n        self,\n        unit_type: UnitTypeId,\n        amount: int = 1,\n        closest_to: Point2 = None,\n        train_only_idle_buildings: bool = True\n    ) -> int:\n        \"\"\"Trains a specified number of units. Trains only one if amount is not specified.\n        Warning: currently has issues with warp gate warp ins\n\n        New function. Please report any bugs!\n\n        Example Zerg::\n\n            self.train(UnitTypeId.QUEEN, 5)\n            # This should queue 5 queens in 5 different townhalls if you have enough townhalls, enough minerals and enough free supply left\n\n        Example Terran::\n\n            # Assuming you have 2 idle barracks with reactors, one barracks without addon and one with techlab\n            # It should only queue 4 marines in the 2 idle barracks with reactors\n            self.train(UnitTypeId.MARINE, 4)\n\n        Example distance to::\n\n            # If you want to train based on distance to a certain point, you can use \"closest_to\"\n            self.train(UnitTypeId.MARINE, 4, closest_to = self.game_info.map_center)\n\n\n        :param unit_type:\n        :param amount:\n        :param closest_to:\n        :param train_only_idle_buildings:\"\"\"\n        # Tech requirement not met\n        if self.tech_requirement_progress(unit_type) < 1:\n            race_dict = {\n                Race.Protoss: PROTOSS_TECH_REQUIREMENT,\n                Race.Terran: TERRAN_TECH_REQUIREMENT,\n                Race.Zerg: ZERG_TECH_REQUIREMENT,\n            }\n            unit_info_id = race_dict[self.race][unit_type]\n            logger.warning(\n                \"{} Trying to produce unit {} in self.train() but tech requirement is not met: {}\".format(\n                    self.time_formatted, unit_type, unit_info_id\n                )\n            )\n            return 0\n\n        # Not affordable\n        if not self.can_afford(unit_type):\n            return 0\n\n        trained_amount = 0\n        # All train structure types: queen can made from hatchery, lair, hive\n        train_structure_type: Set[UnitTypeId] = UNIT_TRAINED_FROM[unit_type]\n        train_structures = self.structures if self.race != Race.Zerg else self.structures | self.larva\n        requires_techlab = any(\n            TRAIN_INFO[structure_type][unit_type].get(\"requires_techlab\", False)\n            for structure_type in train_structure_type\n        )\n        is_protoss = self.race == Race.Protoss\n        is_terran = self.race == Race.Terran\n        can_have_addons = any(\n            u in train_structure_type for u in {UnitTypeId.BARRACKS, UnitTypeId.FACTORY, UnitTypeId.STARPORT}\n        )\n        # Sort structures closest to a point\n        if closest_to is not None:\n            train_structures = train_structures.sorted_by_distance_to(closest_to)\n        elif can_have_addons:\n            # This should sort the structures in ascending order: first structures with reactor, then naked, then with techlab\n            train_structures = train_structures.sorted(\n                key=lambda structure: -1 * (structure.add_on_tag in self.reactor_tags) + 1 *\n                (structure.add_on_tag in self.techlab_tags)\n            )\n\n        structure: Unit\n        for structure in train_structures:\n            # Exit early if we can't afford\n            if not self.can_afford(unit_type):\n                return trained_amount\n            if (\n                # If structure hasn't received an action/order this frame\n                structure.tag not in self.unit_tags_received_action\n                # If structure can train this unit at all\n                and structure.type_id in train_structure_type\n                # Structure has to be completed to be able to train\n                and structure.build_progress == 1\n                # If structure is protoss, it needs to be powered to train\n                and (not is_protoss or structure.is_powered or structure.type_id == UnitTypeId.NEXUS)\n                # Either parameter \"train_only_idle_buildings\" is False or structure is idle or structure has less than 2 orders and has reactor\n                and (\n                    not train_only_idle_buildings\n                    or len(structure.orders) < 1 + int(structure.add_on_tag in self.reactor_tags)\n                )\n                # If structure type_id does not accept addons, it cant require a techlab\n                # Else we have to check if building has techlab as addon\n                and (not requires_techlab or structure.add_on_tag in self.techlab_tags)\n            ):\n                # Warp in at location\n                # TODO: find fast warp in locations either random location or closest to the given parameter \"closest_to\"\n                # TODO: find out which pylons have fast warp in by checking distance to nexus and warpgates.ready\n                if structure.type_id == UnitTypeId.WARPGATE:\n                    pylons = self.structures(UnitTypeId.PYLON)\n                    location = pylons.random.position.random_on_distance(4)\n                    successfully_trained = structure.warp_in(unit_type, location)\n                else:\n                    # Normal train a unit from larva or inside a structure\n                    successfully_trained = self.do(\n                        structure.train(unit_type), subtract_cost=True, subtract_supply=True, ignore_warning=True\n                    )\n                    # Check if structure has reactor: queue same unit again\n                    if (\n                        # Only terran can have reactors\n                        is_terran\n                        # Check if we have enough cost or supply for this unit type\n                        and self.can_afford(unit_type)\n                        # Structure needs to be idle in the current frame\n                        and not structure.orders\n                        # We are at least 2 away from goal\n                        and trained_amount + 1 < amount\n                        # Unit type does not require techlab\n                        and not requires_techlab\n                        # Train structure has reactor\n                        and structure.add_on_tag in self.reactor_tags\n                    ):\n                        trained_amount += 1\n                        # With one command queue=False and one queue=True, you can queue 2 marines in a reactored barracks in one frame\n                        successfully_trained = self.do(\n                            structure.train(unit_type, queue=True),\n                            subtract_cost=True,\n                            subtract_supply=True,\n                            ignore_warning=True,\n                        )\n\n                if successfully_trained:\n                    trained_amount += 1\n                    if trained_amount == amount:\n                        # Target unit train amount reached\n                        return trained_amount\n                else:\n                    # Some error occured and we couldn't train the unit\n                    return trained_amount\n        return trained_amount\n\n    def research(self, upgrade_type: UpgradeId) -> bool:\n        \"\"\"\n        Researches an upgrade from a structure that can research it, if it is idle and powered (protoss).\n        Returns True if the research was started.\n        Return False if the requirement was not met, or the bot did not have enough resources to start the upgrade,\n        or the building to research the upgrade was missing or not idle.\n\n        New function. Please report any bugs!\n\n        Example::\n\n            # Try to research zergling movement speed if we can afford it\n            # and if at least one pool is at build_progress == 1\n            # and we are not researching it yet\n            if self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) == 0 and self.can_afford(UpgradeId.ZERGLINGMOVEMENTSPEED):\n                spawning_pools_ready = self.structures(UnitTypeId.SPAWNINGPOOL).ready\n                if spawning_pools_ready:\n                    self.research(UpgradeId.ZERGLINGMOVEMENTSPEED)\n\n        :param upgrade_type:\n        \"\"\"\n        assert (\n            upgrade_type in UPGRADE_RESEARCHED_FROM\n        ), f\"Could not find upgrade {upgrade_type} in 'research from'-dictionary\"\n\n        # Not affordable\n        if not self.can_afford(upgrade_type):\n            return False\n\n        research_structure_types: UnitTypeId = UPGRADE_RESEARCHED_FROM[upgrade_type]\n        required_tech_building: Optional[UnitTypeId] = RESEARCH_INFO[research_structure_types][upgrade_type].get(\n            \"required_building\", None\n        )\n\n        requirement_met = (\n            required_tech_building is None or self.structure_type_build_progress(required_tech_building) == 1\n        )\n        if not requirement_met:\n            return False\n\n        is_protoss = self.race == Race.Protoss\n\n        # All upgrades right now that can be researched in spire and hatch can also be researched in their morphs\n        equiv_structures = {\n            UnitTypeId.SPIRE: {UnitTypeId.SPIRE, UnitTypeId.GREATERSPIRE},\n            UnitTypeId.GREATERSPIRE: {UnitTypeId.SPIRE, UnitTypeId.GREATERSPIRE},\n            UnitTypeId.HATCHERY: {UnitTypeId.HATCHERY, UnitTypeId.LAIR, UnitTypeId.HIVE},\n            UnitTypeId.LAIR: {UnitTypeId.HATCHERY, UnitTypeId.LAIR, UnitTypeId.HIVE},\n            UnitTypeId.HIVE: {UnitTypeId.HATCHERY, UnitTypeId.LAIR, UnitTypeId.HIVE},\n        }\n        # Convert to a set, or equivalent structures are chosen\n        # Overlord speed upgrade can be researched from hatchery, lair or hive\n        research_structure_types: Set[UnitTypeId] = equiv_structures.get(\n            research_structure_types, {research_structure_types}\n        )\n\n        structure: Unit\n        for structure in self.structures:\n            if (\n                # Structure can research this upgrade\n                structure.type_id in research_structure_types\n                # If structure hasn't received an action/order this frame\n                and structure.tag not in self.unit_tags_received_action\n                # Structure is idle\n                and structure.is_idle\n                # Structure belongs to protoss and is powered (near pylon)\n                and (not is_protoss or structure.is_powered)\n            ):\n                # Can_afford check was already done earlier in this function\n                successful_action: bool = self.do(\n                    structure.research(upgrade_type), subtract_cost=True, ignore_warning=True\n                )\n                return successful_action\n        return False\n\n    def do(\n        self,\n        action: UnitCommand,\n        subtract_cost: bool = False,\n        subtract_supply: bool = False,\n        can_afford_check: bool = False,\n        ignore_warning: bool = False,\n    ) -> bool:\n        \"\"\"Adds a unit action to the 'self.actions' list which is then executed at the end of the frame.\n\n        Training a unit::\n\n            # Train an SCV from a random idle command center\n            cc = self.townhalls.idle.random_or(None)\n            # self.townhalls can be empty or there are no idle townhalls\n            if cc and self.can_afford(UnitTypeId.SCV):\n                cc.train(UnitTypeId.SCV)\n\n        Building a building::\n\n            # Building a barracks at the main ramp, requires 150 minerals and a depot\n            worker = self.workers.random_or(None)\n            barracks_placement_position = self.main_base_ramp.barracks_correct_placement\n            if worker and self.can_afford(UnitTypeId.BARRACKS):\n                worker.build(UnitTypeId.BARRACKS, barracks_placement_position)\n\n        Moving a unit::\n\n            # Move a random worker to the center of the map\n            worker = self.workers.random_or(None)\n            # worker can be None if all are dead\n            if worker:\n                worker.move(self.game_info.map_center)\n\n        :param action:\n        :param subtract_cost:\n        :param subtract_supply:\n        :param can_afford_check:\n        \"\"\"\n        if not self.unit_command_uses_self_do and isinstance(action, bool):\n            if not ignore_warning:\n                warnings.warn(\n                    \"You have used self.do(). Please consider putting 'self.unit_command_uses_self_do = True' in your bot __init__() function or removing self.do().\",\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n            return action\n\n        assert isinstance(\n            action, UnitCommand\n        ), f\"Given unit command is not a command, but instead of type {type(action)}\"\n        if subtract_cost:\n            cost: Cost = self._game_data.calculate_ability_cost(action.ability)\n            if can_afford_check and not (self.minerals >= cost.minerals and self.vespene >= cost.vespene):\n                # Dont do action if can't afford\n                return False\n            self.minerals -= cost.minerals\n            self.vespene -= cost.vespene\n        if subtract_supply and action.ability in abilityid_to_unittypeid:\n            unit_type = abilityid_to_unittypeid[action.ability]\n            required_supply = self.calculate_supply_cost(unit_type)\n            # Overlord has -8\n            if required_supply > 0:\n                self.supply_used += required_supply\n                self.supply_left -= required_supply\n        self.actions.append(action)\n        self.unit_tags_received_action.add(action.unit.tag)\n        return True\n\n    # TODO remove again, because you can just use 'self.do()' and execute '_do_actions' and 'self.actions.clear()' afterwards?\n    async def synchronous_do(self, action: UnitCommand):\n        \"\"\"\n        Not recommended. Use self.do instead to reduce lag.\n        This function is only useful for realtime=True in the first frame of the game to instantly produce a worker\n        and split workers on the mineral patches.\n        \"\"\"\n        assert isinstance(\n            action, UnitCommand\n        ), f\"Given unit command is not a command, but instead of type {type(action)}\"\n        if not self.can_afford(action.ability):\n            logger.warning(f\"Cannot afford action {action}\")\n            return ActionResult.Error\n        r = await self._client.actions(action)\n        if not r:  # success\n            cost = self._game_data.calculate_ability_cost(action.ability)\n            self.minerals -= cost.minerals\n            self.vespene -= cost.vespene\n            self.unit_tags_received_action.add(action.unit.tag)\n        else:\n            logger.error(f\"Error: {r} (action: {action})\")\n        return r\n\n    async def _do_actions(self, actions: List[UnitCommand], prevent_double: bool = True):\n        \"\"\"Used internally by main.py automatically, use self.do() instead!\n\n        :param actions:\n        :param prevent_double:\"\"\"\n        if not actions:\n            return None\n        if prevent_double:\n            actions = list(filter(self.prevent_double_actions, actions))\n        result = await self._client.actions(actions)\n        return result\n\n    def prevent_double_actions(self, action) -> bool:\n        \"\"\"\n        :param action:\n        \"\"\"\n        # Always add actions if queued\n        if action.queue:\n            return True\n        if action.unit.orders:\n            # action: UnitCommand\n            # current_action: UnitOrder\n            current_action = action.unit.orders[0]\n            if current_action.ability.id != action.ability and current_action.ability.exact_id != action.ability:\n                # Different action, return True\n                return True\n            with suppress(AttributeError):\n                if current_action.target == action.target.tag:\n                    # Same action, remove action if same target unit\n                    return False\n            with suppress(AttributeError):\n                if action.target.x == current_action.target.x and action.target.y == current_action.target.y:\n                    # Same action, remove action if same target position\n                    return False\n            return True\n        return True\n\n    async def chat_send(self, message: str, team_only: bool = False):\n        \"\"\"Send a chat message to the SC2 Client.\n\n        Example::\n\n            await self.chat_send(\"Hello, this is a message from my bot!\")\n\n        :param message:\n        :param team_only:\"\"\"\n        assert isinstance(message, str), f\"{message} is not a string\"\n        await self._client.chat_send(message, team_only)\n\n    def in_map_bounds(self, pos: Union[Point2, tuple, list]) -> bool:\n        \"\"\"Tests if a 2 dimensional point is within the map boundaries of the pixelmaps.\n        :param pos:\"\"\"\n        return (\n            self._game_info.playable_area.x <= pos[0] <\n            self._game_info.playable_area.x + self.game_info.playable_area.width and self._game_info.playable_area.y <=\n            pos[1] < self._game_info.playable_area.y + self.game_info.playable_area.height\n        )\n\n    # For the functions below, make sure you are inside the boundaries of the map size.\n    def get_terrain_height(self, pos: Union[Point2, Unit]) -> int:\n        \"\"\"Returns terrain height at a position.\n        Caution: terrain height is different from a unit's z-coordinate.\n\n        :param pos:\"\"\"\n        assert isinstance(pos, (Point2, Unit)), f\"pos is not of type Point2 or Unit\"\n        pos = pos.position.rounded\n        return self._game_info.terrain_height[pos]\n\n    def get_terrain_z_height(self, pos: Union[Point2, Unit]) -> float:\n        \"\"\"Returns terrain z-height at a position.\n\n        :param pos:\"\"\"\n        assert isinstance(pos, (Point2, Unit)), f\"pos is not of type Point2 or Unit\"\n        pos = pos.position.rounded\n        return -16 + 32 * self._game_info.terrain_height[pos] / 255\n\n    def in_placement_grid(self, pos: Union[Point2, Unit]) -> bool:\n        \"\"\"Returns True if you can place something at a position.\n        Remember, buildings usually use 2x2, 3x3 or 5x5 of these grid points.\n        Caution: some x and y offset might be required, see ramp code in game_info.py\n\n        :param pos:\"\"\"\n        assert isinstance(pos, (Point2, Unit)), f\"pos is not of type Point2 or Unit\"\n        pos = pos.position.rounded\n        return self._game_info.placement_grid[pos] == 1\n\n    def in_pathing_grid(self, pos: Union[Point2, Unit]) -> bool:\n        \"\"\"Returns True if a ground unit can pass through a grid point.\n\n        :param pos:\"\"\"\n        assert isinstance(pos, (Point2, Unit)), f\"pos is not of type Point2 or Unit\"\n        pos = pos.position.rounded\n        return self._game_info.pathing_grid[pos] == 1\n\n    def is_visible(self, pos: Union[Point2, Unit]) -> bool:\n        \"\"\"Returns True if you have vision on a grid point.\n\n        :param pos:\"\"\"\n        # more info: https://github.com/Blizzard/s2client-proto/blob/9906df71d6909511907d8419b33acc1a3bd51ec0/s2clientprotocol/spatial.proto#L19\n        assert isinstance(pos, (Point2, Unit)), f\"pos is not of type Point2 or Unit\"\n        pos = pos.position.rounded\n        return self.state.visibility[pos] == 2\n\n    def has_creep(self, pos: Union[Point2, Unit]) -> bool:\n        \"\"\"Returns True if there is creep on the grid point.\n\n        :param pos:\"\"\"\n        assert isinstance(pos, (Point2, Unit)), f\"pos is not of type Point2 or Unit\"\n        pos = pos.position.rounded\n        return self.state.creep[pos] == 1\n\n    def _prepare_start(self, client, player_id, game_info, game_data, realtime: bool = False, base_build: int = -1):\n        \"\"\"\n        Ran until game start to set game and player data.\n\n        :param client:\n        :param player_id:\n        :param game_info:\n        :param game_data:\n        :param realtime:\n        \"\"\"\n        self._client: Client = client\n        self.player_id: int = player_id\n        self._game_info: GameInfo = game_info\n        self._game_data: GameData = game_data\n        self.realtime: bool = realtime\n        self.base_build: int = base_build\n\n        self.race: Race = Race(self._game_info.player_races[self.player_id])\n\n        if len(self._game_info.player_races) == 2:\n            self.enemy_race: Race = Race(self._game_info.player_races[3 - self.player_id])\n\n        self._distances_override_functions(self.distance_calculation_method)\n\n    def _prepare_first_step(self):\n        \"\"\"First step extra preparations. Must not be called before _prepare_step.\"\"\"\n        if self.townhalls:\n            self._game_info.player_start_location = self.townhalls.first.position\n            # Calculate and cache expansion locations forever inside 'self._cache_expansion_locations', this is done to prevent a bug when this is run and cached later in the game\n            _ = self._find_expansion_locations()\n        self._game_info.map_ramps, self._game_info.vision_blockers = self._game_info._find_ramps_and_vision_blockers()\n        self._time_before_step: float = time.perf_counter()\n\n    def _prepare_step(self, state, proto_game_info):\n        \"\"\"\n        :param state:\n        :param proto_game_info:\n        \"\"\"\n        # Set attributes from new state before on_step.\"\"\"\n        self.state: GameState = state  # See game_state.py\n        # update pathing grid, which unfortunately is in GameInfo instead of GameState\n        self._game_info.pathing_grid: PixelMap = PixelMap(\n            proto_game_info.game_info.start_raw.pathing_grid, in_bits=True, mirrored=False\n        )\n        # Required for events, needs to be before self.units are initialized so the old units are stored\n        self._units_previous_map: Dict[int, Unit] = {unit.tag: unit for unit in self.units}\n        self._structures_previous_map: Dict[int, Unit] = {structure.tag: structure for structure in self.structures}\n        self._enemy_units_previous_map: Dict[int, Unit] = {unit.tag: unit for unit in self.enemy_units}\n        self._enemy_structures_previous_map: Dict[int, Unit] = {\n            structure.tag: structure\n            for structure in self.enemy_structures\n        }\n        self._all_units_previous_map: Dict[int, Unit] = {unit.tag: unit for unit in self.all_units}\n\n        self._prepare_units()\n        self.minerals: int = state.common.minerals\n        self.vespene: int = state.common.vespene\n        self.supply_army: int = state.common.food_army\n        self.supply_workers: int = state.common.food_workers  # Doesn't include workers in production\n        self.supply_cap: int = state.common.food_cap\n        self.supply_used: int = state.common.food_used\n        self.supply_left: int = self.supply_cap - self.supply_used\n\n        if self.race == Race.Zerg:\n            # Workaround Zerg supply rounding bug\n            self._correct_zerg_supply()\n        elif self.race == Race.Protoss:\n            self.warp_gate_count: int = state.common.warp_gate_count\n\n        self.idle_worker_count: int = state.common.idle_worker_count\n        self.army_count: int = state.common.army_count\n        self._time_before_step: float = time.perf_counter()\n\n        if self.enemy_race == Race.Random and self.all_enemy_units:\n            self.enemy_race = Race(self.all_enemy_units.first.race)\n\n    def _prepare_units(self):\n        # Set of enemy units detected by own sensor tower, as blips have less unit information than normal visible units\n        self.blips: Set[Blip] = set()\n        self.all_units: Units = Units([], self)\n        self.units: Units = Units([], self)\n        self.workers: Units = Units([], self)\n        self.larva: Units = Units([], self)\n        self.structures: Units = Units([], self)\n        self.townhalls: Units = Units([], self)\n        self.gas_buildings: Units = Units([], self)\n        self.all_own_units: Units = Units([], self)\n        self.enemy_units: Units = Units([], self)\n        self.enemy_structures: Units = Units([], self)\n        self.all_enemy_units: Units = Units([], self)\n        self.resources: Units = Units([], self)\n        self.destructables: Units = Units([], self)\n        self.watchtowers: Units = Units([], self)\n        self.mineral_field: Units = Units([], self)\n        self.vespene_geyser: Units = Units([], self)\n        self.placeholders: Units = Units([], self)\n        self.techlab_tags: Set[int] = set()\n        self.reactor_tags: Set[int] = set()\n\n        worker_types: Set[UnitTypeId] = {UnitTypeId.DRONE, UnitTypeId.DRONEBURROWED, UnitTypeId.SCV, UnitTypeId.PROBE}\n\n        index: int = 0\n        for unit in self.state.observation_raw.units:\n            if unit.is_blip:\n                self.blips.add(Blip(unit))\n            else:\n                unit_type: int = unit.unit_type\n                # Convert these units to effects: reaper grenade, parasitic bomb dummy, forcefield\n                if unit_type in FakeEffectID:\n                    self.state.effects.add(EffectData(unit, fake=True))\n                    continue\n                unit_obj = Unit(unit, self, distance_calculation_index=index, base_build=self.base_build)\n                index += 1\n                self.all_units.append(unit_obj)\n                if unit.display_type == IS_PLACEHOLDER:\n                    self.placeholders.append(unit_obj)\n                    continue\n                alliance = unit.alliance\n                # Alliance.Neutral.value = 3\n                if alliance == 3:\n                    # XELNAGATOWER = 149\n                    if unit_type == 149:\n                        self.watchtowers.append(unit_obj)\n                    # mineral field enums\n                    elif unit_type in mineral_ids:\n                        self.mineral_field.append(unit_obj)\n                        self.resources.append(unit_obj)\n                    # geyser enums\n                    elif unit_type in geyser_ids:\n                        self.vespene_geyser.append(unit_obj)\n                        self.resources.append(unit_obj)\n                    # all destructable rocks\n                    else:\n                        self.destructables.append(unit_obj)\n                # Alliance.Self.value = 1\n                elif alliance == 1:\n                    self.all_own_units.append(unit_obj)\n                    unit_id = unit_obj.type_id\n                    if unit_obj.is_structure:\n                        self.structures.append(unit_obj)\n                        if unit_id in race_townhalls[self.race]:\n                            self.townhalls.append(unit_obj)\n                        elif unit_id in ALL_GAS or unit_obj.vespene_contents:\n                            # TODO: remove \"or unit_obj.vespene_contents\" when a new linux client newer than version 4.10.0 is released\n                            self.gas_buildings.append(unit_obj)\n                        elif unit_id in {\n                            UnitTypeId.TECHLAB,\n                            UnitTypeId.BARRACKSTECHLAB,\n                            UnitTypeId.FACTORYTECHLAB,\n                            UnitTypeId.STARPORTTECHLAB,\n                        }:\n                            self.techlab_tags.add(unit_obj.tag)\n                        elif unit_id in {\n                            UnitTypeId.REACTOR,\n                            UnitTypeId.BARRACKSREACTOR,\n                            UnitTypeId.FACTORYREACTOR,\n                            UnitTypeId.STARPORTREACTOR,\n                        }:\n                            self.reactor_tags.add(unit_obj.tag)\n                    else:\n                        self.units.append(unit_obj)\n                        if unit_id in worker_types:\n                            self.workers.append(unit_obj)\n                        elif unit_id == UnitTypeId.LARVA:\n                            self.larva.append(unit_obj)\n                # Alliance.Enemy.value = 4\n                elif alliance == 4:\n                    self.all_enemy_units.append(unit_obj)\n                    if unit_obj.is_structure:\n                        self.enemy_structures.append(unit_obj)\n                    else:\n                        self.enemy_units.append(unit_obj)\n\n        # Force distance calculation and caching on all units using scipy pdist or cdist\n        if self.distance_calculation_method == 1:\n            _ = self._pdist\n        elif self.distance_calculation_method in {2, 3}:\n            _ = self._cdist\n\n    async def _after_step(self) -> int:\n        \"\"\" Executed by main.py after each on_step function. \"\"\"\n        # Keep track of the bot on_step duration\n        self._time_after_step: float = time.perf_counter()\n        step_duration = self._time_after_step - self._time_before_step\n        self._min_step_time = min(step_duration, self._min_step_time)\n        self._max_step_time = max(step_duration, self._max_step_time)\n        self._last_step_step_time = step_duration\n        self._total_time_in_on_step += step_duration\n        self._total_steps_iterations += 1\n        # Commit and clear bot actions\n        if self.actions:\n            await self._do_actions(self.actions)\n            self.actions.clear()\n        # Clear set of unit tags that were given an order this frame by self.do()\n        self.unit_tags_received_action.clear()\n        # Commit debug queries\n        await self._client._send_debug()\n\n        return self.state.game_loop\n\n    async def _advance_steps(self, steps: int):\n        \"\"\"Advances the game loop by amount of 'steps'. This function is meant to be used as a debugging and testing tool only.\n        If you are using this, please be aware of the consequences, e.g. 'self.units' will be filled with completely new data.\"\"\"\n        await self._after_step()\n        # Advance simulation by exactly \"steps\" frames\n        await self.client.step(steps)\n        state = await self.client.observation()\n        gs = GameState(state.observation)\n        proto_game_info = await self.client._execute(game_info=sc_pb.RequestGameInfo())\n        self._prepare_step(gs, proto_game_info)\n        await self.issue_events()\n        # await self.on_step(-1)\n\n    async def issue_events(self):\n        \"\"\"This function will be automatically run from main.py and triggers the following functions:\n        - on_unit_created\n        - on_unit_destroyed\n        - on_building_construction_started\n        - on_building_construction_complete\n        - on_upgrade_complete\n        \"\"\"\n        await self._issue_unit_dead_events()\n        await self._issue_unit_added_events()\n        await self._issue_building_events()\n        await self._issue_upgrade_events()\n        await self._issue_vision_events()\n\n    async def _issue_unit_added_events(self):\n        for unit in self.units:\n            if unit.tag not in self._units_previous_map and unit.tag not in self._unit_tags_seen_this_game:\n                self._unit_tags_seen_this_game.add(unit.tag)\n                self._units_created[unit.type_id] += 1\n                await self.on_unit_created(unit)\n            elif unit.tag in self._units_previous_map:\n                previous_frame_unit: Unit = self._units_previous_map[unit.tag]\n                # Check if a unit took damage this frame and then trigger event\n                if unit.health < previous_frame_unit.health or unit.shield < previous_frame_unit.shield:\n                    damage_amount = previous_frame_unit.health - unit.health + previous_frame_unit.shield - unit.shield\n                    await self.on_unit_took_damage(unit, damage_amount)\n                # Check if a unit type has changed\n                if previous_frame_unit.type_id != unit.type_id:\n                    await self.on_unit_type_changed(unit, previous_frame_unit.type_id)\n\n    async def _issue_upgrade_events(self):\n        difference = self.state.upgrades - self._previous_upgrades\n        for upgrade_completed in difference:\n            await self.on_upgrade_complete(upgrade_completed)\n        self._previous_upgrades = self.state.upgrades\n\n    async def _issue_building_events(self):\n        for structure in self.structures:\n            if structure.tag not in self._structures_previous_map:\n                if structure.build_progress < 1:\n                    await self.on_building_construction_started(structure)\n                else:\n                    # Include starting townhall\n                    self._units_created[structure.type_id] += 1\n                    await self.on_building_construction_complete(structure)\n            elif structure.tag in self._structures_previous_map:\n                # Check if a structure took damage this frame and then trigger event\n                previous_frame_structure: Unit = self._structures_previous_map[structure.tag]\n                if (\n                    structure.health < previous_frame_structure.health\n                    or structure.shield < previous_frame_structure.shield\n                ):\n                    damage_amount = (\n                        previous_frame_structure.health - structure.health + previous_frame_structure.shield -\n                        structure.shield\n                    )\n                    await self.on_unit_took_damage(structure, damage_amount)\n                # Check if a structure changed its type\n                if previous_frame_structure.type_id != structure.type_id:\n                    await self.on_unit_type_changed(structure, previous_frame_structure.type_id)\n                # Check if structure completed\n                if structure.build_progress == 1 and previous_frame_structure.build_progress < 1:\n                    self._units_created[structure.type_id] += 1\n                    await self.on_building_construction_complete(structure)\n\n    async def _issue_vision_events(self):\n        # Call events for enemy unit entered vision\n        for enemy_unit in self.enemy_units:\n            if enemy_unit.tag not in self._enemy_units_previous_map:\n                await self.on_enemy_unit_entered_vision(enemy_unit)\n        for enemy_structure in self.enemy_structures:\n            if enemy_structure.tag not in self._enemy_structures_previous_map:\n                await self.on_enemy_unit_entered_vision(enemy_structure)\n\n        # Call events for enemy unit left vision\n        enemy_units_left_vision: Set[int] = set(self._enemy_units_previous_map.keys()) - self.enemy_units.tags\n        for enemy_unit_tag in enemy_units_left_vision:\n            await self.on_enemy_unit_left_vision(enemy_unit_tag)\n        enemy_structures_left_vision: Set[int] = (\n            set(self._enemy_structures_previous_map.keys()) - self.enemy_structures.tags\n        )\n        for enemy_structure_tag in enemy_structures_left_vision:\n            await self.on_enemy_unit_left_vision(enemy_structure_tag)\n\n    async def _issue_unit_dead_events(self):\n        for unit_tag in self.state.dead_units & set(self._all_units_previous_map.keys()):\n            await self.on_unit_destroyed(unit_tag)\n\n    async def on_unit_destroyed(self, unit_tag: int):\n        \"\"\"\n        Override this in your bot class.\n        Note that this function uses unit tags and not the unit objects\n        because the unit does not exist any more.\n        This will event will be called when a unit (or structure, friendly or enemy) dies.\n        For enemy units, this only works if the enemy unit was in vision on death.\n\n        :param unit_tag:\n        \"\"\"\n\n    async def on_unit_created(self, unit: Unit):\n        \"\"\"Override this in your bot class. This function is called when a unit is created.\n\n        :param unit:\"\"\"\n\n    async def on_unit_type_changed(self, unit: Unit, previous_type: UnitTypeId):\n        \"\"\"Override this in your bot class. This function is called when a unit type has changed. To get the current UnitTypeId of the unit, use 'unit.type_id'\n\n        This may happen when a larva morphed to an egg, siege tank sieged, a zerg unit burrowed, a hatchery morphed to lair,\n        a corruptor morphed to broodlordcocoon, etc..\n\n        Examples::\n\n            print(f\"My unit changed type: {unit} from {previous_type} to {unit.type_id}\")\n\n        :param unit:\n        :param previous_type:\n        \"\"\"\n\n    async def on_building_construction_started(self, unit: Unit):\n        \"\"\"\n        Override this in your bot class.\n        This function is called when a building construction has started.\n\n        :param unit:\n        \"\"\"\n\n    async def on_building_construction_complete(self, unit: Unit):\n        \"\"\"\n        Override this in your bot class. This function is called when a building\n        construction is completed.\n\n        :param unit:\n        \"\"\"\n\n    async def on_upgrade_complete(self, upgrade: UpgradeId):\n        \"\"\"\n        Override this in your bot class. This function is called with the upgrade id of an upgrade that was not finished last step and is now.\n\n        :param upgrade:\n        \"\"\"\n\n    async def on_unit_took_damage(self, unit: Unit, amount_damage_taken: float):\n        \"\"\"\n        Override this in your bot class. This function is called when your own unit (unit or structure) took damage.\n        It will not be called if the unit died this frame.\n\n        This may be called frequently for terran structures that are burning down, or zerg buildings that are off creep,\n        or terran bio units that just used stimpack ability.\n        TODO: If there is a demand for it, then I can add a similar event for when enemy units took damage\n\n        Examples::\n\n            print(f\"My unit took damage: {unit} took {amount_damage_taken} damage\")\n\n        :param unit:\n        \"\"\"\n\n    async def on_enemy_unit_entered_vision(self, unit: Unit):\n        \"\"\"\n        Override this in your bot class. This function is called when an enemy unit (unit or structure) entered vision (which was not visible last frame).\n\n        :param unit:\n        \"\"\"\n\n    async def on_enemy_unit_left_vision(self, unit_tag: int):\n        \"\"\"\n        Override this in your bot class. This function is called when an enemy unit (unit or structure) left vision (which was visible last frame).\n        Same as the self.on_unit_destroyed event, this function is called with the unit's tag because the unit is no longer visible anymore.\n        If you want to store a snapshot of the unit, use self._enemy_units_previous_map[unit_tag] for units or self._enemy_structures_previous_map[unit_tag] for structures.\n\n        Examples::\n\n            last_known_unit = self._enemy_units_previous_map.get(unit_tag, None) or self._enemy_structures_previous_map[unit_tag]\n            print(f\"Enemy unit left vision, last known location: {last_known_unit.position}\")\n\n        :param unit_tag:\n        \"\"\"\n\n    async def on_before_start(self):\n        \"\"\"\n        Override this in your bot class. This function is called before \"on_start\"\n        and before \"prepare_first_step\" that calculates expansion locations.\n        Not all data is available yet.\n        This function is useful in realtime=True mode to split your workers or start producing the first worker.\n        \"\"\"\n\n    async def on_start(self):\n        \"\"\"\n        Override this in your bot class.\n        At this point, game_data, game_info and the first iteration of game_state (self.state) are available.\n        \"\"\"\n\n    async def on_step(self, iteration: int):\n        \"\"\"\n        You need to implement this function!\n        Override this in your bot class.\n        This function is called on every game step (looped in realtime mode).\n\n        :param iteration:\n        \"\"\"\n        raise NotImplementedError\n\n    async def on_end(self, game_result: Result):\n        \"\"\"Override this in your bot class. This function is called at the end of a game.\n        Unsure if this function will be called on the laddermanager client as the bot process may forcefully be terminated.\n\n        :param game_result:\"\"\"\n\n# --- Snippet Separator ---\n\nasync def on_step(self, iteration):\n        if iteration == 0:\n            await self.chat_send(\"(glhf)\")\n\n        # Draw creep pixelmap for debugging\n        # self.draw_creep_pixelmap()\n\n        # If townhall no longer exists: attack move with all units to enemy start location\n        if not self.townhalls:\n            for unit in self.units.exclude_type({UnitTypeId.EGG, UnitTypeId.LARVA}):\n                unit.attack(self.enemy_start_locations[0])\n            return\n\n        hatch: Unit = self.townhalls[0]\n\n        # Pick a target location\n        target: Point2 = self.enemy_structures.not_flying.random_or(self.enemy_start_locations[0]).position\n\n        # Give all zerglings an attack command\n        for zergling in self.units(UnitTypeId.ZERGLING):\n            zergling.attack(target)\n\n        # Inject hatchery if queen has more than 25 energy\n        for queen in self.units(UnitTypeId.QUEEN):\n            if queen.energy >= 25 and not hatch.has_buff(BuffId.QUEENSPAWNLARVATIMER):\n                queen(AbilityId.EFFECT_INJECTLARVA, hatch)\n\n        # Pull workers out of gas if we have almost enough gas mined, this will stop mining when we reached 100 gas mined\n        if self.vespene >= 88 or self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) > 0:\n            gas_drones: Units = self.workers.filter(lambda w: w.is_carrying_vespene and len(w.orders) < 2)\n            drone: Unit\n            for drone in gas_drones:\n                minerals: Units = self.mineral_field.closer_than(10, hatch)\n                if minerals:\n                    mineral: Unit = minerals.closest_to(drone)\n                    drone.gather(mineral, queue=True)\n\n        # If we have 100 vespene, this will try to research zergling speed once the spawning pool is at 100% completion\n        if self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED\n                                        ) == 0 and self.can_afford(UpgradeId.ZERGLINGMOVEMENTSPEED):\n            spawning_pools_ready: Units = self.structures(UnitTypeId.SPAWNINGPOOL).ready\n            if spawning_pools_ready:\n                self.research(UpgradeId.ZERGLINGMOVEMENTSPEED)\n\n        # If we have less than 2 supply left and no overlord is in the queue: train an overlord\n        if self.supply_left < 2 and self.already_pending(UnitTypeId.OVERLORD) < 1:\n            self.train(UnitTypeId.OVERLORD, 1)\n\n        # While we have less than 88 vespene mined: send drones into extractor one frame at a time\n        if (\n            self.gas_buildings.ready and self.vespene < 88\n            and self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) == 0\n        ):\n            extractor: Unit = self.gas_buildings.first\n            if extractor.surplus_harvesters < 0:\n                self.workers.random.gather(extractor)\n\n        # If we have lost of minerals, make a macro hatchery\n        if self.minerals > 500:\n            for d in range(4, 15):\n                pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                if await self.can_place_single(UnitTypeId.HATCHERY, pos):\n                    self.workers.random.build(UnitTypeId.HATCHERY, pos)\n                    break\n\n        # While we have less than 16 drones, make more drones\n        if self.can_afford(UnitTypeId.DRONE) and self.supply_workers < 16:\n            self.train(UnitTypeId.DRONE)\n\n        # If our spawningpool is completed, start making zerglings\n        if self.structures(UnitTypeId.SPAWNINGPOOL).ready and self.larva and self.can_afford(UnitTypeId.ZERGLING):\n            amount_trained: int = self.train(UnitTypeId.ZERGLING, self.larva.amount)\n\n        # If we have no extractor, build extractor\n        if (\n            self.gas_buildings.amount + self.already_pending(UnitTypeId.EXTRACTOR) == 0\n            and self.can_afford(UnitTypeId.EXTRACTOR) and self.workers\n        ):\n            drone: Unit = self.workers.random\n            target: Unit = self.vespene_geyser.closest_to(drone)\n            drone.build_gas(target)\n\n        # If we have no spawning pool, try to build spawning pool\n        elif self.structures(UnitTypeId.SPAWNINGPOOL).amount + self.already_pending(UnitTypeId.SPAWNINGPOOL) == 0:\n            if self.can_afford(UnitTypeId.SPAWNINGPOOL):\n                for d in range(4, 15):\n                    pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                    if await self.can_place_single(UnitTypeId.SPAWNINGPOOL, pos):\n                        drone: Unit = self.workers.closest_to(pos)\n                        drone.build(UnitTypeId.SPAWNINGPOOL, pos)\n\n        # If we have no queen, try to build a queen if we have a spawning pool compelted\n        elif (\n            self.units(UnitTypeId.QUEEN).amount + self.already_pending(UnitTypeId.QUEEN) < self.townhalls.amount\n            and self.structures(UnitTypeId.SPAWNINGPOOL).ready\n        ):\n            if self.can_afford(UnitTypeId.QUEEN):\n                self.train(UnitTypeId.QUEEN)\n\n# --- Snippet Separator ---\n\nclass CompetitiveBot(BotAI):\n\n    async def on_start(self):\n        self.client.game_step = 2\n\n    async def on_step(self, iteration):\n        if iteration == 0:\n            await self.chat_send(\"(glhf)\")\n\n        # Draw creep pixelmap for debugging\n        # self.draw_creep_pixelmap()\n\n        # If townhall no longer exists: attack move with all units to enemy start location\n        if not self.townhalls:\n            for unit in self.units.exclude_type({UnitTypeId.EGG, UnitTypeId.LARVA}):\n                unit.attack(self.enemy_start_locations[0])\n            return\n\n        hatch: Unit = self.townhalls[0]\n\n        # Pick a target location\n        target: Point2 = self.enemy_structures.not_flying.random_or(self.enemy_start_locations[0]).position\n\n        # Give all zerglings an attack command\n        for zergling in self.units(UnitTypeId.ZERGLING):\n            zergling.attack(target)\n\n        # Inject hatchery if queen has more than 25 energy\n        for queen in self.units(UnitTypeId.QUEEN):\n            if queen.energy >= 25 and not hatch.has_buff(BuffId.QUEENSPAWNLARVATIMER):\n                queen(AbilityId.EFFECT_INJECTLARVA, hatch)\n\n        # Pull workers out of gas if we have almost enough gas mined, this will stop mining when we reached 100 gas mined\n        if self.vespene >= 88 or self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) > 0:\n            gas_drones: Units = self.workers.filter(lambda w: w.is_carrying_vespene and len(w.orders) < 2)\n            drone: Unit\n            for drone in gas_drones:\n                minerals: Units = self.mineral_field.closer_than(10, hatch)\n                if minerals:\n                    mineral: Unit = minerals.closest_to(drone)\n                    drone.gather(mineral, queue=True)\n\n        # If we have 100 vespene, this will try to research zergling speed once the spawning pool is at 100% completion\n        if self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED\n                                        ) == 0 and self.can_afford(UpgradeId.ZERGLINGMOVEMENTSPEED):\n            spawning_pools_ready: Units = self.structures(UnitTypeId.SPAWNINGPOOL).ready\n            if spawning_pools_ready:\n                self.research(UpgradeId.ZERGLINGMOVEMENTSPEED)\n\n        # If we have less than 2 supply left and no overlord is in the queue: train an overlord\n        if self.supply_left < 2 and self.already_pending(UnitTypeId.OVERLORD) < 1:\n            self.train(UnitTypeId.OVERLORD, 1)\n\n        # While we have less than 88 vespene mined: send drones into extractor one frame at a time\n        if (\n            self.gas_buildings.ready and self.vespene < 88\n            and self.already_pending_upgrade(UpgradeId.ZERGLINGMOVEMENTSPEED) == 0\n        ):\n            extractor: Unit = self.gas_buildings.first\n            if extractor.surplus_harvesters < 0:\n                self.workers.random.gather(extractor)\n\n        # If we have lost of minerals, make a macro hatchery\n        if self.minerals > 500:\n            for d in range(4, 15):\n                pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                if await self.can_place_single(UnitTypeId.HATCHERY, pos):\n                    self.workers.random.build(UnitTypeId.HATCHERY, pos)\n                    break\n\n        # While we have less than 16 drones, make more drones\n        if self.can_afford(UnitTypeId.DRONE) and self.supply_workers < 16:\n            self.train(UnitTypeId.DRONE)\n\n        # If our spawningpool is completed, start making zerglings\n        if self.structures(UnitTypeId.SPAWNINGPOOL).ready and self.larva and self.can_afford(UnitTypeId.ZERGLING):\n            amount_trained: int = self.train(UnitTypeId.ZERGLING, self.larva.amount)\n\n        # If we have no extractor, build extractor\n        if (\n            self.gas_buildings.amount + self.already_pending(UnitTypeId.EXTRACTOR) == 0\n            and self.can_afford(UnitTypeId.EXTRACTOR) and self.workers\n        ):\n            drone: Unit = self.workers.random\n            target: Unit = self.vespene_geyser.closest_to(drone)\n            drone.build_gas(target)\n\n        # If we have no spawning pool, try to build spawning pool\n        elif self.structures(UnitTypeId.SPAWNINGPOOL).amount + self.already_pending(UnitTypeId.SPAWNINGPOOL) == 0:\n            if self.can_afford(UnitTypeId.SPAWNINGPOOL):\n                for d in range(4, 15):\n                    pos: Point2 = hatch.position.towards(self.game_info.map_center, d)\n                    if await self.can_place_single(UnitTypeId.SPAWNINGPOOL, pos):\n                        drone: Unit = self.workers.closest_to(pos)\n                        drone.build(UnitTypeId.SPAWNINGPOOL, pos)\n\n        # If we have no queen, try to build a queen if we have a spawning pool compelted\n        elif (\n            self.units(UnitTypeId.QUEEN).amount + self.already_pending(UnitTypeId.QUEEN) < self.townhalls.amount\n            and self.structures(UnitTypeId.SPAWNINGPOOL).ready\n        ):\n            if self.can_afford(UnitTypeId.QUEEN):\n                self.train(UnitTypeId.QUEEN)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a StarCraft II bot using the python-sc2 library. The bot should be designed to play as the Protoss race and follow a strategy of building three bases and three stargates. The bot should manage resources, build structures, train units, and engage in combat. It should use chrono boost on active nexuses, attack with all workers if no nexuses are left, and attack the enemy with void rays when there are more than five. The bot should also manage worker distribution, build pylons when supply is low, train probes on undersaturated nexuses, expand when there are less than three nexuses, build a gateway and a cybernetics core, build gas near completed nexuses, and build stargates when there are less than three but at least three nexuses. The bot should also train void rays at idle stargates when there are at least three townhalls. The bot should be run on the \"(2)CatalystLE\" map against an easy difficulty Protoss computer opponent.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 94, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that performs a SIwave DCIR analysis in HFSS 3D Layout using the pyaedt library. The code should first configure EDB for DCIR analysis by creating a temporary directory and downloading an example board into EDB. Then, it should create pin groups on VRM positive and negative pins, and create a voltage source between these pin groups. Similarly, it should create pin groups on sink component positive and negative pins, and place a current source between these pin groups. After that, it should add a SIwave DCIR analysis, save and close EDB. The code should then launch AEDT, import the configured EDB and analyze DCIR. It should also retrieve and print the loop resistance, current source, and via information from the DCIR element data, as well as the voltage from the DCIR solution data. Finally, the code should close the AEDT project and release the desktop.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def create_resistor_on_pin(self, pos_pin, neg_pin, rvalue=1, resistor_name=\"\"):\n        \"\"\"Create a voltage source.\n\n        Parameters\n        ----------\n        pos_pin : Object\n            Positive Pin.\n        neg_pin : Object\n            Negative Pin.\n        rvalue : float, optional\n            Resistance value. The default is ``1``.\n        resistor_name : str, optional\n            Name of the resistor. The default is ``\"\"``.\n\n        Returns\n        -------\n        str\n            Name of the Resistor.\n\n        Examples\n        --------\n\n        >>> from pyaedt import Edb\n        >>> edbapp = Edb(\"myaedbfolder\", \"project name\", \"release version\")\n        >>> pins =edbapp.core_components.get_pin_from_component(\"U2A5\")\n        >>> edbapp.core_hfss.create_resistor_on_pin(pins[0], pins[1],50,\"res_name\")\n        \"\"\"\n        return self._pedb.core_siwave.create_resistor_on_pin(pos_pin, neg_pin, rvalue, resistor_name)\n\n# --- Snippet Separator ---\n\ndef create_voltage_source_on_pin(self, pos_pin, neg_pin, voltage_value=3.3, phase_value=0, source_name=\"\"):\n        \"\"\"Create a voltage source.\n\n        Parameters\n        ----------\n        pos_pin : Object\n            Positive Pin.\n        neg_pin : Object\n            Negative Pin.\n        voltage_value : float, optional\n            Value for the voltage. The default is ``3.3``.\n        phase_value : optional\n            Value for the phase. The default is ``0``.\n        source_name : str, optional\n            Name of the source. The default is ``\"\"``.\n\n        Returns\n        -------\n        str\n            Source Name\n\n        Examples\n        --------\n\n        >>> from pyaedt import Edb\n        >>> edbapp = Edb(\"myaedbfolder\", \"project name\", \"release version\")\n        >>> pins =edbapp.core_components.get_pin_from_component(\"U2A5\")\n        >>> edbapp.core_hfss.create_voltage_source_on_pin(pins[0], pins[1],50,\"source_name\")\n        \"\"\"\n        return self._pedb.core_siwave.create_voltage_source_on_pin(\n            pos_pin, neg_pin, voltage_value, phase_value, source_name\n        )\n\n# --- Snippet Separator ---\n\ndef create_current_source_on_pin(self, pos_pin, neg_pin, current_value=0.1, phase_value=0, source_name=\"\"):\n        \"\"\"Create a current source.\n\n        Parameters\n        ----------\n        pos_pin : Object\n            Positive Pin.\n        neg_pin : Object\n            Negative Pin.\n        current_value : float, optional\n            Value for the current. The default is ``0.1``.\n        phase_value : optional\n            Value for the phase. The default is ``0``.\n        source_name : str, optional\n            Name of the source. The default is ``\"\"``.\n\n        Returns\n        -------\n        str\n            Source Name.\n\n        Examples\n        --------\n\n        >>> from pyaedt import Edb\n        >>> edbapp = Edb(\"myaedbfolder\", \"project name\", \"release version\")\n        >>> pins =edbapp.core_components.get_pin_from_component(\"U2A5\")\n        >>> edbapp.core_hfss.create_current_source_on_pin(pins[0], pins[1],50,\"source_name\")\n        \"\"\"\n\n        return self._pedb.core_siwave.create_current_source_on_pin(\n            pos_pin, neg_pin, current_value, phase_value, source_name\n        )\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a SIwave DCIR analysis in HFSS 3D Layout using the pyaedt library. The code should first configure EDB for DCIR analysis by creating a temporary directory and downloading an example board into EDB. Then, it should create pin groups on VRM positive and negative pins, and create a voltage source between these pin groups. Similarly, it should create pin groups on sink component positive and negative pins, and place a current source between these pin groups. After that, it should add a SIwave DCIR analysis, save and close EDB. The code should then launch AEDT, import the configured EDB and analyze DCIR. It should also retrieve and print the loop resistance, current source, and via information from the DCIR element data, as well as the voltage from the DCIR solution data. Finally, the code should close the AEDT project and release the desktop.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 95, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that uses the Avalanche library for a question answering task on the SQuAD dataset using the T5 model from HuggingFace's transformers library. The code should include a custom class that extends Avalanche's Naive class to adapt it for machine translation tasks. The main function should load the SQuAD dataset, preprocess it, and divide it into training and validation sets. It should then initialize the T5 model, set up the continual learning scenario with Avalanche, and train the model on the training set. Finally, the code should test the model by asking it a question and printing the model's answer.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class DynamicModule(Module):\n    \"\"\"\n        Dynamic Modules are Avalanche modules that can be incrementally\n        expanded to allow architectural modifications (multi-head\n        classifiers, progressive networks, ...).\n\n        Compared to pytoch Modules, they provide an additional method,\n        `model_adaptation`, which adapts the model given data from the\n        current experience.\n    \"\"\"\n\n    def adaptation(self, dataset: AvalancheDataset = None):\n        \"\"\" Adapt the module (freeze units, add units...) using the current\n        data. Optimizers must be updated after the model adaptation.\n\n        Avalanche strategies call this method to adapt the architecture\n        *before* processing each experience. Strategies also update the\n        optimizer automatically.\n\n        .. warning::\n            As a general rule, you should NOT use this method to train the\n            model. The dataset should be used only to check conditions which\n            require the model's adaptation, such as the discovery of new\n            classes or tasks.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        if self.training:\n            self.train_adaptation(dataset)\n        else:\n            self.eval_adaptation(dataset)\n\n    def train_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at training time.\n\n        Avalanche strategies automatically call this method *before* training\n        on each experience.\n        \"\"\"\n        pass\n\n    def eval_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at evaluation time.\n\n        Avalanche strategies automatically call this method *before* evaluating\n        on each experience.\n\n        .. warning::\n            This method receives the experience's data at evaluation time\n            because some dynamic models need it for adaptation. For example,\n            an incremental classifier needs to be expanded even at evaluation\n            time if new classes are available. However, you should **never**\n            use this data to **train** the module's parameters.\n        \"\"\"\n        pass\n\n# --- Snippet Separator ---\n\ndef adaptation(self, dataset: AvalancheDataset = None):\n        \"\"\" Adapt the module (freeze units, add units...) using the current\n        data. Optimizers must be updated after the model adaptation.\n\n        Avalanche strategies call this method to adapt the architecture\n        *before* processing each experience. Strategies also update the\n        optimizer automatically.\n\n        .. warning::\n            As a general rule, you should NOT use this method to train the\n            model. The dataset should be used only to check conditions which\n            require the model's adaptation, such as the discovery of new\n            classes or tasks.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        if self.training:\n            self.train_adaptation(dataset)\n        else:\n            self.eval_adaptation(dataset)\n\n# --- Snippet Separator ---\n\nclass Dataset(Serializable):\n    \"\"\"\n    Preparing data for model training and inferencing.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        \"\"\"\n        init is designed to finish following steps:\n\n        - setup data\n            - The data related attributes' names should start with '_' so that it will not be saved on disk when serializing.\n\n        - initialize the state of the dataset(info to prepare the data)\n            - The name of essential state for preparing data should not start with '_' so that it could be serialized on disk when serializing.\n\n        The data could specify the info to caculate the essential data for preparation\n        \"\"\"\n        self.setup_data(*args, **kwargs)\n        super().__init__()\n\n    def setup_data(self, *args, **kwargs):\n        \"\"\"\n        Setup the data.\n\n        We split the setup_data function for following situation:\n\n        - User have a Dataset object with learned status on disk.\n\n        - User load the Dataset object from the disk(Note the init function is skiped).\n\n        - User call `setup_data` to load new data.\n\n        - User prepare data for model based on previous status.\n        \"\"\"\n        pass\n\n    def prepare(self, *args, **kwargs) -> object:\n        \"\"\"\n        The type of dataset depends on the model. (It could be pd.DataFrame, pytorch.DataLoader, etc.)\n        The parameters should specify the scope for the prepared data\n        The method should:\n        - process the data\n\n        - return the processed data\n\n        Returns\n        -------\n        object:\n            return the object\n        \"\"\"\n        pass\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the Avalanche library for a question answering task on the SQuAD dataset using the T5 model from HuggingFace's transformers library. The code should include a custom class that extends Avalanche's Naive class to adapt it for machine translation tasks. The main function should load the SQuAD dataset, preprocess it, and divide it into training and validation sets. It should then initialize the T5 model, set up the continual learning scenario with Avalanche, and train the model on the training set. Finally, the code should test the model by asking it a question and printing the model's answer.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 96, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that imports necessary libraries and modules from the porepy library. The code should define two functions, `add_data_darcy` and `add_data_advection`, which add data to a given grid bucket (`gb`) and domain with a specified tolerance (`tol`). The `add_data_darcy` function should add parameters related to Darcy's law, including permeability, source, aperture, and boundary conditions. The `add_data_advection` function should add parameters related to advection, including source, porosity, discharge, and boundary conditions. \n\nThe code should then define variables for tolerance, export folder, time, number of time steps, time step size, export frequency, and a boolean for coarsening. It should also define a dictionary for mesh size and a dictionary for domain boundaries. \n\nThe code should import a grid from a CSV file, compute its geometry, coarsen it if necessary, and assign node ordering. It should then create a Darcy solver, add Darcy data to the grid bucket, solve the Darcy problem, and extract and project the discharge and pressure. It should also compute the total flow rate and export the results to a VTK file.\n\nThe code should then define variables for physics, create advection and mass matrix solvers, add advection data to the grid bucket, and add a time step property to the grid bucket. It should then create matrices and right-hand sides for the advection and mass matrix problems, and perform an LU factorization to speed up the solver. \n\nThe code should then initialize a solution vector and arrays for time and production, and loop over the time steps to update the solution, compute the production, and export the solution to a VTK file every specified number of time steps. Finally, it should export the time steps to a PVD file and save the times and absolute production values to a text file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class EllipticModel():\n    '''\n    Class for solving an incompressible flow problem:\n    \\nabla K \\nabla p = q,\n    where K is the second order permeability tenser, p the fluid pressure\n    and q sinks and sources.\n\n    Parameters in Init:\n    gb: (Grid /GridBucket) a grid or grid bucket object. If gb = GridBucket\n        a Parameter class should be added to each grid bucket data node with\n        keyword 'param'.\n    data: (dictionary) Defaults to None. Only used if gb is a Grid. Should\n          contain a Parameter class with the keyword 'Param'\n    physics: (string): defaults to 'flow'\n\n    Functions:\n    solve(): Calls reassemble and solves the linear system.\n             Returns: the pressure p.\n             Sets attributes: self.x\n    step(): Same as solve, but without reassemble of the matrices\n    reassemble(): Assembles the lhs matrix and rhs array.\n            Returns: lhs, rhs.\n            Sets attributes: self.lhs, self.rhs\n    source_disc(): Defines the discretization of the source term.\n            Returns Source discretization object\n    flux_disc(): Defines the discretization of the flux term.\n            Returns Flux discretization object (E.g., Tpfa)\n    grid(): Returns: the Grid or GridBucket\n    data(): Returns: Data dictionary\n    split(name): Assignes the solution self.x to the data dictionary at each\n                 node in the GridBucket.\n                 Parameters:\n                    name: (string) The keyword assigned to the pressure\n    discharge(): Calls split('pressure'). Then calculate the discharges over each\n                 face in the grids and between edges in the GridBucket\n    save(): calls split('pressure'). Then export the pressure to a vtk file to the\n            folder kwargs['folder_name'] with file name\n            kwargs['file_name'], default values are 'results' for the folder and\n            physics for the file name.\n    '''\n\n    def __init__(self, gb, data=None, physics='flow', **kwargs):\n        self.physics = physics\n        self._gb = gb\n        self.is_GridBucket = isinstance(self._gb, GridBucket)\n        self._data = data\n\n        self.lhs = []\n        self.rhs = []\n        self.x = []\n\n        file_name = kwargs.get('file_name', physics)\n        folder_name = kwargs.get('folder_name', 'results')\n        mesh_kw = kwargs.get('mesh_kw', {})\n\n        tic = time.time()\n        logger.info('Create exporter')\n        self.exporter = Exporter(self._gb, file_name, folder_name, **mesh_kw)\n        logger.info('Elapsed time: ' + str(time.time() - tic))\n\n        self._flux_disc = self.flux_disc()\n        self._source_disc = self.source_disc()\n\n    def solve(self, max_direct=40000, callback=False, **kwargs):\n        \"\"\" Reassemble and solve linear system.\n\n        After the funtion has been called, the attributes lhs and rhs are\n        updated according to the parameter states. Also, the attribute x\n        gives the pressure given the current state.\n\n        TODO: Provide an option to save solver information if multiple\n        systems are to be solved with the same left hand side.\n\n        The function attempts to set up the best linear solver based on the\n        system size. The setup and parameter choices here are still\n        experimental.\n\n        Parameters:\n            max_direct (int): Maximum number of unknowns where a direct solver\n                is applied. If a direct solver can be applied this is usually\n                the most efficient option. However, if the system size is\n                too large compared to available memory, a direct solver becomes\n                extremely slow.\n            callback (boolean, optional): If True iteration information will be\n                output when an iterative solver is applied (system size larger\n                than max_direct)\n\n        Returns:\n            np.array: Pressure state.\n\n        \"\"\"\n        logger.error('Solve elliptic model')\n        # Discretize\n        tic = time.time()\n        logger.warning('Discretize')\n        self.lhs, self.rhs = self.reassemble()\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n\n        # Solve\n        tic = time.time()\n        ls = LSFactory()\n        if self.rhs.size < max_direct:\n            logger.warning('Solve linear system using direct solver')\n            self.x = ls.direct(self.lhs, self.rhs)\n        else:\n            logger.warning('Solve linear system using GMRES')\n            precond = self._setup_preconditioner()\n#            precond = ls.ilu(self.lhs)\n            slv = ls.gmres(self.lhs)\n            self.x, info = slv(self.rhs, M=precond, callback=callback,\n                               maxiter=10000, restart=1500, tol=1e-8)\n            if info == 0:\n                logger.warning('GMRES succeeded.')\n            else:\n                logger.warning('GMRES failed with status ' + str(info))\n\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n        return self.x\n\n    def step(self):\n        return self.solve()\n\n    def reassemble(self):\n        \"\"\"\n        reassemble matrices. This must be called between every time step to\n        update the rhs of the system.\n        \"\"\"\n        lhs_flux, rhs_flux = self._discretize(self._flux_disc)\n        lhs_source, rhs_source = self._discretize(self._source_disc)\n        assert lhs_source.nnz == 0, 'Source lhs different from zero!'\n        self.lhs = lhs_flux\n        self.rhs = rhs_flux + rhs_source\n        return self.lhs, self.rhs\n\n    def source_disc(self):\n        if self.is_GridBucket:\n            return source.IntegralMixedDim(physics=self.physics)\n        else:\n            return source.Integral(physics=self.physics)\n\n    def flux_disc(self):\n        if self.is_GridBucket:\n            return tpfa.TpfaMixedDim(physics=self.physics)\n        else:\n            return tpfa.Tpfa(physics=self.physics)\n\n    def _discretize(self, discr):\n        if self.is_GridBucket:\n            return discr.matrix_rhs(self.grid())\n        else:\n            return discr.matrix_rhs(self.grid(), self.data())\n\n    def grid(self):\n        return self._gb\n\n    def data(self):\n        return self._data\n\n    def split(self, x_name='solution'):\n        self.x_name = x_name\n        self._flux_disc.split(self.grid(), self.x_name, self.x)\n\n    def pressure(self, pressure_name='pressure'):\n        self.pressure_name = pressure_name\n        if self.is_GridBucket:\n            self.split(self.pressure_name)\n        else:\n            self._data[self.pressure_name] = self.x\n\n    def discharge(self, discharge_name='discharge'):\n        if self.is_GridBucket:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       p_name=self.pressure_name)\n        else:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       self.pressure_name,\n                                       self._data)\n\n    def permeability(self, perm_names=['kxx', 'kyy', 'kzz']):\n        \"\"\" Assign permeability to self._data, ready for export to vtk.\n\n        For the moment, we only dump the main diagonals of the permeabliity.\n        Extensions should be trivial if needed.\n\n        Parameters:\n            perm_names (list): Which components to export. Defaults to kxx,\n                kyy and xzz.\n\n        \"\"\"\n\n        def get_ind(n):\n            if n == 'kxx':\n                return 0\n            elif n == 'kyy':\n                return 1\n            elif n == 'kzz':\n                return 2\n            else:\n                raise ValueError('Unknown perm keyword ' + n)\n\n        for n in perm_names:\n            ind = get_ind(n)\n            if self.is_GridBucket:\n                for _, d in self.grid():\n                    d[n] = d['param'].get_permeability().perm[ind, ind, :]\n            else:\n                self._data[n] = self._data['param'].get_permeability()\\\n                    .perm[ind, ind, :]\n\n    def porosity(self, poro_name='porosity'):\n        if self.is_GridBucket:\n            for _, d in self.grid():\n                d[poro_name] = d['param'].get_porosity()\n        else:\n            self._data[poro_name] = self._data['param'].get_porosity()\n\n    def save(self, variables=None, save_every=None):\n        if variables is None:\n            self.exporter.write_vtk()\n        else:\n            if not self.is_GridBucket:\n                variables = {k: self._data[k] for k in variables\n                             if k in self._data}\n            self.exporter.write_vtk(variables)\n\n    # Helper functions for linear solve below\n    def _setup_preconditioner(self):\n        solvers, ind, not_ind = self._assign_solvers()\n\n        def precond(r):\n            x = np.zeros_like(r)\n            for s, i, ni in zip(solvers, ind, not_ind):\n                x[i] += s(r[i])\n            return x\n\n        def precond_mult(r):\n            x = np.zeros_like(r)\n            A = self.lhs\n            for s, i, ni in zip(solvers, ind, not_ind):\n                r_i = r[i] - A[i, :][:, ni] * x[ni]\n                x[i] += s(r_i)\n            return x\n\n        def M(r): return precond(r)\n        return spl.LinearOperator(self.lhs.shape, M)\n\n    def _assign_solvers(self):\n        mat, ind = self._obtain_submatrix()\n        all_ind = np.arange(self.rhs.size)\n        not_ind = [np.setdiff1d(all_ind, i) for i in ind]\n\n        factory = LSFactory()\n        num_mat = len(mat)\n        solvers = np.empty(num_mat, dtype=np.object)\n        for i, A in enumerate(mat):\n            sz = A.shape[0]\n            if sz < 5000:\n                solvers[i] = factory.direct(A)\n            else:\n                # amg solver is pyamg is installed, if not ilu\n                try:\n                    solvers[i] = factory.amg(A, as_precond=True)\n                except ImportError:\n                    solvers[i] = factory.ilu(A)\n\n        return solvers, ind, not_ind\n\n    def _obtain_submatrix(self):\n\n        if isinstance(self.grid(), GridBucket):\n            gb = self.grid()\n            fd = self.flux_disc()\n            mat = []\n            sub_ind = []\n            for g, _ in self.grid():\n                ind = fd.solver.dof_of_grid(gb, g)\n                A = self.lhs[ind, :][:, ind]\n                mat.append(A)\n                sub_ind.append(ind)\n            return mat, sub_ind\n        else:\n            return [self.lhs], [np.arange(self.grid().num_cells)]\n\n# --- Snippet Separator ---\n\ndef write_vtk(self, data=None, time_step=None, grid=None):\n        \"\"\" Interface function to export in VTK the grid and additional data.\n\n        In 2d the cells are represented as polygon, while in 3d as polyhedra.\n        VTK module need to be installed.\n        In 3d the geometry of the mesh needs to be computed.\n\n        To work with python3, the package vtk should be installed in version 7\n        or higher.\n\n        Parameters:\n        data: if g is a single grid then data is a dictionary (see example)\n              if g is a grid bucket then list of names for optional data,\n              they are the keys in the grid bucket (see example).\n        time_step: (optional) in a time dependent problem defines the full name of\n            the file.\n        grid: (optional) in case of changing grid set a new one.\n\n        \"\"\"\n        if self.is_not_vtk:\n            return\n\n        if self.fixed_grid and grid is not None:\n            raise ValueError(\"Inconsistency in exporter setting\")\n        elif not self.fixed_grid and grid is not None:\n            self.gb = grid\n            self.is_GridBucket = isinstance(self.gb, grid_bucket.GridBucket)\n            self._update_gVTK()\n\n        if self.is_GridBucket:\n            self._export_vtk_gb(data, time_step)\n        else:\n            # No need of special naming, create the folder\n            name = self._make_folder(self.folder, self.name)\n            self._export_vtk_single(data, time_step, self.gb, name)\n\n# --- Snippet Separator ---\n\ndef __init__(self, grid, name, folder=None, **kwargs):\n        \"\"\"\n        Parameters:\n        grid: the grid or grid bucket\n        name: the root of file name without any extension.\n        folder: (optional) the folder to save the file. If the folder does not\n            exist it will be created.\n\n        Optional arguments in kwargs:\n        fixed_grid: (optional) in a time dependent simulation specify if the\n            grid changes in time or not. The default is True.\n        binary: export in binary format, default is True.\n\n        How to use:\n        If you need to export a single grid:\n        save = Exporter(g, \"solution\", folder=\"results\")\n        save.write_vtk({\"cells_id\": cells_id, \"pressure\": pressure})\n\n        In a time loop:\n        save = Exporter(gb, \"solution\", folder=\"results\")\n        while time:\n            save.write_vtk({\"conc\": conc}, time_step=i)\n        save.write_pvd(steps*deltaT)\n\n        if you need to export the grid bucket\n        save = Exporter(gb, \"solution\", folder=\"results\")\n        save.write_vtk(gb, [\"cells_id\", \"pressure\"])\n\n        In a time loop:\n        while time:\n            save.write_vtk([\"conc\"], time_step=i)\n        save.write_pvd(steps*deltaT)\n\n        In the case of different physics, change the file name with\n        \"change_name\".\n\n        \"\"\"\n\n        self.gb = grid\n        self.name = name\n        self.folder = folder\n        self.fixed_grid = kwargs.get('fixed_grid', True)\n        self.binary = kwargs.get('binary', True)\n\n        self.is_GridBucket = isinstance(self.gb, grid_bucket.GridBucket)\n        self.is_not_vtk = 'vtk' not in sys.modules\n\n        if self.is_not_vtk:\n            return\n\n        if self.is_GridBucket:\n            self.gb_VTK = np.empty(self.gb.size(), dtype=np.object)\n        else:\n            self.gb_VTK = None\n\n        self.has_numba = 'numba' in sys.modules\n\n        if self.fixed_grid:\n            self._update_gb_VTK()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary libraries and modules from the porepy library. The code should define two functions, `add_data_darcy` and `add_data_advection`, which add data to a given grid bucket (`gb`) and domain with a specified tolerance (`tol`). The `add_data_darcy` function should add parameters related to Darcy's law, including permeability, source, aperture, and boundary conditions. The `add_data_advection` function should add parameters related to advection, including source, porosity, discharge, and boundary conditions. \n\nThe code should then define variables for tolerance, export folder, time, number of time steps, time step size, export frequency, and a boolean for coarsening. It should also define a dictionary for mesh size and a dictionary for domain boundaries. \n\nThe code should import a grid from a CSV file, compute its geometry, coarsen it if necessary, and assign node ordering. It should then create a Darcy solver, add Darcy data to the grid bucket, solve the Darcy problem, and extract and project the discharge and pressure. It should also compute the total flow rate and export the results to a VTK file.\n\nThe code should then define variables for physics, create advection and mass matrix solvers, add advection data to the grid bucket, and add a time step property to the grid bucket. It should then create matrices and right-hand sides for the advection and mass matrix problems, and perform an LU factorization to speed up the solver. \n\nThe code should then initialize a solution vector and arrays for time and production, and loop over the time steps to update the solution, compute the production, and export the solution to a VTK file every specified number of time steps. Finally, it should export the time steps to a PVD file and save the times and absolute production values to a text file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 97, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that performs scatter estimation in PET imaging using the SIRF library. The code should accept command-line arguments for the raw data file, randoms data file, attenuation correction factors file, path to normalization and attenuation files, normalization file, attenuation image file, output prefix for scatter estimates, and a non-interactive mode. The code should then process these options, set up the scatter estimator, and perform the scatter estimation. If the non-interactive mode is not set, the code should display the scatter estimate and plot a sinogram profile. The code should also handle any errors that occur during the process.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class ScatterEstimator():\n    '''\n    Class for estimating the scatter contribution in PET projection data\n\n    This class implements the SSS iterative algorithm from STIR. It\n    is an iterative loop of reconstruction, single scatter estimation,\n    upsampling, tail-fitting.\n\n    Output is an acquisition_data object with the scatter contribution.\n    This can then be added to the randoms to use in PETAcquisitionModel.set_background_term().\n    '''\n    def __init__(self, filename = ''):\n        self.handle = None\n        self.image = None\n        self.name = 'PETScatterEstimator'\n        self.filename = filename\n\n        if not self.filename:\n            self.handle = pystir.cSTIR_newObject(self.name)\n        else:\n            self.handle = pystir.cSTIR_objectFromFile(self.name, self.filename)\n\n        check_status(self.handle)\n\n    def __del__(self):\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n    def set_up(self):\n        \"\"\"\n        Set up.\n\n        Prepare this object for performing scatter estimation;\n        All input has to be set before calling this function.\n        \"\"\"\n        try_calling(pystir.cSTIR_setupScatterEstimator(\n            self.handle))\n\n    def process(self):\n        \"\"\"\n        Runs the scatter estimation.\n\n        You need to run set_up() first.\n        \"\"\"\n        print('ScatterEstimator:: Waiting for the scatter estimation to finish ...')\n        self.output = AcquisitionData()\n        self.output.handle = pystir.cSTIR_runScatterEstimator(self.handle)\n        check_status(self.output.handle)\n        print('ScatterEstimator:: estimation finished.')\n\n    def get_output(self):\n        \"\"\"\n        Return the final scatter estimate.\n        \"\"\"\n        data = AcquisitionData()\n        data.handle = parms.parameter_handle(self.handle, 'PETScatterEstimator', 'output')\n        check_status(data.handle)\n        return data\n\n    def get_num_iterations(self):\n        \"\"\"Get number of iterations of the SSS algorithm to use.\"\"\"\n        return parms.int_par(self.handle, 'PETScatterEstimator', 'num_iterations')\n\n    def set_attenuation_image(self, image):\n        assert_validity(image, ImageData)\n        parms.set_parameter(self.handle, self.name, 'setAttenuationImage', image.handle)\n\n    def set_attenuation_correction_factors(self, arg):\n        assert_validity(arg, AcquisitionData)\n        parms.set_parameter(self.handle, self.name, 'setAttenuationCorrectionFactors', arg.handle)\n\n    def set_input(self, acq_data):\n        assert_validity(acq_data, AcquisitionData)\n        parms.set_parameter(self.handle, self.name, 'setInput', acq_data.handle)\n\n    def set_randoms(self, acq_data):\n        assert_validity(acq_data, AcquisitionData)\n        parms.set_parameter(self.handle, self.name, 'setRandoms', acq_data.handle)\n\n    def set_asm(self, asm):\n        '''Set acquisition sensitivity model (without attenuation!)'''\n        assert_validity(asm, AcquisitionSensitivityModel)\n        parms.set_parameter(self.handle, self.name, 'setASM', asm.handle)\n\n    def set_num_iterations(self, v):\n        \"\"\"Set number of iterations of the SSS algorithm to use.\"\"\"\n        parms.set_int_par(self.handle, 'PETScatterEstimator', 'set_num_iterations', v)\n\n    def set_output_prefix(self, v):\n        \"\"\"\n        Set prefix for filenames with scatter estimates.\n\n        Actual filenames will append the iteration number and the .hs extension\n        as common for STIR Interfile data.\n\n        Set it to the empty string to prevent any output.\n        \"\"\"\n        parms.set_char_par(self.handle, 'PETScatterEstimator', 'set_output_prefix', v)\n\n# --- Snippet Separator ---\n\nclass SingleScatterSimulator():\n    '''\n    Class for simulating the scatter contribution to PET data.\n\n    This class uses the STIR Single Scatter simulation, taking as input an\n    activity and attenuation image, and a acquisition data template.\n\n    WARNING: Currently this class does not use the low-resolution sampling\n    mechanism of STIR. This means that if you give it a full resolution acq_data,\n    you will likely run out of memory and/or time.\n    '''\n    def __init__(self, filename = ''):\n        self.handle = None\n        self.image = None\n        self.name = 'PETSingleScatterSimulator'\n        self.filename = filename\n\n        if not self.filename:\n            self.handle = pystir.cSTIR_newObject(self.name)\n        else:\n            self.handle = pystir.cSTIR_objectFromFile(self.name, self.filename)\n        check_status(self.handle)\n\n    def __del__(self):\n        if self.handle is not None:\n            pyiutil.deleteDataHandle(self.handle)\n\n    def set_up(self, acq_templ, img_templ):\n        \"\"\"Set up.\n\n        Prepare this object for performing forward operations;\n        acq_templ:  an AcquisitionData object used as a template for\n                    creating an AcquisitionData object to store forward\n                    projection;\n        img_templ:  an ImageData object used as a template for checking geometry etc\n\n        attenuation image has to be set first\n        \"\"\"\n        assert_validity(acq_templ, AcquisitionData)\n        assert_validity(img_templ, ImageData)\n\n        # temporarily save the templates in the class\n        self.acq_templ = acq_templ\n        #self.img_templ = img_templ\n\n        try_calling(pystir.cSTIR_setupScatterSimulator(\n            self.handle, acq_templ.handle, img_templ.handle))\n\n    def forward(self, image,  out=None):\n        \"\"\"Return the scatter estimation for the input activity image.\n\n        image   :  an ImageData object.\n\n        set_up() has to be called first.\n        \"\"\"\n        assert_validity(image, ImageData)\n        if out is None:\n            ad = AcquisitionData()\n            ad.handle = pystir.cSTIR_scatterSimulatorFwd(\n                self.handle, image.handle);\n            check_status(ad.handle)\n            return ad\n        ad = out\n        assert_validity(ad, AcquisitionData)\n        try_calling(pystir.cSTIR_scatterSimulatorFwdReplace(\n            self.handle, image.handle, ad.handle))\n\n    def set_attenuation_image(self, image):\n        assert_validity(image, ImageData)\n        parms.set_parameter(self.handle, self.name, 'setAttenuationImage', image.handle)\n\n# --- Snippet Separator ---\n\ndef process(self):\n        \"\"\"\n        Runs the scatter estimation.\n\n        You need to run set_up() first.\n        \"\"\"\n        print('ScatterEstimator:: Waiting for the scatter estimation to finish ...')\n        self.output = AcquisitionData()\n        self.output.handle = pystir.cSTIR_runScatterEstimator(self.handle)\n        check_status(self.output.handle)\n        print('ScatterEstimator:: estimation finished.')\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs scatter estimation in PET imaging using the SIRF library. The code should accept command-line arguments for the raw data file, randoms data file, attenuation correction factors file, path to normalization and attenuation files, normalization file, attenuation image file, output prefix for scatter estimates, and a non-interactive mode. The code should then process these options, set up the scatter estimator, and perform the scatter estimation. If the non-interactive mode is not set, the code should display the scatter estimate and plot a sinogram profile. The code should also handle any errors that occur during the process.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 98, "repo_full_name": "pyomo__mpi-sppy", "instruction": "Generate code that imports necessary modules and functions from the 'hydro' and 'mpisppy' libraries. The code should define a function to parse arguments and create a configuration object with various arguments. Then, define a main function that calls the argument parsing function, checks the length of the branching factors, and creates node names from these factors. The main function should also create scenario names, set up a scenario creator and denouement, and prepare the necessary arguments for the 'vanilla' cylinders. Depending on the configuration, the main function should also set up spokes for the Lagrangian bound and xhat looper bound. The function should then create a wheel spinner with the hub and spoke dictionaries, spin the wheel, and print the best inner and outer bounds. If a certain condition is met, the function should write the first stage and full tree solutions. Finally, the code should call the main function if it is the main module.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class MainFunction(Function):\n    \"\"\"Subclass of Function that allows multiple functions and variables to\n    be defined in a single code string. The code must contain a main() function\n    definition.\n    \"\"\"\n\n    def __init__(self, shader_type, *args, **kwargs):\n        self.shader_type = shader_type\n        self._chains = {}\n        Function.__init__(self, *args, **kwargs)\n\n    @property\n    def signature(self):\n        return ('main', [], 'void')\n\n    @property\n    def version_pragma(self):\n        \"\"\"Return version number and extra qualifiers from pragma if present.\"\"\"\n        m = re.search(parsing.re_version_pragma, self.code)\n        if m is None:\n            return None\n        return int(m.group(1)), m.group(2)\n\n    def definition(self, obj_names, version, shader):\n        code = Function.definition(self, obj_names, version, shader)\n        # strip out version pragma before returning code; this will be\n        # added to the final compiled code later.\n        code = re.sub(parsing.re_version_pragma, '', code)\n        return code\n\n    def static_names(self):\n        if self._static_vars is not None:\n            return self._static_vars\n\n        # parse static variables\n        names = Function.static_names(self)\n\n        # parse all function names + argument names\n        funcs = parsing.find_functions(self.code)\n        for f in funcs:\n            if f[0] == 'main':\n                continue\n            names.append(f[0])\n            for arg in f[1]:\n                names.append(arg[1])\n\n        self._static_vars = names\n        return names\n\n    def add_chain(self, var):\n        \"\"\"Create a new ChainFunction and attach to $var.\"\"\"\n        chain = FunctionChain(var, [])\n        self._chains[var] = chain\n        self[var] = chain\n\n    def add_callback(self, hook, func):\n        self._chains[hook].append(func)\n\n    def remove_callback(self, hook, func):\n        self._chains[hook].remove(func)\n\n# --- Snippet Separator ---\n\ndef run_function_modally(function, progress_maximum, *args, **kwargs):\n    \"\"\"Create a temporary ExperimentWithProgressBar and run it modally.\n\n    This convenience function allows a function to be run with a nice progress bar, without the hassle\n    of setting up an Experiment object.  The function must accept a keyword argument, update_progress,\n    which is a function - it should be called periodically, with a numeric argument that starts at zero\n    and increments up to a final value of progress_maximum.  A sensible default for this argument would\n    be ``lambda p: p``, which is a function that does nothing.\n\n    Positional and keyword arguments are passed through, the only other argument needed is\n    progress_maximum, which sets the final value of progress.\n    \"\"\"\n #   function(*args, **kwargs)\n    e = RunFunctionWithProgressBar(function, progress_maximum=progress_maximum)\n    e.run_modally(*args, **kwargs)\n\n# --- Snippet Separator ---\n\nclass Function(ShaderObject):\n    \"\"\"Representation of a GLSL function\n\n    Objects of this class can be used for re-using and composing GLSL\n    snippets. Each Function consists of a GLSL snippet in the form of\n    a function. The code may have template variables that start with\n    the dollar sign. These stubs can be replaced with expressions using\n    the index operation. Expressions can be:\n\n    * plain text that is inserted verbatim in the code\n    * a Function object or a call to a funcion\n    * a Variable (or Varying) object\n    * float, int, tuple are automatically turned into a uniform Variable\n    * a VertexBuffer is automatically turned into an attribute Variable\n\n    All functions have implicit \"$pre\" and \"$post\" placeholders that may be\n    used to insert code at the beginning and end of the function.\n\n    Examples\n    --------\n    This example shows the basic usage of the Function class::\n\n        vert_code_template = Function('''\n            void main() {\n            gl_Position = $pos;\n            gl_Position.x += $xoffset;\n            gl_Position.y += $yoffset;\n        }''')\n\n        scale_transform = Function('''\n        vec4 transform_scale(vec4 pos){\n            return pos * $scale;\n        }''')\n\n        # If you get the function from a snippet collection, always\n        # create new Function objects to ensure they are 'fresh'.\n        vert_code = Function(vert_code_template)\n        trans1 = Function(scale_transform)\n        trans2 = Function(scale_transform)  # trans2 != trans1\n\n        # Three ways to assign to template variables:\n        #\n        # 1) Assign verbatim code\n        vert_code['xoffset'] = '(3.0 / 3.1415)'\n\n        # 2) Assign a value (this creates a new uniform or attribute)\n        vert_code['yoffset'] = 5.0\n\n        # 3) Assign a function call expression\n        pos_var = Variable('attribute vec4 a_position')\n        vert_code['pos'] = trans1(trans2(pos_var))\n\n        # Transforms also need their variables set\n        trans1['scale'] = 0.5\n        trans2['scale'] = (1.0, 0.5, 1.0, 1.0)\n\n        # You can actually change any code you want, but use this with care!\n        vert_code.replace('gl_Position.y', 'gl_Position.z')\n\n        # Finally, you can set special variables explicitly. This generates\n        # a new statement at the end of the vert_code function.\n        vert_code['gl_PointSize'] = '10.'\n\n\n    If we use ``vert_code.compile()`` we get::\n\n        attribute vec4 a_position;\n        uniform float u_yoffset;\n        uniform float u_scale_1;\n        uniform vec4 u_scale_2;\n        uniform float u_pointsize;\n\n        vec4 transform_scale_1(vec4 pos){\n            return pos * u_scale_1;\n        }\n\n        vec4 transform_scale_2(vec4 pos){\n            return pos * u_scale_2;\n        }\n\n        void main() {\n            gl_Position = transform_scale_1(transform_scale_2(a_position));\n            gl_Position.x += (3.0 / 3.1415);\n            gl_Position.z += u_yoffset;\n\n            gl_PointSize = u_pointsize;\n        }\n\n    Note how the two scale function resulted in two different functions\n    and two uniforms for the scale factors.\n\n    Notes\n    -----\n\n    Function calls:\n\n    As can be seen above, the arguments with which a function is to be\n    called must be specified by calling the Function object. The\n    arguments can be any of the expressions mentioned earlier. If the\n    signature is already specified in the template code, that function\n    itself must be given.\n\n    ::\n\n        code = Function('''\n            void main() {\n                vec4 position = $pos;\n                gl_Position = $scale(position)\n            }\n        ''')\n\n        # Example of a function call with all possible three expressions\n        vert_code['pos'] = func1('3.0', 'uniform float u_param', func2())\n\n        # For scale, the sigfnature is already specified\n        code['scale'] = scale_func  # Must not specify args\n\n    Data for uniform and attribute variables:\n\n    To each variable a value can be associated. In fact, in most cases\n    the Function class is smart enough to be able to create a Variable\n    object if only the data is given.\n\n    ::\n\n        code['offset'] = Variable('uniform float offset')  # No data\n        code['offset'] = Variable('uniform float offset', 3.0)  # With data\n        code['offset'] = 3.0  # -> Uniform Variable\n        position['position'] = VertexBuffer()  # -> attribute Variable\n\n        # Updating variables\n        code['offset'].value = 4.0\n        position['position'].value.set_data(...)\n\n    \"\"\"\n\n    def __init__(self, code, dependencies=None):\n        super(Function, self).__init__()\n\n        # Add depencencies is given. This is to allow people to\n        # manually define deps for a function that they use.\n        if dependencies is not None:\n            for dep in dependencies:\n                self._add_dep(dep)\n\n        self.code = code\n\n        # Expressions replace template variables (also our dependencies)\n        self._expressions = OrderedDict()\n\n        # Verbatim string replacements\n        self._replacements = OrderedDict()\n\n        # Stuff to do at the end\n        self._assignments = OrderedDict()\n\n    def __setitem__(self, key, val):\n        \"\"\"Setting of replacements through a dict-like syntax.\n\n        Each replacement can be:\n        * verbatim code: ``fun1['foo'] = '3.14159'``\n        * a FunctionCall: ``fun1['foo'] = fun2()``\n        * a Variable: ``fun1['foo'] = Variable(...)`` (can be auto-generated)\n        \"\"\"\n        # Check the key. Must be Varying, 'gl_X' or a known template variable\n        if isinstance(key, Variable):\n            if key.vtype == 'varying':\n                if self.name != 'main':\n                    raise Exception(\"Varying assignment only alowed in 'main' \"\n                                    \"function.\")\n                storage = self._assignments\n            else:\n                raise TypeError(\"Variable assignment only allowed for \"\n                                \"varyings, not %s (in %s)\"\n                                % (key.vtype, self.name))\n        elif isinstance(key, str):\n            if any(map(key.startswith,\n                       ('gl_PointSize', 'gl_Position', 'gl_FragColor'))):\n                storage = self._assignments\n            elif key in self.template_vars or key in ('pre', 'post'):\n                storage = self._expressions\n            else:\n                raise KeyError('Invalid template variable %r' % key)\n        else:\n            raise TypeError('In `function[key]` key must be a string or '\n                            'varying.')\n\n        # If values already match, bail out now\n        if eq(storage.get(key), val):\n            return\n\n        # If we are only changing the value (and not the dtype) of a uniform,\n        # we can set that value and return immediately to avoid triggering a\n        # recompile.\n        if val is not None and not isinstance(val, Variable):\n            # We are setting a value. If there is already a variable set here,\n            # try just updating its value.\n            variable = storage.get(key, None)\n            if isinstance(variable, Variable):\n                if np.any(variable.value != val):\n                    variable.value = val\n                    self.changed(value_changed=True)\n                return\n\n            # Could not set variable.value directly; instead we will need\n            # to create a new ShaderObject\n            val = ShaderObject.create(val, ref=key)\n            if variable is val:\n                # This can happen if ShaderObject.create returns the same\n                # object (such as when setting a Transform).\n                return\n\n        # Remove old references, if any\n        oldval = storage.pop(key, None)\n        if oldval is not None:\n            for obj in (key, oldval):\n                if isinstance(obj, ShaderObject):\n                    self._remove_dep(obj)\n\n        # Add new references\n        if val is not None:\n            if isinstance(key, Varying):\n                # tell this varying to inherit properties from\n                # its source attribute / expression.\n                key.link(val)\n\n            # Store value and dependencies\n            storage[key] = val\n            for obj in (key, val):\n                if isinstance(obj, ShaderObject):\n                    self._add_dep(obj)\n\n        # In case of verbatim text, we might have added new template vars\n        if isinstance(val, TextExpression):\n            for var in parsing.find_template_variables(val.expression()):\n                if var not in self.template_vars:\n                    self.template_vars.add(var.lstrip('$'))\n\n        self.changed(code_changed=True, value_changed=True)\n        if logger.level <= logging.DEBUG:\n            import traceback\n            last = traceback.format_list(traceback.extract_stack()[-2:-1])\n            logger.debug(\"Assignment would trigger shader recompile:\\n\"\n                         \"Original: %r\\nReplacement: %r\\nSource: %s\",\n                         oldval, val, ''.join(last))\n\n    def __getitem__(self, key):\n        \"\"\"Return a reference to a program variable from this function.\n\n        This allows variables between functions to be linked together::\n\n            func1['var_name'] = func2['other_var_name']\n\n        In the example above, the two local variables would be assigned to the\n        same program variable whenever func1 and func2 are attached to the same\n        program.\n        \"\"\"\n        try:\n            return self._expressions[key]\n        except KeyError:\n            pass\n\n        try:\n            return self._assignments[key]\n        except KeyError:\n            pass\n\n        if key not in self.template_vars:\n            raise KeyError('Invalid template variable %r' % key)\n        else:\n            raise KeyError('No value known for key %r' % key)\n\n    def __call__(self, *args):\n        \"\"\"Set the signature for this function and return an FunctionCall\n        object. Each argument can be verbatim code or a FunctionCall object.\n        \"\"\"\n        return FunctionCall(self, args)\n\n    def __contains__(self, key):\n        return key in self.template_vars\n\n    # Public API methods\n\n    @property\n    def signature(self):\n        if self._signature is None:\n            try:\n                self._signature = parsing.parse_function_signature(self._code)\n            except Exception as err:\n                raise ValueError('Invalid code: ' + str(err))\n        return self._signature\n\n    @property\n    def name(self):\n        \"\"\"The function name. The name may be mangled in the final code\n        to avoid name clashes.\n        \"\"\"\n        return self.signature[0]\n\n    @property\n    def args(self):\n        \"\"\"\n        List of input arguments in the function signature::\n            [(arg_name, arg_type), ...]\n        \"\"\"\n        return self.signature[1]\n\n    @property\n    def rtype(self):\n        \"\"\"The return type of this function.\"\"\"\n        return self.signature[2]\n\n    @property\n    def code(self):\n        \"\"\"The template code used to generate the definition for this function.\"\"\"\n        return self._code\n\n    @code.setter\n    def code(self, code):\n        # Get and strip code\n        if isinstance(code, Function):\n            code = code._code\n        elif not isinstance(code, str):\n            raise ValueError('Function needs a string or Function; got %s.' %\n                             type(code))\n        self._code = self._clean_code(code)\n\n        # (name, args, rval)\n        self._signature = None\n\n        # $placeholders parsed from the code\n        self._template_vars = None\n\n        # Create static Variable instances for any global variables declared\n        # in the code\n        self._static_vars = None\n\n    @property\n    def template_vars(self):\n        if self._template_vars is None:\n            self._template_vars = self._parse_template_vars()\n        return self._template_vars\n\n    def static_names(self):\n        if self._static_vars is None:\n            self._static_vars = parsing.find_program_variables(self._code)\n        return list(self._static_vars.keys()) + [arg[0] for arg in self.args]\n\n    def replace(self, str1, str2):\n        \"\"\"Set verbatim code replacement\n\n        It is strongly recommended to use function['$foo'] = 'bar' where\n        possible because template variables are less likely to changed\n        than the code itself in future versions of vispy.\n\n        Parameters\n        ----------\n        str1 : str\n            String to replace\n        str2 : str\n            String to replace str1 with\n        \"\"\"\n        if str2 != self._replacements.get(str1, None):\n            self._replacements[str1] = str2\n            self.changed(code_changed=True)\n            # self._last_changed = time.time()\n\n    # Private methods\n\n    def _parse_template_vars(self):\n        \"\"\"Find all template variables in self._code, excluding the function name.\"\"\"\n        template_vars = set()\n        for var in parsing.find_template_variables(self._code):\n            var = var.lstrip('$')\n            if var == self.name:\n                continue\n            if var in ('pre', 'post'):\n                raise ValueError('GLSL uses reserved template variable $%s' %\n                                 var)\n            template_vars.add(var)\n        return template_vars\n\n    def _get_replaced_code(self, names, version, shader):\n        \"\"\"Return code, with new name, expressions, and replacements applied.\"\"\"\n        code = self._code\n\n        # Modify name\n        fname = names[self]\n        code = code.replace(\" \" + self.name + \"(\", \" \" + fname + \"(\")\n\n        # Apply string replacements first -- these may contain $placeholders\n        for key, val in self._replacements.items():\n            code = code.replace(key, val)\n\n        # Apply assignments to the end of the function\n\n        # Collect post lines\n        post_lines = []\n        for key, val in self._assignments.items():\n            if isinstance(key, Variable):\n                key = names[key]\n            if isinstance(val, ShaderObject):\n                val = val.expression(names)\n            line = '    %s = %s;' % (key, val)\n            post_lines.append(line)\n\n        # Add a default $post placeholder if needed\n        if 'post' in self._expressions:\n            post_lines.append('    $post')\n\n        # Apply placeholders for hooks\n        post_text = '\\n'.join(post_lines)\n        if post_text:\n            post_text = '\\n' + post_text + '\\n'\n        code = code.rpartition('}')\n        code = code[0] + post_text + code[1] + code[2]\n\n        # Add a default $pre placeholder if needed\n        if 'pre' in self._expressions:\n            m = re.search(fname + r'\\s*\\([^{]*\\)\\s*{', code)\n            if m is None:\n                raise RuntimeError(\"Cound not find beginning of function '%s'\"\n                                   % fname)\n            ind = m.span()[1]\n            code = code[:ind] + \"\\n    $pre\\n\" + code[ind:]\n\n        # Apply template variables\n        for key, val in self._expressions.items():\n            val = val.expression(names)\n            search = r'\\$' + key + r'($|[^a-zA-Z0-9_])'\n            code = re.sub(search, val+r'\\1', code)\n\n        # Done\n        if '$' in code:\n            v = parsing.find_template_variables(code)\n            logger.warning('Unsubstituted placeholders in code: %s\\n'\n                           '  replacements made: %s',\n                           v, list(self._expressions.keys()))\n\n        return code + '\\n'\n\n    def definition(self, names, version, shader):\n        return self._get_replaced_code(names, version, shader)\n\n    def expression(self, names):\n        return names[self]\n\n    def _clean_code(self, code):\n        \"\"\"Return *code* with indentation and leading/trailing blank lines removed.\"\"\"\n        lines = code.split(\"\\n\")\n        min_indent = 100\n        for line in lines:\n            if line.strip() != \"\":\n                indent = len(line) - len(line.lstrip())\n                min_indent = min(indent, min_indent)\n        if min_indent > 0:\n            lines = [line[min_indent:] for line in lines]\n        code = \"\\n\".join(lines)\n        return code\n\n    def __repr__(self):\n        try:\n            args = ', '.join([' '.join(arg) for arg in self.args])\n        except Exception:\n            return ('<%s (error parsing signature) at 0x%x>' %\n                    (self.__class__.__name__, id(self)))\n        return '<%s \"%s %s(%s)\" at 0x%x>' % (self.__class__.__name__,\n                                             self.rtype,\n                                             self.name,\n                                             args,\n                                             id(self))\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary modules and functions from the 'hydro' and 'mpisppy' libraries. The code should define a function to parse arguments and create a configuration object with various arguments. Then, define a main function that calls the argument parsing function, checks the length of the branching factors, and creates node names from these factors. The main function should also create scenario names, set up a scenario creator and denouement, and prepare the necessary arguments for the 'vanilla' cylinders. Depending on the configuration, the main function should also set up spokes for the Lagrangian bound and xhat looper bound. The function should then create a wheel spinner with the hub and spoke dictionaries, spin the wheel, and print the best inner and outer bounds. If a certain condition is met, the function should write the first stage and full tree solutions. Finally, the code should call the main function if it is the main module.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 99, "repo_full_name": "zulko__moviepy", "instruction": "Generate code that performs the following tasks using the moviepy library:\n\n1. Checks if the required video files exist in the current directory. If not, it attempts to download them using the youtube-dl command-line tool. If the download fails, it outputs an error message and exits the program.\n2. Loads an audio file, extracts a subclip from it, and applies fade-in and fade-out effects. It then analyzes the audio to find its period.\n3. Loads a video file, extracts a subclip from it, and crops it. It then analyzes the video to find a segment that loops well.\n4. Extracts the looping segment from the video, slows it down to match the audio tempo, and makes it loop for the duration of the audio. It then creates a mirrored version of this segment.\n5. Combines the original and mirrored video segments side by side, applies fade-in and fade-out effects, and adds the audio to the video.\n6. Creates a title screen with text overlay on the video and a credits screen with text on a black background.\n7. Concatenates the title screen, video, and credits screen into a final video.\n8. Writes the final video to a file with specified fps, audio bitrate, and bitrate.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def write_videofile(self, filename, fps=None, codec=None,\n                        bitrate=None, audio=True, audio_fps=44100,\n                        preset=\"medium\",\n                        audio_nbytes=4, audio_codec=None,\n                        audio_bitrate=None, audio_bufsize=2000,\n                        temp_audiofile=None,\n                        rewrite_audio=True, remove_temp=True,\n                        write_logfile=False, verbose=True,\n                        threads=None, ffmpeg_params=None):\n\n        \"\"\"Write the clip to a videofile.\n\n        Parameters\n        -----------\n\n        filename\n          Name of the video file to write in.\n          The extension must correspond to the \"codec\" used (see below),\n          or simply be '.avi' (which will work with any codec).\n\n        fps\n          Number of frames per second in the resulting video file. If None is\n          provided, and the clip has an fps attribute, this fps will be used.\n\n        codec\n          Codec to use for image encoding. Can be any codec supported\n          by ffmpeg. If the filename is has extension '.mp4', '.ogv', '.webm',\n          the codec will be set accordingly, but you can still set it if you\n          don't like the default. For other extensions, the output filename\n          must be set accordingly.\n\n          Some examples of codecs are:\n\n          ``'libx264'`` (default codec for file extension ``.mp4``)\n          makes well-compressed videos (quality tunable using 'bitrate').\n\n\n          ``'mpeg4'`` (other codec for extension ``.mp4``) can be an alternative\n          to ``'libx264'``, and produces higher quality videos by default.\n\n\n          ``'rawvideo'`` (use file extension ``.avi``) will produce\n          a video of perfect quality, of possibly very huge size.\n\n\n          ``png`` (use file extension ``.avi``) will produce a video\n          of perfect quality, of smaller size than with ``rawvideo``\n\n\n          ``'libvorbis'`` (use file extension ``.ogv``) is a nice video\n          format, which is completely free/ open source. However not\n          everyone has the codecs installed by default on their machine.\n\n\n          ``'libvpx'`` (use file extension ``.webm``) is tiny a video\n          format well indicated for web videos (with HTML5). Open source.\n\n\n        audio\n          Either ``True``, ``False``, or a file name.\n          If ``True`` and the clip has an audio clip attached, this\n          audio clip will be incorporated as a soundtrack in the movie.\n          If ``audio`` is the name of an audio file, this audio file\n          will be incorporated as a soundtrack in the movie.\n\n        audiofps\n          frame rate to use when generating the sound.\n\n        temp_audiofile\n          the name of the temporary audiofile to be generated and\n          incorporated in the the movie, if any.\n\n        audio_codec\n          Which audio codec should be used. Examples are 'libmp3lame'\n          for '.mp3', 'libvorbis' for 'ogg', 'libfdk_aac':'m4a',\n          'pcm_s16le' for 16-bit wav and 'pcm_s32le' for 32-bit wav.\n          Default is 'libmp3lame', unless the video extension is 'ogv'\n          or 'webm', at which case the default is 'libvorbis'.\n\n        audio_bitrate\n          Audio bitrate, given as a string like '50k', '500k', '3000k'.\n          Will determine the size/quality of audio in the output file.\n          Note that it mainly an indicative goal, the bitrate won't\n          necessarily be the this in the final file.\n\n        preset\n          Sets the time that FFMPEG will spend optimizing the compression.\n          Choices are: ultrafast, superfast, fast, medium, slow, superslow.\n          Note that this does not impact the quality of the video, only the\n          size of the video file. So choose ultrafast when you are in a\n          hurry and file size does not matter.\n\n        threads\n          Number of threads to use for ffmpeg. Can speed up the writing of\n          the video on multicore computers\n\n        ffmpeg_params\n          Any additional ffmpeg parameters you would like to pass, as a list\n          of terms, like ['-option1', 'value1', '-option2', 'value2']\n\n        write_logfile\n          If true, will write log files for the audio and the video.\n          These will be files ending with '.log' with the name of the\n          output file in them.\n\n\n\n        Examples\n        ========\n\n        >>> from moviepy.editor import VideoFileClip\n        >>> clip = VideoFileClip(\"myvideo.mp4\").subclip(100,120)\n        >>> clip.write_videofile(\"my_new_video.mp4\")\n\n        \"\"\"\n\n        name, ext = os.path.splitext(os.path.basename(filename))\n        ext = ext[1:].lower()\n\n        if codec is None:\n\n            try:\n                codec = extensions_dict[ext]['codec'][0]\n            except KeyError:\n                raise ValueError(\"MoviePy couldn't find the codec associated \"\n                                 \"with the filename. Provide the 'codec' parameter in \"\n                                 \"write_videofile.\")\n\n        if audio_codec is None:\n            if (ext in ['ogv', 'webm']):\n                audio_codec = 'libvorbis'\n            else:\n                audio_codec = 'libmp3lame'\n        elif audio_codec == 'raw16':\n            audio_codec = 'pcm_s16le'\n        elif audio_codec == 'raw32':\n            audio_codec = 'pcm_s32le'\n\n        audiofile = audio if is_string(audio) else None\n        make_audio = ((audiofile is None) and (audio == True) and\n                      (self.audio is not None))\n\n        if make_audio:\n            # The audio will be the clip's audio\n            if temp_audiofile is not None:\n                audiofile = temp_audiofile\n\n            else:\n\n                # make a name for the temporary audio file\n\n                if audio_codec in extensions_dict:\n                    audio_ext = audio_codec\n                else:\n                    try:\n                        audio_ext = find_extension(audio_codec)\n                    except ValueError:\n\n                        raise ValueError(\n                            \"The audio_codec you chose is unknown by MoviePy. \"\n                            \"You should report this. In the meantime, you can specify a \"\n                            \"temp_audiofile with the right extension in write_videofile.\")\n\n                audiofile = (name + Clip._TEMP_FILES_PREFIX +\n                             \"wvf_snd.%s\" % audio_ext)\n\n        # enough cpu for multiprocessing ? USELESS RIGHT NOW, WILL COME AGAIN\n        # enough_cpu = (multiprocessing.cpu_count() > 1)\n\n        verbose_print(verbose, \"[MoviePy] >>>> Building video %s\\n\" % filename)\n\n        if make_audio:\n            self.audio.write_audiofile(audiofile, audio_fps,\n                                       audio_nbytes, audio_bufsize,\n                                       audio_codec, bitrate=audio_bitrate,\n                                       write_logfile=write_logfile,\n                                       verbose=verbose)\n\n        ffmpeg_write_video(self, filename, fps, codec,\n                           bitrate=bitrate,\n                           preset=preset,\n                           write_logfile=write_logfile,\n                           audiofile = audiofile,\n                           verbose=verbose, threads=threads,\n                           ffmpeg_params=ffmpeg_params)\n\n        if remove_temp and make_audio:\n            os.remove(audiofile)\n\n        verbose_print(verbose, \"[MoviePy] >>>> Video ready: %s \\n\\n\"%filename)\n\n# --- Snippet Separator ---\n\ndef audio_video_fx(f, clip, *a, **k):\n    \"\"\" Use an audio function on a video/audio clip\n\n    This decorator tells that the function f (audioclip -> audioclip)\n    can be also used on a video clip, at which case it returns a\n    videoclip with unmodified video and modified audio.\n    \"\"\"\n\n    if hasattr(clip, \"audio\"):\n        newclip = clip.copy()\n        if clip.audio is not None:\n            newclip.audio =  f(clip.audio, *a, **k)\n        return newclip\n    else:\n        return f(clip, *a, **k)\n\n# --- Snippet Separator ---\n\ndef preview(clip, fps=15, audio=True, audio_fps=22050,\n             audio_buffersize=3000, audio_nbytes=2):\n    \"\"\" \n    Displays the clip in a window, at the given frames per second\n    (of movie) rate. It will avoid that the clip be played faster\n    than normal, but it cannot avoid the clip to be played slower\n    than normal if the computations are complex. In this case, try\n    reducing the ``fps``.\n\n    Parameters\n    ------------\n\n    fps\n      Number of frames per seconds in the displayed video.\n\n    audio\n      ``True`` (default) if you want the clip's audio be played during\n      the preview.\n\n    audiofps\n      The frames per second to use when generating the audio sound.\n\n    \"\"\"\n\n    import pygame as pg\n\n    # compute and splash the first image\n    screen = pg.display.set_mode(clip.size)\n\n    audio = audio and (clip.audio is not None)\n\n    if audio:\n        # the sound will be played in parrallel. We are not\n        # parralellizing it on different CPUs because it seems that\n        # pygame and openCV already use several cpus it seems.\n\n        # two synchro-flags to tell whether audio and video are ready\n        videoFlag = threading.Event()\n        audioFlag = threading.Event()\n        # launch the thread\n        audiothread = threading.Thread(target=clip.audio.preview,\n            args = (audio_fps,audio_buffersize, audio_nbytes,\n                    audioFlag, videoFlag))\n        audiothread.start()\n\n    img = clip.get_frame(0)\n    imdisplay(img, screen)\n    if audio: # synchronize with audio\n        videoFlag.set() # say to the audio: video is ready\n        audioFlag.wait() # wait for the audio to be ready\n\n    result = []\n\n    t0 = time.time()\n    for t in np.arange(1.0 / fps, clip.duration-.001, 1.0 / fps):\n\n        img = clip.get_frame(t)\n\n        for event in pg.event.get():\n            if event.type == pg.KEYDOWN:\n                if (event.key == pg.K_ESCAPE):\n\n                    if audio:\n                        videoFlag.clear()\n                    print( \"Keyboard interrupt\" )\n                    return result\n\n            elif event.type == pg.MOUSEBUTTONDOWN:\n                x,y = pg.mouse.get_pos()\n                rgb = img[y,x]\n                result.append({'time':t, 'position':(x,y),\n                                'color':rgb})\n                print( \"time, position, color : \", \"%.03f, %s, %s\"%(\n                             t,str((x,y)),str(rgb)))\n\n        t1 = time.time()\n        time.sleep(max(0, t - (t1-t0)) )\n        imdisplay(img, screen)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks using the moviepy library:\n\n1. Checks if the required video files exist in the current directory. If not, it attempts to download them using the youtube-dl command-line tool. If the download fails, it outputs an error message and exits the program.\n2. Loads an audio file, extracts a subclip from it, and applies fade-in and fade-out effects. It then analyzes the audio to find its period.\n3. Loads a video file, extracts a subclip from it, and crops it. It then analyzes the video to find a segment that loops well.\n4. Extracts the looping segment from the video, slows it down to match the audio tempo, and makes it loop for the duration of the audio. It then creates a mirrored version of this segment.\n5. Combines the original and mirrored video segments side by side, applies fade-in and fade-out effects, and adds the audio to the video.\n6. Creates a title screen with text overlay on the video and a credits screen with text on a black background.\n7. Concatenates the title screen, video, and credits screen into a final video.\n8. Writes the final video to a file with specified fps, audio bitrate, and bitrate.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 100, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a simulation using the dd4hep library. The code should initialize a kernel and load a geometry from an XML file located in the environment's 'DD4hepExamplesINSTALL' directory. It should import constants from the kernel's detector description and set up a Geant4 instance with a tracker. The code should also configure the user interface, tracking field, and event actions. It should set up a particle gun with a gamma particle, energy of 5 keV, and multiplicity of 1. The code should also set up a tracker and a physics list, adding various particle groups and processes. Finally, the code should execute the Geant4 instance.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def run(self):\n    \"\"\"setup the geometry and dd4hep and geant4 and do what was asked to be done\"\"\"\n    import ROOT\n    ROOT.PyConfig.IgnoreCommandLineOptions = True\n\n    import DDG4\n    import dd4hep\n\n    self.printLevel = getOutputLevel(self.printLevel)\n\n    kernel = DDG4.Kernel()\n    dd4hep.setPrintLevel(self.printLevel)\n\n    for compactFile in self.compactFile:\n      kernel.loadGeometry(str(\"file:\" + os.path.abspath(compactFile)))\n    detectorDescription = kernel.detectorDescription()\n\n    DDG4.importConstants(detectorDescription)\n\n  # ----------------------------------------------------------------------------------\n\n    # simple = DDG4.Geant4( kernel, tracker='Geant4TrackerAction',calo='Geant4CalorimeterAction')\n    # geant4 = DDG4.Geant4( kernel, tracker='Geant4TrackerCombineAction',calo='Geant4ScintillatorCalorimeterAction')\n    geant4 = DDG4.Geant4(kernel, tracker=self.action.tracker, calo=self.action.calo)\n\n    geant4.printDetectors()\n\n    if self.runType == \"vis\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=True, macro=self.macroFile)\n    elif self.runType == \"qt\":\n      uiaction = geant4.setupUI(typ=\"qt\", vis=True, macro=self.macroFile)\n    elif self.runType == \"run\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=self.macroFile, ui=False)\n    elif self.runType == \"shell\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=True)\n    elif self.runType == \"batch\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=False)\n    else:\n      logger.error(\"unknown runType\")\n      exit(1)\n\n    # User Configuration for the Geant4Phases\n    uiaction.ConfigureCommands = self.ui._commandsConfigure\n    uiaction.InitializeCommands = self.ui._commandsInitialize\n    uiaction.PostRunCommands = self.ui._commandsPostRun\n    uiaction.PreRunCommands = self.ui._commandsPreRun\n    uiaction.TerminateCommands = self.ui._commandsTerminate\n\n    kernel.NumEvents = self.numberOfEvents\n\n    # -----------------------------------------------------------------------------------\n    # setup the magnetic field:\n    self.__setMagneticFieldOptions(geant4)\n\n    # configure geometry creation\n    self.geometry.constructGeometry(kernel, geant4, self.output.geometry)\n\n    # ----------------------------------------------------------------------------------\n    # Configure Run actions\n    run1 = DDG4.RunAction(kernel, 'Geant4TestRunAction/RunInit')\n    kernel.registerGlobalAction(run1)\n    kernel.runAction().add(run1)\n\n    # Configure the random seed, do it before the I/O because we might change the seed!\n    self.random.initialize(DDG4, kernel, self.output.random)\n\n    # Configure the output file format and plugin\n    self.outputConfig.initialize(dd4hepsimulation=self, geant4=geant4)\n\n    actionList = []\n\n    if self.enableGun:\n      gun = DDG4.GeneratorAction(kernel, \"Geant4ParticleGun/\" + \"Gun\")\n      self.gun.setOptions(gun)\n      gun.Standalone = False\n      gun.Mask = 1\n      actionList.append(gun)\n      self.__applyBoostOrSmear(kernel, actionList, 1)\n      logger.info(\"++++ Adding DD4hep Particle Gun ++++\")\n\n    if self.enableG4Gun:\n      # GPS Create something\n      self._g4gun = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/Gun\")\n      self._g4gun.Uses = 'G4ParticleGun'\n      self._g4gun.Mask = 2\n      logger.info(\"++++ Adding Geant4 Particle Gun ++++\")\n      actionList.append(self._g4gun)\n\n    if self.enableG4GPS:\n      # GPS Create something\n      self._g4gps = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/GPS\")\n      self._g4gps.Uses = 'G4GeneralParticleSource'\n      self._g4gps.Mask = 3\n      logger.info(\"++++ Adding Geant4 General Particle Source ++++\")\n      actionList.append(self._g4gps)\n\n    start = 4\n    for index, plugin in enumerate(self.inputConfig.userInputPlugin, start=start):\n      gen = plugin(self)\n      gen.Mask = index\n      start = index + 1\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n      logger.info(\"++++ Adding User Plugin %s ++++\", gen.Name)\n\n    for index, inputFile in enumerate(self.inputFiles, start=start):\n      if inputFile.endswith(\".slcio\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/LCIO%d\" % index)\n        gen.Parameters = self.lcio.getParameters()\n        gen.Input = \"LCIOFileReader|\" + inputFile\n      elif inputFile.endswith(\".stdhep\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/STDHEP%d\" % index)\n        gen.Input = \"LCIOStdHepReader|\" + inputFile\n      elif inputFile.endswith(\".HEPEvt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/HEPEvt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtShort|\" + inputFile\n      elif inputFile.endswith(\".hepevt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepevt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtLong|\" + inputFile\n      elif inputFile.endswith(tuple([\".hepmc\"] + HEPMC3_SUPPORTED_EXTENSIONS)):\n        if self.hepmc3.useHepMC3:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Parameters = self.hepmc3.getParameters()\n          gen.Input = \"HEPMC3FileReader|\" + inputFile\n        else:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Input = \"Geant4EventReaderHepMC|\" + inputFile\n      elif inputFile.endswith(\".pairs\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/GuineaPig%d\" % index)\n        gen.Input = \"Geant4EventReaderGuineaPig|\" + inputFile\n        gen.Parameters = self.guineapig.getParameters()\n      else:\n        # this should never happen because we already check at the top, but in case of some LogicError...\n        raise RuntimeError(\"Unknown input file type: %s\" % inputFile)\n      gen.Sync = self.skipNEvents\n      gen.Mask = index\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n\n    if actionList:\n      self._buildInputStage(geant4, actionList, output_level=self.output.inputStage,\n                            have_mctruth=self._enablePrimaryHandler())\n\n    # ================================================================================================\n\n    # And handle the simulation particles.\n    part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n    kernel.generatorAction().adopt(part)\n    # part.SaveProcesses = ['conv','Decay']\n    part.SaveProcesses = self.part.saveProcesses\n    part.MinimalKineticEnergy = self.part.minimalKineticEnergy\n    part.KeepAllParticles = self.part.keepAllParticles\n    part.PrintEndTracking = self.part.printEndTracking\n    part.PrintStartTracking = self.part.printStartTracking\n    part.MinDistToParentVertex = self.part.minDistToParentVertex\n    part.OutputLevel = self.output.part\n    part.enableUI()\n\n    if self.part.enableDetailedHitsAndParticleInfo:\n      self.part.setDumpDetailedParticleInfo(kernel, DDG4)\n\n    self.part.setupUserParticleHandler(part, kernel, DDG4)\n\n    # =================================================================================\n\n    # Setup global filters for use in sensitive detectors\n    try:\n      self.filter.setupFilters(kernel)\n    except RuntimeError as e:\n      logger.error(\"%s\", e)\n      exit(1)\n\n    # =================================================================================\n    # get lists of trackers and calorimeters in detectorDescription\n\n    trk, cal, unk = self.getDetectorLists(detectorDescription)\n\n    for detectors, function, defFilter, abort in [(trk, geant4.setupTracker, self.filter.tracker, False),\n                                                  (cal, geant4.setupCalorimeter, self.filter.calo, False),\n                                                  (unk, geant4.setupDetector, None, True),\n                                                  ]:\n      try:\n        self.__setupSensitiveDetectors(detectors, function, defFilter, abort)\n      except Exception as e:\n        logger.error(\"Failed setting up sensitive detector %s\", e)\n        raise\n\n  # =================================================================================\n    # Now build the physics list:\n    _phys = self.physics.setupPhysics(kernel, name=self.physicsList)\n\n    # add the G4StepLimiterPhysics to activate the max step limits in volumes\n    ph = DDG4.PhysicsList(kernel, 'Geant4PhysicsList/Myphysics')\n    ph.addPhysicsConstructor(str('G4StepLimiterPhysics'))\n    _phys.add(ph)\n\n    dd4hep.setPrintLevel(self.printLevel)\n\n    kernel.configure()\n    kernel.initialize()\n\n    # GPS\n    if self._g4gun is not None:\n      self._g4gun.generator()\n    if self._g4gps is not None:\n      self._g4gps.generator()\n\n    startUpTime, _sysTime, _cuTime, _csTime, _elapsedTime = os.times()\n\n    kernel.run()\n    kernel.terminate()\n\n    totalTimeUser, totalTimeSys, _cuTime, _csTime, _elapsedTime = os.times()\n    if self.printLevel <= 3:\n      logger.info(\"DDSim            INFO  Total Time:   %3.2f s (User), %3.2f s (System)\" %\n                  (totalTimeUser, totalTimeSys))\n      if self.numberOfEvents != 0:\n        eventTime = totalTimeUser - startUpTime\n        perEventTime = eventTime / self.numberOfEvents\n        logger.info(\"DDSim            INFO  StartUp Time: %3.2f s, Event Processing: %3.2f s (%3.2f s/Event) \"\n                    % (startUpTime, eventTime, perEventTime))\n\n# --- Snippet Separator ---\n\ndef materialScan(opts):\n  kernel = DDG4.Kernel()\n  kernel.loadGeometry(str(opts.compact))\n  DDG4.Core.setPrintFormat(str(\"%-32s %6s %s\"))\n  geant4 = DDG4.Geant4(kernel)\n  # Configure UI\n  geant4.setupCshUI(ui=None)\n  for i in geant4.description.detectors():\n    o = DDG4.DetElement(i.second.ptr())\n    sd = geant4.description.sensitiveDetector(o.name())\n    if sd.isValid():\n      typ = sd.type()\n      if typ in geant4.sensitive_types:\n        geant4.setupDetector(o.name(), geant4.sensitive_types[typ])\n      else:\n        logger.error('+++  %-32s type:%-12s  --> Unknown Sensitive type: %s', o.name(), typ, typ)\n        sys.exit(errno.EINVAL)\n\n  geant4.setupGun(\"Gun\",\n                  Standalone=True,\n                  particle='geantino',\n                  energy=20 * g4units.GeV,\n                  position=opts.position,\n                  direction=opts.direction,\n                  multiplicity=1,\n                  isotrop=False)\n  scan = DDG4.SteppingAction(kernel, 'Geant4GeometryScanner/GeometryScan')\n  kernel.steppingAction().adopt(scan)\n\n  # Now build the physics list:\n  geant4.setupPhysics('QGSP_BERT')\n\n  kernel.configure()\n  kernel.initialize()\n  kernel.NumEvents = 1\n  kernel.run()\n  kernel.terminate()\n  return 0\n\n# --- Snippet Separator ---\n\ndef materialScan(opts):\n  kernel = DDG4.Kernel()\n  kernel.loadGeometry(str(opts.compact))\n  DDG4.Core.setPrintFormat(str(\"%-32s %6s %s\"))\n  geant4 = DDG4.Geant4(kernel)\n  # Configure UI\n  geant4.setupCshUI(ui=None)\n  for i in geant4.description.detectors():\n    o = DDG4.DetElement(i.second.ptr())\n    sd = geant4.description.sensitiveDetector(o.name())\n    if sd.isValid():\n      typ = sd.type()\n      if typ in geant4.sensitive_types:\n        geant4.setupDetector(o.name(), geant4.sensitive_types[typ])\n      else:\n        logger.error('+++  %-32s type:%-12s  --> Unknown Sensitive type: %s', o.name(), typ, typ)\n        sys.exit(errno.EINVAL)\n\n  geant4.setupGun(\"Gun\",\n                  Standalone=True,\n                  particle='geantino',\n                  energy=20 * g4units.GeV,\n                  position=opts.position,\n                  direction=opts.direction,\n                  multiplicity=1,\n                  isotrop=False)\n  scan = DDG4.SteppingAction(kernel, 'Geant4MaterialScanner/MaterialScan')\n  kernel.steppingAction().adopt(scan)\n\n  # Now build the physics list:\n  geant4.setupPhysics('QGSP_BERT')\n\n  kernel.configure()\n  kernel.initialize()\n  kernel.NumEvents = 1\n  kernel.run()\n  kernel.terminate()\n  return 0\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a simulation using the dd4hep library. The code should initialize a kernel and load a geometry from an XML file located in the environment's 'DD4hepExamplesINSTALL' directory. It should import constants from the kernel's detector description and set up a Geant4 instance with a tracker. The code should also configure the user interface, tracking field, and event actions. It should set up a particle gun with a gamma particle, energy of 5 keV, and multiplicity of 1. The code should also set up a tracker and a physics list, adding various particle groups and processes. Finally, the code should execute the Geant4 instance.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 101, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that performs the following tasks using the porepy library:\n\n1. Import necessary modules and define two functions, `add_data` and `plot_over_line`. The `add_data` function should define the permeability, apertures, and boundary conditions for a given grid bucket and domain. The `plot_over_line` function should plot values over a line in a grid bucket.\n\n2. Set a tolerance value and define mesh size parameters and a domain.\n\n3. Import a grid bucket from a CSV file, compute its geometry, coarsen it, and assign node ordering.\n\n4. Use the `add_data` function to assign parameters to the grid bucket.\n\n5. Define a solver using the DualVEMMixDim class for flow, compute the matrix and right-hand side of the system, and solve it.\n\n6. Split the solution, extract the discharge and pressure, and project the discharge.\n\n7. Export the grid bucket to a VTK file, including the pressure and discharge.\n\n8. Define a bounding box and a number of points, and create two sets of points along the x and y axes.\n\n9. Use the `plot_over_line` function to plot the pressure along these lines and save the results to CSV files.\n\n10. Print the diameter of the grid bucket and the number of cells in 2D and 1D.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def dfm_2d_from_csv(f_name, mesh_kwargs, domain=None, return_domain=False,\n                    tol=1e-8, **kwargs):\n    \"\"\"\n    Create the grid bucket from a set of fractures stored in a csv file and a\n    domain. In the csv file, we assume the following structure:\n    FID, START_X, START_Y, END_X, END_Y\n\n    Where FID is the fracture id, START_X and START_Y are the abscissa and\n    coordinate of the starting point, and END_X and END_Y are the abscissa and\n    coordinate of the ending point.\n    Note: the delimiter can be different.\n\n    Parameters:\n        f_name: the file name in CSV format\n        mesh_kwargs: list of additional arguments for the meshing\n        domain: rectangular domain, if not given the bounding-box is computed\n        kwargs: list of argument for the numpy function genfromtxt\n\n    Returns:\n        gb: grid bucket associated to the configuration.\n        domain: if the domain is not given as input parameter, the bounding box\n        is returned.\n\n    \"\"\"\n    pts, edges = lines_from_csv(f_name, tol=tol, **kwargs)\n    f_set = np.array([pts[:, e] for e in edges.T])\n\n    # Define the domain as bounding-box if not defined\n    if domain is None:\n        overlap = kwargs.get('domain_overlap', 0)\n        domain = cg.bounding_box(pts, overlap)\n\n    if return_domain:\n        return meshing.simplex_grid(f_set, domain, **mesh_kwargs), domain\n    else:\n        return meshing.simplex_grid(f_set, domain, **mesh_kwargs)\n\n# --- Snippet Separator ---\n\ndef generate_coarse_grid(g, subdiv):\n    \"\"\" Generate a coarse grid clustering the cells according to the flags\n    given by subdiv. Subdiv should be long as the number of cells in the\n    original grid, it contains integers (possibly not continuous) which\n    represent the cells in the final mesh. If a grid bucket is given the\n    coarsening is applied to the higher dimensional grid.\n\n    The values computed in \"compute_geometry\" are not preserved and they should\n    be computed out from this function.\n\n    Note: there is no check for disconnected cells in the final grid.\n\n    Parameters:\n        g: the grid or grid bucket\n        subdiv: a list of flags, one for each cell of the original grid\n\n    Return:\n        grid: if a grid is given as input, its coarser version is returned.\n        If a grid bucket is given as input, the grid is updated in place.\n\n    How to use:\n    subdiv = np.array([0,0,1,1,1,1,3,4,6,4,6,4])\n    g = generate_coarse_grid(g, subdiv)\n\n    or with a grid bucket:\n    subdiv = np.array([0,0,1,1,1,1,3,4,6,4,6,4])\n    generate_coarse_grid(gb, subdiv)\n\n    \"\"\"\n    if isinstance(g, grid.Grid):\n        generate_coarse_grid_single(g, subdiv, False)\n\n    if isinstance(g, grid_bucket.GridBucket):\n        generate_coarse_grid_gb(g, subdiv)\n\n# --- Snippet Separator ---\n\ndef dfm_3d_from_csv(file_name, tol=1e-4, **mesh_kwargs):\n    \"\"\"\n    Create the grid bucket from a set of 3d fractures stored in a csv file and\n    domain. In the csv file, we assume the following structure\n    - first line describes the domain as a rectangle with\n      X_MIN, Y_MIN, Z_MIN, X_MAX, Y_MAX, Z_MAX\n    - the other lines descibe the N fractures as a list of points\n      P0_X, P0_Y, P0_Z, ...,PN_X, PN_Y, PN_Z\n\n    Parameters:\n        file_name: name of the file\n        tol: (optional) tolerance for the methods\n        mesh_kwargs: kwargs for the gridding, see meshing.simplex_grid\n\n    Return:\n        gb: the grid bucket\n    \"\"\"\n    frac_list, network, domain = network_3d_from_csv(file_name)\n\n    gb = meshing.simplex_grid(domain=domain, network=network, **mesh_kwargs)\n    return gb, domain\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks using the porepy library:\n\n1. Import necessary modules and define two functions, `add_data` and `plot_over_line`. The `add_data` function should define the permeability, apertures, and boundary conditions for a given grid bucket and domain. The `plot_over_line` function should plot values over a line in a grid bucket.\n\n2. Set a tolerance value and define mesh size parameters and a domain.\n\n3. Import a grid bucket from a CSV file, compute its geometry, coarsen it, and assign node ordering.\n\n4. Use the `add_data` function to assign parameters to the grid bucket.\n\n5. Define a solver using the DualVEMMixDim class for flow, compute the matrix and right-hand side of the system, and solve it.\n\n6. Split the solution, extract the discharge and pressure, and project the discharge.\n\n7. Export the grid bucket to a VTK file, including the pressure and discharge.\n\n8. Define a bounding box and a number of points, and create two sets of points along the x and y axes.\n\n9. Use the `plot_over_line` function to plot the pressure along these lines and save the results to CSV files.\n\n10. Print the diameter of the grid bucket and the number of cells in 2D and 1D.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 102, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seedemu library. The emulation should include three layers: Base, Routing, and Ebgp. It should create multiple autonomous systems, each with their own hosts and routers. The routers should join different networks. The autonomous systems should be connected through internet exchanges. The code should also define a function to create a stub autonomous system with a specified ASN and exchange. The function should create hosts and a router for the autonomous system, and join them to a network. The router should also join the specified exchange. The code should also add private peering relationships between different autonomous systems. Finally, the code should add the layers to the emulator and dump the emulator state to a binary file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def makeEmulatorBaseWith5StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef makeEmulatorBaseWith10StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 160, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 161, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 162, 103, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 163, 104, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 164, 104, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(103, [3],  [160, 161, 162], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(104, [3, 4], [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [4],  [163], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(104, [12], [164], PeerRelationship.Provider)\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef makeStubAs(emu: Emulator, base: Base, asn: int, exchange: int,\n    services: List[Service]):\n    \"\"\"!\n    @brief create a new stub AS.\n\n    @param emu reference to the Emulator object.\n    @param base reference to the base layer.\n    @param asn ASN for the newly created AS.\n    @param exchange IXP ID for new newly created AS to join.\n    @param list of instances of Service to install on hosts. One host will be\n    created for each.\n    \"\"\"\n\n    # Create AS and internal network\n    stub_as = base.createAutonomousSystem(asn)\n    stub_as.createNetwork('net0')\n\n    # Create a BGP router \n    # Attach the router to both the internal and external networks\n    router = stub_as.createRouter('router0')\n    router.joinNetwork('net0')\n    router.joinNetwork('ix{}'.format(exchange))\n\n    # Create a host node for each specified service\n    createHostsOnNetwork(emu, stub_as, 'net0', services)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulation using the seedemu library. The emulation should include three layers: Base, Routing, and Ebgp. It should create multiple autonomous systems, each with their own hosts and routers. The routers should join different networks. The autonomous systems should be connected through internet exchanges. The code should also define a function to create a stub autonomous system with a specified ASN and exchange. The function should create hosts and a router for the autonomous system, and join them to a network. The router should also join the specified exchange. The code should also add private peering relationships between different autonomous systems. Finally, the code should add the layers to the emulator and dump the emulator state to a binary file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 103, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that creates a project in Maxwell 2D using the PyAEDT library and runs a transient simulation. The code should import necessary libraries, set non-graphical mode, insert a Maxwell 2D design and save the project. It should create a rectangle and duplicate it, create an air region, assign windings to the sheets and a balloon to the air region, and plot the model. The code should also create a transient setup, create a rectangular plot, solve the model, create output and plot it using PyVista, generate the same plot outside AEDT, and finally close AEDT.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Maxwell3d(Maxwell, FieldAnalysis3D, object):\n    \"\"\"Provides the Maxwell 3D application interface.\n\n    This class allows you to connect to an existing Maxwell 3D design or create a\n    new Maxwell 3D design if one does not exist.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open. The default is ``None``, in which\n        case an attempt is made to get an active project. If no\n        projects are present, an empty project is created.\n    designname : str, optional\n        Name of the design to select. The default is ``None``, in\n        which case an attempt is made to get an active design. If no\n        designs are present, an empty design is created.\n    solution_type : str, optional\n        Solution type to apply to the design. The default is\n        ``None``, in which case the default type is applied.\n    setup_name : str, optional\n        Name of the setup to use as the nominal. The default is\n        ``None``, in which case the active setup is used or\n        nothing is used.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``, in which case\n        the active version or latest installed version is used. This\n        parameter is ignored when Script is launched within AEDT.\n    non_graphical : bool, optional\n        Whether to launch AEDT in non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in graphical\n        mode. This parameter is ignored when a script is launched within\n        AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine. The default is ``True``. This parameter is ignored\n        when Script is launched within AEDT.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit. The default is ``False``.\n    student_version : bool, optional\n        Whether to open the AEDT student version. The default is\n        ``False``. This parameter is ignored when Script is launched\n        within AEDT.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of Maxwell 3D and open the specified\n    project, which is named ``mymaxwell.aedt``.\n\n    >>> from pyaedt import Maxwell3d\n    >>> aedtapp = Maxwell3d(\"mymaxwell.aedt\")\n    pyaedt info: Added design ...\n\n    Create an instance of Maxwell 3D using the 2021 R1 release and open\n    the specified project, which is named ``mymaxwell2.aedt``.\n\n    >>> aedtapp = Maxwell3d(specified_version=\"2021.2\", projectname=\"mymaxwell2.aedt\")\n    pyaedt info: Added design ...\n\n    \"\"\"\n\n    @property  # for legacy purposes\n    def dim(self):\n        \"\"\"Dimensions.\"\"\"\n        return \"3D\"\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        setup_name=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=False,\n        close_on_exit=False,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n        \"\"\"\n        Initialize the ``Maxwell`` class.\n        \"\"\"\n        self.is3d = True\n        FieldAnalysis3D.__init__(\n            self,\n            \"Maxwell 3D\",\n            projectname,\n            designname,\n            solution_type,\n            setup_name,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine,\n            port,\n            aedt_process_id,\n        )\n        Maxwell.__init__(self)\n\n# --- Snippet Separator ---\n\nclass Maxwell2d(Maxwell, FieldAnalysis3D, object):\n    \"\"\"Provides the Maxwell 2D application interface.\n\n    This class allows you to connect to an existing Maxwell 2D design or create a\n    new Maxwell 2D design if one does not exist.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open. The default is ``None``, in which\n        case an attempt is made to get an active project. If no\n        projects are present, an empty project is created.\n    designname : str, optional\n        Name of the design to select. The default is ``None``, in\n        which case an attempt is made to get an active design. If no\n        designs are present, an empty design is created.\n    solution_type : str, optional\n        Solution type to apply to the design. The default is\n        ``None``, in which case the default type is applied.\n    setup_name : str, optional\n        Name of the setup to use as the nominal. The default is\n        ``None``, in which case the active setup is used or\n        nothing is used.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``, in which case\n        the active version or latest installed version is used.\n        This parameter is ignored when Script is launched within AEDT.\n    non_graphical : bool, optional\n        Whether to launch AEDT in non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in graphical mode.\n        This parameter is ignored when a script is launched within AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine. The default is ``True``. This parameter is ignored when Script is launched within AEDT.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit. The default is ``False``.\n    student_version : bool, optional\n        Whether to open the AEDT student version. The default is ``False``.\n        This parameter is ignored when Script is launched within AEDT.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of Maxwell 2D and connect to an existing\n    Maxwell 2D design or create a new Maxwell 2D design if one does\n    not exist.\n\n    >>> from pyaedt import Maxwell2d\n    >>> aedtapp = Maxwell2d()\n\n    Create an instance of Maxwell 2D and link to a project named\n    ``projectname``. If this project does not exist, create one with\n    this name.\n\n    >>> aedtapp = Maxwell2d(projectname)\n\n    Create an instance of Maxwell 2D and link to a design named\n    ``designname`` in a project named ``projectname``.\n\n    >>> aedtapp = Maxwell2d(projectname,designame)\n\n    \"\"\"\n\n    @property  # for legacy purposes\n    def dim(self):\n        \"\"\"Dimensions.\"\"\"\n        return self.modeler.dimension\n\n    @property\n    def geometry_mode(self):\n        \"\"\"Geometry mode.\n\n        References\n        ----------\n\n        >>> oDesign.GetGeometryMode\"\"\"\n        return self.odesign.GetGeometryMode()\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        setup_name=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=False,\n        close_on_exit=False,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n        self.is3d = False\n        FieldAnalysis3D.__init__(\n            self,\n            \"Maxwell 2D\",\n            projectname,\n            designname,\n            solution_type,\n            setup_name,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine,\n            port,\n            aedt_process_id,\n        )\n        Maxwell.__init__(self)\n\n    @property\n    def xy_plane(self):\n        \"\"\"Maxwell 2D plane between `True` and `False`.\"\"\"\n        return self.design_solutions.xy_plane\n\n    @xy_plane.setter\n    @pyaedt_function_handler()\n    def xy_plane(self, value=True):\n        self.design_solutions.xy_plane = value\n\n    @property\n    def model_depth(self):\n        \"\"\"Model depth.\"\"\"\n\n        if \"ModelDepth\" in self.design_properties:\n            value_str = self.design_properties[\"ModelDepth\"]\n            try:\n                a = float_units(value_str)\n            except:\n                a = self.variable_manager[value_str].value\n            finally:\n                return a\n        else:\n            return None\n\n    @model_depth.setter\n    def model_depth(self, value):\n        \"\"\"Set model depth.\"\"\"\n        return self.change_design_settings(\n            {\"ModelDepth\": self._modeler._arg_with_dim(value, self._modeler.model_units)}\n        )\n\n    @pyaedt_function_handler()\n    def generate_design_data(self, linefilter=None, objectfilter=None):\n        \"\"\"Generate a generic set of design data and store it in the extension directory as ``design_data.json``.\n\n        Parameters\n        ----------\n        linefilter : optional\n            The default is ``None``.\n        objectfilter : optional\n            The default is ``None``.\n\n        Returns\n        -------\n        bool\n            ``True`` when successful, ``False`` when failed.\n\n        \"\"\"\n\n        def convert(obj):\n            if isinstance(obj, bool):\n                return str(obj).lower()\n            if isinstance(obj, (list, tuple)):\n                return [convert(item) for item in obj]\n            if isinstance(obj, dict):\n                return {convert(key): convert(value) for key, value in obj.items()}\n            return obj\n\n        solid_bodies = self.modeler.solid_bodies\n        if objectfilter:\n            solid_ids = [i for i, j in self.modeler.object_id_dict.items() if j.name in objectfilter]\n        else:\n            solid_ids = [i for i in list(self.modeler.object_id_dict.keys())]\n        self.design_data = {\n            \"Project Directory\": self.project_path,\n            \"Working Directory\": self.working_directory,\n            \"Library Directories\": self.library_list,\n            \"Dimension\": self.modeler.dimension,\n            \"GeoMode\": self.geometry_mode,\n            \"ModelUnits\": self.modeler.model_units,\n            \"Symmetry\": self.symmetry_multiplier,\n            \"ModelDepth\": self.model_depth,\n            \"ObjectList\": solid_ids,\n            \"LineList\": self.modeler.vertex_data_of_lines(linefilter),\n            \"VarList\": self.variable_manager.variable_names,\n            \"Setups\": self.existing_analysis_setups,\n            \"MaterialProperties\": self.get_object_material_properties(solid_bodies),\n        }\n\n        design_file = os.path.join(self.working_directory, \"design_data.json\")\n        with open(design_file, \"w\") as fps:\n            json.dump(convert(self.design_data), fps, indent=4)\n        return True\n\n    @pyaedt_function_handler()\n    def read_design_data(self):\n        \"\"\"Read back the design data as a dictionary.\n\n        Returns\n        -------\n        dict\n            Dictionary of design data.\n\n        \"\"\"\n        design_file = os.path.join(self.working_directory, \"design_data.json\")\n        with open(design_file, \"r\") as fps:\n            design_data = json.load(fps)\n        return design_data\n\n    @pyaedt_function_handler()\n    def assign_balloon(self, edge_list, bound_name=None):\n        \"\"\"Assign a balloon boundary to a list of edges.\n\n        Parameters\n        ----------\n        edge_list : list\n            List of edges.\n        bound_name : str, optional\n            Name of the boundary. The default is ``None``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Boundary object.\n\n        References\n        ----------\n\n        >>> oModule.AssignBalloon\n        \"\"\"\n        edge_list = self.modeler.convert_to_selections(edge_list, True)\n\n        if not bound_name:\n            bound_name = generate_unique_name(\"Balloon\")\n\n        props2 = OrderedDict({\"Edges\": edge_list})\n        bound = BoundaryObject(self, bound_name, props2, \"Balloon\")\n\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_vector_potential(self, input_edge, vectorvalue=0, bound_name=None):\n        \"\"\"Assign a vector to a list of edges.\n\n        Parameters\n        ----------\n        input_edge : list\n            List of edge names or edge IDs to assign a vector to.\n        vectorvalue : float, optional\n            Value of the vector. The default is ``0``.\n        bound_name : str, optional\n            Name of the boundary. The default is ``None``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Vector Potential Object\n\n        References\n        ----------\n\n        >>> oModule.AssignVectorPotential\n        \"\"\"\n        input_edge = self.modeler.convert_to_selections(input_edge, True)\n\n        if not bound_name:\n            bound_name = generate_unique_name(\"Vector\")\n        if type(input_edge[0]) is str:\n            props2 = OrderedDict({\"Objects\": input_edge, \"Value\": str(vectorvalue), \"CoordinateSystem\": \"\"})\n        else:\n            props2 = OrderedDict({\"Edges\": input_edge, \"Value\": str(vectorvalue), \"CoordinateSystem\": \"\"})\n        bound = BoundaryObject(self, bound_name, props2, \"Vector Potential\")\n\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_master_slave(\n        self, master_edge, slave_edge, reverse_master=False, reverse_slave=False, same_as_master=True, bound_name=None\n    ):\n        \"\"\"Assign master and slave boundary conditions to two edges of the same object.\n\n        Parameters\n        ----------\n        master_edge : int\n            ID of the master edge.\n        slave_edge : int\n            ID of the slave edge.\n        reverse_master : bool, optional\n            Whether to reverse the master edge to the V direction. The default is ``False``.\n        reverse_slave : bool, optional\n            Whether to reverse the master edge to the U direction. The default is ``False``.\n        same_as_master : bool, optional\n            Whether the B-Field of the slave edge and master edge are the same. The default is ``True``.\n        bound_name : str, optional\n            Name of the master boundary. The name of the slave boundary will have a ``_dep`` suffix.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`, :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Master and slave objects.\n\n        References\n        ----------\n\n        >>> oModule.AssignIndependent\n        >>> oModule.AssignDependent\n        \"\"\"\n        master_edge = self.modeler.convert_to_selections(master_edge, True)\n        slave_edge = self.modeler.convert_to_selections(slave_edge, True)\n        if not bound_name:\n            bound_name_m = generate_unique_name(\"Independent\")\n            bound_name_s = generate_unique_name(\"Dependent\")\n        else:\n            bound_name_m = bound_name\n            bound_name_s = bound_name + \"_dep\"\n        props2 = OrderedDict({\"Edges\": master_edge, \"ReverseV\": reverse_master})\n        bound = BoundaryObject(self, bound_name_m, props2, \"Independent\")\n        if bound.create():\n            self.boundaries.append(bound)\n\n            props2 = OrderedDict(\n                {\n                    \"Edges\": slave_edge,\n                    \"ReverseU\": reverse_slave,\n                    \"Independent\": bound_name_m,\n                    \"SameAsMaster\": same_as_master,\n                }\n            )\n            bound2 = BoundaryObject(self, bound_name_s, props2, \"Dependent\")\n            if bound2.create():\n                self.boundaries.append(bound2)\n                return bound, bound2\n            else:\n                return bound, False\n        return False, False\n\n    @pyaedt_function_handler()\n    def assign_end_connection(self, objects, resistance=0, inductance=0, bound_name=None):\n        \"\"\"Assign End connection to a list of objects.\n\n        Parameters\n        ----------\n        objects : list of int or str or :class:`pyaedt.modeler.Object3d.Object3d`\n            List of objects to apply end connection.\n        resistance : float or str, optional\n            Resistance value. If float is provided then it is assumed in Ohm.\n            The default value is \"0ohm\".\n        inductance : float or str, optional\n            Inductance value. If float is provided then it is assumed in Henry.\n            The default value is \"0H\".\n        bound_name : str, optional\n            Name of the End connection boundary.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            New created object.\n\n        References\n        ----------\n\n        >>> oModule.AssignEndConnection\n        \"\"\"\n        if self.solution_type not in [\"EddyCurrent\", \"Transient\"]:\n            self.logger.error(\"Excitation applicable only to Eddy current or Transient Solver.\")\n            return False\n        if len(objects) < 2:\n            self.logger.error(\"At least 2 objects are needed.\")\n            return False\n        objects = self.modeler.convert_to_selections(objects, True)\n        if not bound_name:\n            bound_name = generate_unique_name(\"EndConnection\")\n\n        props = OrderedDict(\n            {\n                \"Objects\": objects,\n                \"ResistanceValue\": self.modeler._arg_with_dim(resistance, \"ohm\"),\n                \"InductanceValue\": self.modeler._arg_with_dim(inductance, \"H\"),\n            }\n        )\n        bound = BoundaryObject(self, bound_name, props, \"EndConnection\")\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n# --- Snippet Separator ---\n\nclass MaxwellCircuit(AnalysisMaxwellCircuit, object):\n    \"\"\"Provides the Maxwell Circuit Editor application interface.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open.  The default is ``None``, in which\n        case an attempt is made to get an active project. If no\n        projects are present, an empty project is created.\n    designname : str, optional\n        Name of the design to select. The default is ``None``, in\n        which case an attempt is made to get an active design. If no\n        designs are present, an empty design is created.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``. If ``None``,\n        the active setup is used or the latest installed version is\n        used.\n    non-graphical : bool, optional\n        Whether to launch AEDT in non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in graphical mode.\n        This parameter is ignored when a script is launched within AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine.  The default is ``True``.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit. The default is ``True``.\n    student_version : bool, optional\n        Whether open AEDT Student Version. The default is ``False``.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of Maxwell Circuit and connect to an existing\n    Maxwell circuit design or create a new Maxwell circuit design if one does\n    not exist.\n\n    >>> from pyaedt import MaxwellCircuit\n    >>> app = MaxwellCircuit()\n\n    Create an instance of Maxwell Circuit and link to a project named\n    ``\"projectname\"``. If this project does not exist, create one with\n    this name.\n\n    >>> app = MaxwellCircuit(projectname)\n\n    Create an instance of Maxwell Circuit and link to a design named\n    ``\"designname\"`` in a project named ``\"projectname\"``.\n\n    >>> app = MaxwellCircuit(projectname, designame)\n\n    Create an instance of Maxwell Circuit and open the specified\n    project, which is named ``\"myfile.aedt\"``.\n\n    >>> app = MaxwellCircuit(\"myfile.aedt\")\n    \"\"\"\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=False,\n        close_on_exit=False,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n        \"\"\"Constructor.\"\"\"\n        AnalysisMaxwellCircuit.__init__(\n            self,\n            \"Maxwell Circuit\",\n            projectname,\n            designname,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine,\n            port,\n            aedt_process_id,\n        )\n\n    @pyaedt_function_handler()\n    def create_schematic_from_netlist(self, file_to_import):\n        \"\"\"Create a circuit schematic from an HSpice net list.\n\n        Supported currently are:\n\n        * R\n        * L\n        * C\n        * Diodes\n\n        Parameters\n        ----------\n        file_to_import : str\n            Full path to the HSpice file.\n\n        Returns\n        -------\n        bool\n            ``True`` when successful, ``False`` when failed.\n\n        \"\"\"\n        xpos = 0\n        ypos = 0\n        delta = 0.0508\n        use_instance = True\n        with open(file_to_import, \"r\") as f:\n            for line in f:\n                mycomp = None\n                fields = line.split(\" \")\n                name = fields[0]\n                if fields[0][0] == \"R\":\n                    value = fields[3][fields[3].find(\"=\") + 1 :].strip()\n                    mycomp = self.modeler.schematic.create_resistor(\n                        name, value, [xpos, ypos], use_instance_id_netlist=use_instance\n                    )\n                elif fields[0][0] == \"L\":\n                    value = fields[3][fields[3].find(\"=\") + 1 :].strip()\n                    mycomp = self.modeler.schematic.create_inductor(\n                        name, value, [xpos, ypos], use_instance_id_netlist=use_instance\n                    )\n                elif fields[0][0] == \"C\":\n                    value = fields[3][fields[3].find(\"=\") + 1 :].strip()\n                    mycomp = self.modeler.schematic.create_capacitor(\n                        name, value, [xpos, ypos], use_instance_id_netlist=use_instance\n                    )\n                elif fields[0][0] == \"D\":\n                    value = fields[3][fields[3].find(\"=\") + 1 :].strip()\n                    mycomp = self.modeler.schematic.create_diode(\n                        name, value, [xpos, ypos], use_instance_id_netlist=use_instance\n                    )\n                if mycomp:\n                    id = 1\n                    for pin in mycomp.pins:\n                        if pin.name == \"CH\" or pin.name == fields[0][0]:\n                            continue\n                        pos = pin.location\n                        if pos[0] < xpos:\n                            angle = 0.0\n                        else:\n                            angle = math.pi\n                        self.modeler.schematic.create_page_port(fields[id], [pos[0], pos[1]], angle)\n                        id += 1\n                    ypos += delta\n                    if ypos > 0.254:\n                        xpos += delta\n                        ypos = 0\n        return True\n\n    def __enter__(self):\n        return self\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a project in Maxwell 2D using the PyAEDT library and runs a transient simulation. The code should import necessary libraries, set non-graphical mode, insert a Maxwell 2D design and save the project. It should create a rectangle and duplicate it, create an air region, assign windings to the sheets and a balloon to the air region, and plot the model. The code should also create a transient setup, create a rectangular plot, solve the model, create output and plot it using PyVista, generate the same plot outside AEDT, and finally close AEDT.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 104, "repo_full_name": "dfki-ric__pytransform3d", "instruction": "Generate code that visualizes a wrench applied to a 6-DOF robot arm. The wrench is assumed to be measured by a force/torque sensor at the tool center point (TCP) of the robot arm due to a spherical mass. The code should include a function to plot the transformation about and along a screw axis, which represents the wrench. The wrench is then transformed from the TCP to the robot's base frame using the adjoint representation of the transformation. The transformed wrench has a force component and a torque component, which are also visualized as a screw. The code should also load a robot model from a URDF file, set joint angles, and plot the robot model and the transformations. The visualization should include the robot arm, the TCP, the spherical mass, and the wrench in both the TCP frame and the base frame. The code should be able to save the visualization as an image. The pytransform3d library is used for transformations and visualizations.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def parse_urdf(urdf_xml, mesh_path=None, package_dir=None, strict_check=True):\n    \"\"\"Parse information from URDF file.\n\n    Parameters\n    ----------\n    urdf_xml : str\n        Robot definition in URDF\n\n    mesh_path : str, optional (default: None)\n        Path in which we search for meshes that are defined in the URDF.\n        Meshes will be ignored if it is set to None and no 'package_dir'\n        is given.\n\n    package_dir : str, optional (default: None)\n        Some URDFs start file names with 'package://' to refer to the ROS\n        package in which these files (textures, meshes) are located. This\n        variable defines to which path this prefix will be resolved.\n\n    strict_check : bool, optional (default: True)\n        Raise a ValueError if the transformation matrix is not numerically\n        close enough to a real transformation matrix. Otherwise we print a\n        warning.\n\n    Returns\n    -------\n    robot_name : str\n        Name of the robot\n\n    links : list of Link\n        Links of the robot\n\n    joints : list of Joint\n        Joints of the robot\n\n    Raises\n    ------\n    UrdfException\n        If URDF is not valid\n    \"\"\"\n    # lxml does not allow whitespaces in the beginning\n    urdf_xml = urdf_xml.strip()\n    # lxml complains about unicode strings that start with unicode encoding\n    # declaration. Hence, we have to convert them to bytes first.\n    urdf_xml = bytes(urdf_xml.encode(\"utf-8\"))\n    try:\n        root = etree.XML(urdf_xml, parser=etree.XMLParser(recover=True))\n    except etree.XMLSyntaxError:\n        raise UrdfException(\"Invalid XML.\")\n\n    # URDF XML schema:\n    # https://github.com/ros/urdfdom/blob/master/xsd/urdf.xsd\n\n    if root.tag != \"robot\":\n        raise UrdfException(\"Robot tag is missing.\")\n\n    tree = etree.ElementTree(root)\n\n    if \"name\" not in root.attrib:\n        raise UrdfException(\"Attribute 'name' is missing in robot tag.\")\n\n    robot_name = root.attrib[\"name\"]\n\n    materials = dict([_parse_material(material)\n                      for material in tree.findall(\"material\")])\n\n    links = [_parse_link(link, materials, mesh_path, package_dir, strict_check)\n             for link in tree.findall(\"link\")]\n\n    link_names = [link.name for link in links]\n    joints = [_parse_joint(joint, link_names, strict_check)\n              for joint in tree.findall(\"joint\")]\n\n    return robot_name, links, joints\n\n# --- Snippet Separator ---\n\ndef adjoint_from_transform(A2B, strict_check=True, check=True):\n    r\"\"\"Compute adjoint representation of a transformation matrix.\n\n    The adjoint representation of a transformation\n    :math:`\\left[Ad_{\\boldsymbol{T}_{BA}}\\right] \\in \\mathbb{R}^{6 \\times 6}`\n    from frame A to frame B translates a twist from frame A to frame B\n    through the adjoint map\n\n    .. math::\n\n        \\mathcal{V}_{B}\n        = \\left[Ad_{\\boldsymbol{T}_{BA}}\\right] \\mathcal{V}_A\n\n    The corresponding transformation matrix operation is\n\n    .. math::\n\n        \\left[\\mathcal{V}_{B}\\right]\n        = \\boldsymbol{T}_{BA} \\left[\\mathcal{V}_A\\right]\n        \\boldsymbol{T}_{BA}^{-1}\n\n    We can also use the adjoint representation to transform a wrench from frame\n    A to frame B:\n\n    .. math::\n\n        \\mathcal{F}_B\n        = \\left[ Ad_{\\boldsymbol{T}_{AB}} \\right]^T \\mathcal{F}_A\n\n    Note that not only the adjoint is transposed but also the transformation is\n    inverted.\n\n    Adjoint representations have the following properties:\n\n    .. math::\n\n        \\left[Ad_{\\boldsymbol{T}_1 \\boldsymbol{T}_2}\\right]\n        = \\left[Ad_{\\boldsymbol{T}_1}\\right]\n        \\left[Ad_{\\boldsymbol{T}_2}\\right]\n\n    .. math::\n\n        \\left[Ad_{\\boldsymbol{T}}\\right]^{-1} =\n        \\left[Ad_{\\boldsymbol{T}^{-1}}\\right]\n\n    For a transformation matrix\n\n    .. math::\n\n        \\boldsymbol T =\n        \\left( \\begin{array}{cc}\n            \\boldsymbol R & \\boldsymbol t\\\\\n            \\boldsymbol 0 & 1\\\\\n        \\end{array} \\right)\n\n    the adjoint is defined as\n\n    .. math::\n\n        \\left[Ad_{\\boldsymbol{T}}\\right]\n        =\n        \\left( \\begin{array}{cc}\n            \\boldsymbol R & \\boldsymbol 0\\\\\n            \\left[\\boldsymbol{t}\\right]_{\\times}\\boldsymbol R & \\boldsymbol R\\\\\n        \\end{array} \\right),\n\n    where :math:`\\left[\\boldsymbol{t}\\right]_{\\times}` is the cross-product\n    matrix (see :func:`~pytransform3d.rotations.cross_product_matrix`) of the\n    translation component.\n\n    Parameters\n    ----------\n    A2B : array-like, shape (4, 4)\n        Transform from frame A to frame B\n\n    strict_check : bool, optional (default: True)\n        Raise a ValueError if the transformation matrix is not numerically\n        close enough to a real transformation matrix. Otherwise we print a\n        warning.\n\n    check : bool, optional (default: True)\n        Check if transformation matrix is valid\n\n    Returns\n    -------\n    adj_A2B : array, shape (6, 6)\n        Adjoint representation of transformation matrix\n    \"\"\"\n    if check:\n        A2B = check_transform(A2B, strict_check)\n\n    R = A2B[:3, :3]\n    p = A2B[:3, 3]\n\n    adj_A2B = np.zeros((6, 6))\n    adj_A2B[:3, :3] = R\n    adj_A2B[3:, :3] = np.dot(cross_product_matrix(p), R)\n    adj_A2B[3:, 3:] = R\n    return adj_A2B\n\n# --- Snippet Separator ---\n\ndef initialize_urdf_transform_manager(tm, robot_name, links, joints):\n    \"\"\"Initializes transform manager from previously parsed URDF data.\n\n    Parameters\n    ----------\n    tm : UrdfTransformManager\n        Transform manager\n\n    robot_name : str\n        Name of the robot\n\n    links : list of Link\n        Links of the robot\n\n    joints : list of Joint\n        Joints of the robot\n    \"\"\"\n    tm.add_transform(links[0].name, robot_name, np.eye(4))\n    _add_links(tm, links)\n    _add_joints(tm, joints)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that visualizes a wrench applied to a 6-DOF robot arm. The wrench is assumed to be measured by a force/torque sensor at the tool center point (TCP) of the robot arm due to a spherical mass. The code should include a function to plot the transformation about and along a screw axis, which represents the wrench. The wrench is then transformed from the TCP to the robot's base frame using the adjoint representation of the transformation. The transformed wrench has a force component and a torque component, which are also visualized as a screw. The code should also load a robot model from a URDF file, set joint angles, and plot the robot model and the transformations. The visualization should include the robot arm, the TCP, the spherical mass, and the wrench in both the TCP frame and the base frame. The code should be able to save the visualization as an image. The pytransform3d library is used for transformations and visualizations.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 105, "repo_full_name": "pyvista__pyvista", "instruction": "Generate code that creates and plots various parametric geometric objects using the pyvista library. The objects to be created and plotted include a Supertoroid, an Ellipsoid, a Partial Parametric Ellipsoid, a Pseudosphere, a Bohemian Dome, a Bour, a Boy's Surface, a Catalan Minimal, a Conic Spiral, a Cross Cap, a Dini, an Enneper, a Figure-8 Klein, a Henneberg, a Klein, a Kuen, a Mobius, a Plucker Conoid, Random Hills, a Roman, a Super Ellipsoid, a Torus, a Circular Arc, and an Extruded Half Arc. The objects should be plotted with light blue color where applicable. For the Partial Parametric Ellipsoid, a specific plotting direction should be used. For the Enneper, the plotting position should be \"yz\". For the Circular Arc and the Extruded Half Arc, specific points and a center should be defined. The Extruded Half Arc should be extruded in the z direction and its edges should be shown in the plot.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def plot_projected_ellipsoid(\n        ax, mean, cov, factor=1.96, wireframe=True, n_steps=20, color=None,\n        alpha=1.0):  # pragma: no cover\n    \"\"\"Plots projected equiprobable ellipsoid in 3D.\n\n    An error ellipsoid shows equiprobable points. This is a projection of a\n    Gaussian distribution in exponential coordinate space to 3D.\n\n    Parameters\n    ----------\n    ax : axis\n        Matplotlib axis.\n\n    mean : array-like, shape (4, 4)\n        Mean pose.\n\n    cov : array-like, shape (6, 6)\n        Covariance in exponential coordinate space.\n\n    factor : float, optional (default: 1.96)\n        Multiple of the standard deviations that should be plotted.\n\n    wireframe : bool, optional (default: True)\n        Plot wireframe of ellipsoid and surface otherwise.\n\n    n_steps : int, optional (default: 20)\n        Number of discrete steps plotted in each dimension.\n\n    color : str, optional (default: None)\n        Color in which the equiprobably lines should be plotted.\n\n    alpha : float, optional (default: 1.0)\n        Alpha value for lines.\n\n    Returns\n    -------\n    ax : axis\n        Matplotlib axis.\n    \"\"\"\n    x, y, z = to_projected_ellipsoid(mean, cov, factor, n_steps)\n\n    if wireframe:\n        ax.plot_wireframe(\n            x, y, z, rstride=2, cstride=2, color=color, alpha=alpha)\n    else:\n        ax.plot_surface(x, y, z, color=color, alpha=alpha, linewidth=0)\n\n    return ax\n\n# --- Snippet Separator ---\n\ndef CircularArc(pointa, pointb, center, resolution=100, negative=False):\n    \"\"\"Create a circular arc defined by two endpoints and a center.\n\n    The number of segments composing the polyline is controlled by\n    setting the object resolution.\n\n    Parameters\n    ----------\n    pointa : sequence[float]\n        Position of the first end point.\n\n    pointb : sequence[float]\n        Position of the other end point.\n\n    center : sequence[float]\n        Center of the circle that defines the arc.\n\n    resolution : int, default: 100\n        The number of segments of the polyline that draws the arc.\n        Resolution of 1 will just create a line.\n\n    negative : bool, default: False\n        By default the arc spans the shortest angular sector between\n        ``pointa`` and ``pointb``.\n\n        By setting this to ``True``, the longest angular sector is\n        used instead (i.e. the negative coterminal angle to the\n        shortest one).\n\n    Returns\n    -------\n    pyvista.PolyData\n        Circular arc mesh.\n\n    Examples\n    --------\n    Create a quarter arc centered at the origin in the xy plane.\n\n    >>> import pyvista\n    >>> arc = pyvista.CircularArc([-1, 0, 0], [0, 1, 0], [0, 0, 0])\n    >>> pl = pyvista.Plotter()\n    >>> _ = pl.add_mesh(arc, color='k', line_width=10)\n    >>> _ = pl.show_bounds(location='all', font_size=30, use_2d=True)\n    >>> _ = pl.view_xy()\n    >>> pl.show()\n    \"\"\"\n    check_valid_vector(pointa, 'pointa')\n    check_valid_vector(pointb, 'pointb')\n    check_valid_vector(center, 'center')\n    if not np.isclose(\n        np.linalg.norm(np.array(pointa) - np.array(center)),\n        np.linalg.norm(np.array(pointb) - np.array(center)),\n    ):\n        raise ValueError(\"pointa and pointb are not equidistant from center\")\n\n    # fix half-arc bug: if a half arc travels directly through the\n    # center point, it becomes a line\n    pointb = list(pointb)\n    pointb[0] -= 1e-10\n    pointb[1] -= 1e-10\n\n    arc = _vtk.vtkArcSource()\n    arc.SetPoint1(*pointa)\n    arc.SetPoint2(*pointb)\n    arc.SetCenter(*center)\n    arc.SetResolution(resolution)\n    arc.SetNegative(negative)\n\n    arc.Update()\n    angle = np.deg2rad(arc.GetAngle())\n    arc = wrap(arc.GetOutput())\n    # Compute distance of every point along circular arc\n    center = np.array(center).ravel()\n    radius = np.sqrt(np.sum((arc.points[0] - center) ** 2, axis=0))\n    angles = np.arange(0.0, 1.0 + 1.0 / resolution, 1.0 / resolution) * angle\n    arc['Distance'] = radius * angles\n    return arc\n\n# --- Snippet Separator ---\n\ndef plot_ellipsoid(ax=None, radii=np.ones(3), A2B=np.eye(4), ax_s=1,\n                   wireframe=True, n_steps=20, alpha=1.0, color=\"k\"):\n    \"\"\"Plot ellipsoid.\n\n    Parameters\n    ----------\n    ax : Matplotlib 3d axis, optional (default: None)\n        If the axis is None, a new 3d axis will be created\n\n    radii : array-like, shape (3,)\n        Radii along the x-axis, y-axis, and z-axis of the ellipsoid.\n\n    A2B : array-like, shape (4, 4)\n        Transform from frame A to frame B\n\n    ax_s : float, optional (default: 1)\n        Scaling of the new matplotlib 3d axis\n\n    wireframe : bool, optional (default: True)\n        Plot wireframe of ellipsoid and surface otherwise\n\n    n_steps : int, optional (default: 20)\n        Number of discrete steps plotted in each dimension\n\n    alpha : float, optional (default: 1)\n        Alpha value of the ellipsoid that will be plotted\n\n    color : str, optional (default: black)\n        Color in which the ellipsoid should be plotted\n\n    Returns\n    -------\n    ax : Matplotlib 3d axis\n        New or old axis\n    \"\"\"\n    if ax is None:\n        ax = make_3d_axis(ax_s)\n\n    radius_x, radius_y, radius_z = radii\n\n    x, y, z = unit_sphere_surface_grid(n_steps)\n    x *= radius_x\n    y *= radius_y\n    z *= radius_z\n\n    x, y, z = transform_surface(A2B, x, y, z)\n\n    if wireframe:\n        ax.plot_wireframe(\n            x, y, z, rstride=2, cstride=2, color=color, alpha=alpha)\n    else:\n        ax.plot_surface(x, y, z, color=color, alpha=alpha, linewidth=0)\n\n    return ax\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates and plots various parametric geometric objects using the pyvista library. The objects to be created and plotted include a Supertoroid, an Ellipsoid, a Partial Parametric Ellipsoid, a Pseudosphere, a Bohemian Dome, a Bour, a Boy's Surface, a Catalan Minimal, a Conic Spiral, a Cross Cap, a Dini, an Enneper, a Figure-8 Klein, a Henneberg, a Klein, a Kuen, a Mobius, a Plucker Conoid, Random Hills, a Roman, a Super Ellipsoid, a Torus, a Circular Arc, and an Extruded Half Arc. The objects should be plotted with light blue color where applicable. For the Partial Parametric Ellipsoid, a specific plotting direction should be used. For the Enneper, the plotting position should be \"yz\". For the Circular Arc and the Extruded Half Arc, specific points and a center should be defined. The Extruded Half Arc should be extruded in the z direction and its edges should be shown in the plot.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 106, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation using the seed-emulator library. The emulation should include base, routing, eBGP, iBGP, OSPF, and web service layers. It should define a function to create a stub autonomous system with a web server and a router that join a network and an internet exchange. The code should create three internet exchanges and multiple stub autonomous systems that join these exchanges. It should also create two autonomous systems with routers that join different networks and internet exchanges. The code should define private peerings between different autonomous systems. Finally, it should add a BGP attacker component that hijacks certain prefixes and joins an internet exchange. The code should merge the BGP attacker with the emulator and render the new emulator. The code should compile the new emulator using Docker and output the result to a specified directory.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class BgpAttackerComponent(Component):\n    \"\"\"!\n    @brief BGP hijacker component.\n    \"\"\"\n\n    __data: Emulator\n    __hijacker_as: AutonomousSystem\n    __prefixes: List[str]\n    __routing: Routing\n    __hijacker: Router\n\n    def __init__(self, attackerAsn: int):\n        \"\"\"!\n        @brief Create a new BGP hijacker.\n\n        @param attackerAsn ASN of the hijacker.\n        \"\"\"\n\n        self.__data = Emulator()\n        self.__prefixes = []\n\n        base = Base()\n        self.__routing = Routing()\n\n        self.__hijacker_as = base.createAutonomousSystem(attackerAsn)\n        self.__hijacker = self.__hijacker_as.createRouter('hijacker')\n\n        self.__data.addLayer(base)\n        self.__data.addLayer(self.__routing)\n        self.__data.addHook(BgpAttackerInjectorHook(self))\n\n    def getHijackerAsn(self) -> int: \n        \"\"\"!\n        @brief Get ASN of the hijacker.\n\n        @returns ASN.\n        \"\"\"\n        return self.__hijacker_as.getAsn()\n\n    def getHijackerRouter(self) -> Router:\n        \"\"\"!\n        @brief Get the router object of the hijacker.\n\n        @returns router.\n        \"\"\"\n        return self.__hijacker\n\n    def get(self) -> Emulator:\n        \"\"\"!\n        @brief Get the emulator with attacker.\n\n        Merge the emulator to install the component.\n        \"\"\"\n        return self.__data\n\n    def addHijackedPrefix(self, prefix: str) -> BgpAttackerComponent:\n        \"\"\"!\n        @brief Add a prefix to hijack.\n\n        @param prefix prefix in CIDR notation.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self.__prefixes.append(prefix)\n\n        return self\n\n    def getHijackedPrefixes(self) -> List[str]:\n        \"\"\"!\n        @brief Get hijacked prefixes.\n\n        @returns list of prefixes.\n        \"\"\"\n        return self.__prefixes\n\n    def joinInternetExchange(self, ix: str, addr: str) -> BgpAttackerComponent:\n        \"\"\"!\n        @brief Join an internet exchange.\n\n        @param ix internet exchange network name.\n        @param addr address in the exchange.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self.__hijacker.joinNetwork(ix, addr)\n\n        return self\n\n# --- Snippet Separator ---\n\ndef makeEmulatorBaseWith5StubASAndHosts(hosts_per_stub_as: int) -> Emulator:\n    ###############################################################################\n    emu     = Emulator()\n    base    = Base()\n    routing = Routing()\n    ebgp    = Ebgp()\n    ibgp    = Ibgp()\n    ospf    = Ospf()\n\n\n    ###############################################################################\n\n    ix100 = base.createInternetExchange(100)\n    ix101 = base.createInternetExchange(101)\n    ix102 = base.createInternetExchange(102)\n    ix103 = base.createInternetExchange(103)\n    ix104 = base.createInternetExchange(104)\n\n    # Customize names (for visualization purpose)\n    ix100.getPeeringLan().setDisplayName('NYC-100')\n    ix101.getPeeringLan().setDisplayName('San Jose-101')\n    ix102.getPeeringLan().setDisplayName('Chicago-102')\n    ix103.getPeeringLan().setDisplayName('Miami-103')\n    ix104.getPeeringLan().setDisplayName('Boston-104')\n\n\n    ###############################################################################\n    # Create Transit Autonomous Systems \n\n    ## Tier 1 ASes\n    makeTransitAs(base, 2, [100, 101, 102], \n        [(100, 101), (101, 102)] \n    )\n\n    makeTransitAs(base, 3, [100, 103, 104], \n        [(100, 103), (103, 104)]\n    )\n\n    makeTransitAs(base, 4, [100, 102, 104], \n        [(100, 104), (102, 104)]\n    )\n\n    ## Tier 2 ASes\n    makeTransitAs(base, 12, [101, 104], [(101, 104)])\n\n\n    ###############################################################################\n    # Create single-homed stub ASes. \"None\" means create a host only \n\n    makeStubAsWithHosts(emu, base, 150, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 151, 100, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 152, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 153, 101, hosts_per_stub_as)\n    makeStubAsWithHosts(emu, base, 154, 102, hosts_per_stub_as)\n\n\n    ###############################################################################\n    # Peering via RS (route server). The default peering mode for RS is PeerRelationship.Peer, \n    # which means each AS will only export its customers and their own prefixes. \n    # We will use this peering relationship to peer all the ASes in an IX.\n    # None of them will provide transit service for others. \n\n    ebgp.addRsPeers(100, [2, 3, 4])\n    ebgp.addRsPeers(102, [2, 4])\n    ebgp.addRsPeers(104, [3, 4])\n\n    # To buy transit services from another autonomous system, \n    # we will use private peering  \n\n    ebgp.addPrivatePeerings(100, [2],  [150, 151], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(100, [3],  [150], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(101, [2],  [12], PeerRelationship.Provider)\n    ebgp.addPrivatePeerings(101, [12], [152, 153], PeerRelationship.Provider)\n\n    ebgp.addPrivatePeerings(102, [2, 4],  [154], PeerRelationship.Provider)\n\n\n    # Add layers to the emulator\n    emu.addLayer(base)\n    emu.addLayer(routing)\n    emu.addLayer(ebgp)\n    emu.addLayer(ibgp)\n    emu.addLayer(ospf)\n\n    return emu\n\n# --- Snippet Separator ---\n\ndef makeTransitAs(base: Base, asn: int, exchanges: List[int],\n    intra_ix_links: List[Tuple[int, int]]) -> AutonomousSystem:\n    \"\"\"!\n    @brief create a transit AS.\n\n    @param base reference to the base layer.\n    @param asn ASN of the newly created AS.\n    @param exchanges list of IXP IDs to join.\n    @param intra_ix_links list of tuple of IXP IDs, to create intra-IX links at.\n\n    @returns transit AS object.\n    \"\"\"\n\n    transit_as = base.createAutonomousSystem(asn)\n\n    routers: Dict[int, Router] = {}\n\n    # Create a BGP router for each internet exchange (for peering purpose)\n    for ix in exchanges:\n        routers[ix] = transit_as.createRouter('r{}'.format(ix))\n        routers[ix].joinNetwork('ix{}'.format(ix))\n\n    # For each pair, create an internal network to connect the BGP routers\n    # from two internet exchanges. There is no need to create a full-mesh\n    # network among the BGP routers. As long as they can reach each other\n    # over a single or multiple hops, it is OK.\n    for (a, b) in intra_ix_links:\n        name = 'net_{}_{}'.format(a, b)\n\n        transit_as.createNetwork(name)\n        routers[a].joinNetwork(name)\n        routers[b].joinNetwork(name)\n\n    return transit_as\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulation using the seed-emulator library. The emulation should include base, routing, eBGP, iBGP, OSPF, and web service layers. It should define a function to create a stub autonomous system with a web server and a router that join a network and an internet exchange. The code should create three internet exchanges and multiple stub autonomous systems that join these exchanges. It should also create two autonomous systems with routers that join different networks and internet exchanges. The code should define private peerings between different autonomous systems. Finally, it should add a BGP attacker component that hijacks certain prefixes and joins an internet exchange. The code should merge the BGP attacker with the emulator and render the new emulator. The code should compile the new emulator using Docker and output the result to a specified directory.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 107, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that performs the following tasks using the porepy library:\n\n1. Define a function to add data to a given grid bucket. This function should define the permeability, apertures, and boundary conditions for each grid in the bucket. It should also assign coupling permeability for each edge in the grid bucket.\n\n2. Define a function to write a network of points to a CSV file. The network should be defined as a string and written to the file.\n\n3. Define a main function that takes in a permeability factor, a description, and a mesh size. This function should create a grid bucket from the CSV file, compute its geometry, and optionally generate a coarse grid. It should then assign parameters to the grid bucket, solve a system of equations using the DualVEMMixDim solver, and extract and project the solution. The function should also export the results to a VTK file and print out some information about the grid bucket.\n\n4. Define two functions, one for blocking and one for permeable scenarios. Each function should call the main function with different permeability factors and mesh sizes.\n\n5. Finally, call the two functions for blocking and permeable scenarios.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def generate_coarse_grid(g, subdiv):\n    \"\"\" Generate a coarse grid clustering the cells according to the flags\n    given by subdiv. Subdiv should be long as the number of cells in the\n    original grid, it contains integers (possibly not continuous) which\n    represent the cells in the final mesh. If a grid bucket is given the\n    coarsening is applied to the higher dimensional grid.\n\n    The values computed in \"compute_geometry\" are not preserved and they should\n    be computed out from this function.\n\n    Note: there is no check for disconnected cells in the final grid.\n\n    Parameters:\n        g: the grid or grid bucket\n        subdiv: a list of flags, one for each cell of the original grid\n\n    Return:\n        grid: if a grid is given as input, its coarser version is returned.\n        If a grid bucket is given as input, the grid is updated in place.\n\n    How to use:\n    subdiv = np.array([0,0,1,1,1,1,3,4,6,4,6,4])\n    g = generate_coarse_grid(g, subdiv)\n\n    or with a grid bucket:\n    subdiv = np.array([0,0,1,1,1,1,3,4,6,4,6,4])\n    generate_coarse_grid(gb, subdiv)\n\n    \"\"\"\n    if isinstance(g, grid.Grid):\n        generate_coarse_grid_single(g, subdiv, False)\n\n    if isinstance(g, grid_bucket.GridBucket):\n        generate_coarse_grid_gb(g, subdiv)\n\n# --- Snippet Separator ---\n\ndef write_vtk(self, data=None, time_step=None, grid=None):\n        \"\"\" Interface function to export in VTK the grid and additional data.\n\n        In 2d the cells are represented as polygon, while in 3d as polyhedra.\n        VTK module need to be installed.\n        In 3d the geometry of the mesh needs to be computed.\n\n        To work with python3, the package vtk should be installed in version 7\n        or higher.\n\n        Parameters:\n        data: if g is a single grid then data is a dictionary (see example)\n              if g is a grid bucket then list of names for optional data,\n              they are the keys in the grid bucket (see example).\n        time_step: (optional) in a time dependent problem defines the full name of\n            the file.\n        grid: (optional) in case of changing grid set a new one.\n\n        \"\"\"\n        if self.is_not_vtk:\n            return\n\n        if self.fixed_grid and grid is not None:\n            raise ValueError(\"Inconsistency in exporter setting\")\n        elif not self.fixed_grid and grid is not None:\n            self.gb = grid\n            self.is_GridBucket = isinstance(self.gb, grid_bucket.GridBucket)\n            self._update_gVTK()\n\n        if self.is_GridBucket:\n            self._export_vtk_gb(data, time_step)\n        else:\n            # No need of special naming, create the folder\n            name = self._make_folder(self.folder, self.name)\n            self._export_vtk_single(data, time_step, self.gb, name)\n\n# --- Snippet Separator ---\n\ndef dfm_2d_from_csv(f_name, mesh_kwargs, domain=None, return_domain=False,\n                    tol=1e-8, **kwargs):\n    \"\"\"\n    Create the grid bucket from a set of fractures stored in a csv file and a\n    domain. In the csv file, we assume the following structure:\n    FID, START_X, START_Y, END_X, END_Y\n\n    Where FID is the fracture id, START_X and START_Y are the abscissa and\n    coordinate of the starting point, and END_X and END_Y are the abscissa and\n    coordinate of the ending point.\n    Note: the delimiter can be different.\n\n    Parameters:\n        f_name: the file name in CSV format\n        mesh_kwargs: list of additional arguments for the meshing\n        domain: rectangular domain, if not given the bounding-box is computed\n        kwargs: list of argument for the numpy function genfromtxt\n\n    Returns:\n        gb: grid bucket associated to the configuration.\n        domain: if the domain is not given as input parameter, the bounding box\n        is returned.\n\n    \"\"\"\n    pts, edges = lines_from_csv(f_name, tol=tol, **kwargs)\n    f_set = np.array([pts[:, e] for e in edges.T])\n\n    # Define the domain as bounding-box if not defined\n    if domain is None:\n        overlap = kwargs.get('domain_overlap', 0)\n        domain = cg.bounding_box(pts, overlap)\n\n    if return_domain:\n        return meshing.simplex_grid(f_set, domain, **mesh_kwargs), domain\n    else:\n        return meshing.simplex_grid(f_set, domain, **mesh_kwargs)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks using the porepy library:\n\n1. Define a function to add data to a given grid bucket. This function should define the permeability, apertures, and boundary conditions for each grid in the bucket. It should also assign coupling permeability for each edge in the grid bucket.\n\n2. Define a function to write a network of points to a CSV file. The network should be defined as a string and written to the file.\n\n3. Define a main function that takes in a permeability factor, a description, and a mesh size. This function should create a grid bucket from the CSV file, compute its geometry, and optionally generate a coarse grid. It should then assign parameters to the grid bucket, solve a system of equations using the DualVEMMixDim solver, and extract and project the solution. The function should also export the results to a VTK file and print out some information about the grid bucket.\n\n4. Define two functions, one for blocking and one for permeable scenarios. Each function should call the main function with different permeability factors and mesh sizes.\n\n5. Finally, call the two functions for blocking and permeable scenarios.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 108, "repo_full_name": "nanophotonics__nplab", "instruction": "Generate code that creates an experiment using the nplab library. The experiment should involve a shutter and a spectrometer. The experiment should open and close the shutter, wait for a specified amount of time, and then take a spectrum. The experiment should also have a user interface that allows the user to control the irradiation time and wait time. The code should also include a GUI for the experiment that includes a data browser, spectrometer controls, and shutter controls. The experiment and its GUI should be tested using dummy spectrometer and shutter.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Experiment(Instrument):\n    \"\"\"A class representing an experimental protocol.\n\n    This base class is a subclass of Instrument, so it provides all the GUI\n    code and data management that instruments have.  It's also got an\n    improved logging mechanism, designed for use as a status display, and some\n    template methods for running a long experiment in the background.\n    \"\"\"\n\n    latest_data = DumbNotifiedProperty(doc=\"The last dataset/group we acquired\")\n    log_messages = DumbNotifiedProperty(doc=\"Log messages from the latest run\")\n    log_to_console = False\n    experiment_can_be_safely_aborted = False # set to true if you want to suppress warnings about ExperimentStopped\n\n    def __init__(self):\n        \"\"\"Create an instance of the Experiment class\"\"\"\n        super(Experiment, self).__init__()\n        self._stop_event = threading.Event()\n        self._finished_event = threading.Event()\n        self._experiment_thread = None\n        self.log_messages = \"\"\n\n    def prepare_to_run(self, *args, **kwargs):\n        \"\"\"This method is always run in the foreground thread before run()\n\n        Use this method if you might need to pop up a GUI, for example.  The\n        most common use of this would be to create a data group or to ensure\n        the current data file exists - doing that in run() could give rise\n        to nasty threading problems.  By default, it does nothing.\n\n        The arguments are passed through from start() to here, so you should\n        either use or ignore them as appropriate.  These are the same args\n        as are passed to run(), so if one of the two functions requires an\n        argument you should make sure the other won't fail if the same\n        argument is passed to it (simple rule: accept *args, **kwargs in\n        both, in addition to any arguments you might have).\n        \"\"\"\n        pass\n\n    def run(self, *args, **kwargs):\n        \"\"\"This method should be the meat of the experiment (needs overriden).\n\n        This is where your experiment code goes.  Note that you should use\n        `self.wait_or_stop()` to pause your experiment between readings, to\n        allow the background thread to be stopped if necessary.\n\n        If you set `self.latest_data`, this may be used to display your\n        results in real time.  You can also use `self.log()` to output text\n        describing the experiment's progress; this may be picked up and \n        displayed graphically or in the console.\n\n        The arguments are passed through from start() to here, so you should\n        either use or ignore them as appropriate.  These are the same args\n        as are passed to run(), so if one of the two functions requires an\n        argument you should make sure the other won't fail if the same\n        argument is passed to it (simple rule: accept *args, **kwargs in\n        both, in addition to any arguments you might have).\n        \"\"\"\n        NotImplementedError(\"The run() method of an Experiment must be overridden!\")\n\n    def wait_or_stop(self, timeout, raise_exception=True):\n        \"\"\"Wait for the specified time in seconds.  Stop if requested.\n\n        This waits for a given time, unless the experiment has been manually \n        stopped, in which case it will terminate the thread by raising an\n        ExperimentStopped exception.  You should call this whenever your\n        experiment is in a state that would be OK to stop, such as between\n        readings.\n\n        If raise_exception is False, it will simply return False when the\n        experiment should stop.  This is appropriate if you want to use it in a\n        while loop, e.g. ``while self.wait_or_stop(10,raise_exception=False):``\n\n        You may want to explicitly handle the ExperimentStopped exception to\n        close down cleanly.\n        \"\"\"\n        if self._stop_event.wait(timeout):\n            if raise_exception:\n                raise ExperimentStopped()\n        return True\n\n    @background_action\n    @locked_action\n    def run_in_background(self, *args, **kwargs):\n        \"\"\"Run the experiment in a background thread.\n\n        This is important in order to keep the GUI responsive.\n        \"\"\"\n        self.log_messages = \"\"\n        self._stop_event.clear()\n        self._finished_event.clear()\n        self.run(*args, **kwargs)\n        self._finished_event.set()\n\n    def start(self, *args, **kwargs):\n        \"\"\"Start the experiment running in a background thread.  See run_in_background.\"\"\"\n        assert self.running == False, \"Can't start the experiment when it is already running!\"\n        self.prepare_to_run(*args, **kwargs)\n        self._experiment_thread = self.run_in_background(*args, **kwargs)\n\n    def stop(self, join=False):\n        \"\"\"Stop the experiment running, if supported.  May take a little while.\"\"\"\n        self._stop_event.set()\n        if join:\n            try:\n                self._experiment_thread.join()\n            except ExperimentStopped as e:\n                if not self.experiment_can_be_safely_aborted:\n                    raise e\n\n    @property\n    def running(self):\n        \"\"\"Whether the experiment is currently running in the background.\"\"\"\n        return background_actions_running(self)\n\n    def log(self, message):\n        \"\"\"Log a message to the current HDF5 file and to the experiment's history\"\"\"\n        self.log_messages += message + \"\\n\"\n        if self.log_to_console:\n            print(message)\n        super(Experiment, self).log(message)\n\n# --- Snippet Separator ---\n\nclass Shutter(Instrument):\n    \"\"\"A generic instrument class for optical shutters.\n\n    An optical shutter can be \"Open\" (allowing light to pass) or \"Closed\" (not\n    allowing light through).  This generic class provides a GUI and some\n    convenience methods.  You can set and (usually) check the state of the\n    shutter using the property `Shutter.state` which is a string that's either\n    \"Open\" or \"Closed\".  If you need a boolean answer, use `Shutter.is_open()`\n    or `Shutter.is_closed`.  There's also `expose()` that opens for a number\n    of seconds, and `toggle()` that changes state.\n\n    # Subclassing Notes\n    The minimum required subclassing effort is overriding `set_state` to open\n    and close the shutter.  Overriding get_state allows you to read back the\n    state of the shutter.  If you want to emulate that (i.e. keep track of\n    the state of the shutter in software) subclass `ShutterWithEmulatedRead`\n    and make sure you call its `__init__` method in your initialisation code.\n    \"\"\"\n    def __init__(self):\n        super(Shutter, self).__init__()\n\n    def toggle(self):\n        \"\"\"Toggle the state of the shutter.\n\n        The default behaviour will emulate a toggle command if none exists.\n        \"\"\"\n        try:\n            if self.is_closed():\n                self.state = \"Open\"\n            else:\n                self.state = \"Closed\"\n        except NotImplementedError:\n            raise NotImplementedError(\"This shutter has no way to toggle!\"\"\")\n\n    @contextlib.contextmanager\n    def hold(self, state=\"Open\", default_state=\"Closed\"):\n        \"\"\"Hold the shutter in a given state (for use in a `with` block).\n\n        This returns a context manager, so it can be used in a `with` block,\n        so that the shutter is held in the given position (default Open) while\n        something else happens, then returns to its previous state (usually\n        Closed) afterwards, even if exceptions occur.\n\n        If the shutter can't report it's current state it should raise a\n        `NotImplementedError` (this is the default) in which case we will \n        default to closing the shutter afterwards unless `default_state` has\n        been set in which case we use that.\n\n        In the future, this might block other threads from touching the \n        shutter - currently it does not.\n        \"\"\"\n        try:\n            oldstate = self.state\n        except NotImplementedError:\n            oldstate = default_state\n        try:\n            self.state = state\n            yield\n        finally:\n            self.state = oldstate\n\n    def expose(self, time_in_seconds):\n        \"\"\"Open the shutter for a specified time, then close again.\n\n        This function will block until the exposure is over.  NB if you \n        override this function in a subclass, take care with what happens to\n        reads/writes of the self.state property.  If you are in a subclass\n        of `ShutterWithEmulatedRead` you might need to update\n        `_last_set_state`.\n        \"\"\"\n        with self.hold(\"Open\"):\n            time.sleep(time_in_seconds)\n\n    def get_state(self):\n        \"\"\"Whether the shutter is 'Open' or 'Closed'.\"\"\"\n        raise NotImplementedError(\"This shutter has no way to get its state!\"\"\")\n\n    def set_state(self, value):\n        \"\"\"Set the shutter to be either 'Open' or 'Closed'.\"\"\"\n        raise NotImplementedError(\"This shutter has no way to set its state!\"\"\")\n\n    def open_shutter(self):\n        \"\"\"Open the shutter.\"\"\"\n        self._set_state_proxy(\"Open\")\n\n    def close_shutter(self):\n        \"\"\"Close the shutter.\"\"\"\n        self._set_state_proxy(\"Closed\")\n\n    # This slightly ugly hack means it's not necessary to redefine the \n    # state property every time it's subclassed.\n    def _get_state_proxy(self):\n        \"\"\"The state of the shutter - should either be \"Open\" or \"Closed\".\"\"\"\n        return self.get_state()\n\n    def _set_state_proxy(self, state):\n        self.set_state(state)\n        self._last_set_state = state.title() # Remember what state we're in\n\n    state = property(_get_state_proxy, _set_state_proxy)\n\n    def is_open(self):\n        \"\"\"Return `True` if the shutter is open.\"\"\"\n        return self.state.title() == \"Open\"\n\n    def is_closed(self):\n        \"\"\"Return `True` if the shutter is closed.\"\"\"\n        return self.state.title() == \"Closed\"\n\n    def get_qt_ui(self):\n        \"\"\"Return a graphical interface for the shutter.\"\"\"\n        return ShutterUI(self)\n\n# --- Snippet Separator ---\n\nclass ExperimentGuiMixin(object):\n    \"\"\"This class will add a basic GUI to an experiment, showing logs & data.\n\n    The `get_control_widget()` method is essentially empty, and is intended\n    to be overridden with useful settings for the experiment, for example \n    using a QuickControlBox.\n    \"\"\"\n    def get_qt_ui(self):\n        \"\"\"Create a Qt Widget representing the experiment.\"\"\"\n        return ExperimentWidget(self)\n\n    def get_data_widget(self):\n        \"\"\"Create a QWidget that shows the latest data\"\"\"\n        return DataWidget(self)\n\n    def get_log_widget(self):\n        \"\"\"A widget that displays logs in a scrolling display.\"\"\"\n        return LogWidget(self)\n\n    def get_control_widget(self):\n        \"\"\"Return a widget that controls the experiment's settings.\"\"\"\n        return QuickControlBox()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an experiment using the nplab library. The experiment should involve a shutter and a spectrometer. The experiment should open and close the shutter, wait for a specified amount of time, and then take a spectrum. The experiment should also have a user interface that allows the user to control the irradiation time and wait time. The code should also include a GUI for the experiment that includes a data browser, spectrometer controls, and shutter controls. The experiment and its GUI should be tested using dummy spectrometer and shutter.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 109, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that demonstrates the use of the parallelized CCSD with K-point sampling in the pyscf library. The code should create a supercell composed of replicated units and run a molecular Hartree-Fock program using integrals between periodic gaussians. It should then call a molecular CC method for gamma point calculation and perform k-point calculations for the same system. The code should also calculate the differences between gamma/k-point mean-field, ccsd, ip-eomccsd, and ea-eomccsd calculations and print these differences.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def Ppath(self, point=\"\", node=\"\", x=\"\", y=\"\", z=\"\", cs=\"\", **kwargs):\n        \"\"\"\n        APDL Command: PPATH\n\n        Defines a path by picking or defining nodes, or locations on the\n        currently active working plane, or by entering specific coordinate\n        locations.\n\n        Parameters\n        ----------\n        point\n            The point number.  It must be greater than zero and less than or\n            equal to the nPts value specified on the PATH command if graphical\n            picking is not being used.\n\n        node\n            The node number defining this point.  If blank, use the X, Y, Z\n            coordinates to define the point.  A valid node number will override\n            X, Y, Z coordinate arguments.\n\n        x, y, z\n            The location of the point in the global Cartesian coordinate\n            system.  Use these arguments only if you omit the NODE argument.\n\n        cs\n            The coordinate system for interpolation of the path between the\n            previous point and this point.  Omit this argument if you wish to\n            use the currently active (CSYS) coordinate system.  If the\n            coordinate system of two adjacent points is different, the CS value\n            of the latter point will be used.\n\n        Notes\n        -----\n        For linearized stress calculations, the path must be defined with\n        nodes.\n\n        This command is designed and works best in interactive (GUI) mode,\n        using the menu paths listed below. For command line operations, issue\n        PPATH,P to define your path by picking nodes.\n\n        For information on displaying paths you have defined, see Defining Data\n        to be Retrieved in the Basic Analysis Guide.\n\n        \"\"\"\n        command = \"PPATH,%s,%s,%s,%s,%s,%s\" % (str(point), str(node), str(x), str(y), str(z), str(cs))\n        return self.Run(command, **kwargs)\n\n# --- Snippet Separator ---\n\ndef barnes_point(sq_dist, values, kappa, gamma=None):\n    r\"\"\"Generate a single pass Barnes interpolation value for a point.\n\n    The calculated value is based on the given distances, kappa and gamma values. This\n    is calculated as an inverse distance-weighted average of the points in the neighborhood,\n    with weights given as:\n\n    .. math:: w = e ^ \\frac{-r^2}{\\kappa}\n\n    * :math:`\\kappa` is a scaling parameter\n    * :math:`r` is the distance to a point.\n\n    For more information see [Barnes1964]_ or [Koch1983]_.\n\n    Parameters\n    ----------\n    sq_dist: (N, ) numpy.ndarray\n        Squared distance between observations and grid point\n    values: (N, ) numpy.ndarray\n        Observation values in same order as sq_dist\n    kappa: float\n        Response parameter for barnes interpolation.\n    gamma: float\n        Adjustable smoothing parameter for the barnes interpolation. Default 1.\n\n    Returns\n    -------\n    value: float\n        Interpolation value for grid point.\n\n    \"\"\"\n    if gamma is None:\n        gamma = 1\n    weights = tools.barnes_weights(sq_dist, kappa, gamma)\n    total_weights = np.sum(weights)\n\n    return sum(v * (w / total_weights) for (w, v) in zip(weights, values))\n\n# --- Snippet Separator ---\n\nclass CodePointMapping:\n    \"\"\"Map integer values to font code points.\"\"\"\n\n    def __init__(self, num, font_start, font_jumps=None, char_jumps=None):\n        \"\"\"Initialize the instance.\n\n        Parameters\n        ----------\n        num : int\n            The number of values that will be mapped\n        font_start : int\n            The first code point in the font to use in the mapping\n        font_jumps : list[int, int], optional\n            Sequence of code point jumps in the font. These are places where the next\n            font code point does not correspond to a new input code. This is usually caused\n            by there being multiple symbols for a single code. Defaults to :data:`None`, which\n            indicates no jumps.\n        char_jumps : list[int, int], optional\n            Sequence of code jumps. These are places where the next code value does not\n            have a valid code point in the font. This usually comes from place in the WMO\n            table where codes have no symbol. Defaults to :data:`None`, which indicates no\n            jumps.\n\n        \"\"\"\n        next_font_jump = self._safe_pop(font_jumps)\n        next_char_jump = self._safe_pop(char_jumps)\n        font_point = font_start\n        self.chrs = []\n        code = 0\n        while code < num:\n            if next_char_jump and code >= next_char_jump[0]:\n                jump_len = next_char_jump[1]\n                code += jump_len\n                self.chrs.extend([''] * jump_len)\n                next_char_jump = self._safe_pop(char_jumps)\n            else:\n                self.chrs.append(chr(font_point))\n                if next_font_jump and code >= next_font_jump[0]:\n                    font_point += next_font_jump[1]\n                    next_font_jump = self._safe_pop(font_jumps)\n                code += 1\n                font_point += 1\n\n    @staticmethod\n    def _safe_pop(lst):\n        \"\"\"Safely pop from a list.\n\n        Returns None if list empty.\n\n        \"\"\"\n        return lst.pop(0) if lst else None\n\n    def __call__(self, code):\n        \"\"\"Return the Unicode code point corresponding to `code`.\n\n        If code >= 1000, then an alternate code point is returned, with the thousands\n        digit indicating which alternate.\n        \"\"\"\n        if code < 1000:\n            return self.chrs[code]\n        else:\n            alt = code // 1000\n            code %= 1000\n            return self.alt_char(code, alt)\n\n    def __len__(self):\n        \"\"\"Return the number of codes supported by this mapping.\"\"\"\n        return len(self.chrs)\n\n    def alt_char(self, code, alt):\n        \"\"\"Get one of the alternate code points for a given value.\n\n        In the WMO tables, some code have multiple symbols. This allows getting that\n        symbol rather than main one.\n\n        Parameters\n        ----------\n        code : int\n            The code for looking up the font code point\n        alt : int\n            The number of the alternate symbol\n\n        Returns\n        -------\n        int\n            The appropriate code point in the font\n\n        \"\"\"\n        return chr(ord(self(code)) + alt)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that demonstrates the use of the parallelized CCSD with K-point sampling in the pyscf library. The code should create a supercell composed of replicated units and run a molecular Hartree-Fock program using integrals between periodic gaussians. It should then call a molecular CC method for gamma point calculation and perform k-point calculations for the same system. The code should also calculate the differences between gamma/k-point mean-field, ccsd, ip-eomccsd, and ea-eomccsd calculations and print these differences.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 110, "repo_full_name": "synerbi__sirf", "instruction": "Generate code that performs a few steps of steepest ascent for the maximization of Poisson log-likelihood objective function using subset gradients. The code should allow the user to specify the reconstruction engine, raw data file, path to data files, number of steepest descent steps, whether to use locally optimal steepest ascent, verbosity, and whether to show plots or not. The code should import the specified engine module from the sirf library, process the command-line options, and define a function to truncate the image. The main function should create an acquisition model, read PET acquisition data from the specified file, create a filter that zeroes the image outside a cylinder of the same diameter as the image xy-section size, create an initial image estimate, create an objective function of Poisson logarithmic likelihood type compatible with the acquisition data type, and perform the steepest descent steps. If anything goes wrong, the code should catch and display the error information.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def make_Poisson_loglikelihood(acq_data, likelihood_type='LinearModelForMean',\n                               acq_model=None):\n    \"\"\"Makes Poisson loglikelihood.\n\n    Selects the objective function based on the acquisition data and likelihood\n    model types.\n    \"\"\"\n    # only this objective function is implemented for now\n    if likelihood_type == 'LinearModelForMean':\n        obj_fun = PoissonLogLikelihoodWithLinearModelForMeanAndProjData()\n        obj_fun.set_acquisition_data(acq_data)\n    else:\n        raise error(\n            'only PoissonLogLikelihoodWithLinearModelForMeanAndProjData ' +\n            'is currently implemented in SIRF')\n    if acq_model is not None:\n        obj_fun.set_acquisition_model(acq_model)\n    return obj_fun\n\n# --- Snippet Separator ---\n\nclass AcquisitionModelUsingNiftyPET(AcquisitionModel):\n        \"\"\"PET acquisition model that uses NiftyPET projector.\n\n        Class for a PET acquisition model that uses the NiftyPET projector\n        for G in AcquisitionModel (F).\n        \"\"\"\n\n        def __init__(self):\n            \"\"\"Create an AcquisitionModelUsingNiftyPET object.\"\"\"\n            super(AcquisitionModelUsingNiftyPET, self).__init__()\n            self.name = 'AcqModUsingNiftyPET'\n            self.handle = pystir.cSTIR_newObject(self.name)\n            check_status(self.handle)\n\n        def __del__(self):\n            \"\"\"del.\"\"\"\n            if self.handle is not None:\n                pyiutil.deleteDataHandle(self.handle)\n\n        def set_cuda_verbosity(self, verbosity):\n            \"\"\"Set the verbosity of the CUDA code.\"\"\"\n            if verbosity:\n                v = 1\n            else:\n                v = 0\n            parms.set_int_par(self.handle, self.name, 'cuda_verbosity', v)\n\n        def set_use_truncation(self, use_truncation):\n            \"\"\"Set use truncation.\n\n            Whether or not to truncate FOV before forward- and\n            after back-projection.\n            \"\"\"\n            if use_truncation:\n                v = 1\n            else:\n                v = 0\n            parms.set_int_par(self.handle, self.name, 'use_truncation', v)\n\n# --- Snippet Separator ---\n\ndef write_driver(self, nodes, read_write_info, prefix, postfix,\n                     region_name, writer=FortranWriter()):\n        # pylint: disable=too-many-arguments\n        '''This function uses the ``get_driver_as_string()`` function to get a\n        a stand-alone driver, and then writes this source code to a file. The\n        file name is derived from the region name:\n        \"driver-\"+module_name+\"_\"+region_name+\".F90\"\n\n        :param nodes: a list of nodes containing the body of the driver\n            routine.\n        :type nodes: List[:py:class:`psyclone.psyir.nodes.Node`]\n        :param read_write_info: information about all input and output \\\n            parameters.\n        :type read_write_info: :py:class:`psyclone.psyir.tools.ReadWriteInfo`\n        :param str prefix: the prefix to use for each PSyData symbol, \\\n            e.g. 'extract' as prefix will create symbols `extract_psydata`.\n        :param str postfix: a postfix that is appended to an output variable \\\n            to create the corresponding variable that stores the output \\\n            value from the kernel data file. The caller must guarantee that \\\n            no name clashes are created when adding the postfix to a variable \\\n            and that the postfix is consistent between extract code and \\\n            driver code (see 'ExtractTrans.determine_postfix()').\n        :param Tuple[str,str] region_name: an optional name to \\\n            use for this PSyData area, provided as a 2-tuple containing a \\\n            location name followed by a local name. The pair of strings \\\n            should uniquely identify a region.\n        :param writer: a backend visitor to convert PSyIR \\\n            representation to the selected language. It defaults to \\\n            the FortranWriter.\n        :type writer: \\\n            :py:class:`psyclone.psyir.backend.language_writer.LanguageWriter`\n\n        '''\n        code = self.get_driver_as_string(nodes, read_write_info, prefix,\n                                         postfix, region_name, writer=writer)\n        fll = FortLineLength()\n        code = fll.process(code)\n        if not code:\n            # This indicates an error that was already printed,\n            # so ignore it here.\n            return\n        module_name, local_name = region_name\n        with open(f\"driver-{module_name}-{local_name}.F90\", \"w\",\n                  encoding='utf-8') as out:\n            out.write(code)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a few steps of steepest ascent for the maximization of Poisson log-likelihood objective function using subset gradients. The code should allow the user to specify the reconstruction engine, raw data file, path to data files, number of steepest descent steps, whether to use locally optimal steepest ascent, verbosity, and whether to show plots or not. The code should import the specified engine module from the sirf library, process the command-line options, and define a function to truncate the image. The main function should create an acquisition model, read PET acquisition data from the specified file, create a filter that zeroes the image outside a cylinder of the same diameter as the image xy-section size, create an initial image estimate, create an objective function of Poisson logarithmic likelihood type compatible with the acquisition data type, and perform the steepest descent steps. If anything goes wrong, the code should catch and display the error information.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 111, "repo_full_name": "hiddensymmetries__simsopt", "instruction": "Generate code that solves a FOCUS-like Stage II coil optimization problem for finite build coils using the simsopt library. The code should approximate each finite build coil using a multifilament approach and model the multifilament pack. The objective function should be defined as a combination of the squared flux, curve length penalty, and coil-to-coil distance penalty. The code should also include the initialization of the boundary magnetic surface, creation of equally spaced curves and multifilament grid, application of stellarator and rotation symmetries, and definition of the Biot-Savart law. The code should perform a Taylor test and run the optimization using the L-BFGS-B method from scipy.optimize. The output should be saved in VTK format.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def coil_optimization(s, bs, base_curves, curves, out_dir=''):\n    \"\"\"\n    Optimize the coils for the QA, QH, or other configurations.\n\n    Args:\n        s: plasma boundary.\n        bs: Biot Savart class object, presumably representing the\n          magnetic fields generated by the coils.\n        base_curves: List of CurveXYZFourier class objects.\n        curves: List of Curve class objects.\n        out_dir: Path or string for the output directory for saved files.\n\n    Returns:\n        bs: Biot Savart class object, presumably representing the\n          OPTIMIZED magnetic fields generated by the coils.\n    \"\"\"\n\n    from simsopt.geo import CurveLength, CurveCurveDistance, \\\n        MeanSquaredCurvature, LpCurveCurvature, CurveSurfaceDistance\n    from simsopt.objectives import QuadraticPenalty\n    from simsopt.geo import curves_to_vtk\n    from simsopt.objectives import SquaredFlux\n\n    out_dir = Path(out_dir)\n    nphi = len(s.quadpoints_phi)\n    ntheta = len(s.quadpoints_theta)\n    ncoils = len(base_curves)\n\n    # Weight on the curve lengths in the objective function:\n    LENGTH_WEIGHT = 1e-4\n\n    # Threshold and weight for the coil-to-coil distance penalty in the objective function:\n    CC_THRESHOLD = 0.1\n    CC_WEIGHT = 1e-1\n\n    # Threshold and weight for the coil-to-surface distance penalty in the objective function:\n    CS_THRESHOLD = 0.1\n    CS_WEIGHT = 1e-2\n\n    # Threshold and weight for the curvature penalty in the objective function:\n    CURVATURE_THRESHOLD = 0.1\n    CURVATURE_WEIGHT = 1e-9\n\n    # Threshold and weight for the mean squared curvature penalty in the objective function:\n    MSC_THRESHOLD = 0.1\n    MSC_WEIGHT = 1e-9\n\n    MAXITER = 500  # number of iterations for minimize\n\n    # Define the objective function:\n    Jf = SquaredFlux(s, bs)\n    Jls = [CurveLength(c) for c in base_curves]\n    Jccdist = CurveCurveDistance(curves, CC_THRESHOLD, num_basecurves=ncoils)\n    Jcsdist = CurveSurfaceDistance(curves, s, CS_THRESHOLD)\n    Jcs = [LpCurveCurvature(c, 2, CURVATURE_THRESHOLD) for c in base_curves]\n    Jmscs = [MeanSquaredCurvature(c) for c in base_curves]\n\n    # Form the total objective function.\n    JF = Jf \\\n        + LENGTH_WEIGHT * sum(Jls) \\\n        + CC_WEIGHT * Jccdist \\\n        + CS_WEIGHT * Jcsdist \\\n        + CURVATURE_WEIGHT * sum(Jcs) \\\n        + MSC_WEIGHT * sum(QuadraticPenalty(J, MSC_THRESHOLD) for J in Jmscs)\n\n    def fun(dofs):\n        \"\"\" Function for coil optimization grabbed from stage_two_optimization.py \"\"\"\n        JF.x = dofs\n        J = JF.J()\n        grad = JF.dJ()\n        jf = Jf.J()\n        BdotN = np.mean(np.abs(np.sum(bs.B().reshape((nphi, ntheta, 3)) * s.unitnormal(), axis=2)))\n        outstr = f\"J={J:.1e}, Jf={jf:.1e}, ⟨B·n⟩={BdotN:.1e}\"\n        cl_string = \", \".join([f\"{J.J():.1f}\" for J in Jls])\n        kap_string = \", \".join(f\"{np.max(c.kappa()):.1f}\" for c in base_curves)\n        msc_string = \", \".join(f\"{J.J():.1f}\" for J in Jmscs)\n        outstr += f\", Len=sum([{cl_string}])={sum(J.J() for J in Jls):.1f}, ϰ=[{kap_string}], ∫ϰ²/L=[{msc_string}]\"\n        outstr += f\", C-C-Sep={Jccdist.shortest_distance():.2f}, C-S-Sep={Jcsdist.shortest_distance():.2f}\"\n        outstr += f\", ║∇J║={np.linalg.norm(grad):.1e}\"\n        print(outstr)\n        return J, grad\n\n    print(\"\"\"\n    ################################################################################\n    ### Perform a Taylor test ######################################################\n    ################################################################################\n    \"\"\")\n    f = fun\n    dofs = JF.x\n    np.random.seed(1)\n    h = np.random.uniform(size=dofs.shape)\n\n    J0, dJ0 = f(dofs)\n    dJh = sum(dJ0 * h)\n    for eps in [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]:\n        J1, _ = f(dofs + eps*h)\n        J2, _ = f(dofs - eps*h)\n        print(\"err\", (J1-J2)/(2*eps) - dJh)\n\n    print(\"\"\"\n    ################################################################################\n    ### Run the optimisation #######################################################\n    ################################################################################\n    \"\"\")\n    res = minimize(fun, dofs, jac=True, method='L-BFGS-B', options={'maxiter': MAXITER, 'maxcor': 300}, tol=1e-15)\n    curves_to_vtk(curves, out_dir / \"curves_opt\")\n    bs.set_points(s.gamma().reshape((-1, 3)))\n    return bs\n\n# --- Snippet Separator ---\n\ndef create_equally_spaced_curves(ncurves, nfp, stellsym, R0=1.0, R1=0.5, order=6, numquadpoints=None):\n    \"\"\"\n    Create ``ncurves`` curves of type\n    :obj:`~simsopt.geo.curvexyzfourier.CurveXYZFourier` of order\n    ``order`` that will result in circular equally spaced coils (major\n    radius ``R0`` and minor radius ``R1``) after applying\n    :obj:`~simsopt.field.coil.coils_via_symmetries`.\n\n    Usage example: create 4 base curves, which are then rotated 3 times and\n    flipped for stellarator symmetry:\n\n    .. code-block::\n\n        base_curves = create_equally_spaced_curves(4, 3, stellsym=True)\n        base_currents = [Current(1e5) for c in base_curves]\n        coils = coils_via_symmetries(base_curves, base_currents, 3, stellsym=True)\n    \"\"\"\n    if numquadpoints is None:\n        numquadpoints = 15 * order\n    curves = []\n    from simsopt.geo.curvexyzfourier import CurveXYZFourier\n    for i in range(ncurves):\n        curve = CurveXYZFourier(numquadpoints, order)\n        angle = (i+0.5)*(2*np.pi)/((1+int(stellsym))*nfp*ncurves)\n        curve.set(\"xc(0)\", cos(angle)*R0)\n        curve.set(\"xc(1)\", cos(angle)*R1)\n        curve.set(\"yc(0)\", sin(angle)*R0)\n        curve.set(\"yc(1)\", sin(angle)*R1)\n        # The the next line, the minus sign is for consistency with\n        # Vmec.external_current(), so the coils create a toroidal field of the\n        # proper sign and free-boundary equilibrium works following stage-2 optimization.\n        curve.set(\"zs(1)\", -R1)\n        curve.x = curve.x  # need to do this to transfer data to C++\n        curves.append(curve)\n    return curves\n\n# --- Snippet Separator ---\n\ndef initialize_coils(config_flag, TEST_DIR, s, out_dir=''):\n    \"\"\"\n    Initializes coils for each of the target configurations that are\n    used for permanent magnet optimization.\n\n    Args:\n        config_flag: String denoting the stellarator configuration \n          being initialized.\n        TEST_DIR: String denoting where to find the input files.\n        out_dir: Path or string for the output directory for saved files.\n        s: plasma boundary surface.\n    Returns:\n        base_curves: List of CurveXYZ class objects.\n        curves: List of Curve class objects.\n        coils: List of Coil class objects.\n    \"\"\"\n    from simsopt.geo import create_equally_spaced_curves\n    from simsopt.field import Current, Coil, coils_via_symmetries\n    from simsopt.geo import curves_to_vtk\n\n    out_dir = Path(out_dir)\n    if 'muse' in config_flag:\n        # Load in pre-optimized coils\n        coils_filename = TEST_DIR / 'muse_tf_coils.focus'\n        base_curves, base_currents, ncoils = read_focus_coils(coils_filename)\n        coils = []\n        for i in range(ncoils):\n            coils.append(Coil(base_curves[i], base_currents[i]))\n        base_currents[0].fix_all()\n\n        # fix all the coil shapes\n        for i in range(ncoils):\n            base_curves[i].fix_all()\n    elif config_flag == 'qh':\n        # generate planar TF coils\n        ncoils = 4\n        R0 = s.get_rc(0, 0)\n        R1 = s.get_rc(1, 0) * 2\n        order = 5\n\n        # qh needs to be scaled to 0.1 T on-axis magnetic field strength\n        from simsopt.mhd.vmec import Vmec\n        vmec_file = 'wout_LandremanPaul2021_QH_reactorScale_lowres_reference.nc'\n        total_current = Vmec(TEST_DIR / vmec_file).external_current() / (2 * s.nfp) / 8.75 / 5.69674966667\n        base_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order, numquadpoints=128)\n        base_currents = [(Current(total_current / ncoils * 1e-5) * 1e5) for _ in range(ncoils-1)]\n        total_current = Current(total_current)\n        total_current.fix_all()\n        base_currents += [total_current - sum(base_currents)]\n        coils = coils_via_symmetries(base_curves, base_currents, s.nfp, True)\n\n        # fix all the coil shapes so only the currents are optimized\n        for i in range(ncoils):\n            base_curves[i].fix_all()\n    elif config_flag == 'qa':\n        # generate planar TF coils\n        ncoils = 8\n        R0 = 1.0\n        R1 = 0.65\n        order = 5\n\n        # qa needs to be scaled to 0.1 T on-axis magnetic field strength\n        from simsopt.mhd.vmec import Vmec\n        vmec_file = 'wout_LandremanPaul2021_QA_lowres.nc'\n        total_current = Vmec(TEST_DIR / vmec_file).external_current() / (2 * s.nfp) / 7.131\n        base_curves = create_equally_spaced_curves(ncoils, s.nfp, stellsym=True, R0=R0, R1=R1, order=order, numquadpoints=128)\n        base_currents = [(Current(total_current / ncoils * 1e-5) * 1e5) for _ in range(ncoils-1)]\n        total_current = Current(total_current)\n        total_current.fix_all()\n        base_currents += [total_current - sum(base_currents)]\n        coils = coils_via_symmetries(base_curves, base_currents, s.nfp, True)\n        # fix all the coil shapes so only the currents are optimized\n        for i in range(ncoils):\n            base_curves[i].fix_all()\n\n    # Initialize the coil curves and save the data to vtk\n    curves = [c.curve for c in coils]\n    curves_to_vtk(curves, out_dir / \"curves_init\")\n    return base_curves, curves, coils\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that solves a FOCUS-like Stage II coil optimization problem for finite build coils using the simsopt library. The code should approximate each finite build coil using a multifilament approach and model the multifilament pack. The objective function should be defined as a combination of the squared flux, curve length penalty, and coil-to-coil distance penalty. The code should also include the initialization of the boundary magnetic surface, creation of equally spaced curves and multifilament grid, application of stellarator and rotation symmetries, and definition of the Biot-Savart law. The code should perform a Taylor test and run the optimization using the L-BFGS-B method from scipy.optimize. The output should be saved in VTK format.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 112, "repo_full_name": "aidasoft__dd4hep", "instruction": "Generate code that sets up a dd4hep simulation using Python configuration. The code should import necessary modules and set up logging. It should define a function that runs the simulation. In this function, it should import additional modules, set up the kernel, load the geometry from a file, import constants, and configure the Geant4 interface. It should also set up the tracking field, event actions, and the particle gun. The code should handle simulation particles, build the physics list, and start the engine. If the script is run as the main program, it should call the function to run the simulation.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def setConfigureFunction(self, newModule):\n        \"\"\"\n        Set an execution function that executes a simulation instance.\n\n        Args:\n            executionFunction: (sim: SimulationBaseClass) => None\n                A function with one parameter, a simulation instance.\n                The function will be called after the creationFunction and configurationFunction in each simulation run.\n                It must execute the simulation.\n                Its return value is not used.\n        \"\"\"\n        self.simParams.configureFunction = newModule\n\n# --- Snippet Separator ---\n\ndef setExecutionFunction(self, newModule):\n        \"\"\"\n        Set an execution function that executes a simulation instance.\n\n        Args:\n            executionFunction: (sim: SimulationBaseClass) => None\n                A function with one parameter, a simulation instance.\n                The function will be called after the creationFunction and configurationFunction in each simulation run.\n                It must execute the simulation.\n                Its return value is not used.\n        \"\"\"\n        self.simParams.executionFunction = newModule\n\n# --- Snippet Separator ---\n\ndef run(self):\n    \"\"\"setup the geometry and dd4hep and geant4 and do what was asked to be done\"\"\"\n    import ROOT\n    ROOT.PyConfig.IgnoreCommandLineOptions = True\n\n    import DDG4\n    import dd4hep\n\n    self.printLevel = getOutputLevel(self.printLevel)\n\n    kernel = DDG4.Kernel()\n    dd4hep.setPrintLevel(self.printLevel)\n\n    for compactFile in self.compactFile:\n      kernel.loadGeometry(str(\"file:\" + os.path.abspath(compactFile)))\n    detectorDescription = kernel.detectorDescription()\n\n    DDG4.importConstants(detectorDescription)\n\n  # ----------------------------------------------------------------------------------\n\n    # simple = DDG4.Geant4( kernel, tracker='Geant4TrackerAction',calo='Geant4CalorimeterAction')\n    # geant4 = DDG4.Geant4( kernel, tracker='Geant4TrackerCombineAction',calo='Geant4ScintillatorCalorimeterAction')\n    geant4 = DDG4.Geant4(kernel, tracker=self.action.tracker, calo=self.action.calo)\n\n    geant4.printDetectors()\n\n    if self.runType == \"vis\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=True, macro=self.macroFile)\n    elif self.runType == \"qt\":\n      uiaction = geant4.setupUI(typ=\"qt\", vis=True, macro=self.macroFile)\n    elif self.runType == \"run\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=self.macroFile, ui=False)\n    elif self.runType == \"shell\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=True)\n    elif self.runType == \"batch\":\n      uiaction = geant4.setupUI(typ=\"tcsh\", vis=False, macro=None, ui=False)\n    else:\n      logger.error(\"unknown runType\")\n      exit(1)\n\n    # User Configuration for the Geant4Phases\n    uiaction.ConfigureCommands = self.ui._commandsConfigure\n    uiaction.InitializeCommands = self.ui._commandsInitialize\n    uiaction.PostRunCommands = self.ui._commandsPostRun\n    uiaction.PreRunCommands = self.ui._commandsPreRun\n    uiaction.TerminateCommands = self.ui._commandsTerminate\n\n    kernel.NumEvents = self.numberOfEvents\n\n    # -----------------------------------------------------------------------------------\n    # setup the magnetic field:\n    self.__setMagneticFieldOptions(geant4)\n\n    # configure geometry creation\n    self.geometry.constructGeometry(kernel, geant4, self.output.geometry)\n\n    # ----------------------------------------------------------------------------------\n    # Configure Run actions\n    run1 = DDG4.RunAction(kernel, 'Geant4TestRunAction/RunInit')\n    kernel.registerGlobalAction(run1)\n    kernel.runAction().add(run1)\n\n    # Configure the random seed, do it before the I/O because we might change the seed!\n    self.random.initialize(DDG4, kernel, self.output.random)\n\n    # Configure the output file format and plugin\n    self.outputConfig.initialize(dd4hepsimulation=self, geant4=geant4)\n\n    actionList = []\n\n    if self.enableGun:\n      gun = DDG4.GeneratorAction(kernel, \"Geant4ParticleGun/\" + \"Gun\")\n      self.gun.setOptions(gun)\n      gun.Standalone = False\n      gun.Mask = 1\n      actionList.append(gun)\n      self.__applyBoostOrSmear(kernel, actionList, 1)\n      logger.info(\"++++ Adding DD4hep Particle Gun ++++\")\n\n    if self.enableG4Gun:\n      # GPS Create something\n      self._g4gun = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/Gun\")\n      self._g4gun.Uses = 'G4ParticleGun'\n      self._g4gun.Mask = 2\n      logger.info(\"++++ Adding Geant4 Particle Gun ++++\")\n      actionList.append(self._g4gun)\n\n    if self.enableG4GPS:\n      # GPS Create something\n      self._g4gps = DDG4.GeneratorAction(kernel, \"Geant4GeneratorWrapper/GPS\")\n      self._g4gps.Uses = 'G4GeneralParticleSource'\n      self._g4gps.Mask = 3\n      logger.info(\"++++ Adding Geant4 General Particle Source ++++\")\n      actionList.append(self._g4gps)\n\n    start = 4\n    for index, plugin in enumerate(self.inputConfig.userInputPlugin, start=start):\n      gen = plugin(self)\n      gen.Mask = index\n      start = index + 1\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n      logger.info(\"++++ Adding User Plugin %s ++++\", gen.Name)\n\n    for index, inputFile in enumerate(self.inputFiles, start=start):\n      if inputFile.endswith(\".slcio\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/LCIO%d\" % index)\n        gen.Parameters = self.lcio.getParameters()\n        gen.Input = \"LCIOFileReader|\" + inputFile\n      elif inputFile.endswith(\".stdhep\"):\n        gen = DDG4.GeneratorAction(kernel, \"LCIOInputAction/STDHEP%d\" % index)\n        gen.Input = \"LCIOStdHepReader|\" + inputFile\n      elif inputFile.endswith(\".HEPEvt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/HEPEvt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtShort|\" + inputFile\n      elif inputFile.endswith(\".hepevt\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepevt%d\" % index)\n        gen.Input = \"Geant4EventReaderHepEvtLong|\" + inputFile\n      elif inputFile.endswith(tuple([\".hepmc\"] + HEPMC3_SUPPORTED_EXTENSIONS)):\n        if self.hepmc3.useHepMC3:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Parameters = self.hepmc3.getParameters()\n          gen.Input = \"HEPMC3FileReader|\" + inputFile\n        else:\n          gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/hepmc%d\" % index)\n          gen.Input = \"Geant4EventReaderHepMC|\" + inputFile\n      elif inputFile.endswith(\".pairs\"):\n        gen = DDG4.GeneratorAction(kernel, \"Geant4InputAction/GuineaPig%d\" % index)\n        gen.Input = \"Geant4EventReaderGuineaPig|\" + inputFile\n        gen.Parameters = self.guineapig.getParameters()\n      else:\n        # this should never happen because we already check at the top, but in case of some LogicError...\n        raise RuntimeError(\"Unknown input file type: %s\" % inputFile)\n      gen.Sync = self.skipNEvents\n      gen.Mask = index\n      actionList.append(gen)\n      self.__applyBoostOrSmear(kernel, actionList, index)\n\n    if actionList:\n      self._buildInputStage(geant4, actionList, output_level=self.output.inputStage,\n                            have_mctruth=self._enablePrimaryHandler())\n\n    # ================================================================================================\n\n    # And handle the simulation particles.\n    part = DDG4.GeneratorAction(kernel, \"Geant4ParticleHandler/ParticleHandler\")\n    kernel.generatorAction().adopt(part)\n    # part.SaveProcesses = ['conv','Decay']\n    part.SaveProcesses = self.part.saveProcesses\n    part.MinimalKineticEnergy = self.part.minimalKineticEnergy\n    part.KeepAllParticles = self.part.keepAllParticles\n    part.PrintEndTracking = self.part.printEndTracking\n    part.PrintStartTracking = self.part.printStartTracking\n    part.MinDistToParentVertex = self.part.minDistToParentVertex\n    part.OutputLevel = self.output.part\n    part.enableUI()\n\n    if self.part.enableDetailedHitsAndParticleInfo:\n      self.part.setDumpDetailedParticleInfo(kernel, DDG4)\n\n    self.part.setupUserParticleHandler(part, kernel, DDG4)\n\n    # =================================================================================\n\n    # Setup global filters for use in sensitive detectors\n    try:\n      self.filter.setupFilters(kernel)\n    except RuntimeError as e:\n      logger.error(\"%s\", e)\n      exit(1)\n\n    # =================================================================================\n    # get lists of trackers and calorimeters in detectorDescription\n\n    trk, cal, unk = self.getDetectorLists(detectorDescription)\n\n    for detectors, function, defFilter, abort in [(trk, geant4.setupTracker, self.filter.tracker, False),\n                                                  (cal, geant4.setupCalorimeter, self.filter.calo, False),\n                                                  (unk, geant4.setupDetector, None, True),\n                                                  ]:\n      try:\n        self.__setupSensitiveDetectors(detectors, function, defFilter, abort)\n      except Exception as e:\n        logger.error(\"Failed setting up sensitive detector %s\", e)\n        raise\n\n  # =================================================================================\n    # Now build the physics list:\n    _phys = self.physics.setupPhysics(kernel, name=self.physicsList)\n\n    # add the G4StepLimiterPhysics to activate the max step limits in volumes\n    ph = DDG4.PhysicsList(kernel, 'Geant4PhysicsList/Myphysics')\n    ph.addPhysicsConstructor(str('G4StepLimiterPhysics'))\n    _phys.add(ph)\n\n    dd4hep.setPrintLevel(self.printLevel)\n\n    kernel.configure()\n    kernel.initialize()\n\n    # GPS\n    if self._g4gun is not None:\n      self._g4gun.generator()\n    if self._g4gps is not None:\n      self._g4gps.generator()\n\n    startUpTime, _sysTime, _cuTime, _csTime, _elapsedTime = os.times()\n\n    kernel.run()\n    kernel.terminate()\n\n    totalTimeUser, totalTimeSys, _cuTime, _csTime, _elapsedTime = os.times()\n    if self.printLevel <= 3:\n      logger.info(\"DDSim            INFO  Total Time:   %3.2f s (User), %3.2f s (System)\" %\n                  (totalTimeUser, totalTimeSys))\n      if self.numberOfEvents != 0:\n        eventTime = totalTimeUser - startUpTime\n        perEventTime = eventTime / self.numberOfEvents\n        logger.info(\"DDSim            INFO  StartUp Time: %3.2f s, Event Processing: %3.2f s (%3.2f s/Event) \"\n                    % (startUpTime, eventTime, perEventTime))\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a dd4hep simulation using Python configuration. The code should import necessary modules and set up logging. It should define a function that runs the simulation. In this function, it should import additional modules, set up the kernel, load the geometry from a file, import constants, and configure the Geant4 interface. It should also set up the tracking field, event actions, and the particle gun. The code should handle simulation particles, build the physics list, and start the engine. If the script is run as the main program, it should call the function to run the simulation.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 113, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that initializes a parser with three arguments: the path to a blend file, the path to a haven directory, and the output directory. Then, initialize the blenderproc library and load the blend file into the scene. Set a random hdri from the haven directory as the background. Define a point light, set its location and energy level. Compute a point of interest and sample five camera poses around it. Enable normal and depth rendering, render the pipeline, and write the data to a .hdf5 container in the specified output directory.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def Mplib(self, r_w_opt=\"\", path=\"\", **kwargs):\n        \"\"\"\n        APDL Command: /MPLIB\n\n        Sets the default material library read and write paths.\n\n        Parameters\n        ----------\n        r-w_opt\n            Determines what path is being set.  Possible values are:\n\n            READ - Set the read path.\n\n            WRITE - Set the write path.\n\n            STAT - Report what read and write paths are currently in use.\n\n        path\n            The directory path to be used for material library files.\n\n        Notes\n        -----\n        The /MPLIB command sets two path strings used in conjunction with the\n        material library feature and the MPREAD and MPWRITE commands.\n\n        For MPREAD, when you use the LIB option and no directory path is given\n        in the file name, the command searches for the file in these locations:\n        the current working directory, the user's home directory, the user-\n        specified material library directory (as defined by the\n        /MPLIB,READ,PATH command), and /ansys_dir/matlib.\n\n        For MPWRITE, when you use the LIB option and the directory portion of\n        the specification for the material library file is blank, the command\n        writes the material  library file to the directory specified by the\n        /MPLIB,WRITE,PATH command (if that path has been set).  If the path has\n        not been set, the default is to write the file to the current working\n        directory.\n\n        The Material Library files supplied with the distribution disks are\n        meant for demonstration purposes only.  These files are not intended\n        for use in customer applications.\n\n        \"\"\"\n        command = \"/MPLIB,%s,%s\" % (str(r_w_opt), str(path))\n        return self.Run(command, **kwargs)\n\n# --- Snippet Separator ---\n\ndef _parse_cli(arg_list, input=True):\n    parser = argparse.ArgumentParser(\n        description=\"Animation engine for explanatory math videos\",\n        epilog=\"Made with <3 by the manim community devs\",\n    )\n    if input:\n        parser.add_argument(\n            \"file\", help=\"path to file holding the python code for the scene\",\n        )\n        parser.add_argument(\n            \"scene_names\",\n            nargs=\"*\",\n            help=\"Name of the Scene class you want to see\",\n            default=[\"\"],\n        )\n        parser.add_argument(\n            \"-o\",\n            \"--output_file\",\n            help=\"Specify the name of the output file, if \"\n            \"it should be different from the scene class name\",\n            default=\"\",\n        )\n\n    # The following use (action='store_const', const=True) instead of\n    # the built-in (action='store_true').  This is because the latter\n    # will default to False if not specified, while the former sets no\n    # default value.  Since we want to set the default value in\n    # manim.cfg rather than here, we use the former.\n    parser.add_argument(\n        \"-p\",\n        \"--preview\",\n        action=\"store_const\",\n        const=True,\n        help=\"Automatically open the saved file once its done\",\n    )\n    parser.add_argument(\n        \"-f\",\n        \"--show_file_in_finder\",\n        action=\"store_const\",\n        const=True,\n        help=\"Show the output file in finder\",\n    )\n    parser.add_argument(\n        \"-q\", \"--quiet\", action=\"store_const\", const=True, help=\"Quiet mode\",\n    )\n    parser.add_argument(\n        \"--sound\",\n        action=\"store_const\",\n        const=True,\n        help=\"Play a success/failure sound\",\n    )\n    parser.add_argument(\n        \"--leave_progress_bars\",\n        action=\"store_const\",\n        const=True,\n        help=\"Leave progress bars displayed in terminal\",\n    )\n    parser.add_argument(\n        \"-a\",\n        \"--write_all\",\n        action=\"store_const\",\n        const=True,\n        help=\"Write all the scenes from a file\",\n    )\n    parser.add_argument(\n        \"-w\",\n        \"--write_to_movie\",\n        action=\"store_const\",\n        const=True,\n        help=\"Render the scene as a movie file\",\n    )\n    parser.add_argument(\n        \"-s\",\n        \"--save_last_frame\",\n        action=\"store_const\",\n        const=True,\n        help=\"Save the last frame (and do not save movie)\",\n    )\n    parser.add_argument(\n        \"-g\",\n        \"--save_pngs\",\n        action=\"store_const\",\n        const=True,\n        help=\"Save each frame as a png\",\n    )\n    parser.add_argument(\n        \"-i\",\n        \"--save_as_gif\",\n        action=\"store_const\",\n        const=True,\n        help=\"Save the video as gif\",\n    )\n\n    # The default value of the following is set in manim.cfg\n    parser.add_argument(\n        \"-c\", \"--color\", help=\"Background color\",\n    )\n    parser.add_argument(\n        \"--background_opacity\", help=\"Background opacity\",\n    )\n    parser.add_argument(\n        \"--media_dir\", help=\"directory to write media\",\n    )\n    # video_group = parser.add_mutually_exclusive_group()\n    # video_group.add_argument(\n    #     \"--video_dir\",\n    #     help=\"directory to write file tree for video\",\n    # )\n    # parser.add_argument(\n    #     \"--tex_dir\",\n    #     help=\"directory to write tex\",\n    # )\n    # parser.add_argument(\n    #     \"--text_dir\",\n    #     help=\"directory to write text\",\n    # )\n    parser.add_argument(\n        \"--tex_template\", help=\"Specify a custom TeX template file\",\n    )\n\n    # All of the following use (action=\"store_true\"). This means that\n    # they are by default False.  In contrast to the previous ones that\n    # used (action=\"store_const\", const=True), the following do not\n    # correspond to a single configuration option.  Rather, they\n    # override several options at the same time.\n\n    # The following overrides -w, -a, -g, and -i\n    parser.add_argument(\n        \"--dry_run\",\n        action=\"store_true\",\n        help=\"Do a dry run (render scenes but generate no output files)\",\n    )\n\n    # The following overrides PNG_MODE, MOVIE_FILE_EXTENSION, and\n    # BACKGROUND_OPACITY\n    parser.add_argument(\n        \"-t\",\n        \"--transparent\",\n        action=\"store_true\",\n        help=\"Render to a movie file with an alpha channel\",\n    )\n\n    # The following are mutually exclusive and each overrides\n    # FRAME_RATE, PIXEL_HEIGHT, and PIXEL_WIDTH,\n    parser.add_argument(\n        \"-l\",\n        \"--low_quality\",\n        action=\"store_true\",\n        help=\"Render at low quality (for fastest rendering)\",\n    )\n    parser.add_argument(\n        \"-m\",\n        \"--medium_quality\",\n        action=\"store_true\",\n        help=\"Render at medium quality (for much faster rendering)\",\n    )\n    parser.add_argument(\n        \"-e\",\n        \"--high_quality\",\n        action=\"store_true\",\n        help=\"Render at high quality (for slightly faster rendering)\",\n    )\n    parser.add_argument(\n        \"-k\",\n        \"--fourk_quality\",\n        action=\"store_true\",\n        help=\"Render at 4K quality (slower rendering)\",\n    )\n\n    # This overrides any of the above\n    parser.add_argument(\n        \"-r\", \"--resolution\", help='Resolution, passed as \"height,width\"',\n    )\n\n    # This sets FROM_ANIMATION_NUMBER and UPTO_ANIMATION_NUMBER\n    parser.add_argument(\n        \"-n\",\n        \"--from_animation_number\",\n        help=\"Start rendering not from the first animation, but\"\n        \"from another, specified by its index.  If you pass\"\n        'in two comma separated values, e.g. \"3,6\", it will end'\n        \"the rendering at the second value\",\n    )\n\n    # Specify the manim.cfg file\n    parser.add_argument(\n        \"--config_file\", help=\"Specify the configuration file\",\n    )\n\n    return parser.parse_args(arg_list)\n\n# --- Snippet Separator ---\n\ndef main(args):\n    '''\n    Parses and checks the command line arguments, calls the generate\n    function if all is well, catches any errors and outputs the\n    results.\n\n    :param args: the list of command-line arguments that PSyclone has \\\n        been invoked with.\n    :type args: List[str]\n\n    '''\n    # pylint: disable=too-many-statements,too-many-branches\n\n    # Make sure we have the supported APIs defined in the Config singleton,\n    # but postpone loading the config file till the command line was parsed\n    # in case that the user specifies a different config file.\n    Config.get(do_not_load_file=True)\n\n    parser = argparse.ArgumentParser(\n        description='Run the PSyclone code generator on a particular file')\n    parser.add_argument('-oalg', help='filename of transformed algorithm code')\n    parser.add_argument(\n        '-opsy', help='filename of generated PSy code')\n    parser.add_argument('-okern',\n                        help='directory in which to put transformed kernels, '\n                        'default is the current working directory.')\n    parser.add_argument('-api',\n                        help=f'choose a particular api from '\n                        f'{str(Config.get().supported_apis)}, '\n                        f'default \\'{Config.get().default_api}\\'.')\n    parser.add_argument('filename', help='algorithm-layer source code')\n    parser.add_argument('-s', '--script', help='filename of a PSyclone'\n                        ' optimisation script')\n    parser.add_argument(\n        '-d', '--directory', default=[], action=\"append\", help='path to a '\n        'root directory structure containing kernel source code. Multiple '\n        'roots can be specified by using multiple -d arguments.')\n    # Make the default an empty list so that we can check whether the\n    # user has supplied a value(s) later\n    parser.add_argument(\n        '-I', '--include', default=[], action=\"append\",\n        help='path to Fortran INCLUDE or module files')\n    parser.add_argument(\n        '-l', '--limit', dest='limit', default='off',\n        choices=['off', 'all', 'output'],\n        help='limit the Fortran line length to 132 characters (default '\n        '\\'%(default)s\\'). Use \\'all\\' to apply limit to both input and '\n        'output Fortran. Use \\'output\\' to apply line-length limit to output '\n        'Fortran only.')\n    parser.add_argument(\n        '-dm', '--dist_mem', dest='dist_mem', action='store_true',\n        help='generate distributed memory code')\n    parser.add_argument(\n        '-nodm', '--no_dist_mem', dest='dist_mem', action='store_false',\n        help='do not generate distributed memory code')\n    parser.add_argument(\n        '--kernel-renaming', default=\"multiple\",\n        choices=configuration.VALID_KERNEL_NAMING_SCHEMES,\n        help=\"Naming scheme to use when re-naming transformed kernels\")\n    parser.add_argument(\n        '--profile', '-p', action=\"append\", choices=Profiler.SUPPORTED_OPTIONS,\n        help=\"Add profiling hooks for either 'kernels' or 'invokes'\")\n    parser.set_defaults(dist_mem=Config.get().distributed_memory)\n\n    parser.add_argument(\"--config\", help=\"Config file with \"\n                        \"PSyclone specific options.\")\n    parser.add_argument(\n        '--version', '-v', action='version',\n        version=f'PSyclone version: {__VERSION__}',\n        help=f'Display version information ({__VERSION__})')\n\n    args = parser.parse_args(args)\n\n    if args.profile:\n        Profiler.set_options(args.profile)\n\n    # If an output directory has been specified for transformed kernels\n    # then check that it is valid\n    if args.okern:\n        if not os.path.exists(args.okern):\n            print(f\"Specified kernel output directory ({args.okern}) does \"\n                  f\"not exist.\", file=sys.stderr)\n            sys.exit(1)\n        if not os.access(args.okern, os.W_OK):\n            print(f\"Cannot write to specified kernel output directory \"\n                  f\"({args.okern}).\", file=sys.stderr)\n            sys.exit(1)\n        kern_out_path = args.okern\n    else:\n        # We write any transformed kernels to the current working directory\n        kern_out_path = os.getcwd()\n\n    # If no config file name is specified, args.config is none\n    # and config will load the default config file.\n    Config.get().load(args.config)\n\n    # Check API, if none is specified, take the setting from the config file\n    if args.api is None:\n        # No command line option, use the one specified in Config - which\n        # is either based on a parameter in the config file, or otherwise\n        # the default:\n        api = Config.get().api\n    elif args.api not in Config.get().supported_apis:\n        print(f\"Unsupported API '{args.api}' specified. Supported APIs are \"\n              f\"{Config.get().supported_apis}.\", file=sys.stderr)\n        sys.exit(1)\n    else:\n        # There is a valid API specified on the command line. Set it\n        # as API in the config object as well.\n        api = args.api\n        Config.get().api = api\n\n    # The Configuration manager checks that the supplied path(s) is/are\n    # valid so protect with a try\n    try:\n        if args.include:\n            Config.get().include_paths = args.include\n        else:\n            # Default is to instruct fparser2 to look in the directory\n            # containing the file being parsed\n            Config.get().include_paths = [\"./\"]\n    except ConfigurationError as err:\n        print(str(err), file=sys.stderr)\n        sys.exit(1)\n\n    try:\n        alg, psy = generate(args.filename, api=api,\n                            kernel_paths=args.directory,\n                            script_name=args.script,\n                            line_length=(args.limit == 'all'),\n                            distributed_memory=args.dist_mem,\n                            kern_out_path=kern_out_path,\n                            kern_naming=args.kernel_renaming)\n    except NoInvokesError:\n        _, exc_value, _ = sys.exc_info()\n        print(f\"Warning: {exc_value}\")\n        # no invoke calls were found in the algorithm file so we do\n        # not need to process it, or generate any psy layer code, so\n        # output the original algorithm file and set the psy file to\n        # be empty\n        with open(args.filename, encoding=\"utf8\") as alg_file:\n            alg = alg_file.read()\n        psy = \"\"\n    except (OSError, IOError, ParseError, GenerationError,\n            RuntimeError):\n        _, exc_value, _ = sys.exc_info()\n        print(exc_value, file=sys.stderr)\n        sys.exit(1)\n    except Exception:  # pylint: disable=broad-except\n        print(\"Error, unexpected exception, please report to the authors:\",\n              file=sys.stderr)\n        traceback.print_exception(*sys.exc_info(), file=sys.stderr)\n        sys.exit(1)\n    if args.limit != 'off':\n        # Limit the line length of the output Fortran to ensure it conforms\n        # to the 132 characters mandated by the standard.\n        fll = FortLineLength()\n        psy_str = fll.process(str(psy))\n        alg_str = fll.process(str(alg))\n    else:\n        psy_str = str(psy)\n        alg_str = str(alg)\n    if args.oalg is not None:\n        with open(args.oalg, mode='w', encoding=\"utf8\") as alg_file:\n            alg_file.write(alg_str)\n    else:\n        print(f\"Transformed algorithm code:\\n{alg_str}\")\n\n    if not psy_str:\n        # empty file so do not output anything\n        pass\n    elif args.opsy is not None:\n        with open(args.opsy, mode='w', encoding=\"utf8\") as psy_file:\n            psy_file.write(psy_str)\n    else:\n        print(f\"Generated psy layer code:\\n{psy_str}\")\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that initializes a parser with three arguments: the path to a blend file, the path to a haven directory, and the output directory. Then, initialize the blenderproc library and load the blend file into the scene. Set a random hdri from the haven directory as the background. Define a point light, set its location and energy level. Compute a point of interest and sample five camera poses around it. Enable normal and depth rendering, render the pipeline, and write the data to a .hdf5 container in the specified output directory.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 114, "repo_full_name": "nucypher__nucypher", "instruction": "Generate code that sets up a secure data sharing policy using the nucypher library. The code should perform the following tasks:\n\n1. Set up logging and environment variables for the Ethereum RPC endpoint, wallet filepath, and Alice's Ethereum address.\n2. Connect to the Ethereum provider and layer 2 provider.\n3. Unlock Alice's Ethereum wallet using a password.\n4. Set up Alice's payment method using the SubscriptionManagerPayment class.\n5. Create an instance of Alice with her Ethereum address, signer, domain, Ethereum provider URI, and payment method.\n6. Start Alice's learning loop.\n7. Create a policy label and get the policy public key associated with the label.\n8. Generate heart rate samples using a heart monitor and save them as a file.\n9. Get the public keys of the recipient (Doctor) and create an instance of Bob with these keys.\n10. Set up policy details such as the policy expiration date and m-out-of-n shares.\n11. Grant access to Bob by creating a policy and sending it to the NuCypher network.\n12. Store additional information about the policy in a JSON file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Alice(Character, BlockchainPolicyAuthor):\n    banner = ALICE_BANNER\n    _default_crypto_powerups = [SigningPower, DecryptingPower, DelegatingPower]\n\n    def __init__(self,\n\n                 # Mode\n                 is_me: bool = True,\n                 federated_only: bool = False,\n                 eth_provider_uri: str = None,\n                 signer=None,\n\n                 # Ownership\n                 checksum_address: str = None,\n\n                 # M of N\n                 threshold: Optional[int] = None,\n                 shares: Optional[int] = None,\n\n                 # Policy Value\n                 rate: int = None,\n                 duration: int = None,\n                 payment_method: PaymentMethod = None,\n\n                 # Policy Storage\n                 store_policy_credentials: bool = None,\n\n                 # Middleware\n                 timeout: int = 10,  # seconds  # TODO: configure  NRN\n                 network_middleware: RestMiddleware = None,\n\n                 *args, **kwargs) -> None:\n\n        #\n        # Fallback Policy Values\n        #\n\n        self.timeout = timeout\n\n        if is_me:\n            self.threshold = threshold\n            self.shares = shares\n\n            self._policy_queue = Queue()\n            self._policy_queue.put(READY)\n        else:\n            self.threshold = STRANGER_ALICE\n            self.shares = STRANGER_ALICE\n\n        Character.__init__(self,\n                           known_node_class=Ursula,\n                           is_me=is_me,\n                           federated_only=federated_only,\n                           eth_provider_uri=eth_provider_uri,\n                           checksum_address=checksum_address,\n                           network_middleware=network_middleware,\n                           *args, **kwargs)\n\n        if is_me and not federated_only:  # TODO: #289\n            if not eth_provider_uri:\n                raise ValueError('ETH Provider URI is required to init a decentralized character.')\n\n            blockchain = BlockchainInterfaceFactory.get_interface(eth_provider_uri=self.eth_provider_uri)\n            signer = signer or Web3Signer(blockchain.client)  # fallback to web3 provider by default for Alice.\n            self.transacting_power = TransactingPower(account=self.checksum_address, signer=signer)\n            self._crypto_power.consume_power_up(self.transacting_power)\n            BlockchainPolicyAuthor.__init__(self,\n                                            domain=self.domain,\n                                            transacting_power=self.transacting_power,\n                                            registry=self.registry,\n                                            eth_provider_uri=eth_provider_uri)\n\n        self.log = Logger(self.__class__.__name__)\n        if is_me:\n\n            # Policy Payment\n            if federated_only and not payment_method:\n                # Federated payments are free by default.\n                payment_method = FreeReencryptions()\n            if not payment_method:\n                raise ValueError('payment_method is a required argument for a local Alice.')\n            self.payment_method = payment_method\n            self.rate = rate\n            self.duration = duration\n\n            # Settings\n            self.active_policies = dict()\n            self.revocation_kits = dict()\n            self.store_policy_credentials = store_policy_credentials\n\n            self.log.info(self.banner)\n\n    def add_active_policy(self, active_policy):\n        \"\"\"\n        Adds a Policy object that is active on the NuCypher network to Alice's\n        `active_policies` dictionary by the policy ID.\n        \"\"\"\n        if active_policy.hrac in self.active_policies:\n            raise KeyError(\"Policy already exists in active_policies.\")\n        self.active_policies[active_policy.hrac] = active_policy\n\n    def generate_kfrags(self,\n                        bob: 'Bob',\n                        label: bytes,\n                        threshold: int = None,\n                        shares: int = None\n                        ) -> List:\n        \"\"\"\n        Generates re-encryption key frags (\"KFrags\") and returns them.\n\n        These KFrags can be used by Ursula to re-encrypt a Capsule for Bob so\n        that he can activate the Capsule.\n\n        :param bob: Bob instance which will be able to decrypt messages re-encrypted with these kfrags.\n        :param m: Minimum number of kfrags needed to activate a Capsule.\n        :param n: Total number of kfrags to generate\n        \"\"\"\n\n        bob_encrypting_key = bob.public_keys(DecryptingPower)\n        delegating_power = self._crypto_power.power_ups(DelegatingPower)\n        policy_key_and_kfrags = delegating_power.generate_kfrags(bob_pubkey_enc=bob_encrypting_key,\n                                                                 signer=self.stamp.as_umbral_signer(),\n                                                                 label=label,\n                                                                 threshold=threshold or self.threshold,\n                                                                 shares=shares or self.shares)\n        return policy_key_and_kfrags\n\n    def create_policy(self, bob: \"Bob\", label: bytes, **policy_params):\n        \"\"\"\n        Create a Policy so that Bob has access to all resources under label.\n        Generates KFrags and attaches them.\n        \"\"\"\n\n        policy_params = self.generate_policy_parameters(**policy_params)\n        shares = policy_params.pop('shares')\n\n        # Generate KFrags\n        public_key, kfrags = self.generate_kfrags(bob=bob,\n                                                  label=label,\n                                                  threshold=policy_params['threshold'],\n                                                  shares=shares)\n        payload = dict(label=label,\n                       bob=bob,\n                       kfrags=kfrags,\n                       public_key=public_key,\n                       **policy_params)\n\n        if self.federated_only:\n            # Use known nodes\n            policy = FederatedPolicy(publisher=self, **payload)\n        else:\n            # Sample from blockchain\n            payload.update(**policy_params)\n            policy = BlockchainPolicy(publisher=self, **payload)\n\n        return policy\n\n    def generate_policy_parameters(self,\n                                   threshold: Optional[int] = None,\n                                   shares: Optional[int] = None,\n                                   duration: Optional[int] = None,\n                                   commencement: Optional[maya.MayaDT] = None,\n                                   expiration: Optional[maya.MayaDT] = None,\n                                   value: Optional[int] = None,\n                                   rate: Optional[int] = None,\n                                   payment_method: Optional[PaymentMethod] = None\n                                   ) -> dict:\n        \"\"\"Construct policy creation from default parameters or overrides.\"\"\"\n\n        if not duration and not expiration:\n            raise ValueError(\"Policy end time must be specified as 'expiration' or 'duration', got neither.\")\n\n        # Merge injected and default params.\n        threshold = threshold or self.threshold\n        shares = shares or self.shares\n        duration = duration or self.duration\n        rate = rate if rate is not None else self.rate  # TODO conflict with CLI default value, see #1709\n        payment_method = payment_method or self.payment_method\n\n        # Calculate Policy Rate, Duration, and Value\n        quote = self.payment_method.quote(\n            shares=shares,\n            duration=duration,\n            commencement=commencement.epoch if commencement else None,\n            expiration=expiration.epoch if expiration else None,\n            rate=rate,\n            value=value\n        )\n\n        params = dict(\n            payment_method=payment_method,\n            threshold=threshold,\n            shares=shares,\n            duration=quote.duration,\n            commencement=quote.commencement,\n            expiration=quote.expiration,\n            rate=quote.rate,\n            value=quote.value\n        )\n        return params\n\n    def _check_grant_requirements(self, policy):\n        \"\"\"Called immediately before granting.\"\"\"\n        # TODO: Do not allow policies with an expiration beyond a node unbonding time.\n\n        # Policy Probationary Period\n        # TODO: Remove when the time is right.\n        # from nucypher.config.constants import END_OF_POLICIES_PROBATIONARY_PERIOD\n        # if policy.expiration > END_OF_POLICIES_PROBATIONARY_PERIOD:\n        #     raise RuntimeError(f\"The requested duration for this policy (until {policy.expiration}) exceeds the \"\n        #                        f\"probationary period ({END_OF_POLICIES_PROBATIONARY_PERIOD}).\")\n\n    def grant(self,\n              bob: \"Bob\",\n              label: bytes,\n              ursulas: set = None,\n              timeout: int = None,\n              **policy_params):\n\n        timeout = timeout or self.timeout\n\n        #\n        # Policy Creation\n        #\n\n        if ursulas:\n            # This might be the first time alice learns about the handpicked Ursulas.\n            for handpicked_ursula in ursulas:\n                self.remember_node(node=handpicked_ursula)\n\n        policy = self.create_policy(bob=bob, label=label, **policy_params)\n        self._check_grant_requirements(policy=policy)\n        self.log.debug(f\"Generated new policy proposal {policy} ... \")\n\n        #\n        # We'll find n Ursulas by default.  It's possible to \"play the field\" by trying different\n        # value and expiration combinations on a limited number of Ursulas;\n        # Users may decide to inject some market strategies here.\n        #\n\n        # If we're federated only, we need to block to make sure we have enough nodes.\n        if self.federated_only and len(self.known_nodes) < policy.shares:\n            good_to_go = self.block_until_number_of_known_nodes_is(number_of_nodes_to_know=policy.shares,\n                                                                   learn_on_this_thread=True,\n                                                                   timeout=timeout)\n            if not good_to_go:\n                raise ValueError(\n                    \"To make a Policy in federated mode, you need to know about \"\n                    \"all the Ursulas you need (in this case, {}); there's no other way to \"\n                    \"know which nodes to use.  Either pass them here or when you make the Policy, \"\n                    \"or run the learning loop on a network with enough Ursulas.\".format(policy.shares))\n\n        self.log.debug(f\"Enacting {policy} ... \")\n        enacted_policy = policy.enact(network_middleware=self.network_middleware, ursulas=ursulas)\n\n        self.add_active_policy(enacted_policy)\n        return enacted_policy\n\n    def get_policy_encrypting_key_from_label(self, label: bytes) -> PublicKey:\n        alice_delegating_power = self._crypto_power.power_ups(DelegatingPower)\n        policy_pubkey = alice_delegating_power.get_pubkey_from_label(label)\n        return policy_pubkey\n\n    def revoke(self,\n               policy: Policy,\n               onchain: bool = True,  # forced to False for federated mode\n               offchain: bool = True\n               ) -> Tuple[TxReceipt, Dict[ChecksumAddress, Tuple['Revocation', Exception]]]:\n\n        if not (offchain or onchain):\n            raise ValueError('offchain or onchain must be True to issue revocation')\n\n        receipt, failed = dict(), dict()\n\n        if onchain and (not self.federated_only):\n            pass\n            # TODO: Decouple onchain revocation from SubscriptionManager or deprecate.\n            # receipt = self.policy_agent.revoke_policy(policy_id=bytes(policy.hrac),\n            #                                           transacting_power=self._crypto_power.power_ups(TransactingPower))\n\n        if offchain:\n            \"\"\"\n            Parses the treasure map and revokes onchain arrangements in it.\n            If any nodes cannot be revoked, then the node_id is added to a\n            dict as a key, and the revocation and Ursula's response is added as\n            a value.\n            \"\"\"\n            try:\n                # Wait for a revocation threshold of nodes to be known ((n - m) + 1)\n                revocation_threshold = ((policy.shares - policy.threshold) + 1)\n                self.block_until_specific_nodes_are_known(\n                    policy.revocation_kit.revokable_addresses,\n                    allow_missing=(policy.shares - revocation_threshold))\n            except self.NotEnoughTeachers:\n                raise  # TODO  NRN\n\n            for node_id in policy.revocation_kit.revokable_addresses:\n                ursula = self.known_nodes[node_id]\n                revocation = policy.revocation_kit[node_id]\n                try:\n                    response = self.network_middleware.request_revocation(ursula, revocation)\n                except self.network_middleware.NotFound:\n                    failed[node_id] = (revocation, self.network_middleware.NotFound)\n                except self.network_middleware.UnexpectedResponse:\n                    failed[node_id] = (revocation, self.network_middleware.UnexpectedResponse)\n                else:\n                    if response.status_code != 200:\n                        message = f\"Failed to revocation for node {node_id} with status code {response.status_code}\"\n                        raise self.ActorError(message)\n\n        return receipt, failed\n\n    def decrypt_message_kit(self, label: bytes, message_kit: MessageKit) -> List[bytes]:\n        \"\"\"\n        Decrypt this Alice's own encrypted data.\n\n        I/O signatures match Bob's retrieve interface.\n        \"\"\"\n\n        delegating_power = self._crypto_power.power_ups(DelegatingPower)\n        decrypting_power = delegating_power.get_decrypting_power_from_label(label)\n        cleartext = decrypting_power.decrypt_message_kit(message_kit)\n\n        # TODO: why does it return a list of cleartexts but takes a single message kit?\n        # Shouldn't it be able to take a list of them too?\n        return [cleartext]\n\n# --- Snippet Separator ---\n\nclass Amonia(Alice):\n    \"\"\"\n    Separated at birth, Alice's sister is lighter than air and has a pungent smell.\n    \"\"\"\n\n    @classmethod\n    def from_lawful_alice(cls, alice):\n        alice_clone = copy(alice)\n        alice_clone.__class__ = cls\n        return alice_clone\n\n    def grant_without_paying(self, *args, **kwargs):\n        \"\"\"I take what I want for free.\"\"\"\n\n        def what_do_you_mean_you_dont_tip(policy, *args, **kwargs):\n            return b\"He convinced me, gimme back my $\"\n\n        with patch(\"nucypher.policy.policies.BlockchainPolicy._publish\", what_do_you_mean_you_dont_tip):\n            return super().grant(*args, **kwargs)\n\n    def circumvent_safegaurds_and_grant_without_paying(self, *args, **kwargs):\n        \"\"\"\n        I am not Alice, and I needn't abide by her sensibilities or raise her Exceptions.\n\n        Can I grant for free if I change the client code to my liking?\n        \"\"\"\n        with patch(\"nucypher.policy.policies.Policy._publish\", self.grant_without_paying):\n            return self.grant_without_paying(*args, **kwargs)\n\n# --- Snippet Separator ---\n\ndef grant(self,\n              bob: \"Bob\",\n              label: bytes,\n              ursulas: set = None,\n              timeout: int = None,\n              **policy_params):\n\n        timeout = timeout or self.timeout\n\n        #\n        # Policy Creation\n        #\n\n        if ursulas:\n            # This might be the first time alice learns about the handpicked Ursulas.\n            for handpicked_ursula in ursulas:\n                self.remember_node(node=handpicked_ursula)\n\n        policy = self.create_policy(bob=bob, label=label, **policy_params)\n        self._check_grant_requirements(policy=policy)\n        self.log.debug(f\"Generated new policy proposal {policy} ... \")\n\n        #\n        # We'll find n Ursulas by default.  It's possible to \"play the field\" by trying different\n        # value and expiration combinations on a limited number of Ursulas;\n        # Users may decide to inject some market strategies here.\n        #\n\n        # If we're federated only, we need to block to make sure we have enough nodes.\n        if self.federated_only and len(self.known_nodes) < policy.shares:\n            good_to_go = self.block_until_number_of_known_nodes_is(number_of_nodes_to_know=policy.shares,\n                                                                   learn_on_this_thread=True,\n                                                                   timeout=timeout)\n            if not good_to_go:\n                raise ValueError(\n                    \"To make a Policy in federated mode, you need to know about \"\n                    \"all the Ursulas you need (in this case, {}); there's no other way to \"\n                    \"know which nodes to use.  Either pass them here or when you make the Policy, \"\n                    \"or run the learning loop on a network with enough Ursulas.\".format(policy.shares))\n\n        self.log.debug(f\"Enacting {policy} ... \")\n        enacted_policy = policy.enact(network_middleware=self.network_middleware, ursulas=ursulas)\n\n        self.add_active_policy(enacted_policy)\n        return enacted_policy\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a secure data sharing policy using the nucypher library. The code should perform the following tasks:\n\n1. Set up logging and environment variables for the Ethereum RPC endpoint, wallet filepath, and Alice's Ethereum address.\n2. Connect to the Ethereum provider and layer 2 provider.\n3. Unlock Alice's Ethereum wallet using a password.\n4. Set up Alice's payment method using the SubscriptionManagerPayment class.\n5. Create an instance of Alice with her Ethereum address, signer, domain, Ethereum provider URI, and payment method.\n6. Start Alice's learning loop.\n7. Create a policy label and get the policy public key associated with the label.\n8. Generate heart rate samples using a heart monitor and save them as a file.\n9. Get the public keys of the recipient (Doctor) and create an instance of Bob with these keys.\n10. Set up policy details such as the policy expiration date and m-out-of-n shares.\n11. Grant access to Bob by creating a policy and sending it to the NuCypher network.\n12. Store additional information about the policy in a JSON file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 115, "repo_full_name": "1200wd__bitcoinlib", "instruction": "Generate code that imports all functions from the encoding module of the bitcoinlib library. The code should then define a list of examples for base conversion, each example being a tuple of values to be converted, the base of the original value, and the base to which it should be converted. The code should then iterate over this list, printing each example and its result after conversion using the change_base function from the imported module.\n\nNext, the code should demonstrate the conversion of Bitcoin addresses to public key hashes. It should define a list of Bitcoin addresses, iterate over this list, and for each address, print the address and its corresponding public key hash obtained using the addr_to_pubkeyhash function.\n\nThe code should then demonstrate the conversion from public key hashes to Bitcoin addresses by calling the pubkeyhash_to_addr function with specific public key hashes and printing the results.\n\nThe code should also demonstrate the creation of a public key hash from a redeem script by calling the hash160 and to_bytes functions on a given redeem script and printing the hexadecimal string representation of the result.\n\nThe code should then convert a DER encoded signature to a different format using the convert_der_sig function and print the result.\n\nNext, the code should demonstrate the conversion of an integer to a varbyte integer and back using the int_to_varbyteint and varbyteint_to_int functions, respectively, and print the results.\n\nThe code should then normalize a list of data using the normalize_string and normalize_var functions and print the results.\n\nFinally, the code should demonstrate the conversion of a Bech32 address to a public key hash using the addr_bech32_to_pubkeyhash function and the conversion of a public key hash to a Bech32 address using the pubkeyhash_to_addr_bech32 function, and print the results.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def add_value(self, location, name, fmt='.0f', units=None, **kwargs):\n        r\"\"\"Add a numeric value to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. The conversion of the data values to\n        a string is controlled by `fmt`. The units required for plotting can also\n        be passed in using `units`, which will cause the data to be converted before\n        plotting.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        fmt : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_text\n\n        \"\"\"\n        self[location] = (self.PlotTypes.value, name, (fmt, units, kwargs))\n\n# --- Snippet Separator ---\n\ndef pubkeyhash_to_addr(pubkeyhash, prefix=None, encoding='base58', witver=0):\n    \"\"\"\n    Convert public key hash to base58 encoded address\n\n    Wrapper for the :func:`pubkeyhash_to_addr_base58` and :func:`pubkeyhash_to_addr_bech32` method\n\n    :param pubkeyhash: Public key hash\n    :type pubkeyhash: bytes, str\n    :param prefix: Prefix version byte of network, default is bitcoin '\\x00'\n    :type prefix: str, bytes\n    :param encoding: Encoding of address to calculate: base58 or bech32. Default is base58\n    :type encoding: str\n    :param witver: Witness version used. Currently used for Taproot addresses with witver=1. Ignored for base58 addresses\n    :type witver: int\n\n    :return str: Base58 or bech32 encoded address\n\n    \"\"\"\n    if encoding == 'base58':\n        if prefix is None:\n            prefix = b'\\x00'\n        return pubkeyhash_to_addr_base58(pubkeyhash, prefix)\n    elif encoding == 'bech32':\n        if prefix is None:\n            prefix = 'bc'\n        return pubkeyhash_to_addr_bech32(pubkeyhash, prefix, witver)\n    else:\n        raise EncodingError(\"Encoding %s not supported\" % encoding)\n\n# --- Snippet Separator ---\n\ndef pubkeyhash_to_addr_bech32(pubkeyhash, prefix='bc', witver=0, separator='1', checksum_xor=1):\n    \"\"\"\n    Encode public key hash as bech32 encoded (segwit) address\n\n    >>> pubkeyhash_to_addr_bech32('21c1bc695a56f47991e95ff26856e50f78d3c118')\n    'bc1qy8qmc6262m68ny0ftlexs4h9paud8sgce3sf84'\n\n    Format of address is prefix/hrp + seperator + bech32 address + checksum\n\n    For more information see BIP173 proposal at https://github.com/bitcoin/bips/blob/master/bip-0173.mediawiki\n\n    :param pubkeyhash: Public key hash\n    :type pubkeyhash: str, bytes\n    :param prefix: Address prefix or Human-readable part. Default is 'bc' an abbreviation of Bitcoin. Use 'tb' for testnet.\n    :type prefix: str\n    :param witver: Witness version between 0 and 16\n    :type witver: int\n    :param separator: Separator char between hrp and data, should always be left to '1' otherwise it's not standard.\n    :type separator: str\n    :param checksum_xor: checksum 1 for bech32 v0 addresses and 0x2bc830a3 for bech32m v1+ addresses\n    :type checksum_xor: int\n\n    :return str: Bech32 encoded address\n    \"\"\"\n\n    pubkeyhash = list(to_bytes(pubkeyhash))\n\n    # To simplify and speedup: assume pubkeyhash of size 20, 32 and 40 does not contain witness version and size byte\n    if len(pubkeyhash) not in [20, 32, 40]:\n        if pubkeyhash[0] != 0:\n            witver = pubkeyhash[0] - 0x50\n        if pubkeyhash[1] != len(pubkeyhash[2:]):\n            raise EncodingError(\"Incorrect pubkeyhash length\")\n        pubkeyhash = pubkeyhash[2:]\n\n    if witver > 16:\n        raise EncodingError(\"Witness version must be between 0 and 16\")\n\n    if checksum_xor == BECH32M_CONST and not witver:\n        witver = 1\n    elif witver > 0:\n        checksum_xor = BECH32M_CONST\n\n    data = [witver] + convertbits(pubkeyhash, 8, 5)\n\n    # Expand the HRP into values for checksum computation\n    hrp_expanded = [ord(x) >> 5 for x in prefix] + [0] + [ord(x) & 31 for x in prefix]\n    polymod = _bech32_polymod(hrp_expanded + data + [0, 0, 0, 0, 0, 0]) ^ checksum_xor\n    checksum = [(polymod >> 5 * (5 - i)) & 31 for i in range(6)]\n\n    return prefix + separator + _array_to_codestring(data, 'bech32') + _array_to_codestring(checksum, 'bech32')\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports all functions from the encoding module of the bitcoinlib library. The code should then define a list of examples for base conversion, each example being a tuple of values to be converted, the base of the original value, and the base to which it should be converted. The code should then iterate over this list, printing each example and its result after conversion using the change_base function from the imported module.\n\nNext, the code should demonstrate the conversion of Bitcoin addresses to public key hashes. It should define a list of Bitcoin addresses, iterate over this list, and for each address, print the address and its corresponding public key hash obtained using the addr_to_pubkeyhash function.\n\nThe code should then demonstrate the conversion from public key hashes to Bitcoin addresses by calling the pubkeyhash_to_addr function with specific public key hashes and printing the results.\n\nThe code should also demonstrate the creation of a public key hash from a redeem script by calling the hash160 and to_bytes functions on a given redeem script and printing the hexadecimal string representation of the result.\n\nThe code should then convert a DER encoded signature to a different format using the convert_der_sig function and print the result.\n\nNext, the code should demonstrate the conversion of an integer to a varbyte integer and back using the int_to_varbyteint and varbyteint_to_int functions, respectively, and print the results.\n\nThe code should then normalize a list of data using the normalize_string and normalize_var functions and print the results.\n\nFinally, the code should demonstrate the conversion of a Bech32 address to a public key hash using the addr_bech32_to_pubkeyhash function and the conversion of a public key hash to a Bech32 address using the pubkeyhash_to_addr_bech32 function, and print the results.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 116, "repo_full_name": "continualai__avalanche", "instruction": "Generate code that trains and evaluates a model on the CLEAR benchmark using the Avalanche library. The code should define a set of hyperparameters, create a learning rate scheduler, and define a main function. In the main function, it should initialize a ResNet18 model, define normalization and transformation operations for the training and testing data, and set up logging to Tensorboard, a text file, and stdout. It should also define an evaluation plugin with various metrics. Depending on the evaluation protocol, it should set a seed value and create a CLEAR benchmark. The code should then move the model to the appropriate device, define an SGD optimizer, and create a learning rate scheduler. It should also define a continual learning strategy using the Naive method from Avalanche. The code should then run a training loop, saving the model after each experience and evaluating it on the test stream. Finally, it should generate an accuracy matrix and compute the CLEAR metrics, logging these results to a text file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class DynamicModule(Module):\n    \"\"\"\n        Dynamic Modules are Avalanche modules that can be incrementally\n        expanded to allow architectural modifications (multi-head\n        classifiers, progressive networks, ...).\n\n        Compared to pytoch Modules, they provide an additional method,\n        `model_adaptation`, which adapts the model given data from the\n        current experience.\n    \"\"\"\n\n    def adaptation(self, dataset: AvalancheDataset = None):\n        \"\"\" Adapt the module (freeze units, add units...) using the current\n        data. Optimizers must be updated after the model adaptation.\n\n        Avalanche strategies call this method to adapt the architecture\n        *before* processing each experience. Strategies also update the\n        optimizer automatically.\n\n        .. warning::\n            As a general rule, you should NOT use this method to train the\n            model. The dataset should be used only to check conditions which\n            require the model's adaptation, such as the discovery of new\n            classes or tasks.\n\n        :param dataset: data from the current experience.\n        :return:\n        \"\"\"\n        if self.training:\n            self.train_adaptation(dataset)\n        else:\n            self.eval_adaptation(dataset)\n\n    def train_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at training time.\n\n        Avalanche strategies automatically call this method *before* training\n        on each experience.\n        \"\"\"\n        pass\n\n    def eval_adaptation(self, dataset: AvalancheDataset):\n        \"\"\" Module's adaptation at evaluation time.\n\n        Avalanche strategies automatically call this method *before* evaluating\n        on each experience.\n\n        .. warning::\n            This method receives the experience's data at evaluation time\n            because some dynamic models need it for adaptation. For example,\n            an incremental classifier needs to be expanded even at evaluation\n            time if new classes are available. However, you should **never**\n            use this data to **train** the module's parameters.\n        \"\"\"\n        pass\n\n# --- Snippet Separator ---\n\ndef __init__(self, model: Module, optimizer: Optimizer,\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        BaseStrategy is the super class of all task-based continual learning\n        strategies. It implements a basic training loop and callback system\n        that allows to execute code at each experience of the training loop.\n        Plugins can be used to implement callbacks to augment the training\n        loop with additional behavior (e.g. a memory buffer for replay).\n\n        **Scenarios**\n        This strategy supports several continual learning scenarios:\n\n        * class-incremental scenarios (no task labels)\n        * multi-task scenarios, where task labels are provided)\n        * multi-incremental scenarios, where the same task may be revisited\n\n        The exact scenario depends on the data stream and whether it provides\n        the task labels.\n\n        **Training loop**\n        The training loop is organized as follows::\n            train\n                train_exp  # for each experience\n                    adapt_train_dataset\n                    train_dataset_adaptation\n                    make_train_dataloader\n                    train_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        **Evaluation loop**\n        The evaluation loop is organized as follows::\n            eval\n                eval_exp  # for each experience\n                    adapt_eval_dataset\n                    eval_dataset_adaptation\n                    make_eval_dataloader\n                    eval_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device where the model will be allocated.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience and before training on the first experience.\n                if >0: calls `eval` every `eval_every` epochs, at the end\n                    of all the epochs for a single experience and before\n                    training on the first experience.\n        \"\"\"\n        self._criterion = criterion\n\n        self.model: Module = model\n        \"\"\" PyTorch model. \"\"\"\n\n        self.optimizer = optimizer\n        \"\"\" PyTorch optimizer. \"\"\"\n\n        self.train_epochs: int = train_epochs\n        \"\"\" Number of training epochs. \"\"\"\n\n        self.train_mb_size: int = train_mb_size\n        \"\"\" Training mini-batch size. \"\"\"\n\n        self.eval_mb_size: int = train_mb_size if eval_mb_size is None \\\n            else eval_mb_size\n        \"\"\" Eval mini-batch size. \"\"\"\n\n        self.device = device\n        \"\"\" PyTorch device where the model will be allocated. \"\"\"\n\n        self.plugins = [] if plugins is None else plugins\n        \"\"\" List of `StrategyPlugin`s. \"\"\"\n\n        if evaluator is None:\n            evaluator = EvaluationPlugin()\n        self.plugins.append(evaluator)\n        self.evaluator = evaluator\n        \"\"\" EvaluationPlugin used for logging and metric computations. \"\"\"\n\n        self.eval_every = eval_every\n        \"\"\" Frequency of the evaluation during training. \"\"\"\n\n        ###################################################################\n        # State variables. These are updated during the train/eval loops. #\n        ###################################################################\n        self.training_exp_counter = 0\n        \"\"\" Counts the number of training steps. +1 at the end of each \n        experience. \"\"\"\n\n        self.epoch: Optional[int] = None\n        \"\"\" Epoch counter. \"\"\"\n\n        self.experience = None\n        \"\"\" Current experience. \"\"\"\n\n        self.adapted_dataset = None\n        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n        append data to it (e.g. for replay). \n\n        .. note:: \n            This dataset may contain samples from different experiences. If you \n            want the original data for the current experience  \n            use :attr:`.BaseStrategy.experience`.\n        \"\"\"\n\n        self.dataloader = None\n        \"\"\" Dataloader. \"\"\"\n\n        self.mb_it = None\n        \"\"\" Iteration counter. Reset at the start of a new epoch. \"\"\"\n\n        self.mbatch = None\n        \"\"\" Current mini-batch. \"\"\"\n\n        self.mb_output = None\n        \"\"\" Model's output computed on the current mini-batch. \"\"\"\n\n        self.loss = None\n        \"\"\" Loss of the current mini-batch. \"\"\"\n\n        self.is_training: bool = False\n        \"\"\" True if the strategy is in training mode. \"\"\"\n\n        self.current_eval_stream = None\n        \"\"\"User-provided evaluation stream on `eval` call\"\"\"\n\n        self._stop_training = False\n\n        self._warn_for_disabled_plugins_callbacks()\n        self._warn_for_disabled_metrics_callbacks()\n\n# --- Snippet Separator ---\n\nclass JointTraining(BaseStrategy):\n    def __init__(self, model: Module, optimizer: Optimizer, criterion,\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger):\n        \"\"\"\n        JointTraining performs joint training (also called offline training) on\n        the entire stream of data. This means that it is not a continual\n        learning strategy but it can be used as an \"offline\" upper bound for\n        them.\n\n        .. warnings also::\n            Currently :py:class:`JointTraining` adapts its own dataset.\n            Please check that the plugins you are using do not implement\n            :py:meth:`adapt_trainin_dataset`. Otherwise, they are incompatible\n            with :py:class:`JointTraining`.\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device to run the model.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        \"\"\"\n        super().__init__(model, optimizer, criterion, train_mb_size,\n                         train_epochs, eval_mb_size, device, plugins, evaluator)\n        # JointTraining can be trained only once.\n        self._is_fitted = False\n\n    def train(self, experiences: Union[Experience, Sequence[Experience]],\n              eval_streams: Optional[Sequence[Union[Experience,\n                                                    Sequence[\n                                                        Experience]]]] = None,\n              **kwargs):\n        \"\"\" Training loop. if experiences is a single element trains on it.\n        If it is a sequence, trains the model on each experience in order.\n        This is different from joint training on the entire stream.\n        It returns a dictionary with last recorded value for each metric.\n\n        :param experiences: single Experience or sequence.\n        :param eval_streams: list of streams for evaluation.\n            If None: use training experiences for evaluation.\n            Use [] if you do not want to evaluate during training.\n\n        :return: dictionary containing last recorded value for\n            each metric name.\n        \"\"\"\n        self.is_training = True\n        self.model.train()\n        self.model.to(self.device)\n\n        if self._is_fitted:\n            raise AlreadyTrainedError(\n                \"JointTraining can be trained only once. \"\n                \"Please call the train method once on the entire stream.\"\n            )\n\n        # Normalize training and eval data.\n        if isinstance(experiences, Experience):\n            experiences = [experiences]\n        if eval_streams is None:\n            eval_streams = [experiences]\n        for i, exp in enumerate(eval_streams):\n            if isinstance(exp, Experience):\n                eval_streams[i] = [exp]\n\n        self._experiences = experiences\n        self.before_training(**kwargs)\n        for exp in experiences:\n            self.train_exp(exp, eval_streams, **kwargs)\n            # Joint training only needs a single step because\n            # it concatenates all the data at once.\n            break\n        self.after_training(**kwargs)\n\n        res = self.evaluator.get_last_metrics()\n        self._is_fitted = True\n        return res\n\n    def train_dataset_adaptation(self, **kwargs):\n        \"\"\" Concatenates all the datastream. \"\"\"\n        self.adapted_dataset = self._experiences[0].dataset\n        for exp in self._experiences[1:]:\n            cat_data = AvalancheConcatDataset([self.adapted_dataset,\n                                               exp.dataset])\n            self.adapted_dataset = cat_data\n        self.adapted_dataset = self.adapted_dataset.train()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that trains and evaluates a model on the CLEAR benchmark using the Avalanche library. The code should define a set of hyperparameters, create a learning rate scheduler, and define a main function. In the main function, it should initialize a ResNet18 model, define normalization and transformation operations for the training and testing data, and set up logging to Tensorboard, a text file, and stdout. It should also define an evaluation plugin with various metrics. Depending on the evaluation protocol, it should set a seed value and create a CLEAR benchmark. The code should then move the model to the appropriate device, define an SGD optimizer, and create a learning rate scheduler. It should also define a continual learning strategy using the Naive method from Avalanche. The code should then run a training loop, saving the model after each experience and evaluating it on the test stream. Finally, it should generate an accuracy matrix and compute the CLEAR metrics, logging these results to a text file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 117, "repo_full_name": "dlr-rm__blenderproc", "instruction": "Generate code that uses the blenderproc library to load a 3D scene from an .obj file and texture files, specified by command line arguments. The code should initialize the blenderproc library, load the scene, and label its objects based on a provided mapping. It should then separate walls, floors, and ceilings into distinct objects and assign them appropriate labels. The code should also make lamp and ceiling objects emit light. It should then create a bounding volume hierarchy (BVH) tree containing all objects in the scene. The code should sample camera locations and rotations above the floor, ensuring there are no obstacles in front of the camera and that the scene coverage score is not too low. If these conditions are met, the camera pose should be added. The code should enable normal, depth, and segmentation rendering. Finally, it should render the scene and write the data to a .hdf5 file in a specified output directory.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def scene_coverage_score(cam2world_matrix: Union[Matrix, np.ndarray], special_objects: list = None,\n                         special_objects_weight: float = 2, sqrt_number_of_rays: int = 10) -> float:\n    \"\"\" Evaluate the interestingness/coverage of the scene.\n\n    This module tries to look at as many objects at possible, this might lead to\n    a focus on the same objects from similar angles.\n\n    Only for SUNCG and 3D Front:\n        The least interesting objects: walls, ceilings, floors.\n\n    :param cam2world_matrix: The world matrix which describes the camera pose to check.\n    :param special_objects: Objects that weights differently in calculating whether the scene is interesting or not,\n                            uses the coarse_grained_class or if not SUNCG, 3D Front, the category_id.\n    :param special_objects_weight: Weighting factor for more special objects, used to estimate how interesting the\n                                   scene is. Default: 2.0.\n    :param sqrt_number_of_rays: The square root of the number of rays which will be used to determine the\n                                visible objects.\n    :return: the scoring of the scene.\n    \"\"\"\n    cam2world_matrix = Matrix(cam2world_matrix)\n\n    if special_objects is None:\n        special_objects = []\n    cam_ob = bpy.context.scene.camera\n    cam = cam_ob.data\n\n    num_of_rays = sqrt_number_of_rays * sqrt_number_of_rays\n    score = 0.0\n    objects_hit: defaultdict = defaultdict(int)\n\n    # Get position of the corners of the near plane\n    frame = cam.view_frame(scene=bpy.context.scene)\n    # Bring to world space\n    frame = [cam2world_matrix @ v for v in frame]\n\n    # Compute vectors along both sides of the plane\n    vec_x = frame[1] - frame[0]\n    vec_y = frame[3] - frame[0]\n\n    # Go in discrete grid-like steps over plane\n    position = cam2world_matrix.to_translation()\n    for x in range(0, sqrt_number_of_rays):\n        for y in range(0, sqrt_number_of_rays):\n            # Compute current point on plane\n            end = frame[0] + vec_x * x / float(sqrt_number_of_rays - 1) + vec_y * y / float(sqrt_number_of_rays - 1)\n            # Send ray from the camera position through the current point on the plane\n            hit, _, _, _, hit_object, _ = bpy.context.scene.ray_cast(bpy.context.evaluated_depsgraph_get(),\n                                                                     position, end - position)\n\n            if hit:\n                is_of_special_dataset = \"is_suncg\" in hit_object or \"is_3d_front\" in hit_object\n                is_suncg_object = \"suncg_type\" in hit_object and hit_object[\"suncg_type\"] == \"Object\"\n                is_front_3d_object = \"3D_future_type\" in hit_object and hit_object[\"3D_future_type\"] == \"Object\"\n                if is_of_special_dataset and is_suncg_object or is_of_special_dataset and is_front_3d_object:\n                    # calculate the score based on the type of the object,\n                    # wall, floor and ceiling objects have 0 score\n                    if \"coarse_grained_class\" in hit_object:\n                        object_class = hit_object[\"coarse_grained_class\"]\n                        objects_hit[object_class] += 1\n                        if object_class in special_objects:\n                            score += special_objects_weight\n                        else:\n                            score += 1\n                    else:\n                        score += 1\n                elif \"category_id\" in hit_object:\n                    object_class = hit_object[\"category_id\"]\n                    if object_class in special_objects:\n                        score += special_objects_weight\n                    else:\n                        score += 1\n                    objects_hit[object_class] += 1\n                else:\n                    objects_hit[hit_object] += 1\n                    score += 1\n    # For a scene with three different objects, the starting variance is 1.0, increases/decreases by '1/3' for\n    # each object more/less, excluding floor, ceiling and walls\n    scene_variance = len(objects_hit) / 3.0\n    for object_hit_value in objects_hit.values():\n        # For an object taking half of the scene, the scene_variance is halved, this penalizes non-even\n        # distribution of the objects in the scene\n        scene_variance *= 1.0 - object_hit_value / float(num_of_rays)\n    score = scene_variance * (score / float(num_of_rays))\n    return score\n\n# --- Snippet Separator ---\n\ndef load_blend(path: str, obj_types: Optional[Union[List[str], str]] = None, name_regrex: Optional[str] = None,\n               data_blocks: Union[List[str], str] = \"objects\", link: bool = False) -> List[Entity]:\n    \"\"\"\n    Loads entities (everything that can be stored in a .blend file's folders, see Blender's documentation for\n    bpy.types.ID for more info) that match a name pattern from a specified .blend file's section/data_block.\n\n    :param path: Path to a .blend file.\n    :param obj_types: The type of objects to load. This parameter is only relevant when `data_blocks`\n                      is set to `\"objects\"`. Available options are: ['mesh', 'curve', 'hair', 'armature',\n                      'empty', 'light', 'camera']\n    :param name_regrex: Regular expression representing a name pattern of entities' (everything that can be\n                        stored in a .blend file's folders, see Blender's documentation for bpy.types.ID\n                        for more info) names.\n    :param data_blocks: The data block or a list of data blocks which should be loaded from the given .blend file.\n                        Available options are: ['armatures', 'cameras', 'curves', 'hairs', 'images', 'lights',\n                        'materials', 'meshes', 'objects', 'textures']\n    :param link: whether to link instead of append data blocks from .blend file. Linked objects can not be modified.\n    :return: The list of loaded mesh objects.\n    \"\"\"\n    if obj_types is None:\n        obj_types = [\"mesh\", \"empty\"]\n    # get a path to a .blend file\n    path = resolve_path(path)\n    data_blocks = _BlendLoader.validate_and_standardizes_configured_list(data_blocks, _BlendLoader.valid_data_blocks,\n                                                                         \"data block\")\n    obj_types = _BlendLoader.validate_and_standardizes_configured_list(obj_types, _BlendLoader.valid_object_types,\n                                                                       \"object type\")\n\n    # Remember which orphans existed beforehand\n    orphans_before = collect_all_orphan_data_blocks()\n\n    # Start importing blend file. All objects that should be imported need to be copied from \"data_from\" to \"data_to\"\n    with bpy.data.libraries.load(path, link=link) as (data_from, data_to):\n        for data_block in data_blocks:\n            # Verify that the given data block is valid\n            if hasattr(data_from, data_block):\n                # Find all entities of this data block that match the specified pattern\n                data_to_entities = []\n                for entity_name in getattr(data_from, data_block):\n                    if not name_regrex or re.fullmatch(name_regrex, entity_name) is not None:\n                        data_to_entities.append(entity_name)\n                # Import them\n                setattr(data_to, data_block, data_to_entities)\n                print(\"Imported \" + str(len(data_to_entities)) + \" \" + data_block)\n            else:\n                raise Exception(\"No such data block: \" + data_block)\n\n    # Go over all imported objects again\n    loaded_objects: List[Entity] = []\n    for data_block in data_blocks:\n        # Some adjustments that only affect objects\n        if data_block == \"objects\":\n            for obj in getattr(data_to, data_block):\n                # Check that the object type is desired\n                if obj.type.lower() in obj_types:\n                    # Link objects to the scene\n                    bpy.context.collection.objects.link(obj)\n                    loaded_objects.append(convert_to_entity_subclass(obj))\n\n                    # If a camera was imported\n                    if obj.type == 'CAMERA':\n                        # Make it the active camera in the scene\n                        bpy.context.scene.camera = obj\n\n                        # Find the maximum frame number of its key frames\n                        max_keyframe = -1\n                        if obj.animation_data is not None:\n                            fcurves = obj.animation_data.action.fcurves\n                            for curve in fcurves:\n                                keyframe_points = curve.keyframe_points\n                                for keyframe in keyframe_points:\n                                    max_keyframe = max(max_keyframe, keyframe.co[0])\n\n                        # Set frame_end to the next free keyframe\n                        bpy.context.scene.frame_end = max_keyframe + 1\n                else:\n                    # Remove object again if its type is not desired\n                    bpy.data.objects.remove(obj, do_unlink=True)\n            print(\"Selected \" + str(len(loaded_objects)) + \" of the loaded objects by type\")\n        else:\n            loaded_objects.extend(getattr(data_to, data_block))\n\n    # As some loaded objects were deleted again due to their type, we need also to remove the dependent\n    # data blocks that were also loaded and are now orphans\n    _BlendLoader.purge_added_orphans(orphans_before, data_to)\n    return loaded_objects\n\n# --- Snippet Separator ---\n\ndef light_suncg_scene(lightbulb_emission_strength: float = 15, lampshade_emission_strength: float = 7,\n                      ceiling_emission_strength: float = 1.5):\n    \"\"\" Makes the lamps, windows and ceilings object emit light.\n\n    :param lightbulb_emission_strength: The emission strength that should be used for light bulbs. Default: 15\n    :param lampshade_emission_strength: The emission strength that should be used for lamp shades. Default: 7\n    :param ceiling_emission_strength: The emission strength that should be used for the ceiling. Default: 1.5\n    \"\"\"\n    # Read in the materials for lights and windows\n    lights, windows = Utility.read_suncg_lights_windows_materials()\n\n    collection_of_mats: Dict[str, Dict[str, Material]] = {\"lamp\": {}, \"window\": {}, \"ceiling\": {}}\n\n    # Make some objects emit lights\n    for obj in get_all_mesh_objects():\n        if obj.has_cp(\"modelId\"):\n            obj_id = obj.get_cp(\"modelId\")\n\n            # In the case of the lamp\n            if obj_id in lights:\n                _SuncgLighting.make_lamp_emissive(obj, lights[obj_id], collection_of_mats, lightbulb_emission_strength,\n                                                  lampshade_emission_strength)\n\n            # Make the windows emit light\n            if obj_id in windows:\n                _SuncgLighting.make_window_emissive(obj, collection_of_mats)\n\n            # Also make ceilings slightly emit light\n            if obj.get_name().startswith(\"Ceiling#\"):\n                _SuncgLighting.make_ceiling_emissive(obj, collection_of_mats, ceiling_emission_strength)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the blenderproc library to load a 3D scene from an .obj file and texture files, specified by command line arguments. The code should initialize the blenderproc library, load the scene, and label its objects based on a provided mapping. It should then separate walls, floors, and ceilings into distinct objects and assign them appropriate labels. The code should also make lamp and ceiling objects emit light. It should then create a bounding volume hierarchy (BVH) tree containing all objects in the scene. The code should sample camera locations and rotations above the floor, ensuring there are no obstacles in front of the camera and that the scene coverage score is not too low. If these conditions are met, the camera pose should be added. The code should enable normal, depth, and segmentation rendering. Finally, it should render the scene and write the data to a .hdf5 file in a specified output directory.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 118, "repo_full_name": "stfc__psyclone", "instruction": "Generate code that imports necessary transformations and constants from the 'psyclone' library. The code should define several boolean variables to control the application of different transformations. Then, define a function that applies a series of transformations to a given 'psy' object. The transformations should include redundant computation, asynchronous halo exchanges, OpenMP colouring, and intrinsic inlining. The function should iterate over all invokes in the 'psy' object and apply the transformations according to the defined boolean variables. The function should also handle TransformationErrors. Finally, the function should return the transformed 'psy' object.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def trans(psy):\n    '''Transforms all synchronous halo exchanges into asynchronous halo\n    exchanges and moves the halo exchange start part of each\n    asynchronous halo exchange as early as possible in the schedule in\n    order to maximise the overlap of communication and\n    computation. Also outputs a textual view of the transformed PSyIR\n    representing the PSy-layer.\n\n    :param psy: a PSyclone PSy object which captures the algorithm and \\\n        kernel information required by PSyclone.\n    :type psy: subclass of :py:class:`psyclone.psyGen.PSy`\n\n    '''\n    # Create the required transformations\n    async_hex = Dynamo0p3AsyncHaloExchangeTrans()\n    move_trans = MoveTrans()\n\n    # Iterate over the invokes in this algorithm file\n    invokes = psy.invokes.invoke_list\n    for invoke in invokes:\n\n        # Get the schedule (the PSyIR representation of the PSy-layer)\n        schedule = invoke.schedule\n        # Split any synchronous halo exchanges into asynchronous halo exchanges\n        for hex_node in schedule.walk(DynHaloExchange):\n            async_hex.apply(hex_node)\n\n        # Move any halo exchange starts as early as possible in the\n        # schedule to maximise overlap of compute and comms within the\n        # invoke.\n        for hex_start_node in reversed(schedule.walk(DynHaloExchangeStart)):\n            idx = hex_start_node.position\n            parent = hex_start_node.parent\n            # Move halo exchange start node up one node at a time\n            # until there is an exception (which indicates the move is\n            # invalid). No need to check for idx == 0 as a negative\n            # index wraps to the end of the list which will be\n            # invalid.\n            try:\n                while True:\n                    move_trans.apply(parent[idx], parent[idx-1])\n                    idx -= 1\n            except TransformationError:\n                pass\n\n        # Take a look at the modified PSy-layer PSyIR\n        print(schedule.view())\n\n# --- Snippet Separator ---\n\ndef trans(psy):\n    '''A PSyclone-script compliant transformation function.\n\n    :param psy: The PSy layer object to apply transformations to.\n    :type psy: :py:class:`psyclone.psyGen.PSy`\n\n    :returns: the transformed PSy layer object.\n    :rtype: :py:class:`psyclone.psyGen.PSy`\n\n    '''\n    # Get the Schedule of the target routine\n    sched = psy.invokes.get('tra_adv').schedule\n\n    # Find the outer, 'iteration' loop\n    tloop = None\n    for node in sched.children:\n        if isinstance(node, Loop) and node.loop_type == \"tracers\":\n            tloop = node\n            break\n\n    # Loop through the children of the loop body and transform those\n    # that are over levels\n\n    print(sched.view())\n\n# --- Snippet Separator ---\n\ndef trans(psy):\n    '''A PSyclone-script compliant transformation function that is\n    bespoke for the tracer-advection mini-app. It encloses the\n    body of the iteration loop within a KERNELS region and then\n    applies COLLAPSE(2) to every latitude-longitude loop nest\n    within that.\n\n    :param psy: The PSy layer object to apply transformations to.\n    :type psy: :py:class:`psyclone.psyGen.PSy`\n\n    :returns: the transformed PSy layer object.\n    :rtype: :py:class:`psyclone.psyGen.PSy`\n\n    '''\n    # Get the Schedule of the target routine\n    sched = psy.invokes.get('tra_adv').schedule\n\n    # Find the outer, 'iteration' loop\n    tloop = None\n    for node in sched.children:\n        if isinstance(node, Loop) and node.loop_type == \"tracers\":\n            tloop = node\n            break\n    ACC_KERNELS_TRANS.apply(tloop.loop_body)\n\n    loops = tloop.walk(Loop)\n    for loop in loops:\n        if loop.loop_type == \"lat\":\n            try:\n                ACC_LOOP_TRANS.apply(loop, options={\"collapse\": 2})\n            except TransformationError:\n                pass\n\n    # Finally, enclose the whole of the 'iteration' loop within\n    # a data region\n    ACC_DATA_TRANS.apply(tloop)\n\n    print(sched.view())\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary transformations and constants from the 'psyclone' library. The code should define several boolean variables to control the application of different transformations. Then, define a function that applies a series of transformations to a given 'psy' object. The transformations should include redundant computation, asynchronous halo exchanges, OpenMP colouring, and intrinsic inlining. The function should iterate over all invokes in the 'psy' object and apply the transformations according to the defined boolean variables. The function should also handle TransformationErrors. Finally, the function should return the transformed 'psy' object.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 119, "repo_full_name": "seed-labs__seed-emulator", "instruction": "Generate code that creates an emulation environment using the seed-emulator library. The environment should include a ransomware service, a Tor service, and a DNS layer. \n\nFor the ransomware service, create a ransomware attacker and 16 ransomware victims. The attacker should be installed on a host in an autonomous system and should not support botnet or Tor. The victims should be installed on hosts and should not support botnet. \n\nFor the Tor service, create different types of Tor nodes including directory authorities, clients, relays, exits, and a hidden service. The hidden service should be linked to the ransomware attacker. \n\nFor the DNS layer, create a root server, TLD and ccTLD servers, second-level zone servers, and a local DNS server. The servers should have appropriate zones and records. \n\nFinally, compile the emulator using a Docker compiler with custom base images for the victim and attacker nodes. Copy necessary files to the output directory and make a script executable.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class CymruIpOriginService(Service):\n    \"\"\"!\n    @brief Cymru's IP info service.\n\n    Cymru's IP info service is used by various traceroute utilities to map IP\n    address to ASN (using DNS). This service loads the prefix list within the\n    simulation and creates ASN mappings for them, so with proper local DNS\n    configured, nodes can see the ASN when doing traceroute. \n\n    This layer hosts the domain cymru.com.\n    \"\"\"\n\n    __records: List[str]\n    __dns: DomainNameService\n\n    def __init__(self):\n        \"\"\"!\n        @brief CymruIpOriginService constructor\n        \"\"\"\n        super().__init__()\n        self.__records = []\n        self.addDependency('DomainNameService', True, True)\n        self.addDependency('Base', False, False)\n\n    def _createServer(self) -> Server:\n        return CymruIpOriginServer()\n\n    def getName(self) -> str:\n        return 'CymruIpOriginService'\n\n    def addMapping(self, prefix: str, asn: int) -> CymruIpOriginService:\n        \"\"\"!\n        @brief Add new prefix -> asn mapping.\n\n        @param prefix prefix.\n        @param asn asn.\n\n        @throws AssertionError if prefix invalid.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        [pfx, cidr] = prefix.split('/')\n        cidr = int(cidr)\n        assert cidr <= 24, 'Invalid prefix.'\n        prefix = IPv4Network(prefix)\n\n        sub_cidr = 24\n        num_8s = 3\n\n        if cidr >= 0:\n            sub_cidr = 8\n            num_8s = 1\n\n        if cidr >= 9:\n            sub_cidr = 16\n            num_8s = 2\n\n        if cidr >= 17:\n            sub_cidr = 24\n            num_8s = 3\n\n        for net in prefix.subnets(new_prefix = sub_cidr):\n            record = '*.'\n            record += '.'.join(reversed(str(net).split('.')[0:3]))\n            record += '.origin.asn TXT \"{} | {} | ZZ | SEED | 0000-00-00\"'.format(asn, net)\n            self.__records.append(record)\n\n        return self\n\n    def getRecords(self) -> List[str]:\n        \"\"\"!\n        @brief get generated records.\n\n        @return list of records.\n        \"\"\"\n        return self.__records\n\n    def addRecord(self, record: str) -> CymruIpOriginService:\n        \"\"\"!\n        @brief add record directly to the cymru zone. You should use addMapping\n        to add mapping and not addRecord, unless you know what you are doing.\n\n        @returns self, for chaining API calls.\n        \"\"\"\n        self.__records.append(record)\n\n        return self\n\n    def _doInstall(self, node: Node, server: Server): \n        assert False, 'CymruIpOriginService is not a real service and should not be installed this way. Please install a DomainNameService on the node and host the zone \"cymru.com.\" yourself.'\n\n\n    def configure(self, emulator: Emulator):\n        reg = emulator.getRegistry()\n\n        mappings: List[Tuple[str, str]] = []\n\n        self._log('Collecting all networks in the simulation...')\n        for regobj in reg.getAll().items():\n            [(asn, type, name), obj] = regobj\n            if type != 'net': continue\n            net: Network = obj\n            if asn == 'ix': asn = name.replace('ix', '')\n\n            asn_val = 0\n            try:\n                asn_val = int(asn)\n            except ValueError:\n                asn_val = 0\n\n            mappings.append((net.getPrefix(), asn_val))\n\n        for mapping in mappings:\n            (prefix, asn) = mapping\n            self.addMapping(str(prefix), asn)\n\n        self._log('Creating \"cymru.com.\" zone...')\n        dns: DomainNameService = reg.get('seedemu', 'layer', 'DomainNameService')\n        zone = dns.getZone('cymru.com.')\n        self.__dns = dns\n\n        self._log('Adding mappings...')\n        for record in self.__records:\n            zone.addRecord(record)\n\n        return super().configure(emulator)        \n\n    def print(self, indent: int) -> str:\n        out = ' ' * indent\n        out += 'CymruIpOriginService\\n'\n\n        return out\n\n# --- Snippet Separator ---\n\nclass ReverseDomainNameService(Service):\n    \"\"\"!\n    @brief Reverse DNS. This service hosts the in-addr.arpa. zone and resolve\n    IP addresses to nodename-netname.nodetype.asn.net\n    \"\"\"\n\n    __dns: DomainNameService\n\n    def __init__(self):\n        \"\"\"!\n        @brief ReverseDomainNameService constructor\n        \"\"\"\n        super().__init__()\n        self.addDependency('DomainNameService', True, False)\n        self.addDependency('Base', False, False)\n\n    def getName(self) -> str:\n        return 'ReverseDomainNameService'\n\n    def _createServer(self) -> Server:\n        return ReverseDomainNameServer()\n\n    def install(self, vnode: str) -> Server:\n        assert False, 'ReverseDomainNameService is not a real service and should not be installed this way. Please install a DomainNameService on the node and host the zone \"in-addr.arpa.\" yourself.'\n\n    def configure(self, emulator: Emulator):\n        reg = emulator.getRegistry()\n\n        self._log('Creating \"in-addr.arpa.\" zone...')\n        self.__dns = reg.get('seedemu', 'layer', 'DomainNameService')\n        zone = self.__dns.getZone('in-addr.arpa.')\n\n        self._log('Collecting IP addresses...')\n        for ([scope, type, name], obj) in reg.getAll().items():\n            if type != 'rnode' and type != 'hnode': continue\n            self._log('Collecting {}/{}/{}...'.format(scope, type, name))\n\n            if scope == 'ix':\n                scope = name\n                name = 'rs'\n            else: scope = 'as' + scope\n\n            node: Node = obj\n            for iface in node.getInterfaces():\n                addr = '.'.join(reversed(str(iface.getAddress()).split('.')))\n                netname = iface.getNet().getName()\n                record = '{} PTR {}-{}.{}.{}.net.'.format(addr, name, netname, type, scope).replace('_', '-')\n                zone.addRecord(record)\n\n        return super().configure(emulator)\n\n    def print(self, indent: int) -> str:\n        out = ' ' * indent\n        out += 'ReverseDomainNameService\\n'\n\n        return out\n\n# --- Snippet Separator ---\n\ndef _doInstall(self, node: Node, server: Server): \n        assert False, 'CymruIpOriginService is not a real service and should not be installed this way. Please install a DomainNameService on the node and host the zone \"cymru.com.\" yourself.'\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates an emulation environment using the seed-emulator library. The environment should include a ransomware service, a Tor service, and a DNS layer. \n\nFor the ransomware service, create a ransomware attacker and 16 ransomware victims. The attacker should be installed on a host in an autonomous system and should not support botnet or Tor. The victims should be installed on hosts and should not support botnet. \n\nFor the Tor service, create different types of Tor nodes including directory authorities, clients, relays, exits, and a hidden service. The hidden service should be linked to the ransomware attacker. \n\nFor the DNS layer, create a root server, TLD and ccTLD servers, second-level zone servers, and a local DNS server. The servers should have appropriate zones and records. \n\nFinally, compile the emulator using a Docker compiler with custom base images for the victim and attacker nodes. Copy necessary files to the output directory and make a script executable.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 120, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that imports necessary modules and defines variables for maximum iterations, theta, and k from command line arguments. The code should initialize a simplified friction problem on a halfedge polygon mesh using the fealpy library. It should then create a loop for the maximum number of iterations, where in each iteration, it solves the problem, calculates residuals and high order terms, and saves the results and error data to a file. The code should also plot the mesh and save it as an image file. If the current iteration is not the last one, the code should refine the mesh based on the residuals. After the loop, the code should save the final error data to a file and display a multi-rate plot.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def create_read_in_code(program, psy_data, read_write_info, postfix):\n        '''This function creates the code that reads in the NetCDF file\n        produced during extraction. For each:\n\n        - variable that is read-only, it will declare the symbol and add code\n          that reads in the variable using the PSyData library.\n        - variable that is read and written, it will create code to read in the\n          variable that is read, and create a new variable with the same name\n          and \"_post\" added which is read in to store the values from the\n          NetCDF file after the instrumented region was executed. In the end,\n          the variable that was read and written should have the same value\n          as the corresponding \"_post\" variable.\n        - variable that is written only, it will create a variable with \"_post\"\n          as postfix that reads in the output data from the NetCDF file. It\n          then also declares a variable without postfix (which will be the\n          parameter to the function), allocates it based on the shape of\n          the corresponding \"_post\" variable, and initialises it with 0.\n\n        :param program: the PSyIR Routine to which any code must \\\n            be added. It also contains the symbol table to be used.\n        :type program: :py:class:`psyclone.psyir.nodes.Routine`\n        :param psy_data: the PSyData symbol to be used.\n        :type psy_data: :py:class:`psyclone.psyir.symbols.DataSymbol`\n        :param read_write_info: information about all input and output \\\n            parameters.\n        :type read_write_info: :py:class:`psyclone.psyir.tools.ReadWriteInfo`\n        :param str postfix: a postfix that is added to a variable to \\\n            create the corresponding variable that stores the output \\\n            value from the kernel data file.\n\n        :returns: a list with all output parameters, i.e. variables that \\\n            need to be verified after executing the kernel. Each entry is \\\n            a 2-tuple containing the symbol of the computed variable, and \\\n            the symbol of the variable that contains the value read from \\\n            the file.\n        :rtype: list of 2-tuples of \\\n            :py:class:`psyclone.psyir.symbols.Symbol`\n\n        '''\n        symbol_table = program.scope.symbol_table\n        read_var = f\"{psy_data.name}%ReadVariable\"\n\n        # Collect all output symbols to later create the tests for\n        # correctness. This list stores 2-tuples: first one the\n        # variable that stores the output from the kernel, the second\n        # one the variable that stores the output values read from the\n        # file. The content of these two variables should be identical\n        # at the end.\n        output_symbols = []\n\n        # First handle variables that are read:\n        # -------------------------------------\n        for signature in read_write_info.signatures_read:\n            # Find the right symbol for the variable. Note that all variables\n            # in the input and output list have been detected as being used\n            # when the variable accesses were analysed. Therefore, these\n            # variables have References, and will already have been declared\n            # in the symbol table (in add_all_kernel_symbols).\n            sig_str = str(signature)\n            sym = symbol_table.lookup_with_tag(sig_str)\n            name_lit = Literal(sig_str, CHARACTER_TYPE)\n            ExtractDriverCreator.add_call(program, read_var,\n                                          [name_lit, Reference(sym)])\n\n        # Then handle all variables that are written (note that some\n        # variables might be read and written)\n        for signature in read_write_info.signatures_written:\n            # Find the right symbol for the variable. Note that all variables\n            # in the input and output list have been detected as being used\n            # when the variable accesses were analysed. Therefore, these\n            # variables have References, and will already have been declared\n            # in the symbol table (in add_all_kernel_symbols).\n            sig_str = str(signature)\n            sym = symbol_table.lookup_with_tag(sig_str)\n\n            # The variable is written (and maybe read as well)\n            # ------------------------------------------------\n            # Declare a 'post' variable of the same type and\n            # read in its value.\n            post_name = sig_str+postfix\n            post_sym = symbol_table.new_symbol(post_name,\n                                               symbol_type=DataSymbol,\n                                               datatype=sym.datatype)\n            ExtractDriverCreator.add_call(program, read_var,\n                                          [Literal(post_name, CHARACTER_TYPE),\n                                           Reference(post_sym)])\n\n            # Now if a variable is written to, but not read, the variable\n            # is not allocated. So we need to allocate it and set it to 0.\n            if not read_write_info.is_read(signature):\n                if isinstance(post_sym.datatype, ArrayType):\n                    alloc = IntrinsicCall.create(\n                        IntrinsicCall.Intrinsic.ALLOCATE,\n                        [Reference(sym), (\"mold\", Reference(post_sym))])\n                    program.addchild(alloc)\n                set_zero = Assignment.create(Reference(sym),\n                                             Literal(\"0\", INTEGER_TYPE))\n                program.addchild(set_zero)\n            output_symbols.append((sym, post_sym))\n        return output_symbols\n\n# --- Snippet Separator ---\n\ndef write_driver(self, nodes, read_write_info, prefix, postfix,\n                     region_name, writer=FortranWriter()):\n        # pylint: disable=too-many-arguments\n        '''This function uses the ``get_driver_as_string()`` function to get a\n        a stand-alone driver, and then writes this source code to a file. The\n        file name is derived from the region name:\n        \"driver-\"+module_name+\"_\"+region_name+\".F90\"\n\n        :param nodes: a list of nodes containing the body of the driver\n            routine.\n        :type nodes: List[:py:class:`psyclone.psyir.nodes.Node`]\n        :param read_write_info: information about all input and output \\\n            parameters.\n        :type read_write_info: :py:class:`psyclone.psyir.tools.ReadWriteInfo`\n        :param str prefix: the prefix to use for each PSyData symbol, \\\n            e.g. 'extract' as prefix will create symbols `extract_psydata`.\n        :param str postfix: a postfix that is appended to an output variable \\\n            to create the corresponding variable that stores the output \\\n            value from the kernel data file. The caller must guarantee that \\\n            no name clashes are created when adding the postfix to a variable \\\n            and that the postfix is consistent between extract code and \\\n            driver code (see 'ExtractTrans.determine_postfix()').\n        :param Tuple[str,str] region_name: an optional name to \\\n            use for this PSyData area, provided as a 2-tuple containing a \\\n            location name followed by a local name. The pair of strings \\\n            should uniquely identify a region.\n        :param writer: a backend visitor to convert PSyIR \\\n            representation to the selected language. It defaults to \\\n            the FortranWriter.\n        :type writer: \\\n            :py:class:`psyclone.psyir.backend.language_writer.LanguageWriter`\n\n        '''\n        code = self.get_driver_as_string(nodes, read_write_info, prefix,\n                                         postfix, region_name, writer=writer)\n        fll = FortLineLength()\n        code = fll.process(code)\n        if not code:\n            # This indicates an error that was already printed,\n            # so ignore it here.\n            return\n        module_name, local_name = region_name\n        with open(f\"driver-{module_name}-{local_name}.F90\", \"w\",\n                  encoding='utf-8') as out:\n            out.write(code)\n\n# --- Snippet Separator ---\n\ndef solve_transport_fixed_boundary(\n    parameterisation: GeometryParameterisation,\n    transport_solver: CodesSolver,\n    gs_solver: FemGradShafranovFixedBoundary,\n    kappa95_t: float,\n    delta95_t: float,\n    lcar_mesh: float = 0.15,\n    max_iter: int = 30,\n    iter_err_max: float = 1e-5,\n    max_inner_iter: int = 20,\n    inner_iter_err_max: float = 1e-4,\n    relaxation: float = 0.2,\n    transport_run_mode: Union[str, BaseRunMode] = \"run\",\n    mesh_filename: str = \"FixedBoundaryEquilibriumMesh\",\n    plot: bool = False,\n    debug: bool = False,\n    gif: bool = False,\n    refine: bool = False,\n    num_levels: int = 2,\n    distance: float = 1.0,\n):\n    \"\"\"\n    Solve the plasma fixed boundary problem using delta95 and kappa95 as target\n    values and iterating on a transport solver to have consistency with pprime\n    and ffprime.\n\n    Parameters\n    ----------\n    parameterisation:\n        Geometry parameterisation class for the plasma\n    transport_solver:\n        Transport Solver to call\n    gs_solver:\n        Grad-Shafranov Solver instance\n    kappa95_t:\n        Target value for kappa at 95%\n    delta95_t:\n        Target value for delta at 95%\n    lcar_mesh:\n        Value of the characteristic length used to generate the mesh to solve the\n        Grad-Shafranov problem\n    max_iter:\n        Maximum number of iteration between Grad-Shafranov and the transport solver\n    iter_err_max:\n        Convergence maximum error to stop the iteration\n    max_inner_iter:\n        Maximum number of inner iterations on the flux functions\n    inner_iter_err_max:\n        Inner convergence error on when iterating flux functions\n    relaxation:\n        Iteration relaxing factor\n    transport_run_mode:\n        Run mode for transport solver\n    mesh_filename:\n        filename for mesh output file\n    plot:\n        Whether or not to plot\n    refine:\n        Whether or not the mesh should be refined around the magnetic axis\n    num_levels:\n        number of refinement levels\n    distance:\n        maximum distance from the magnetic axis to which the refinement will be applied\n\n    Returns\n    -------\n    equilibrium: FixedBoundaryEquilibrium\n        Final fixed boundary equilibrium result from the transport <-> fixed boundary\n        equilibrium solve\n    \"\"\"\n    kappa_95 = kappa95_t\n    delta_95 = delta95_t\n\n    directory = get_bluemira_path(\"\", subfolder=\"generated_data\")\n    mesh_name_msh = mesh_filename + \".msh\"\n\n    paramet_params = PlasmaFixedBoundaryParams(\n        **{\n            k: v\n            for k, v in zip(\n                parameterisation.variables.names, parameterisation.variables.values\n            )\n            if k in PlasmaFixedBoundaryParams.fields()\n        }\n    )\n\n    transport_params = TransportSolverParams.from_frame(\n        deepcopy(transport_solver.params)\n    )\n\n    lcfs_options = {\n        \"face\": {\"lcar\": lcar_mesh, \"physical_group\": \"plasma_face\"},\n        \"lcfs\": {\"lcar\": lcar_mesh, \"physical_group\": \"lcfs\"},\n    }\n\n    plot = any((plot, debug, gif))\n    folder = try_get_bluemira_path(\"\", subfolder=\"generated_data\", allow_missing=False)\n    figname = \"Transport iteration \"\n    f, ax = None, None\n\n    for n_iter in range(max_iter):\n        transp_out_params, x, pprime, ffprime = _run_transport_solver(\n            transport_solver, transport_params, transport_run_mode\n        )\n\n        f_pprime = interp1d(x, pprime, fill_value=\"extrapolate\")\n        f_ffprime = interp1d(x, ffprime, fill_value=\"extrapolate\")\n\n        psi_plasmod = transport_solver.get_profile(\"psi\")\n        x_psi_plasmod = np.sqrt(psi_plasmod / psi_plasmod[-1])\n\n        q = transport_solver.get_profile(\"q\")\n        press = transport_solver.get_profile(\"pressure\")\n        q_func = interp1d(x, q, fill_value=\"extrapolate\")\n        p_func = interp1d(x, press, fill_value=\"extrapolate\")\n\n        if plot:\n            if ax is not None:\n                for axis in ax.flat:\n                    axis.clear()\n            f, ax = plot_default_profiles(transport_solver, show=False, f=f, ax=ax)\n\n            f.suptitle(figname + str(n_iter))\n            plt.pause(PLT_PAUSE)\n            if debug or gif:\n                save_figure(\n                    f,\n                    figname + str(n_iter),\n                    save=True,\n                    folder=folder,\n                    dpi=DPI_GIF,\n                )\n\n        plasma = create_plasma_xz_cross_section(\n            parameterisation,\n            transport_params,\n            paramet_params,\n            kappa_95,\n            delta_95,\n            lcfs_options,\n            f\"from equilibrium iteration {n_iter}\",\n        )\n\n        mesh = create_mesh(\n            plasma,\n            directory,\n            mesh_filename,\n            mesh_name_msh,\n        )\n\n        # store the created mesh as coarse mesh\n        coarse_mesh = mesh\n\n        gs_solver.set_mesh(mesh)\n\n        points = gs_solver.mesh.coordinates()\n        psi2d_0 = np.zeros(len(points))\n\n        for n_iter_inner in range(max_inner_iter):\n            gs_solver.set_profiles(\n                f_pprime,\n                f_ffprime,\n                transp_out_params.I_p.value,\n                transp_out_params.B_0.value,\n                transp_out_params.R_0.value,\n            )\n\n            bluemira_print(\n                f\"Solving fixed boundary Grad-Shafranov...[inner iteration: {n_iter_inner}]\"\n            )\n\n            equilibrium = gs_solver.solve(\n                plot=plot,\n                debug=debug,\n                gif=gif,\n                figname=f\"{n_iter} Fixed boundary equilibrium iteration \",\n            )\n\n            x1d, flux_surfaces = get_flux_surfaces_from_mesh(\n                mesh, gs_solver.psi_norm_2d, x_1d=x_psi_plasmod\n            )\n\n            x1d, volume, _, g2, g3 = calc_metric_coefficients(\n                flux_surfaces, gs_solver.grad_psi, x1d, gs_solver.psi_ax\n            )\n            _, _, _, pprime, _, ffprime = calc_curr_dens_profiles(\n                x1d,\n                p_func(x1d),\n                q_func(x1d),\n                g2,\n                g3,\n                volume,\n                transp_out_params.I_p.value,\n                transp_out_params.B_0.value,\n                transp_out_params.R_0.value,\n                gs_solver.psi_ax,\n                gs_solver.psi_b,\n            )\n\n            f_pprime = interp1d(x1d, pprime, fill_value=\"extrapolate\")\n            f_ffprime = interp1d(x1d, ffprime, fill_value=\"extrapolate\")\n\n            psi2d = np.array([gs_solver.psi(p) for p in points])\n\n            eps_psi2d = np.linalg.norm(psi2d - psi2d_0, ord=2) / np.linalg.norm(\n                psi2d, ord=2\n            )\n\n            if eps_psi2d < inner_iter_err_max:\n                break\n            else:\n                bluemira_print(f\"Error on psi2d = {eps_psi2d} > {inner_iter_err_max}\")\n                psi2d_0 = psi2d\n                if refine:\n                    magnetic_axis = find_magnetic_axis(gs_solver.psi, gs_solver.mesh)\n                    magnetic_axis = np.array([magnetic_axis[0], magnetic_axis[1], 0])\n                    mesh = refine_mesh(coarse_mesh, magnetic_axis, distance, num_levels)\n                    bluemira_print(f\"Mesh refined on magnetic axis {magnetic_axis[:2]}\")\n                    gs_solver.set_mesh(mesh)\n\n        _, kappa_95, delta_95 = calculate_plasma_shape_params(\n            gs_solver.psi_norm_2d,\n            mesh,\n            np.sqrt(0.95),\n        )\n\n        iter_err = _update_delta_kappa(\n            paramet_params,\n            kappa_95,\n            kappa95_t,\n            delta_95,\n            delta95_t,\n            relaxation,\n        )\n\n        bluemira_print(\n            f\"{transport_solver.name} <-> Fixed boundary G-S iter {n_iter} : {iter_err:.3E}\"\n        )\n\n        if iter_err <= iter_err_max:\n            message = bluemira_print\n            line_1 = f\"successfully converged in {n_iter} iterations\"\n            ltgt = \"<\"\n            break\n\n        # update parameters\n        for name, value in asdict(paramet_params).items():\n            parameterisation.adjust_variable(name, value)\n\n    else:\n        # If we don't break we didn't converge\n        message = bluemira_warn\n        line_1 = f\"did not converge within {max_iter} iterations\"\n        ltgt = \">\"\n\n    message(\n        f\"{transport_solver.name} <-> Fixed boundary G-S {line_1}:\\n\\t\"\n        f\"Target kappa_95: {kappa95_t:.3f}\\n\\t\"\n        f\"Actual kappa_95: {kappa_95:.3f}\\n\\t\"\n        f\"Target delta_95: {delta95_t:.3f}\\n\\t\"\n        f\"Actual delta_95: {delta_95:.3f}\\n\\t\"\n        f\"Error: {iter_err:.3E} {ltgt} {iter_err_max:.3E}\\n\"\n    )\n\n    if gif:\n        make_gif(folder, figname, clean=not debug)\n    return equilibrium\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary modules and defines variables for maximum iterations, theta, and k from command line arguments. The code should initialize a simplified friction problem on a halfedge polygon mesh using the fealpy library. It should then create a loop for the maximum number of iterations, where in each iteration, it solves the problem, calculates residuals and high order terms, and saves the results and error data to a file. The code should also plot the mesh and save it as an image file. If the current iteration is not the last one, the code should refine the mesh based on the residuals. After the loop, the code should save the final error data to a file and display a multi-rate plot.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 121, "repo_full_name": "bokeh__bokeh", "instruction": "Generate code that creates a Bokeh application to visualize population data. The application should include two plots: a population pyramid and a line chart showing known and predicted population values. The population pyramid should be divided into male and female sections, and the line chart should differentiate between known and predicted values. The application should allow users to select a year and a location, and the plots should update based on these selections. The application should be served using Bokeh server and the layout should be saved into an HTML file named \"widget.html\". The application should continue running until it is manually stopped.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def on_pause(self):\n        '''Event handler called when Pause mode is requested. You should\n        return True if your app can go into Pause mode, otherwise\n        return False and your application will be stopped.\n\n        You cannot control when the application is going to go into this mode.\n        It's determined by the Operating System and mostly used for mobile\n        devices (android/ios) and for resizing.\n\n        The default return value is True.\n\n        .. versionadded:: 1.1.0\n        .. versionchanged:: 1.10.0\n            The default return value is now True.\n        '''\n        return True\n\n# --- Snippet Separator ---\n\ndef shutdown(self):\n        \"\"\"\n        This command shuts down the application and the RemoteEx program. Response is sent before shutdown.\n        The usefulness of this command is limited because it cannot be sent once the application has hung. Restarting of\n        the remote application if an error has occurred should be done by other means (example: Power off and on the\n        computer from remote and starting the RemoteEx from the autostart).\n\n        :return:\n        \"\"\"\n        self.send_command('Shutdown')\n\n# --- Snippet Separator ---\n\ndef add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a Bokeh application to visualize population data. The application should include two plots: a population pyramid and a line chart showing known and predicted population values. The population pyramid should be divided into male and female sections, and the line chart should differentiate between known and predicted values. The application should allow users to select a year and a location, and the plots should update based on these selections. The application should be served using Bokeh server and the layout should be saved into an HTML file named \"widget.html\". The application should continue running until it is manually stopped.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 122, "repo_full_name": "federatedai__fate", "instruction": "Generate code that uses the fate library to create a pipeline for a binary classification task using a heterogenous neural network (HeteroNN). The pipeline should include the following components: a Reader to read the training data, a DataTransform to preprocess the data, an Intersection to find the common instances between the guest and host data, a HeteroNN for the model training, and an Evaluation for model evaluation. The HeteroNN should be configured with specific parameters such as epochs, learning rate, batch size, and task type. The neural network should include a guest bottom model, a guest top model, a host bottom model, and an interactive layer. The models should be defined using the torch library and added to the HeteroNN. The HeteroNN should be compiled with a specific optimizer and loss function. The pipeline should be compiled and fitted with the training data. Finally, the summary of the HeteroNN component should be printed. The code should also include a main function that accepts a configuration file as an argument and runs the pipeline.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def main():\n    # parties config\n    guest = 9999\n    host = 10000\n    arbiter = 10000\n\n    # specify input data name & namespace in database\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    guest_eval_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_eval_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    # initialize pipeline\n    pipeline = PipeLine()\n    # set job initiator\n    pipeline.set_initiator(role=\"guest\", party_id=guest)\n    # set participants information\n    pipeline.set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    # define Reader components to read in data\n    reader_0 = Reader(name=\"reader_0\")\n    # configure Reader for guest\n    reader_0.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_train_data)\n    # configure Reader for host\n    reader_0.get_party_instance(role=\"host\", party_id=host).component_param(table=host_train_data)\n\n    reader_1 = Reader(name=\"reader_1\")\n    reader_1.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_1.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n\n    # define DataIO components\n    dataio_0 = DataIO(name=\"dataio_0\")\n    dataio_1 = DataIO(name=\"dataio_1\")\n\n    # get DataIO party instance of guest\n    dataio_0_guest_party_instance = dataio_0.get_party_instance(role=\"guest\", party_id=guest)\n    # configure DataIO for guest\n    dataio_0_guest_party_instance.component_param(with_label=True, output_format=\"dense\")\n    # get and configure DataIO party instance of host\n    dataio_0.get_party_instance(role=\"host\", party_id=host).component_param(with_label=False)\n\n    # define Intersection components\n    intersection_0 = Intersection(name=\"intersection_0\")\n    intersection_1 = Intersection(name=\"intersection_1\")\n\n    # define HeteroLR component\n    hetero_lr_0 = HeteroLR(name=\"hetero_lr_0\", early_stop=\"weight_diff\", learning_rate=0.15, optimizer=\"rmsprop\",\n                           max_iter=10, early_stopping_rounds=2, validation_freqs=1)\n\n    # add components to pipeline, in order of task execution\n    pipeline.add_component(reader_0)\n    pipeline.add_component(reader_1)\n    pipeline.add_component(dataio_0, data=Data(data=reader_0.output.data))\n    # set dataio_1 to replicate model from dataio_0\n    pipeline.add_component(dataio_1, data=Data(data=reader_1.output.data), model=Model(dataio_0.output.model))\n    # set data input sources of intersection components\n    pipeline.add_component(intersection_0, data=Data(data=dataio_0.output.data))\n    pipeline.add_component(intersection_1, data=Data(data=dataio_1.output.data))\n    # set train & validate data of hetero_lr_0 component\n    pipeline.add_component(\n        hetero_lr_0,\n        data=Data(\n            train_data=intersection_0.output.data,\n            validate_data=intersection_1.output.data))\n\n    # compile pipeline once finished adding modules, this step will form conf and dsl files for running job\n    pipeline.compile()\n\n    # fit model\n    pipeline.fit()\n    # query component summary\n    import json\n    print(json.dumps(pipeline.get_component(\"hetero_lr_0\").get_summary(), indent=4))\n\n    # predict\n    # deploy required components\n    pipeline.deploy_component([dataio_0, intersection_0, hetero_lr_0])\n\n    # initiate predict pipeline\n    predict_pipeline = PipeLine()\n\n    reader_2 = Reader(name=\"reader_2\")\n    reader_2.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_2.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n    # add data reader onto predict pipeline\n    predict_pipeline.add_component(reader_2)\n    # add selected components from train pipeline onto predict pipeline\n    # specify data source\n    predict_pipeline.add_component(pipeline,\n                                   data=Data(predict_input={pipeline.dataio_0.input.data: reader_2.output.data}))\n    # run predict model\n    predict_pipeline.predict()\n\n# --- Snippet Separator ---\n\nclass HeteroNNParam(BaseParam):\n    \"\"\"\n    Parameters used for Homo Neural Network.\n\n    Args:\n        task_type: str, task type of hetero nn model, one of 'classification', 'regression'.\n        config_type: str, accept \"keras\" only.\n        bottom_nn_define: a dict represents the structure of bottom neural network.\n        interactive_layer_define: a dict represents the structure of interactive layer.\n        interactive_layer_lr: float, the learning rate of interactive layer.\n        top_nn_define: a dict represents the structure of top neural network.\n        optimizer: optimizer method, accept following types:\n            1. a string, one of \"Adadelta\", \"Adagrad\", \"Adam\", \"Adamax\", \"Nadam\", \"RMSprop\", \"SGD\"\n            2. a dict, with a required key-value pair keyed by \"optimizer\",\n                with optional key-value pairs such as learning rate.\n            defaults to \"SGD\"\n        loss:  str, a string to define loss function used\n        early_stopping_rounds: int, default: None\n        Will stop training if one metric doesn’t improve in last early_stopping_round rounds\n        metrics: list, default: None\n            Indicate when executing evaluation during train process, which metrics will be used. If not set,\n            default metrics for specific task type will be used. As for binary classification, default metrics are\n            ['auc', 'ks'], for regression tasks, default metrics are ['root_mean_squared_error', 'mean_absolute_error'],\n            [ACCURACY, PRECISION, RECALL] for multi-classification task\n        use_first_metric_only: bool, default: False\n            Indicate whether to use the first metric in `metrics` as the only criterion for early stopping judgement.\n        epochs: int, the maximum iteration for aggregation in training.\n        batch_size : int, batch size when updating model.\n            -1 means use all data in a batch. i.e. Not to use mini-batch strategy.\n            defaults to -1.\n        early_stop : str, accept 'diff' only in this version, default: 'diff'\n            Method used to judge converge or not.\n                a)\tdiff： Use difference of loss between two iterations to judge whether converge.\n        validation_freqs: None or positive integer or container object in python. Do validation in training process or Not.\n                  if equals None, will not do validation in train process;\n                  if equals positive integer, will validate data every validation_freqs epochs passes;\n                  if container object in python, will validate data if epochs belong to this container.\n                    e.g. validation_freqs = [10, 15], will validate data when epoch equals to 10 and 15.\n                  Default: None\n                  The default value is None, 1 is suggested. You can set it to a number larger than 1 in order to\n                  speed up training by skipping validation rounds. When it is larger than 1, a number which is\n                  divisible by \"epochs\" is recommended, otherwise, you will miss the validation scores\n                  of last training epoch.\n        floating_point_precision: None or integer, if not None, means use floating_point_precision-bit to speed up calculation,\n                                   e.g.: convert an x to round(x * 2**floating_point_precision) during Paillier operation, divide\n                                          the result by 2**floating_point_precision in the end.\n        drop_out_keep_rate: float, should betweend 0 and 1, if not equals to 1.0, will enabled drop out\n    \"\"\"\n\n    def __init__(self,\n                 task_type='classification',\n                 config_type=\"keras\",\n                 bottom_nn_define=None,\n                 top_nn_define=None,\n                 interactive_layer_define=None,\n                 interactive_layer_lr=0.9,\n                 optimizer='SGD',\n                 loss=None,\n                 epochs=100,\n                 batch_size=-1,\n                 early_stop=\"diff\",\n                 tol=1e-5,\n                 encrypt_param=EncryptParam(),\n                 encrypted_mode_calculator_param=EncryptedModeCalculatorParam(),\n                 predict_param=PredictParam(),\n                 cv_param=CrossValidationParam(),\n                 validation_freqs=None,\n                 early_stopping_rounds=None,\n                 metrics=None,\n                 use_first_metric_only=True,\n                 selector_param=SelectorParam(),\n                 floating_point_precision=23,\n                 drop_out_keep_rate=1.0,\n                 callback_param=CallbackParam()):\n        super(HeteroNNParam, self).__init__()\n\n        self.task_type = task_type\n        self.config_type = config_type\n        self.bottom_nn_define = bottom_nn_define\n        self.interactive_layer_define = interactive_layer_define\n        self.interactive_layer_lr = interactive_layer_lr\n        self.top_nn_define = top_nn_define\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.early_stop = early_stop\n        self.tol = tol\n        self.optimizer = optimizer\n        self.loss = loss\n        self.validation_freqs = validation_freqs\n        self.early_stopping_rounds = early_stopping_rounds\n        self.metrics = metrics or []\n        self.use_first_metric_only = use_first_metric_only\n\n        self.encrypt_param = copy.deepcopy(encrypt_param)\n        self.encrypted_model_calculator_param = encrypted_mode_calculator_param\n        self.predict_param = copy.deepcopy(predict_param)\n        self.cv_param = copy.deepcopy(cv_param)\n\n        self.selector_param = selector_param\n        self.floating_point_precision = floating_point_precision\n\n        self.drop_out_keep_rate = drop_out_keep_rate\n\n        self.callback_param = copy.deepcopy(callback_param)\n\n    def check(self):\n        self.optimizer = self._parse_optimizer(self.optimizer)\n        supported_config_type = [\"keras\"]\n\n        if self.task_type not in [\"classification\", \"regression\"]:\n            raise ValueError(\"config_type should be classification or regression\")\n\n        if self.config_type not in supported_config_type:\n            raise ValueError(f\"config_type should be one of {supported_config_type}\")\n\n        if not isinstance(self.tol, (int, float)):\n            raise ValueError(\"tol should be numeric\")\n\n        if not isinstance(self.epochs, int) or self.epochs <= 0:\n            raise ValueError(\"epochs should be a positive integer\")\n\n        if self.bottom_nn_define and not isinstance(self.bottom_nn_define, dict):\n            raise ValueError(\"bottom_nn_define should be a dict defining the structure of neural network\")\n\n        if self.top_nn_define and not isinstance(self.top_nn_define, dict):\n            raise ValueError(\"top_nn_define should be a dict defining the structure of neural network\")\n\n        if self.interactive_layer_define is not None and not isinstance(self.interactive_layer_define, dict):\n            raise ValueError(\n                \"the interactive_layer_define should be a dict defining the structure of interactive layer\")\n\n        if self.batch_size != -1:\n            if not isinstance(self.batch_size, int) \\\n                    or self.batch_size < consts.MIN_BATCH_SIZE:\n                raise ValueError(\n                    \" {} not supported, should be larger than 10 or -1 represent for all data\".format(self.batch_size))\n\n        if self.early_stop != \"diff\":\n            raise ValueError(\"early stop should be diff in this version\")\n\n        if self.validation_freqs is None:\n            pass\n        elif isinstance(self.validation_freqs, int):\n            if self.validation_freqs < 1:\n                raise ValueError(\"validation_freqs should be larger than 0 when it's integer\")\n        elif not isinstance(self.validation_freqs, collections.Container):\n            raise ValueError(\"validation_freqs should be None or positive integer or container\")\n\n        if self.early_stopping_rounds and not isinstance(self.early_stopping_rounds, int):\n            raise ValueError(\"early stopping rounds should be None or int larger than 0\")\n        if self.early_stopping_rounds and isinstance(self.early_stopping_rounds, int):\n            if self.early_stopping_rounds < 1:\n                raise ValueError(\"early stopping should be larger than 0 when it's integer\")\n            if not self.validation_freqs:\n                raise ValueError(\"If early stopping rounds is setting, validation_freqs should not be null\")\n\n        if self.metrics is not None and not isinstance(self.metrics, list):\n            raise ValueError(\"metrics should be a list\")\n\n        if not isinstance(self.use_first_metric_only, bool):\n            raise ValueError(\"use_first_metric_only should be a boolean\")\n\n        if self.floating_point_precision is not None and \\\n                (not isinstance(self.floating_point_precision, int) or\n                 self.floating_point_precision < 0 or self.floating_point_precision > 64):\n            raise ValueError(\"floating point precision should be null or a integer between 0 and 64\")\n\n        self.encrypt_param.check()\n        self.encrypted_model_calculator_param.check()\n        self.predict_param.check()\n\n    @staticmethod\n    def _parse_optimizer(opt):\n        \"\"\"\n        Examples:\n\n            1. \"optimize\": \"SGD\"\n            2. \"optimize\": {\n                \"optimizer\": \"SGD\",\n                \"learning_rate\": 0.05\n            }\n        \"\"\"\n\n        kwargs = {}\n        if isinstance(opt, str):\n            return SimpleNamespace(optimizer=opt, kwargs=kwargs)\n        elif isinstance(opt, dict):\n            optimizer = opt.get(\"optimizer\", kwargs)\n            if not optimizer:\n                raise ValueError(f\"optimizer config: {opt} invalid\")\n            kwargs = {k: v for k, v in opt.items() if k != \"optimizer\"}\n            return SimpleNamespace(optimizer=optimizer, kwargs=kwargs)\n        elif opt is None:\n            return None\n        else:\n            raise ValueError(f\"invalid type for optimize: {type(opt)}\")\n\n# --- Snippet Separator ---\n\nclass HeteroNNParam(BaseParam):\n    \"\"\"\n    Parameters used for Hetero Neural Network.\n\n    Args:\n        task_type: str, task type of hetero nn model, one of 'classification', 'regression'.\n        config_type: str, accept \"keras\" only.\n        bottom_nn_define: a dict represents the structure of bottom neural network.\n        interactive_layer_define: a dict represents the structure of interactive layer.\n        interactive_layer_lr: float, the learning rate of interactive layer.\n        top_nn_define: a dict represents the structure of top neural network.\n        optimizer: optimizer method, accept following types:\n            1. a string, one of \"Adadelta\", \"Adagrad\", \"Adam\", \"Adamax\", \"Nadam\", \"RMSprop\", \"SGD\"\n            2. a dict, with a required key-value pair keyed by \"optimizer\",\n                with optional key-value pairs such as learning rate.\n            defaults to \"SGD\"\n        loss:  str, a string to define loss function used\n        epochs: int, the maximum iteration for aggregation in training.\n        batch_size : int, batch size when updating model.\n            -1 means use all data in a batch. i.e. Not to use mini-batch strategy.\n            defaults to -1.\n        early_stop : str, accept 'diff' only in this version, default: 'diff'\n            Method used to judge converge or not.\n                a)\tdiff： Use difference of loss between two iterations to judge whether converge.\n        floating_point_precision: None or integer, if not None, means use floating_point_precision-bit to speed up calculation,\n                                   e.g.: convert an x to round(x * 2**floating_point_precision) during Paillier operation, divide\n                                          the result by 2**floating_point_precision in the end.\n        drop_out_keep_rate: float, should betweend 0 and 1, if not equals to 1.0, will enabled drop out\n        callback_param: CallbackParam object\n    \"\"\"\n\n    def __init__(self,\n                 task_type='classification',\n                 config_type=\"keras\",\n                 bottom_nn_define=None,\n                 top_nn_define=None,\n                 interactive_layer_define=None,\n                 interactive_layer_lr=0.9,\n                 optimizer='SGD',\n                 loss=None,\n                 epochs=100,\n                 batch_size=-1,\n                 early_stop=\"diff\",\n                 tol=1e-5,\n                 encrypt_param=EncryptParam(),\n                 encrypted_mode_calculator_param=EncryptedModeCalculatorParam(),\n                 predict_param=PredictParam(),\n                 cv_param=CrossValidationParam(),\n                 validation_freqs=None,\n                 early_stopping_rounds=None,\n                 metrics=None,\n                 use_first_metric_only=True,\n                 selector_param=SelectorParam(),\n                 floating_point_precision=23,\n                 drop_out_keep_rate=1.0,\n                 callback_param=CallbackParam()):\n        super(HeteroNNParam, self).__init__()\n\n        self.task_type = task_type\n        self.config_type = config_type\n        self.bottom_nn_define = bottom_nn_define\n        self.interactive_layer_define = interactive_layer_define\n        self.interactive_layer_lr = interactive_layer_lr\n        self.top_nn_define = top_nn_define\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.early_stop = early_stop\n        self.tol = tol\n        self.optimizer = optimizer\n        self.loss = loss\n        self.validation_freqs = validation_freqs\n        self.early_stopping_rounds = early_stopping_rounds\n        self.metrics = metrics or []\n        self.use_first_metric_only = use_first_metric_only\n\n        self.encrypt_param = copy.deepcopy(encrypt_param)\n        self.encrypted_model_calculator_param = encrypted_mode_calculator_param\n        self.predict_param = copy.deepcopy(predict_param)\n        self.cv_param = copy.deepcopy(cv_param)\n\n        self.selector_param = selector_param\n        self.floating_point_precision = floating_point_precision\n\n        self.drop_out_keep_rate = drop_out_keep_rate\n\n        self.callback_param = copy.deepcopy(callback_param)\n\n    def check(self):\n        self.optimizer = self._parse_optimizer(self.optimizer)\n        supported_config_type = [\"keras\"]\n\n        if self.task_type not in [\"classification\", \"regression\"]:\n            raise ValueError(\"config_type should be classification or regression\")\n\n        if self.config_type not in supported_config_type:\n            raise ValueError(f\"config_type should be one of {supported_config_type}\")\n\n        if not isinstance(self.tol, (int, float)):\n            raise ValueError(\"tol should be numeric\")\n\n        if not isinstance(self.epochs, int) or self.epochs <= 0:\n            raise ValueError(\"epochs should be a positive integer\")\n\n        if self.bottom_nn_define and not isinstance(self.bottom_nn_define, dict):\n            raise ValueError(\"bottom_nn_define should be a dict defining the structure of neural network\")\n\n        if self.top_nn_define and not isinstance(self.top_nn_define, dict):\n            raise ValueError(\"top_nn_define should be a dict defining the structure of neural network\")\n\n        if self.interactive_layer_define is not None and not isinstance(self.interactive_layer_define, dict):\n            raise ValueError(\n                \"the interactive_layer_define should be a dict defining the structure of interactive layer\")\n\n        if self.batch_size != -1:\n            if not isinstance(self.batch_size, int) \\\n                    or self.batch_size < consts.MIN_BATCH_SIZE:\n                raise ValueError(\n                    \" {} not supported, should be larger than 10 or -1 represent for all data\".format(self.batch_size))\n\n        if self.early_stop != \"diff\":\n            raise ValueError(\"early stop should be diff in this version\")\n\n        if self.metrics is not None and not isinstance(self.metrics, list):\n            raise ValueError(\"metrics should be a list\")\n\n        if self.floating_point_precision is not None and \\\n                (not isinstance(self.floating_point_precision, int) or\n                 self.floating_point_precision < 0 or self.floating_point_precision > 63):\n            raise ValueError(\"floating point precision should be null or a integer between 0 and 63\")\n\n        if not isinstance(self.drop_out_keep_rate, (float, int)) or self.drop_out_keep_rate < 0.0 or \\\n                self.drop_out_keep_rate > 1.0:\n            raise ValueError(\"drop_out_keep_rate should be in range [0.0, 1.0]\")\n\n        self.encrypt_param.check()\n        self.encrypted_model_calculator_param.check()\n        self.predict_param.check()\n        self.selector_param.check()\n\n        descr = \"hetero nn param's \"\n\n        for p in [\"early_stopping_rounds\", \"validation_freqs\",\n                  \"use_first_metric_only\"]:\n            if self._deprecated_params_set.get(p):\n                if \"callback_param\" in self.get_user_feeded():\n                    raise ValueError(f\"{p} and callback param should not be set simultaneously，\"\n                                     f\"{self._deprecated_params_set}, {self.get_user_feeded()}\")\n                else:\n                    self.callback_param.callbacks = [\"PerformanceEvaluate\"]\n                break\n\n        if self._warn_to_deprecate_param(\"validation_freqs\", descr, \"callback_param's 'validation_freqs'\"):\n            self.callback_param.validation_freqs = self.validation_freqs\n\n        if self._warn_to_deprecate_param(\"early_stopping_rounds\", descr, \"callback_param's 'early_stopping_rounds'\"):\n            self.callback_param.early_stopping_rounds = self.early_stopping_rounds\n\n        if self._warn_to_deprecate_param(\"metrics\", descr, \"callback_param's 'metrics'\"):\n            if self.metrics:\n                self.callback_param.metrics = self.metrics\n\n        if self._warn_to_deprecate_param(\"use_first_metric_only\", descr, \"callback_param's 'use_first_metric_only'\"):\n            self.callback_param.use_first_metric_only = self.use_first_metric_only\n\n    @staticmethod\n    def _parse_optimizer(opt):\n        \"\"\"\n        Examples:\n\n            1. \"optimize\": \"SGD\"\n            2. \"optimize\": {\n                \"optimizer\": \"SGD\",\n                \"learning_rate\": 0.05\n            }\n        \"\"\"\n\n        kwargs = {}\n        if isinstance(opt, str):\n            return SimpleNamespace(optimizer=opt, kwargs=kwargs)\n        elif isinstance(opt, dict):\n            optimizer = opt.get(\"optimizer\", kwargs)\n            if not optimizer:\n                raise ValueError(f\"optimizer config: {opt} invalid\")\n            kwargs = {k: v for k, v in opt.items() if k != \"optimizer\"}\n            return SimpleNamespace(optimizer=optimizer, kwargs=kwargs)\n        elif opt is None:\n            return None\n        else:\n            raise ValueError(f\"invalid type for optimize: {type(opt)}\")\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the fate library to create a pipeline for a binary classification task using a heterogenous neural network (HeteroNN). The pipeline should include the following components: a Reader to read the training data, a DataTransform to preprocess the data, an Intersection to find the common instances between the guest and host data, a HeteroNN for the model training, and an Evaluation for model evaluation. The HeteroNN should be configured with specific parameters such as epochs, learning rate, batch size, and task type. The neural network should include a guest bottom model, a guest top model, a host bottom model, and an interactive layer. The models should be defined using the torch library and added to the HeteroNN. The HeteroNN should be compiled with a specific optimizer and loss function. The pipeline should be compiled and fitted with the training data. Finally, the summary of the HeteroNN component should be printed. The code should also include a main function that accepts a configuration file as an argument and runs the pipeline.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 123, "repo_full_name": "manimcommunity__manim", "instruction": "Generate code that creates several scenes using the manim library. The first scene should display a LaTeX title and a mathematical equation, then transform the title and fade out the equation. Afterwards, it should create a grid, display a title for it, apply a non-linear function to the grid, and transform the grid title. The second scene should create a square, transform it into a circle, and then fade it out. The third scene should create a square and apply a pointwise function to it. The fourth scene should display a text and a mathematical equation. The fifth scene should create a square and a decimal number that updates its position and value based on the square's position. The sixth scene should create several shapes and a large pi symbol, and then spiral them in and fade them out. The last scene should create three triangles with different line joints.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def add_title(self, title, scale_factor=1.5, animate=False):\n        \"\"\"\n        Adds a title, after scaling it, adding a background rectangle,\n        moving it to the top and adding it to foreground_mobjects adding\n        it as a local variable of self. Returns the Scene.\n\n        Parameters\n        ----------\n        title : str,TexMobject,TextMobject\n            What the title should be.\n\n        scale_factor : int, float, optional\n            How much the title should be scaled by.\n\n        animate : bool\n            Whether or not to animate the addition.\n\n        Returns\n        -------\n        LinearTransformationScene\n            The scene with the title added to it.\n        \"\"\"\n        if not isinstance(title, Mobject):\n            title = TextMobject(title).scale(scale_factor)\n        title.to_edge(UP)\n        title.add_background_rectangle()\n        if animate:\n            self.play(Write(title))\n        self.add_foreground_mobject(title)\n        self.title = title\n        return self\n\n# --- Snippet Separator ---\n\nclass Anchor(Node):\n    \"\"\"\n    Anchor is a node derives parts of its transform from some other\n    coordinate system in the scene.\n\n    The purpose is to allow children of an Anchor to draw using a position\n    (and optionally rotation) specified by one coordinate system, and scaling/\n    projection specified by another.\n\n    For example, text attached to a point in a 3D scene should be drawn in\n    a coordinate system with a simple relationship to the screen pixels, but\n    should derive its location from a position within the 3D coordinate\n    system::\n\n        root = Box()\n        view = ViewBox(parent=box)\n        plot = LineVisual(parent=ViewBox)\n        anchor = Anchor(parent=root, anchor_to=plot, anchor_pos=(10, 0))\n        text = Text(parent=anchor,\n                    text=\"Always points to (10,0) relative to line.\")\n\n    \"\"\"\n\n# --- Snippet Separator ---\n\nclass LinearTransformationScene(VectorScene):\n    \"\"\"\n    This scene contains special methods that make it\n    especially suitable for showing Linear Transformations.\n    \"\"\"\n\n    CONFIG = {\n        \"include_background_plane\": True,\n        \"include_foreground_plane\": True,\n        \"foreground_plane_kwargs\": {\n            \"x_max\": config[\"frame_width\"] / 2,\n            \"x_min\": -config[\"frame_width\"] / 2,\n            \"y_max\": config[\"frame_width\"] / 2,\n            \"y_min\": -config[\"frame_width\"] / 2,\n            \"faded_line_ratio\": 0,\n        },\n        \"background_plane_kwargs\": {\n            \"color\": GREY,\n            \"axis_config\": {\"stroke_color\": LIGHT_GREY,},\n            \"axis_config\": {\"color\": GREY,},\n            \"background_line_style\": {\"stroke_color\": GREY, \"stroke_width\": 1,},\n        },\n        \"show_coordinates\": False,\n        \"show_basis_vectors\": True,\n        \"basis_vector_stroke_width\": 6,\n        \"i_hat_color\": X_COLOR,\n        \"j_hat_color\": Y_COLOR,\n        \"leave_ghost_vectors\": False,\n        \"t_matrix\": [[3, 0], [1, 2]],\n    }\n\n    def setup(self):\n        # The has_already_setup attr is to not break all the old Scenes\n        if hasattr(self, \"has_already_setup\"):\n            return\n        self.has_already_setup = True\n        self.background_mobjects = []\n        self.foreground_mobjects = []\n        self.transformable_mobjects = []\n        self.moving_vectors = []\n        self.transformable_labels = []\n        self.moving_mobjects = []\n\n        self.t_matrix = np.array(self.t_matrix)\n        self.background_plane = NumberPlane(**self.background_plane_kwargs)\n\n        if self.show_coordinates:\n            self.background_plane.add_coordinates()\n        if self.include_background_plane:\n            self.add_background_mobject(self.background_plane)\n        if self.include_foreground_plane:\n            self.plane = NumberPlane(**self.foreground_plane_kwargs)\n            self.add_transformable_mobject(self.plane)\n        if self.show_basis_vectors:\n            self.basis_vectors = self.get_basis_vectors(\n                i_hat_color=self.i_hat_color, j_hat_color=self.j_hat_color,\n            )\n            self.moving_vectors += list(self.basis_vectors)\n            self.i_hat, self.j_hat = self.basis_vectors\n            self.add(self.basis_vectors)\n\n    def add_special_mobjects(self, mob_list, *mobs_to_add):\n        \"\"\"\n        Adds mobjects to a separate list that can be tracked,\n        if these mobjects have some extra importance.\n\n        Parameters\n        ----------\n        mob_list : list\n            The special list to which you want to add\n            these mobjects.\n\n        *mobs_to_add : Mobject\n            The mobjects to add.\n\n        \"\"\"\n        for mobject in mobs_to_add:\n            if mobject not in mob_list:\n                mob_list.append(mobject)\n                self.add(mobject)\n\n    def add_background_mobject(self, *mobjects):\n        \"\"\"\n        Adds the mobjects to the special list\n        self.background_mobjects.\n\n        Parameters\n        ----------\n        *mobjects : Mobject\n            The mobjects to add to the list.\n        \"\"\"\n        self.add_special_mobjects(self.background_mobjects, *mobjects)\n\n    # TODO, this conflicts with Scene.add_fore\n    def add_foreground_mobject(self, *mobjects):\n        \"\"\"\n        Adds the mobjects to the special list\n        self.foreground_mobjects.\n\n        Parameters\n        ----------\n        *mobjects : Mobject\n            The mobjects to add to the list\n        \"\"\"\n        self.add_special_mobjects(self.foreground_mobjects, *mobjects)\n\n    def add_transformable_mobject(self, *mobjects):\n        \"\"\"\n        Adds the mobjects to the special list\n        self.transformable_mobjects.\n\n        Parameters\n        ----------\n        *mobjects : Mobject\n            The mobjects to add to the list.\n        \"\"\"\n        self.add_special_mobjects(self.transformable_mobjects, *mobjects)\n\n    def add_moving_mobject(self, mobject, target_mobject=None):\n        \"\"\"\n        Adds the mobject to the special list\n        self.moving_mobject, and adds a property\n        to the mobject called mobject.target, which\n        keeps track of what the mobject will move to\n        or become etc.\n\n        Parameters\n        ----------\n        mobject : Mobject\n            The mobjects to add to the list\n\n        target_mobject : Mobject, optional\n            What the moving_mobject goes to, etc.\n        \"\"\"\n        mobject.target = target_mobject\n        self.add_special_mobjects(self.moving_mobjects, mobject)\n\n    def get_unit_square(self, color=YELLOW, opacity=0.3, stroke_width=3):\n        \"\"\"\n        Returns a unit square for the current NumberPlane.\n\n        Parameters\n        ----------\n        color : str, optional\n            The string of the hex color code of the color wanted.\n\n        opacity : float, int, optional\n            The opacity of the square\n\n        stroke_width : int, float, optional\n            The stroke_width in pixels of the border of the square\n\n        Returns\n        -------\n        Square\n        \"\"\"\n        square = self.square = Rectangle(\n            color=color,\n            width=self.plane.get_x_unit_size(),\n            height=self.plane.get_y_unit_size(),\n            stroke_color=color,\n            stroke_width=stroke_width,\n            fill_color=color,\n            fill_opacity=opacity,\n        )\n        square.move_to(self.plane.coords_to_point(0, 0), DL)\n        return square\n\n    def add_unit_square(self, animate=False, **kwargs):\n        \"\"\"\n        Adds a unit square to the scene via\n        self.get_unit_square.\n\n        Parameters\n        ----------\n        animate (bool)\n            Whether or not to animate the addition\n            with DrawBorderThenFill.\n        **kwargs\n            Any valid keyword arguments of\n            self.get_unit_square()\n\n        Returns\n        -------\n        Square\n            The unit square.\n        \"\"\"\n        square = self.get_unit_square(**kwargs)\n        if animate:\n            self.play(\n                DrawBorderThenFill(square), Animation(Group(*self.moving_vectors))\n            )\n        self.add_transformable_mobject(square)\n        self.bring_to_front(*self.moving_vectors)\n        self.square = square\n        return self\n\n    def add_vector(self, vector, color=YELLOW, **kwargs):\n        \"\"\"\n        Adds a vector to the scene, and puts it in the special\n        list self.moving_vectors.\n\n        Parameters\n        ----------\n        vector : Arrow,list,tuple,np.ndarray\n            It can be a pre-made graphical vector, or the\n            coordinates of one.\n\n        color : str\n            The string of the hex color of the vector.\n            This is only taken into consideration if\n            'vector' is not an Arrow. Defaults to YELLOW.\n\n        **kwargs\n            Any valid keyword argument of VectorScene.add_vector.\n\n        Returns\n        -------\n        Arrow\n            The arrow representing the vector.\n        \"\"\"\n        vector = VectorScene.add_vector(self, vector, color=color, **kwargs)\n        self.moving_vectors.append(vector)\n        return vector\n\n    def write_vector_coordinates(self, vector, **kwargs):\n        \"\"\"\n        Returns a column matrix indicating the vector coordinates,\n        after writing them to the screen, and adding them to the\n        special list self.foreground_mobjects\n\n        Parameters\n        ----------\n        vector : Arrow\n            The arrow representing the vector.\n\n        **kwargs\n            Any valid keyword arguments of VectorScene.write_vector_coordinates\n\n        Returns\n        -------\n        Matrix\n            The column matrix representing the vector.\n        \"\"\"\n        coords = VectorScene.write_vector_coordinates(self, vector, **kwargs)\n        self.add_foreground_mobject(coords)\n        return coords\n\n    def add_transformable_label(\n        self, vector, label, transformation_name=\"L\", new_label=None, **kwargs\n    ):\n        \"\"\"\n        Method for creating, and animating the addition of\n        a transformable label for the vector.\n\n        Parameters\n        ----------\n        vector : Vector\n            The vector for which the label must be added.\n\n        label : TexMobject,str\n            The TexMobject/string of the label.\n\n        transformation_name : str, TexMobject, optional\n            The name to give the transformation as a label.\n\n        new_label : TexMobject,str, optional\n            What the label should display after a Linear Transformation\n\n        **kwargs\n            Any valid keyword argument of get_vector_label\n\n        Returns\n        -------\n        TexMobject\n            The TexMobject of the label.\n        \"\"\"\n        label_mob = self.label_vector(vector, label, **kwargs)\n        if new_label:\n            label_mob.target_text = new_label\n        else:\n            label_mob.target_text = \"%s(%s)\" % (\n                transformation_name,\n                label_mob.get_tex_string(),\n            )\n        label_mob.vector = vector\n        label_mob.kwargs = kwargs\n        if \"animate\" in label_mob.kwargs:\n            label_mob.kwargs.pop(\"animate\")\n        self.transformable_labels.append(label_mob)\n        return label_mob\n\n    def add_title(self, title, scale_factor=1.5, animate=False):\n        \"\"\"\n        Adds a title, after scaling it, adding a background rectangle,\n        moving it to the top and adding it to foreground_mobjects adding\n        it as a local variable of self. Returns the Scene.\n\n        Parameters\n        ----------\n        title : str,TexMobject,TextMobject\n            What the title should be.\n\n        scale_factor : int, float, optional\n            How much the title should be scaled by.\n\n        animate : bool\n            Whether or not to animate the addition.\n\n        Returns\n        -------\n        LinearTransformationScene\n            The scene with the title added to it.\n        \"\"\"\n        if not isinstance(title, Mobject):\n            title = TextMobject(title).scale(scale_factor)\n        title.to_edge(UP)\n        title.add_background_rectangle()\n        if animate:\n            self.play(Write(title))\n        self.add_foreground_mobject(title)\n        self.title = title\n        return self\n\n    def get_matrix_transformation(self, matrix):\n        \"\"\"\n        Returns a function corresponding to the linear\n        transformation represented by the matrix passed.\n\n        Parameters\n        ----------\n        matrix : np.ndarray, list, tuple\n            The matrix.\n        \"\"\"\n        return self.get_transposed_matrix_transformation(np.array(matrix).T)\n\n    def get_transposed_matrix_transformation(self, transposed_matrix):\n        \"\"\"\n        Returns a function corresponding to the linear\n        transformation represented by the transposed\n        matrix passed.\n\n        Parameters\n        ----------\n        matrix : np.ndarray, list, tuple\n            The matrix.\n        \"\"\"\n        transposed_matrix = np.array(transposed_matrix)\n        if transposed_matrix.shape == (2, 2):\n            new_matrix = np.identity(3)\n            new_matrix[:2, :2] = transposed_matrix\n            transposed_matrix = new_matrix\n        elif transposed_matrix.shape != (3, 3):\n            raise Exception(\"Matrix has bad dimensions\")\n        return lambda point: np.dot(point, transposed_matrix)\n\n    def get_piece_movement(self, pieces):\n        \"\"\"\n        This method returns an animation that moves an arbitrary\n        mobject in \"pieces\" to its corresponding .target value.\n        If self.leave_ghost_vectors is True, ghosts of the original\n        positions/mobjects are left on screen\n\n        Parameters\n        ----------\n        pieces : list, tuple, np.array\n            The pieces for which the movement must be shown.\n\n        Returns\n        -------\n        Animation\n            The animation of the movement.\n        \"\"\"\n        start = VGroup(*pieces)\n        target = VGroup(*[mob.target for mob in pieces])\n        if self.leave_ghost_vectors:\n            self.add(start.copy().fade(0.7))\n        return Transform(start, target, lag_ratio=0)\n\n    def get_moving_mobject_movement(self, func):\n        \"\"\"\n        This method returns an animation that moves a mobject\n        in \"self.moving_mobjects\"  to its corresponding .target value.\n        func is a function that determines where the .target goes.\n\n        Parameters\n        ----------\n\n        func : function\n            The function that determines where the .target of\n            the moving mobject goes.\n\n        Returns\n        -------\n        Animation\n            The animation of the movement.\n        \"\"\"\n        for m in self.moving_mobjects:\n            if m.target is None:\n                m.target = m.copy()\n            target_point = func(m.get_center())\n            m.target.move_to(target_point)\n        return self.get_piece_movement(self.moving_mobjects)\n\n    def get_vector_movement(self, func):\n        \"\"\"\n        This method returns an animation that moves a mobject\n        in \"self.moving_vectors\"  to its corresponding .target value.\n        func is a function that determines where the .target goes.\n\n        Parameters\n        ----------\n\n        func : function\n            The function that determines where the .target of\n            the moving mobject goes.\n\n        Returns\n        -------\n        Animation\n            The animation of the movement.\n        \"\"\"\n        for v in self.moving_vectors:\n            v.target = Vector(func(v.get_end()), color=v.get_color())\n            norm = get_norm(v.target.get_end())\n            if norm < 0.1:\n                v.target.get_tip().scale_in_place(norm)\n        return self.get_piece_movement(self.moving_vectors)\n\n    def get_transformable_label_movement(self):\n        \"\"\"\n        This method returns an animation that moves all labels\n        in \"self.transformable_labels\" to its corresponding .target .\n\n        Returns\n        -------\n        Animation\n            The animation of the movement.\n        \"\"\"\n        for l in self.transformable_labels:\n            l.target = self.get_vector_label(l.vector.target, l.target_text, **l.kwargs)\n        return self.get_piece_movement(self.transformable_labels)\n\n    def apply_matrix(self, matrix, **kwargs):\n        \"\"\"\n        Applies the transformation represented by the\n        given matrix to the number plane, and each vector/similar\n        mobject on it.\n\n        Parameters\n        ----------\n        matrix : np.ndarray, list, tuple\n            The matrix.\n        **kwargs\n            Any valid keyword argument of self.apply_transposed_matrix()\n        \"\"\"\n        self.apply_transposed_matrix(np.array(matrix).T, **kwargs)\n\n    def apply_inverse(self, matrix, **kwargs):\n        \"\"\"\n        This method applies the linear transformation\n        represented by the inverse of the passed matrix\n        to the number plane, and each vector/similar mobject on it.\n\n        Parameters\n        ----------\n        matrix : np.ndarray, list, tuple\n            The matrix whose inverse is to be applied.\n        **kwargs\n            Any valid keyword argument of self.apply_matrix()\n        \"\"\"\n        self.apply_matrix(np.linalg.inv(matrix), **kwargs)\n\n    def apply_transposed_matrix(self, transposed_matrix, **kwargs):\n        \"\"\"\n        Applies the transformation represented by the\n        given transposed matrix to the number plane,\n        and each vector/similar mobject on it.\n\n        Parameters\n        ----------\n        matrix : np.ndarray, list, tuple\n            The matrix.\n        **kwargs\n            Any valid keyword argument of self.apply_function()\n        \"\"\"\n        func = self.get_transposed_matrix_transformation(transposed_matrix)\n        if \"path_arc\" not in kwargs:\n            net_rotation = np.mean(\n                [angle_of_vector(func(RIGHT)), angle_of_vector(func(UP)) - np.pi / 2]\n            )\n            kwargs[\"path_arc\"] = net_rotation\n        self.apply_function(func, **kwargs)\n\n    def apply_inverse_transpose(self, t_matrix, **kwargs):\n        \"\"\"\n        Applies the inverse of the transformation represented\n        by the given transposed matrix to the number plane and each\n        vector/similar mobject on it.\n\n        Parameters\n        ----------\n        matrix : np.ndarray, list, tuple\n            The matrix.\n        **kwargs\n            Any valid keyword argument of self.apply_transposed_matrix()\n        \"\"\"\n        t_inv = np.linalg.inv(np.array(t_matrix).T).T\n        self.apply_transposed_matrix(t_inv, **kwargs)\n\n    def apply_nonlinear_transformation(self, function, **kwargs):\n        \"\"\"\n        Applies the non-linear transformation represented\n        by the given function to the number plane and each\n        vector/similar mobject on it.\n\n        Parameters\n        ----------\n        function : Function\n            The function.\n        **kwargs\n            Any valid keyword argument of self.apply_function()\n        \"\"\"\n        self.plane.prepare_for_nonlinear_transform()\n        self.apply_function(function, **kwargs)\n\n    def apply_function(self, function, added_anims=[], **kwargs):\n        \"\"\"\n        Applies the given function to each of the mobjects in\n        self.transformable_mobjects, and plays the animation showing\n        this.\n\n        Parameters\n        ----------\n        function : Function\n            The function that affects each point\n            of each mobject in self.transformable_mobjects.\n\n        added_anims : list, optional\n            Any other animations that need to be played\n            simulataneously with this.\n\n        **kwargs\n            Any valid keyword argument of a self.play() call.\n        \"\"\"\n        if \"run_time\" not in kwargs:\n            kwargs[\"run_time\"] = 3\n        anims = (\n            [\n                ApplyPointwiseFunction(function, t_mob)\n                for t_mob in self.transformable_mobjects\n            ]\n            + [\n                self.get_vector_movement(function),\n                self.get_transformable_label_movement(),\n                self.get_moving_mobject_movement(function),\n            ]\n            + [Animation(f_mob) for f_mob in self.foreground_mobjects]\n            + added_anims\n        )\n        self.play(*anims, **kwargs)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates several scenes using the manim library. The first scene should display a LaTeX title and a mathematical equation, then transform the title and fade out the equation. Afterwards, it should create a grid, display a title for it, apply a non-linear function to the grid, and transform the grid title. The second scene should create a square, transform it into a circle, and then fade it out. The third scene should create a square and apply a pointwise function to it. The fourth scene should display a text and a mathematical equation. The fifth scene should create a square and a decimal number that updates its position and value based on the square's position. The sixth scene should create several shapes and a large pi symbol, and then spiral them in and fade them out. The last scene should create three triangles with different line joints.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 124, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that performs the following tasks using the pyaedt library:\n\n1. Creates a temporary folder and prints its path.\n2. Downloads an example file into the temporary folder.\n3. Sets the non-graphical mode and launches AEDT in graphical mode using SI units.\n4. Initializes AEDT and launches HFSS 3D Layout.\n5. If the AEDT file already exists, it removes it and saves the project in the temporary folder.\n6. Prints the boundaries from the setups object.\n7. Hides all nets and then makes only two specified nets visible.\n8. Plots the two specified nets.\n9. Makes all layers visible.\n10. Changes the color of a specified layer.\n11. Disables the visibility of components for the top and bottom layers.\n12. Fits all so that all can be visualized.\n13. Closes the project and releases the desktop.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Maxwell3d(Maxwell, FieldAnalysis3D, object):\n    \"\"\"Provides the Maxwell 3D application interface.\n\n    This class allows you to connect to an existing Maxwell 3D design or create a\n    new Maxwell 3D design if one does not exist.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open. The default is ``None``, in which\n        case an attempt is made to get an active project. If no\n        projects are present, an empty project is created.\n    designname : str, optional\n        Name of the design to select. The default is ``None``, in\n        which case an attempt is made to get an active design. If no\n        designs are present, an empty design is created.\n    solution_type : str, optional\n        Solution type to apply to the design. The default is\n        ``None``, in which case the default type is applied.\n    setup_name : str, optional\n        Name of the setup to use as the nominal. The default is\n        ``None``, in which case the active setup is used or\n        nothing is used.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``, in which case\n        the active version or latest installed version is used. This\n        parameter is ignored when Script is launched within AEDT.\n    non_graphical : bool, optional\n        Whether to launch AEDT in non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in graphical\n        mode. This parameter is ignored when a script is launched within\n        AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine. The default is ``True``. This parameter is ignored\n        when Script is launched within AEDT.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit. The default is ``False``.\n    student_version : bool, optional\n        Whether to open the AEDT student version. The default is\n        ``False``. This parameter is ignored when Script is launched\n        within AEDT.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of Maxwell 3D and open the specified\n    project, which is named ``mymaxwell.aedt``.\n\n    >>> from pyaedt import Maxwell3d\n    >>> aedtapp = Maxwell3d(\"mymaxwell.aedt\")\n    pyaedt info: Added design ...\n\n    Create an instance of Maxwell 3D using the 2021 R1 release and open\n    the specified project, which is named ``mymaxwell2.aedt``.\n\n    >>> aedtapp = Maxwell3d(specified_version=\"2021.2\", projectname=\"mymaxwell2.aedt\")\n    pyaedt info: Added design ...\n\n    \"\"\"\n\n    @property  # for legacy purposes\n    def dim(self):\n        \"\"\"Dimensions.\"\"\"\n        return \"3D\"\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        setup_name=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=False,\n        close_on_exit=False,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n        \"\"\"\n        Initialize the ``Maxwell`` class.\n        \"\"\"\n        self.is3d = True\n        FieldAnalysis3D.__init__(\n            self,\n            \"Maxwell 3D\",\n            projectname,\n            designname,\n            solution_type,\n            setup_name,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine,\n            port,\n            aedt_process_id,\n        )\n        Maxwell.__init__(self)\n\n# --- Snippet Separator ---\n\ndef __init__(\n        self,\n        exclude_vars=None,\n        model=None,\n        global_model_file_name=DefaultCheckpointFileName.GLOBAL_MODEL,\n        best_global_model_file_name=DefaultCheckpointFileName.BEST_GLOBAL_MODEL,\n        source_ckpt_file_full_name=None,\n    ):\n        \"\"\"Persist pytorch-based model to/from file system.\n\n        This Model Persistor tries to load PT model data in following three ways:\n\n            1. Load from a specified source checkpoint file\n            2. Load from a location from the app folder\n            3. Load from a torch model object\n\n        The Persistor tries method 1 first if the source_ckpt_file_full_name is specified;\n        If source_ckpt_file_full_name is not specified, it tries the method 2;\n        If no checkpoint location is specified in the app folder, it tries method 3.\n\n        Method 2 - Load from a location from the app folder\n        It is assumed that the app folder must contain the environments.json file. Among other things, this\n        JSON file must specify where to find the checkpoint file. It does so with two JSON elements:\n\n            - APP_CKPT_DIR: specifies the folder (within the app) where the checkpoint file resides.\n            - APP_CKPT: specifies the base file name of the checkpoint\n\n        Here is an example of the environments.json content::\n\n            {\n                \"APP_CKPT_DIR\": \"model\",\n                \"APP_CKPT\": \"pretrained_model.pt\"\n            }\n\n        In this example, the checkpoint file is located in the \"model\" folder within the app and is named\n        pretrained_model.pt.\n\n        Method 3 - Load from a torch model object. In this case, the 'model' arg must be a valid torch\n        model, or the component ID of a valid torch model included in the \"components\" section of\n        your config_fed_server.json.\n\n        If all 3 methods fail, system_panic() is called.\n\n        If checkpoint folder name is specified, then global model and best global model will be saved to it;\n        Otherwise they will be saved directly in the app folder.\n\n        Args:\n            exclude_vars (str, optional): regex expression specifying weight vars to be excluded from training. Defaults to None.\n            model (str, optional): torch model object or component id of the model object. Defaults to None.\n            global_model_file_name (str, optional): file name for saving global model. Defaults to DefaultCheckpointFileName.GLOBAL_MODEL.\n            best_global_model_file_name (str, optional): file name for saving best global model. Defaults to DefaultCheckpointFileName.BEST_GLOBAL_MODEL.\n            source_ckpt_file_full_name (str, optional): full file name for source model checkpoint file. Defaults to None.\n\n        Raises:\n            ValueError: when source_ckpt_file_full_name does not exist\n        \"\"\"\n        super().__init__()\n        self.exclude_vars = re.compile(exclude_vars) if exclude_vars else None\n        self.model = model\n        self.log_dir = None\n        self.ckpt_preload_path = None\n        self.persistence_manager = None\n        self.ckpt_dir_env_key = EnvironmentKey.CHECKPOINT_DIR\n        self.ckpt_file_name_env_key = EnvironmentKey.CHECKPOINT_FILE_NAME\n        self.global_model_file_name = global_model_file_name\n        self.best_global_model_file_name = best_global_model_file_name\n        self.source_ckpt_file_full_name = source_ckpt_file_full_name\n\n        self.default_train_conf = None\n\n        if source_ckpt_file_full_name and not os.path.exists(source_ckpt_file_full_name):\n            raise ValueError(\"specified source checkpoint model file {} does not exist\")\n\n# --- Snippet Separator ---\n\ndef __init__(self, template_file):\n        \"\"\"Manages the folder structure for provisioned projects.\n\n        Sets the template_file containing scripts and configs to put into startup folders, creates directories for the\n        participants, and moves the provisioned project to the final location at the end\n        ($WORKSPACE/$PROJECT_NAME/prod_XX). WorkspaceBuilder manages and sets the number in prod_XX by incrementing from\n        the last time provision was run for this project in this workspace, starting with 00 to a max of 99.\n\n        Each time the provisioning tool runs, it requires a workspace folder in the local file system.  The workspace\n        will have the following folder structure:\n\n        .. code-block:: text\n\n          $WORKSPACE/    <--- this is assigned by -w option of provision command (default is workspace)\n            $PROJECT_NAME/  <--- this is the name value in the project.yml file\n              prod_00/   <--- a new prod_NN folder is created if provision does not have any errors.\n              prod_01/\n              ...\n              resources/ <--- this folder stores resources for other builders to load\n              state/     <--- this folder stores persistent information (such as certificates) so subsequent runs of the provision command can load the state back.\n              wip/  <--- this is only used during runtime, and will be removed when the provision command exits\n\n        Args:\n            template_file: name of template file containing scripts and configs to put into startup folders\n        \"\"\"\n        self.template_file = template_file\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs the following tasks using the pyaedt library:\n\n1. Creates a temporary folder and prints its path.\n2. Downloads an example file into the temporary folder.\n3. Sets the non-graphical mode and launches AEDT in graphical mode using SI units.\n4. Initializes AEDT and launches HFSS 3D Layout.\n5. If the AEDT file already exists, it removes it and saves the project in the temporary folder.\n6. Prints the boundaries from the setups object.\n7. Hides all nets and then makes only two specified nets visible.\n8. Plots the two specified nets.\n9. Makes all layers visible.\n10. Changes the color of a specified layer.\n11. Disables the visibility of components for the top and bottom layers.\n12. Fits all so that all can be visualized.\n13. Closes the project and releases the desktop.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 125, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that creates a cell using the pyscf.pbc.gto.Cell() function, with specified atom positions, basis, pseudo, a, unit, and verbosity. Then, perform KHF and KMP2 calculations with 2x2x2 k-points, and print the KMP2 energy per unit cell. Repeat the KHF and KMP2 calculations for a single k-point calculation. Then, perform a single k-point calculation using the RHF method, and print the RMP2 energy per unit cell at the k-point. Also, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Convert the RHF object to UHF and GHF objects, and for each, perform a UMP2 and GMP2 calculation respectively, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Print the UMP2 and GMP2 energy per unit cell at the k-point, and the total energy based on the MP2 density matrices.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def add_mm_charges(scf_method, atoms_or_coords, charges, unit=None):\n    '''Embedding the one-electron (non-relativistic) potential generated by MM\n    point charges into QM Hamiltonian.\n\n    The total energy includes the regular QM energy, the interaction between\n    the nuclei in QM region and the MM charges, and the static Coulomb\n    interaction between the electron density and the MM charges. It does not\n    include the static Coulomb interactions of the MM point charges, the MM\n    energy, the vdw interaction or other bonding/non-bonding effects between\n    QM region and MM particles.\n\n    Args:\n        scf_method : a HF or DFT object\n\n        atoms_or_coords : 2D array, shape (N,3)\n            MM particle coordinates\n        charges : 1D array\n            MM particle charges\n    Kwargs:\n        unit : str\n            Bohr, AU, Ang (case insensitive). Default is the same to mol.unit\n\n    Returns:\n        Same method object as the input scf_method with modified 1e Hamiltonian\n\n    Note:\n        1. if MM charge and X2C correction are used together, function mm_charge\n        needs to be applied after X2C decoration (.x2c method), eg\n        mf = mm_charge(scf.RHF(mol).x2c()), [(0.5,0.6,0.8)], [-0.5]).\n        2. Once mm_charge function is applied on the SCF object, it\n        affects all the post-HF calculations eg MP2, CCSD, MCSCF etc\n\n    Examples:\n\n    >>> mol = gto.M(atom='H 0 0 0; F 0 0 1', basis='ccpvdz', verbose=0)\n    >>> mf = mm_charge(dft.RKS(mol), [(0.5,0.6,0.8)], [-0.3])\n    >>> mf.kernel()\n    -101.940495711284\n    '''\n    mol = scf_method.mol\n    if unit is None:\n        unit = mol.unit\n    mm_mol = mm_mole.create_mm_mol(atoms_or_coords, charges, unit)\n    return qmmm_for_scf(scf_method, mm_mol)\n\n# --- Snippet Separator ---\n\ndef dip_moment(cell, dm_kpts, unit='Debye', verbose=logger.NOTE,\n               grids=None, rho=None, kpts=np.zeros((1,3))):\n    ''' Dipole moment in the unit cell.\n\n    Args:\n         cell : an instance of :class:`Cell`\n\n         dm_kpts (two lists of ndarrays) : KUHF density matrices of k-points\n\n    Return:\n        A list: the dipole moment on x, y and z components\n    '''\n    dm_kpts = dm_kpts[0] + dm_kpts[1]\n    return khf.dip_moment(cell, dm_kpts, unit, verbose, grids, rho, kpts)\n\n# --- Snippet Separator ---\n\ndef get_jk(mf, cell, dm, hermi=1, vhfopt=None, kpt=np.zeros(3),\n           kpts_band=None, with_j=True, with_k=True, omega=None, **kwargs):\n    '''Get the Coulomb (J) and exchange (K) AO matrices for the given density matrix.\n\n    Args:\n        dm : ndarray or list of ndarrays\n            A density matrix or a list of density matrices\n\n    Kwargs:\n        hermi : int\n            Whether J, K matrix is hermitian\n            | 0 : no hermitian or symmetric\n            | 1 : hermitian\n            | 2 : anti-hermitian\n        vhfopt :\n            A class which holds precomputed quantities to optimize the\n            computation of J, K matrices\n        kpt : (3,) ndarray\n            The \"inner\" dummy k-point at which the DM was evaluated (or\n            sampled).\n        kpts_band : (3,) ndarray or (*,3) ndarray\n            An arbitrary \"band\" k-point at which J and K are evaluated.\n\n    Returns:\n        The function returns one J and one K matrix, corresponding to the input\n        density matrix (both order and shape).\n    '''\n    return df.FFTDF(cell).get_jk(dm, hermi, kpt, kpts_band, with_j, with_k,\n                                 omega, exxdiv=mf.exxdiv)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a cell using the pyscf.pbc.gto.Cell() function, with specified atom positions, basis, pseudo, a, unit, and verbosity. Then, perform KHF and KMP2 calculations with 2x2x2 k-points, and print the KMP2 energy per unit cell. Repeat the KHF and KMP2 calculations for a single k-point calculation. Then, perform a single k-point calculation using the RHF method, and print the RMP2 energy per unit cell at the k-point. Also, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Convert the RHF object to UHF and GHF objects, and for each, perform a UMP2 and GMP2 calculation respectively, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Print the UMP2 and GMP2 energy per unit cell at the k-point, and the total energy based on the MP2 density matrices.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 126, "repo_full_name": "nvidia__nvflare", "instruction": "Generate code that defines a class named `SupervisedMonaiProstateDittoLearner` which inherits from `SupervisedMonaiProstateLearner`. This class should have an initializer that accepts parameters for the training configuration filename, the number of aggregation epochs, the number of ditto model epochs, and the training task name. The initializer should also set up a `SupervisedPTDittoHelper` instance.\n\nThe class should have a `train_config` method that initializes the superclass and sets up a `UNet` model and an `Adam` optimizer for the `SupervisedPTDittoHelper` instance.\n\nThe class should also have a `train` method that performs a training task pipeline for Ditto. This method should handle abort signals, update local model weights with received weights, load Ditto personalized model, perform local training on the reference model and personalized model, validate the Ditto model each round, compute the delta model, and return a shareable object with the updated local model.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class PTDittoHelper(object):\n    def __init__(\n        self, criterion, model, optimizer, device, app_dir: str, ditto_lambda: float = 0.1, model_epochs: int = 1\n    ):\n        \"\"\"Helper to be used with Ditto components.\n        Implements the functions used for the algorithm proposed in\n        Li et al. \"Ditto: Fair and Robust Federated Learning Through Personalization\"\n        (https://arxiv.org/abs/2012.04221) using PyTorch.\n\n        Args:\n            criterion: base loss criterion\n            model: the personalized model of Ditto method\n            optimizer: training optimizer for personalized model\n            device: device for personalized model training\n            app_dir: needed for local personalized model saving\n            ditto_lambda: lambda weight for Ditto prox loss term when combining with the base loss, defaults to 0.1\n            model_epochs: training epoch for personalized model, defaults to 1\n\n        Returns:\n            None\n        \"\"\"\n\n        self.criterion = criterion\n        self.model = model\n        self.optimizer = optimizer\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = device\n        self.model_epochs = model_epochs\n        # initialize Ditto criterion\n        self.prox_criterion = PTFedProxLoss(mu=ditto_lambda)\n        # check criterion, model, and optimizer type\n        if not isinstance(self.criterion, torch.nn.modules.loss._Loss):\n            raise ValueError(f\"criterion component must be torch loss. \" f\"But got: {type(self.criterion)}\")\n        if not isinstance(self.model, torch.nn.Module):\n            raise ValueError(f\"model component must be torch model. \" f\"But got: {type(self.model)}\")\n        if not isinstance(self.optimizer, torch.optim.Optimizer):\n            raise ValueError(f\"optimizer component must be torch optimizer. \" f\"But got: {type(self.optimizer)}\")\n        if not isinstance(self.device, torch.device):\n            raise ValueError(f\"device component must be torch device. \" f\"But got: {type(self.device)}\")\n\n        # initialize other recording related parameters\n        self.epoch_global = 0\n        self.epoch_of_start_time = 0\n        self.best_metric: int = 0\n        self.model_file_path = os.path.join(app_dir, \"personalized_model.pt\")\n        self.best_model_file_path = os.path.join(app_dir, \"best_personalized_model.pt\")\n\n    def load_model(self, global_weights):\n        # load local model from last round's record if model exist,\n        # otherwise initialize from global model weights for the first round.\n        if os.path.exists(self.model_file_path):\n            model_data = torch.load(self.model_file_path)\n            self.model.load_state_dict(model_data[\"model\"])\n            self.epoch_of_start_time = model_data[\"epoch\"]\n        else:\n            self.model.load_state_dict(global_weights)\n            self.epoch_of_start_time = 0\n        if os.path.exists(self.best_model_file_path):\n            model_data = torch.load(self.best_model_file_path)\n            self.best_metric = model_data[\"best_metric\"]\n\n    def save_model(self, is_best=False):\n        # save personalized model locally\n        model_weights = self.model.state_dict()\n        save_dict = {\"model\": model_weights, \"epoch\": self.epoch_global}\n        if is_best:\n            save_dict.update({\"best_metric\": self.best_metric})\n            torch.save(save_dict, self.best_model_file_path)\n        else:\n            torch.save(save_dict, self.model_file_path)\n\n    def update_metric_save_model(self, metric):\n        self.save_model(is_best=False)\n        if metric > self.best_metric:\n            self.best_metric = metric\n            self.save_model(is_best=True)\n\n    @abstractmethod\n    def local_train(self, train_loader, model_global, abort_signal: Signal, writer):\n        # Train personal model for self.model_epochs, and keep track of curves\n        # This part is task dependent, need customization\n        # Basic idea is to train personalized model with prox term as compare to model_global\n        raise NotImplementedError\n\n# --- Snippet Separator ---\n\ndef __init__(\n        self, criterion, model, optimizer, device, app_dir: str, ditto_lambda: float = 0.1, model_epochs: int = 1\n    ):\n        \"\"\"Helper to be used with Ditto components.\n        Implements the functions used for the algorithm proposed in\n        Li et al. \"Ditto: Fair and Robust Federated Learning Through Personalization\"\n        (https://arxiv.org/abs/2012.04221) using PyTorch.\n\n        Args:\n            criterion: base loss criterion\n            model: the personalized model of Ditto method\n            optimizer: training optimizer for personalized model\n            device: device for personalized model training\n            app_dir: needed for local personalized model saving\n            ditto_lambda: lambda weight for Ditto prox loss term when combining with the base loss, defaults to 0.1\n            model_epochs: training epoch for personalized model, defaults to 1\n\n        Returns:\n            None\n        \"\"\"\n\n        self.criterion = criterion\n        self.model = model\n        self.optimizer = optimizer\n        if device is None:\n            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        else:\n            self.device = device\n        self.model_epochs = model_epochs\n        # initialize Ditto criterion\n        self.prox_criterion = PTFedProxLoss(mu=ditto_lambda)\n        # check criterion, model, and optimizer type\n        if not isinstance(self.criterion, torch.nn.modules.loss._Loss):\n            raise ValueError(f\"criterion component must be torch loss. \" f\"But got: {type(self.criterion)}\")\n        if not isinstance(self.model, torch.nn.Module):\n            raise ValueError(f\"model component must be torch model. \" f\"But got: {type(self.model)}\")\n        if not isinstance(self.optimizer, torch.optim.Optimizer):\n            raise ValueError(f\"optimizer component must be torch optimizer. \" f\"But got: {type(self.optimizer)}\")\n        if not isinstance(self.device, torch.device):\n            raise ValueError(f\"device component must be torch device. \" f\"But got: {type(self.device)}\")\n\n        # initialize other recording related parameters\n        self.epoch_global = 0\n        self.epoch_of_start_time = 0\n        self.best_metric: int = 0\n        self.model_file_path = os.path.join(app_dir, \"personalized_model.pt\")\n        self.best_model_file_path = os.path.join(app_dir, \"best_personalized_model.pt\")\n\n# --- Snippet Separator ---\n\ndef __init__(self, model: Module, optimizer: Optimizer,\n                 criterion=CrossEntropyLoss(),\n                 train_mb_size: int = 1, train_epochs: int = 1,\n                 eval_mb_size: int = 1, device='cpu',\n                 plugins: Optional[Sequence['StrategyPlugin']] = None,\n                 evaluator=default_logger, eval_every=-1):\n        \"\"\"\n        BaseStrategy is the super class of all task-based continual learning\n        strategies. It implements a basic training loop and callback system\n        that allows to execute code at each experience of the training loop.\n        Plugins can be used to implement callbacks to augment the training\n        loop with additional behavior (e.g. a memory buffer for replay).\n\n        **Scenarios**\n        This strategy supports several continual learning scenarios:\n\n        * class-incremental scenarios (no task labels)\n        * multi-task scenarios, where task labels are provided)\n        * multi-incremental scenarios, where the same task may be revisited\n\n        The exact scenario depends on the data stream and whether it provides\n        the task labels.\n\n        **Training loop**\n        The training loop is organized as follows::\n            train\n                train_exp  # for each experience\n                    adapt_train_dataset\n                    train_dataset_adaptation\n                    make_train_dataloader\n                    train_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        **Evaluation loop**\n        The evaluation loop is organized as follows::\n            eval\n                eval_exp  # for each experience\n                    adapt_eval_dataset\n                    eval_dataset_adaptation\n                    make_eval_dataloader\n                    eval_epoch  # for each epoch\n                        # forward\n                        # backward\n                        # model update\n\n        :param model: PyTorch model.\n        :param optimizer: PyTorch optimizer.\n        :param criterion: loss function.\n        :param train_mb_size: mini-batch size for training.\n        :param train_epochs: number of training epochs.\n        :param eval_mb_size: mini-batch size for eval.\n        :param device: PyTorch device where the model will be allocated.\n        :param plugins: (optional) list of StrategyPlugins.\n        :param evaluator: (optional) instance of EvaluationPlugin for logging\n            and metric computations. None to remove logging.\n        :param eval_every: the frequency of the calls to `eval` inside the\n            training loop.\n                if -1: no evaluation during training.\n                if  0: calls `eval` after the final epoch of each training\n                    experience and before training on the first experience.\n                if >0: calls `eval` every `eval_every` epochs, at the end\n                    of all the epochs for a single experience and before\n                    training on the first experience.\n        \"\"\"\n        self._criterion = criterion\n\n        self.model: Module = model\n        \"\"\" PyTorch model. \"\"\"\n\n        self.optimizer = optimizer\n        \"\"\" PyTorch optimizer. \"\"\"\n\n        self.train_epochs: int = train_epochs\n        \"\"\" Number of training epochs. \"\"\"\n\n        self.train_mb_size: int = train_mb_size\n        \"\"\" Training mini-batch size. \"\"\"\n\n        self.eval_mb_size: int = train_mb_size if eval_mb_size is None \\\n            else eval_mb_size\n        \"\"\" Eval mini-batch size. \"\"\"\n\n        self.device = device\n        \"\"\" PyTorch device where the model will be allocated. \"\"\"\n\n        self.plugins = [] if plugins is None else plugins\n        \"\"\" List of `StrategyPlugin`s. \"\"\"\n\n        if evaluator is None:\n            evaluator = EvaluationPlugin()\n        self.plugins.append(evaluator)\n        self.evaluator = evaluator\n        \"\"\" EvaluationPlugin used for logging and metric computations. \"\"\"\n\n        self.eval_every = eval_every\n        \"\"\" Frequency of the evaluation during training. \"\"\"\n\n        ###################################################################\n        # State variables. These are updated during the train/eval loops. #\n        ###################################################################\n        self.training_exp_counter = 0\n        \"\"\" Counts the number of training steps. +1 at the end of each \n        experience. \"\"\"\n\n        self.epoch: Optional[int] = None\n        \"\"\" Epoch counter. \"\"\"\n\n        self.experience = None\n        \"\"\" Current experience. \"\"\"\n\n        self.adapted_dataset = None\n        \"\"\" Data used to train. It may be modified by plugins. Plugins can \n        append data to it (e.g. for replay). \n\n        .. note:: \n            This dataset may contain samples from different experiences. If you \n            want the original data for the current experience  \n            use :attr:`.BaseStrategy.experience`.\n        \"\"\"\n\n        self.dataloader = None\n        \"\"\" Dataloader. \"\"\"\n\n        self.mb_it = None\n        \"\"\" Iteration counter. Reset at the start of a new epoch. \"\"\"\n\n        self.mbatch = None\n        \"\"\" Current mini-batch. \"\"\"\n\n        self.mb_output = None\n        \"\"\" Model's output computed on the current mini-batch. \"\"\"\n\n        self.loss = None\n        \"\"\" Loss of the current mini-batch. \"\"\"\n\n        self.is_training: bool = False\n        \"\"\" True if the strategy is in training mode. \"\"\"\n\n        self.current_eval_stream = None\n        \"\"\"User-provided evaluation stream on `eval` call\"\"\"\n\n        self._stop_training = False\n\n        self._warn_for_disabled_plugins_callbacks()\n        self._warn_for_disabled_metrics_callbacks()\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that defines a class named `SupervisedMonaiProstateDittoLearner` which inherits from `SupervisedMonaiProstateLearner`. This class should have an initializer that accepts parameters for the training configuration filename, the number of aggregation epochs, the number of ditto model epochs, and the training task name. The initializer should also set up a `SupervisedPTDittoHelper` instance.\n\nThe class should have a `train_config` method that initializes the superclass and sets up a `UNet` model and an `Adam` optimizer for the `SupervisedPTDittoHelper` instance.\n\nThe class should also have a `train` method that performs a training task pipeline for Ditto. This method should handle abort signals, update local model weights with received weights, load Ditto personalized model, perform local training on the reference model and personalized model, validate the Ditto model each round, compute the delta model, and return a shareable object with the updated local model.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 127, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that calculates the coupling matrix for singlet energy transfer (SET) and triplet energy transfer (TET) between two molecules using the pyscf library. The code should perform CIS calculations for the excited states of two molecules, calculate the intermolecular 2e integrals, transform these integrals to MO basis, and compute the J-type and K-type coupling. The code should also include functions to compute the Coulomb integrals and exchange integrals across the two molecules, and to evaluate the coupling term including J, K and DFT XC contributions. The code should finally evaluate the overall coupling term.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def energy_2body(agf2, gf, se):\n    ''' Calculates the two-body energy using analytically integrated\n        Galitskii-Migdal formula. The formula is symmetric and only\n        one side needs to be calculated.\n\n    Args:\n        gf : GreensFunction\n            Auxiliaries of the Green's function\n        se : SelfEnergy\n            Auxiliaries of the self-energy\n\n    Returns\n        Two-body energy\n    '''\n\n    assert type(gf) is aux.GreensFunction\n    assert type(se) is aux.SelfEnergy\n\n    gf_occ = gf.get_occupied()\n    se_vir = se.get_virtual()\n\n    e2b = 0.0\n\n    for l in mpi_helper.nrange(gf_occ.naux):\n        vxl = gf_occ.coupling[:,l]\n        vxk = se_vir.coupling\n        dlk = gf_occ.energy[l] - se_vir.energy\n\n        vv = vxk * vxl[:,None]\n        e2b += lib.einsum('xk,yk,k->', vv, vv.conj(), 1./dlk)\n\n    e2b *= 2\n\n    mpi_helper.barrier()\n    e2b = mpi_helper.allreduce(e2b)\n\n    return np.ravel(e2b.real)[0]\n\n# --- Snippet Separator ---\n\ndef build_se_part(agf2, eri, gf_occ, gf_vir, os_factor=1.0, ss_factor=1.0):\n    ''' Builds either the auxiliaries of the occupied self-energy,\n        or virtual if :attr:`gf_occ` and :attr:`gf_vir` are swapped.\n\n    Args:\n        eri : _ChemistsERIs\n            Electronic repulsion integrals\n        gf_occ : GreensFunction\n            Occupied Green's function\n        gf_vir : GreensFunction\n            Virtual Green's function\n\n    Kwargs:\n        os_factor : float\n            Opposite-spin factor for spin-component-scaled (SCS)\n            calculations. Default 1.0\n        ss_factor : float\n            Same-spin factor for spin-component-scaled (SCS)\n            calculations. Default 1.0\n\n    Returns:\n        :class:`SelfEnergy`\n    '''\n\n    cput0 = (logger.process_clock(), logger.perf_counter())\n    log = logger.Logger(agf2.stdout, agf2.verbose)\n\n    assert type(gf_occ) is aux.GreensFunction\n    assert type(gf_vir) is aux.GreensFunction\n\n    nmo = eri.nmo\n    nocc, nvir = gf_occ.naux, gf_vir.naux\n    naux = agf2.with_df.get_naoaux()\n    tol = agf2.weight_tol\n    facs = dict(os_factor=os_factor, ss_factor=ss_factor)\n\n    ei, ci = gf_occ.energy, gf_occ.coupling\n    ea, ca = gf_vir.energy, gf_vir.coupling\n\n    qxi, qja = _make_qmo_eris_incore(agf2, eri, (ci, ci, ca))\n\n    himem_required = naux*(nvir+nmo) + (nocc*nvir)*(2*nmo+1) + (2*nmo**2)\n    himem_required *= 8e-6\n    himem_required *= lib.num_threads()\n\n    if ((himem_required*1.05 + lib.current_memory()[0]) > agf2.max_memory\n            and agf2.allow_lowmem_build) or agf2.allow_lowmem_build == 'force':\n        log.debug('Thread-private memory overhead %.3f exceeds max_memory, using '\n                  'low-memory version.', himem_required)\n        vv, vev = _agf2.build_mats_dfragf2_lowmem(qxi, qja, ei, ea, **facs)\n    else:\n        vv, vev = _agf2.build_mats_dfragf2_incore(qxi, qja, ei, ea, **facs)\n\n    e, c = _agf2.cholesky_build(vv, vev)\n    se = aux.SelfEnergy(e, c, chempot=gf_occ.chempot)\n    se.remove_uncoupled(tol=tol)\n\n    if not (agf2.frozen is None or agf2.frozen == 0):\n        mask = ragf2.get_frozen_mask(agf2)\n        coupling = np.zeros((nmo, se.naux))\n        coupling[mask] = se.coupling\n        se = aux.SelfEnergy(se.energy, coupling, chempot=se.chempot)\n\n    log.timer('se part', *cput0)\n\n    return se\n\n# --- Snippet Separator ---\n\nclass UAGF2(ragf2.RAGF2):\n    ''' Unrestricted AGF2 with canonical HF reference\n\n    Attributes:\n        verbose : int\n            Print level. Default value equals to :class:`Mole.verbose`\n        max_memory : float or int\n            Allowed memory in MB. Default value equals to :class:`Mole.max_memory`\n        incore_complete : bool\n            Avoid all I/O. Default is False.\n        conv_tol : float\n            Convergence threshold for AGF2 energy. Default value is 1e-7\n        conv_tol_rdm1 : float\n            Convergence threshold for first-order reduced density matrix.\n            Default value is 1e-8.\n        conv_tol_nelec : float\n            Convergence threshold for the number of electrons. Default\n            value is 1e-6.\n        max_cycle : int\n            Maximum number of AGF2 iterations. Default value is 50.\n        max_cycle_outer : int\n            Maximum number of outer Fock loop iterations. Default\n            value is 20.\n        max_cycle_inner : int\n            Maximum number of inner Fock loop iterations. Default\n            value is 50.\n        weight_tol : float\n            Threshold in spectral weight of auxiliaries to be considered\n            zero. Default 1e-11.\n        diis : bool or lib.diis.DIIS\n            Whether to use DIIS, can also be a lib.diis.DIIS object. Default\n            value is True.\n        diis_space : int\n            DIIS space size. Default value is 8.\n        diis_min_space : int\n            Minimum space of DIIS. Default value is 1.\n        fock_diis_space : int\n            DIIS space size for Fock loop iterations. Default value is 6.\n        fock_diis_min_space :\n            Minimum space of DIIS. Default value is 1.\n        os_factor : float\n            Opposite-spin factor for spin-component-scaled (SCS)\n            calculations. Default 1.0\n        ss_factor : float\n            Same-spin factor for spin-component-scaled (SCS)\n            calculations. Default 1.0\n        damping : float\n            Damping factor for the self-energy. Default value is 0.0\n\n    Saved results\n\n        e_corr : float\n            AGF2 correlation energy\n        e_tot : float\n            Total energy (HF + correlation)\n        e_1b : float\n            One-body part of :attr:`e_tot`\n        e_2b : float\n            Two-body part of :attr:`e_tot`\n        e_init : float\n            Initial correlation energy (truncated MP2)\n        converged : bool\n            Whether convergence was successful\n        se : tuple of SelfEnergy\n            Auxiliaries of the self-energy for each spin\n        gf : tuple of GreensFunction\n            Auxiliaries of the Green's function for each spin\n    '''\n\n    energy_1body = energy_1body\n    energy_2body = energy_2body\n    fock_loop = fock_loop\n    build_se_part = build_se_part\n\n    def ao2mo(self, mo_coeff=None):\n        ''' Get the electronic repulsion integrals in MO basis.\n        '''\n\n        nmo = max(self.nmo)\n        mem_incore = ((nmo*(nmo+1)//2)**2) * 8/1e6\n        mem_now = lib.current_memory()[0]\n\n        if (self._scf._eri is not None and\n                (mem_incore+mem_now < self.max_memory or self.incore_complete)):\n            eri = _make_mo_eris_incore(self, mo_coeff)\n        else:\n            logger.warn(self, 'MO eris are outcore - this may be very '\n                              'slow for agf2. increasing max_memory or '\n                              'using density fitting is recommended.')\n            eri = _make_mo_eris_outcore(self, mo_coeff)\n\n        return eri\n\n    def make_rdm1(self, gf=None):\n        ''' Compute the one-body reduced density matrix in MO basis.\n\n        Kwargs:\n            gf : tuple of GreensFunction\n                Auxiliaries of the Green's functions for each spin\n\n        Returns:\n            tuple of ndarray of density matrices\n        '''\n\n        if gf is None: gf = self.gf\n        if gf is None: gf = self.init_gf()\n\n        rdm1_a = gf[0].make_rdm1(occupancy=1)\n        rdm1_b = gf[1].make_rdm1(occupancy=1)\n\n        return (rdm1_a, rdm1_b)\n\n    def get_fock(self, eri=None, gf=None, rdm1=None):\n        ''' Computes the physical space Fock matrix in MO basis.\n        '''\n\n        if eri is None: eri = self.ao2mo()\n        if gf is None: gf = self.gf\n\n        return get_fock(self, eri, gf=gf, rdm1=rdm1)\n\n    def energy_mp2(self, mo_energy=None, se=None):\n        if mo_energy is None: mo_energy = self.mo_energy\n        if se is None: se = self.build_se(gf=self.gf)\n\n        self.e_init = energy_mp2(self, mo_energy, se)\n\n        return self.e_init\n\n    def init_gf(self, frozen=False):\n        ''' Builds the Hartree-Fock Green's function.\n\n        Returns:\n            tuple of :class:`GreensFunction`, tuple of :class:`SelfEnergy`\n        '''\n\n        nmoa, nmob = self.nmo\n        nocca, noccb = self.nocc\n\n        energy = self.mo_energy\n        coupling = (np.eye(nmoa), np.eye(nmob))\n\n        focka = np.diag(energy[0])\n        fockb = np.diag(energy[1])\n\n        cpt_a = binsearch_chempot(focka, nmoa, nocca, occupancy=1)[0]\n        cpt_b = binsearch_chempot(fockb, nmob, noccb, occupancy=1)[1]\n\n        if frozen:\n            mask = get_frozen_mask(self)\n            energy = (energy[0][mask[0]], energy[1][mask[1]])\n            coupling = (coupling[0][:,mask[0]], coupling[1][:,mask[1]])\n\n        gf_a = aux.GreensFunction(energy[0], coupling[0], chempot=cpt_a)\n        gf_b = aux.GreensFunction(energy[1], coupling[1], chempot=cpt_b)\n\n        gf = (gf_a, gf_b)\n\n        return gf\n\n    def build_gf(self, eri=None, gf=None, se=None):\n        ''' Builds the auxiliaries of the Green's functions by solving\n            the Dyson equation for each spin.\n\n        Kwargs:\n            eri : _ChemistsERIs\n                Electronic repulsion integrals\n            gf : tuple of GreensFunction\n                Auxiliaries of the Green's function for each spin\n            se : tuple of SelfEnergy\n                Auxiliaries of the self-energy for each spin\n\n        Returns:\n            tuple of :class:`GreensFunction`\n        '''\n\n        if eri is None: eri = self.ao2mo()\n        if gf is None: gf = self.gf\n        if gf is None: gf = self.init_gf()\n        if se is None: se = self.build_se(eri, gf)\n\n        focka, fockb = self.get_fock(eri, gf)\n\n        gf_a = se[0].get_greens_function(focka)\n        gf_b = se[1].get_greens_function(fockb)\n\n        return (gf_a, gf_b)\n\n    def build_se(self, eri=None, gf=None, os_factor=None, ss_factor=None, se_prev=None):\n        ''' Builds the auxiliaries of the self-energy.\n\n        Args:\n            eri : _ChemistsERIs\n                Electronic repulsion integrals\n            gf : tuple of GreensFunction\n                Auxiliaries of the Green's function\n\n        Kwargs:\n            os_factor : float\n                Opposite-spin factor for spin-component-scaled (SCS)\n                calculations. Default 1.0\n            ss_factor : float\n                Same-spin factor for spin-component-scaled (SCS)\n                calculations. Default 1.0\n            se_prev : SelfEnergy\n                Previous self-energy for damping. Default value is None\n\n        Returns\n            tuple of :class:`SelfEnergy`\n        '''\n\n        if eri is None: eri = self.ao2mo()\n        if gf is None: gf = self.gf\n        if gf is None: gf = self.init_gf()\n\n        if os_factor is None: os_factor = self.os_factor\n        if ss_factor is None: ss_factor = self.ss_factor\n\n        facs = dict(os_factor=os_factor, ss_factor=ss_factor)\n        gf_occ = (gf[0].get_occupied(), gf[1].get_occupied())\n        gf_vir = (gf[0].get_virtual(), gf[1].get_virtual())\n\n        se_occ = self.build_se_part(eri, gf_occ, gf_vir, **facs)\n        se_vir = self.build_se_part(eri, gf_vir, gf_occ, **facs)\n\n        se_a = aux.combine(se_occ[0], se_vir[0])\n        se_b = aux.combine(se_occ[1], se_vir[1])\n\n        if se_prev is not None and self.damping != 0.0:\n            se_a_prev, se_b_prev = se_prev\n            se_a.coupling *= np.sqrt(1.0-self.damping)\n            se_b.coupling *= np.sqrt(1.0-self.damping)\n            se_a_prev.coupling *= np.sqrt(self.damping)\n            se_b_prev.coupling *= np.sqrt(self.damping)\n            se_a = aux.combine(se_a, se_a_prev)\n            se_b = aux.combine(se_b, se_b_prev)\n            se_a = se_a.compress(n=(None,0))\n            se_b = se_b.compress(n=(None,0))\n\n        return (se_a, se_b)\n\n    def run_diis(self, se, diis=None):\n        ''' Runs the direct inversion of the iterative subspace for the\n            self-energy.\n\n        Args:\n            se : SelfEnergy\n                Auxiliaries of the self-energy\n            diis : lib.diis.DIIS\n                DIIS object\n\n        Returns:\n            tuple of :class:`SelfEnergy`\n        '''\n\n        if diis is None:\n            return se\n\n        se_occ_a, se_occ_b = (se[0].get_occupied(), se[1].get_occupied())\n        se_vir_a, se_vir_b = (se[0].get_virtual(), se[1].get_virtual())\n\n        vv_occ_a = np.dot(se_occ_a.coupling, se_occ_a.coupling.T)\n        vv_occ_b = np.dot(se_occ_b.coupling, se_occ_b.coupling.T)\n        vv_vir_a = np.dot(se_vir_a.coupling, se_vir_a.coupling.T)\n        vv_vir_b = np.dot(se_vir_b.coupling, se_vir_b.coupling.T)\n\n        vev_occ_a = np.dot(se_occ_a.coupling * se_occ_a.energy[None], se_occ_a.coupling.T)\n        vev_occ_b = np.dot(se_occ_b.coupling * se_occ_b.energy[None], se_occ_b.coupling.T)\n        vev_vir_a = np.dot(se_vir_a.coupling * se_vir_a.energy[None], se_vir_a.coupling.T)\n        vev_vir_b = np.dot(se_vir_b.coupling * se_vir_b.energy[None], se_vir_b.coupling.T)\n\n        dat = np.array([vv_occ_a, vv_vir_a, vev_occ_a, vev_vir_a,\n                        vv_occ_b, vv_vir_b, vev_occ_b, vev_vir_b])\n        dat = diis.update(dat)\n        vv_occ_a, vv_vir_a, vev_occ_a, vev_vir_a, \\\n                vv_occ_b, vv_vir_b, vev_occ_b, vev_vir_b = dat\n\n        se_occ_a = aux.SelfEnergy(*_agf2.cholesky_build(vv_occ_a, vev_occ_a), chempot=se[0].chempot)\n        se_vir_a = aux.SelfEnergy(*_agf2.cholesky_build(vv_vir_a, vev_vir_a), chempot=se[0].chempot)\n        se_occ_b = aux.SelfEnergy(*_agf2.cholesky_build(vv_occ_b, vev_occ_b), chempot=se[1].chempot)\n        se_vir_b = aux.SelfEnergy(*_agf2.cholesky_build(vv_vir_b, vev_vir_b), chempot=se[1].chempot)\n        se = (aux.combine(se_occ_a, se_vir_a), aux.combine(se_occ_b, se_vir_b))\n\n        return se\n\n    def density_fit(self, auxbasis=None, with_df=None):\n        from pyscf.agf2 import dfuagf2\n        myagf2 = dfuagf2.DFUAGF2(self._scf)\n        myagf2.__dict__.update(self.__dict__)\n\n        if with_df is not None:\n            myagf2.with_df = with_df\n\n        if auxbasis is not None and myagf2.with_df.auxbasis != auxbasis:\n            import copy\n            myagf2.with_df = copy.copy(myagf2.with_df)\n            myagf2.with_df.auxbasis = auxbasis\n\n        return myagf2\n\n    def get_ip(self, gf, nroots=5):\n        gf_occ = (gf[0].get_occupied(), gf[1].get_occupied())\n        spin = np.array([0,]*gf_occ[0].naux + [1,]*gf_occ[1].naux)\n        e_ip = np.concatenate([gf_occ[0].energy, gf_occ[1].energy], axis=0)\n        v_ip = np.concatenate([gf_occ[0].coupling, gf_occ[1].coupling], axis=1)\n\n        mask = np.argsort(e_ip)\n        spin = list(spin[mask][-nroots:])[::-1]\n        e_ip = list(-e_ip[mask][-nroots:])[::-1]\n        v_ip = list(v_ip[:,mask][:,-nroots:].T)[::-1]\n\n        return e_ip, v_ip, spin\n\n    def ipagf2(self, nroots=5):\n        e_ip, v_ip, spin = self.get_ip(self.gf, nroots=nroots)\n\n        for n, en, vn, sn in zip(range(nroots), e_ip, v_ip, spin):\n            qpwt = np.linalg.norm(vn)**2\n            tag = ['alpha', 'beta'][sn]\n            logger.note(self, 'IP energy level %d E = %.16g  QP weight = %0.6g  (%s)', n, en, qpwt, tag)\n\n        if nroots == 1:\n            return e_ip[0], v_ip[0]\n        else:\n            return e_ip, v_ip\n\n    def get_ea(self, gf, nroots=5):\n        gf_vir = (gf[0].get_virtual(), gf[1].get_virtual())\n        spin = np.array([0,]*gf_vir[0].naux + [1,]*gf_vir[1].naux)\n        e_ea = np.concatenate([gf_vir[0].energy, gf_vir[1].energy], axis=0)\n        v_ea = np.concatenate([gf_vir[0].coupling, gf_vir[1].coupling], axis=1)\n\n        mask = np.argsort(e_ea)\n        spin = list(spin[mask][:nroots])\n        e_ea = list(e_ea[mask][:nroots])\n        v_ea = list(v_ea[:,mask][:,:nroots].T)\n\n        return e_ea, v_ea, spin\n\n    def eaagf2(self, nroots=5):\n        e_ea, v_ea, spin = self.get_ea(self.gf, nroots=nroots)\n\n        for n, en, vn, sn in zip(range(nroots), e_ea, v_ea, spin):\n            qpwt = np.linalg.norm(vn)**2\n            tag = ['alpha', 'beta'][sn]\n            logger.note(self, 'EA energy level %d E = %.16g  QP weight = %0.6g  (%s)', n, en, qpwt, tag)\n\n        if nroots == 1:\n            return e_ea[0], v_ea[0]\n        else:\n            return e_ea, v_ea\n\n\n    @property\n    def nocc(self):\n        if self._nocc is None:\n            self._nocc = (np.sum(self.mo_occ[0] > 0), np.sum(self.mo_occ[1] > 0))\n        return self._nocc\n    @nocc.setter\n    def nocc(self, val):\n        self._nocc = val\n\n    @property\n    def nmo(self):\n        if self._nmo is None:\n            self._nmo = (self.mo_occ[0].size, self.mo_occ[1].size)\n        return self._nmo\n    @nmo.setter\n    def nmo(self, val):\n        self._nmo = val\n\n    @property\n    def qmo_energy(self):\n        return (self.gf[0].energy, self.gf[1].energy)\n\n    @property\n    def qmo_coeff(self):\n        ''' Gives the couplings in AO basis '''\n        return (np.dot(self.mo_coeff[0], self.gf[0].coupling),\n                np.dot(self.mo_coeff[1], self.gf[1].coupling))\n\n    @property\n    def qmo_occ(self):\n        coeff_a = self.gf[0].get_occupied().coupling\n        coeff_b = self.gf[1].get_occupied().coupling\n        occ_a = np.linalg.norm(coeff_a, axis=0) ** 2\n        occ_b = np.linalg.norm(coeff_b, axis=0) ** 2\n        vir_a = np.zeros_like(self.gf[0].get_virtual().energy)\n        vir_b = np.zeros_like(self.gf[1].get_virtual().energy)\n        qmo_occ_a = np.concatenate([occ_a, vir_a])\n        qmo_occ_b = np.concatenate([occ_b, vir_b])\n        return qmo_occ_a, qmo_occ_b\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that calculates the coupling matrix for singlet energy transfer (SET) and triplet energy transfer (TET) between two molecules using the pyscf library. The code should perform CIS calculations for the excited states of two molecules, calculate the intermolecular 2e integrals, transform these integrals to MO basis, and compute the J-type and K-type coupling. The code should also include functions to compute the Coulomb integrals and exchange integrals across the two molecules, and to evaluate the coupling term including J, K and DFT XC contributions. The code should finally evaluate the overall coupling term.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 128, "repo_full_name": "jchanvfx__nodegraphqt", "instruction": "Generate code that defines a set of functions to manipulate a node graph using the nodegraphqt library. The functions should allow to zoom in, zoom out, reset zoom, set layout direction to horizontal or vertical, open, import, save, and clear a session, clear undo history, copy, cut, paste, delete, extract, clear connections of, select all, clear selection of, invert selection of, disable, duplicate, expand group of nodes, fit the zoom level to selected nodes, show undo list, set pipe style to curved, straight or angled, set background grid to none, dots or lines, auto layout nodes downstream or upstream, and toggle node search.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class NodeGraph(QtCore.QObject):\n\n    node_selected = QtCore.Signal(NodeObject)\n\n    def __init__(self, parent=None):\n        super(NodeGraph, self).__init__(parent)\n        self._model = NodeGraphModel()\n        self._viewer = NodeViewer()\n        self._undo_stack = QUndoStack(self)\n        self._init_actions()\n        self._wire_signals()\n\n    def _wire_signals(self):\n        self._viewer.moved_nodes.connect(self._on_nodes_moved)\n        self._viewer.search_triggered.connect(self._on_search_triggered)\n        self._viewer.connection_changed.connect(self._on_connection_changed)\n        self._viewer.node_selected.connect(self._on_node_selected)\n\n    def _init_actions(self):\n        # setup tab search shortcut.\n        tab = QAction('Search Nodes', self)\n        tab.setShortcut('tab')\n        tab.triggered.connect(self._toggle_tab_search)\n        self._viewer.addAction(tab)\n        setup_actions(self)\n\n    def _toggle_tab_search(self):\n        \"\"\"\n        toggle the tab search widget.\n        \"\"\"\n        self._viewer.tab_search_set_nodes(NodeVendor.names)\n        self._viewer.tab_search_toggle()\n\n    def _on_nodes_moved(self, node_data):\n        \"\"\"\n        called when selected nodes in the viewer has changed position.\n\n        Args:\n            node_data (dict): {<node_view>: <previous_pos>}\n        \"\"\"\n        self._undo_stack.beginMacro('moved nodes')\n        for node_view, prev_pos in node_data.items():\n            node = self._model.nodes[node_view.id]\n            self._undo_stack.push(NodeMovedCmd(node, node.pos(), prev_pos))\n        self._undo_stack.endMacro()\n\n    def _on_node_selected(self, node_id):\n        \"\"\"\n        called when a node in the viewer is selected on left click.\n        (emits the node object when the node is clicked)\n\n        Args:\n            node_id (str): node id emitted by the viewer.\n        \"\"\"\n        node = self.get_node_by_id(node_id)\n        self.node_selected.emit(node)\n\n    def _on_search_triggered(self, node_type, pos):\n        \"\"\"\n        called when the tab search widget is triggered in the viewer.\n\n        Args:\n            node_type (str): node identifier.\n            pos (tuple): x,y position for the node.\n        \"\"\"\n        self.create_node(node_type, pos=pos)\n\n    def _on_connection_changed(self, disconnected, connected):\n        \"\"\"\n        called when a pipe connection has been changed in the viewer.\n\n        Args:\n            disconnected (list[list[widgets.port.PortItem]):\n                pair list of port view items.\n            connected (list[list[widgets.port.PortItem]]):\n                pair list of port view items.\n        \"\"\"\n        if not (disconnected or connected):\n            return\n\n        label = 'connected node(s)' if connected else 'disconnected node(s)'\n        ptypes = {'in': 'inputs', 'out': 'outputs'}\n\n        self._undo_stack.beginMacro(label)\n        for p1_view, p2_view in disconnected:\n            node1 = self._model.nodes[p1_view.node.id]\n            node2 = self._model.nodes[p2_view.node.id]\n            port1 = getattr(node1, ptypes[p1_view.port_type])()[p1_view.name]\n            port2 = getattr(node2, ptypes[p2_view.port_type])()[p2_view.name]\n            port1.disconnect_from(port2)\n        for p1_view, p2_view in connected:\n            node1 = self._model.nodes[p1_view.node.id]\n            node2 = self._model.nodes[p2_view.node.id]\n            port1 = getattr(node1, ptypes[p1_view.port_type])()[p1_view.name]\n            port2 = getattr(node2, ptypes[p2_view.port_type])()[p2_view.name]\n            port1.connect_to(port2)\n        self._undo_stack.endMacro()\n\n    @property\n    def model(self):\n        \"\"\"\n        Return the node graph model.\n\n        Returns:\n            NodeGraphModel: model object.\n        \"\"\"\n        return self._model\n\n    def show(self):\n        \"\"\"\n        Show node graph viewer widget.\n        \"\"\"\n        self._viewer.show()\n\n    def hide(self):\n        \"\"\"\n        Hide node graph viewer widget.\n        \"\"\"\n        self._viewer.hide()\n\n    def close(self):\n        \"\"\"\n        Close node graph viewer widget.\n        \"\"\"\n        self._viewer.close()\n\n    def viewer(self):\n        \"\"\"\n        Return the node graph viewer widget object.\n\n        Returns:\n            NodeGraphQt.widgets.viewer.NodeViewer: viewer widget.\n        \"\"\"\n        return self._viewer\n\n    def scene(self):\n        \"\"\"\n        Return the scene object.\n\n        Returns:\n            NodeGraphQt.widgets.scene.NodeScene: node scene.\n        \"\"\"\n        return self._viewer.scene()\n\n    def undo_stack(self):\n        \"\"\"\n        Returns the undo stack used in the node graph\n\n        Returns:\n            QUndoStack: undo stack.\n        \"\"\"\n        return self._undo_stack\n\n    def begin_undo(self, name='undo'):\n        \"\"\"\n        Start of an undo block followed by a end_undo().\n\n        Args:\n            name (str): name for the undo block.\n        \"\"\"\n        self._undo_stack.beginMacro(name)\n\n    def end_undo(self):\n        \"\"\"\n        End of an undo block started by begin_undo().\n        \"\"\"\n        self._undo_stack.endMacro()\n\n    def context_menu(self):\n        \"\"\"\n        Returns a node graph context menu object.\n\n        Returns:\n            ContextMenu: node graph context menu object instance.\n        \"\"\"\n        return self._viewer.context_menu()\n\n    def acyclic(self):\n        \"\"\"\n        Returns true if the current node graph is acyclic.\n\n        Returns:\n            bool: true if acyclic.\n        \"\"\"\n        return self._model.acyclic\n\n    def set_acyclic(self, mode=True):\n        \"\"\"\n        Set the node graph to be acyclic or not. (default=True)\n\n        Args:\n            mode (bool): false to disable acyclic.\n        \"\"\"\n        self._model.acyclic = mode\n        self._viewer.acyclic = mode\n\n    def set_pipe_layout(self, layout='curved'):\n        \"\"\"\n        Set node graph pipes to be drawn straight or curved by default\n        all pipes are set curved. (default='curved')\n\n        Args:\n            layout (str): 'straight' or 'curved'\n        \"\"\"\n        self._viewer.set_pipe_layout(layout)\n\n    def fit_to_selection(self):\n        \"\"\"\n        Sets the zoom level to fit selected nodes.\n        If no nodes are selected then all nodes in the graph will be framed.\n        \"\"\"\n        nodes = self.selected_nodes() or self.all_nodes()\n        if not nodes:\n            return\n        self._viewer.zoom_to_nodes([n.view for n in nodes])\n\n    def reset_zoom(self):\n        \"\"\"\n        Reset the zoom level\n        \"\"\"\n        self._viewer.reset_zoom()\n\n    def set_zoom(self, zoom=0):\n        \"\"\"\n        Set the zoom factor of the Node Graph the default is 0.0\n\n        Args:\n            zoom (float): zoom factor max zoom out -0.9 max zoom in 2.0\n        \"\"\"\n        self._viewer.set_zoom(zoom)\n\n    def get_zoom(self):\n        \"\"\"\n        Get the current zoom level of the node graph.\n\n        Returns:\n            float: the current zoom level.\n        \"\"\"\n        return self._viewer.get_zoom()\n\n    def center_on(self, nodes=None):\n        \"\"\"\n        Center the node graph on the given nodes or all nodes by default.\n\n        Args:\n            nodes (list[NodeGraphQt.Node]): a list of nodes.\n        \"\"\"\n        self._viewer.center_selection(nodes)\n\n    def center_selection(self):\n        \"\"\"\n        Center the node graph on the current selected nodes.\n        \"\"\"\n        nodes = self._viewer.selected_nodes()\n        self._viewer.center_selection(nodes)\n\n    def registered_nodes(self):\n        \"\"\"\n        Return a list of all node types that have been registered.\n        To register a node see \"NodeGraphWidget.register_node()\"\n\n        Returns:\n            list[str]: node types.\n        \"\"\"\n        return sorted(NodeVendor.nodes.keys())\n\n    def register_node(self, node, alias=None):\n        \"\"\"\n        Register a node.\n\n        Args:\n            node (NodeGraphQt.Node): node object.\n            alias (str): custom alias name for the node type.\n        \"\"\"\n        NodeVendor.register_node(node, alias)\n\n    def create_node(self, node_type, name=None, selected=True, color=None, pos=None):\n        \"\"\"\n        Create a new node in the node graph.\n        To list all node types see \"NodeGraphWidget.registered_nodes()\"\n\n        Args:\n            node_type (str): node instance type.\n            name (str): set name of the node.\n            selected (bool): set created node to be selected.\n            color (tuple or str): node color (255, 255, 255) or '#FFFFFF'.\n            pos (tuple): set position of the node (x, y).\n\n        Returns:\n            NodeGraphQt.Node: created instance of a node.\n        \"\"\"\n        NodeInstance = NodeVendor.create_node_instance(node_type)\n        if NodeInstance:\n            node = NodeInstance()\n            node._graph = self\n            node.update()\n\n            self._undo_stack.beginMacro('created node')\n            self._undo_stack.push(NodeAddedCmd(self, node, pos))\n            if name:\n                node.set_name(name)\n            else:\n                node.set_name(node.NODE_NAME)\n            if color:\n                if isinstance(color, str):\n                    color = color[1:] if color[0] is '#' else color\n                    color = tuple(int(color[i:i + 2], 16) for i in (0, 2, 4))\n                node.set_color(*color)\n            node.set_selected(selected)\n            self._undo_stack.endMacro()\n            return node\n        raise Exception('\\n\\n>> Cannot find node:\\t\"{}\"\\n'.format(node_type))\n\n    def add_node(self, node):\n        \"\"\"\n        Add a node into the node graph.\n\n        Args:\n            node (NodeGraphQt.Node): node object.\n        \"\"\"\n        assert isinstance(node, NodeObject), 'node must be a Node instance.'\n        node._graph = self\n        node.NODE_NAME = self.get_unique_name(node.NODE_NAME)\n        node.model.name = node.NODE_NAME\n        node.update()\n        self._undo_stack.push(NodeAddedCmd(self, node))\n\n    def delete_node(self, node):\n        \"\"\"\n        Remove the node from the node graph.\n\n        Args:\n            node (NodeGraphQt.Node): node object.\n        \"\"\"\n        assert isinstance(node, NodeObject), \\\n            'node must be a instance of a NodeObject.'\n        self._undo_stack.push(NodeRemovedCmd(self, node))\n\n    def delete_nodes(self, nodes):\n        \"\"\"\n        Remove a list of nodes from the node graph.\n\n        Args:\n            nodes (list[NodeGraphQt.Node]): list of node instances.\n        \"\"\"\n        self._undo_stack.beginMacro('deleted nodes')\n        [self.delete_node(n) for n in nodes]\n        self._undo_stack.endMacro()\n\n    def all_nodes(self):\n        \"\"\"\n        Return all nodes in the node graph.\n\n        Returns:\n            list[NodeGraphQt.Node]: list of nodes.\n        \"\"\"\n        return list(self._model.nodes.values())\n\n    def selected_nodes(self):\n        \"\"\"\n        Return all selected nodes that are in the node graph.\n\n        Returns:\n            list[NodeGraphQt.Node]: list of nodes.\n        \"\"\"\n        nodes = []\n        for item in self._viewer.selected_nodes():\n            node = self._model.nodes[item.id]\n            nodes.append(node)\n        return nodes\n\n    def select_all(self):\n        \"\"\"\n        Select all nodes in the current node graph.\n        \"\"\"\n        self._undo_stack.beginMacro('select all')\n        for node in self.all_nodes():\n            node.set_selected(True)\n        self._undo_stack.endMacro()\n\n    def clear_selection(self):\n        \"\"\"\n        Clears the selection in the node graph.\n        \"\"\"\n        self._undo_stack.beginMacro('deselected nodes')\n        for node in self.all_nodes():\n            node.set_selected(False)\n        self._undo_stack.endMacro()\n\n    def get_node_by_id(self, node_id=None):\n        \"\"\"\n        Get the node object by it's id.\n\n        Args:\n            node_id (str): node id\n\n        Returns:\n            NodeGraphQt.NodeObject: node object.\n        \"\"\"\n        return self._model.nodes.get(node_id)\n\n    def get_node_by_name(self, name):\n        \"\"\"\n        Returns node object that matches the name.\n\n        Args:\n            name (str): name of the node.\n        Returns:\n            NodeGraphQt.Node: node object.\n        \"\"\"\n        for node_id, node in self._model.nodes.items():\n            if node.name() == name:\n                return node\n\n    def get_unique_name(self, name):\n        \"\"\"\n        return a unique node name for the node.\n\n        Args:\n            name (str): node name.\n\n        Returns:\n            str: unique node name.\n        \"\"\"\n        name = ' '.join(name.split())\n        node_names = [n.name() for n in self.all_nodes()]\n        if name not in node_names:\n            return name\n\n        regex = re.compile('[\\w ]+(?: )*(\\d+)')\n        search = regex.search(name)\n        if not search:\n            for x in range(1, len(node_names) + 1):\n                new_name = '{} {}'.format(name, x)\n                if new_name not in node_names:\n                    return new_name\n\n        version = search.group(1)\n        name = name[:len(version) * -1].strip()\n        for x in range(1, len(node_names) + 1):\n            new_name = '{} {}'.format(name, x)\n            if new_name not in node_names:\n                return new_name\n\n    def current_session(self):\n        \"\"\"\n        returns the file path to the currently loaded session.\n\n        Returns:\n            str: path to the currently loaded session\n        \"\"\"\n        return self._model.session\n\n    def clear_session(self):\n        \"\"\"\n        clear the loaded node layout session.\n        \"\"\"\n        for n in self.all_nodes():\n            self.delete_node(n)\n        self._undo_stack.clear()\n        self._model.session = None\n\n    def _serialize(self, nodes):\n        \"\"\"\n        serialize nodes to a dict.\n\n        Args:\n            nodes (list[NodeGraphQt.Nodes]): list of node instances.\n\n        Returns:\n            dict: serialized data.\n        \"\"\"\n        serial_data = {'nodes': {}, 'connections': []}\n        nodes_data = {}\n        for n in nodes:\n\n            # update the node model.\n            n.update_model()\n\n            nodes_data.update(n.model.to_dict)\n\n        for n_id, n_data in nodes_data.items():\n            serial_data['nodes'][n_id] = n_data\n\n            inputs = n_data.pop('inputs') if n_data.get('inputs') else {}\n            outputs = n_data.pop('outputs') if n_data.get('outputs') else {}\n\n            for pname, conn_data in inputs.items():\n                for conn_id, prt_names in conn_data.items():\n                    for conn_prt in prt_names:\n                        pipe = {'in': [n_id, pname], 'out': [conn_id, conn_prt]}\n                        if pipe not in serial_data['connections']:\n                            serial_data['connections'].append(pipe)\n\n            for pname, conn_data in outputs.items():\n                for conn_id, prt_names in conn_data.items():\n                    for conn_prt in prt_names:\n                        pipe = {'out': [n_id, pname], 'in': [conn_id, conn_prt]}\n                        if pipe not in serial_data['connections']:\n                            serial_data['connections'].append(pipe)\n\n        if not serial_data['connections']:\n            serial_data.pop('connections')\n\n        return serial_data\n\n    def _deserialize(self, data, relative_pos=False, pos=None):\n        \"\"\"\n        deserialize node data.\n\n        Args:\n            data (dict): node data.\n            relative_pos (bool): position node relative to the cursor.\n\n        Returns:\n            list[NodeGraphQt.Nodes]: list of node instances.\n        \"\"\"\n        nodes = {}\n\n        # build the nodes.\n        for n_id, n_data in data.get('nodes', {}).items():\n            identifier = n_data['type']\n            NodeInstance = NodeVendor.create_node_instance(identifier)\n            if NodeInstance:\n                node = NodeInstance()\n                node._graph = self\n\n                name = self.get_unique_name(n_data.get('name', node.NODE_NAME))\n                n_data['name'] = name\n\n                # set properties.\n                for prop, val in node.model.properties.items():\n                    if prop in n_data.keys():\n                        setattr(node.model, prop, n_data[prop])\n\n                # set custom properties.\n                for prop, val in n_data.get('custom', {}).items():\n                    if prop in node.model.custom_properties.keys():\n                        node.model.custom_properties[prop] = val\n\n                node.update()\n\n                self._undo_stack.push(\n                    NodeAddedCmd(self, node, n_data.get('pos'))\n                )\n                nodes[n_id] = node\n\n        # build the connections.\n        for connection in data.get('connections', []):\n            nid, pname = connection.get('in', ('', ''))\n            in_node = nodes.get(nid)\n            if not in_node:\n                continue\n            in_port = in_node.inputs().get(pname) if in_node else None\n\n            nid, pname = connection.get('out', ('', ''))\n            out_node = nodes.get(nid)\n            if not out_node:\n                continue\n            out_port = out_node.outputs().get(pname) if out_node else None\n\n            if in_port and out_port:\n                self._undo_stack.push(PortConnectedCmd(in_port, out_port))\n\n        node_objs = list(nodes.values())\n        if relative_pos:\n            self._viewer.move_nodes([n.view for n in node_objs])\n            [setattr(n.model, 'pos', n.view.pos) for n in node_objs]\n        elif pos:\n            self._viewer.move_nodes([n.view for n in node_objs], pos=pos)\n\n        return node_objs\n\n    def save_session(self, file_path):\n        \"\"\"\n        Saves the current node graph session layout to a JSON formatted file.\n\n        Args:\n            file_path (str): path to the saved node layout.\n        \"\"\"\n        serliazed_data = self._serialize(self.selected_nodes())\n        file_path = file_path.strip()\n        with open(file_path, 'w') as file_out:\n            json.dump(serliazed_data, file_out, indent=2, separators=(',', ':'))\n\n    def load_session(self, file_path):\n        \"\"\"\n        Load node graph session layout file.\n\n        Args:\n            file_path (str): path to the serialized layout file.\n        \"\"\"\n        file_path = file_path.strip()\n        if not os.path.isfile(file_path):\n            raise IOError('file does not exist.')\n\n        self.clear_session()\n\n        try:\n            with open(file_path) as data_file:\n                layout_data = json.load(data_file)\n        except Exception as e:\n            layout_data = None\n            print('Cannot read data from file.\\n{}'.format(e))\n\n        if not layout_data:\n            return\n\n        self._deserialize(layout_data)\n        self._undo_stack.clear()\n        self._model.session = file_path\n\n    def copy_nodes(self, nodes=None):\n        \"\"\"\n        copy nodes to the clipboard by default this method copies\n        the selected nodes from the node graph.\n\n        Args:\n            nodes (list[NodeGraphQt.Node]): list of node instances.\n        \"\"\"\n        nodes = nodes or self.selected_nodes()\n        if not nodes:\n            return False\n        clipboard = QClipboard()\n        serial_data = self._serialize(nodes)\n        serial_str = json.dumps(serial_data)\n        if serial_str:\n            clipboard.setText(serial_str)\n            return True\n        return False\n\n    def paste_nodes(self):\n        \"\"\"\n        Pastes nodes from the clipboard.\n        \"\"\"\n        clipboard = QClipboard()\n        cb_string = clipboard.text()\n        if not cb_string:\n            return\n\n        self._undo_stack.beginMacro('pasted nodes')\n        serial_data = json.loads(cb_string)\n        self.clear_selection()\n        nodes = self._deserialize(serial_data, True)\n        [n.set_selected(True) for n in nodes]\n        self._undo_stack.endMacro()\n\n    def duplicate_nodes(self, nodes):\n        \"\"\"\n        Create duplicates nodes.\n\n        Args:\n            nodes (list[NodeGraphQt.Node]): list of node objects.\n        Returns:\n            list[NodeGraphQt.Node]: list of duplicated node instances.\n        \"\"\"\n        if not nodes:\n            return\n\n        self._undo_stack.beginMacro('duplicated nodes')\n\n        self.clear_selection()\n        serial = self._serialize(nodes)\n        new_nodes = self._deserialize(serial)\n        offset = 50\n        for n in new_nodes:\n            x, y = n.pos()\n            n.set_pos(x + offset, y + offset)\n            n.set_property('selected', True)\n\n        self._undo_stack.endMacro()\n        return new_nodes\n\n    def disable_nodes(self, nodes, mode=None):\n        \"\"\"\n        Disable/Enable specified nodes.\n\n        Args:\n            nodes (list[NodeGraphQt.Node]): list of node instances.\n            mode (bool): (optional) disable state of the nodes.\n        \"\"\"\n        if not nodes:\n            return\n        if mode is None:\n            mode = not nodes[0].disabled()\n        if len(nodes) > 1:\n            text = {False: 'enabled', True: 'disabled'}[mode]\n            text = '{} ({}) nodes'.format(text, len(nodes))\n            self._undo_stack.beginMacro(text)\n            [n.set_disabled(mode) for n in nodes]\n            self._undo_stack.endMacro()\n            return\n        nodes[0].set_disabled(mode)\n\n# --- Snippet Separator ---\n\ndef setup_actions(graph):\n    \"\"\"\n    build the base node graph menu commands.\n\n    Args:\n        graph (NodeGraphQt.NodeGraph):\n    \"\"\"\n    root_menu = graph.context_menu()\n    file_menu = root_menu.add_menu('&File')\n    edit_menu = root_menu.add_menu('&Edit')\n\n    # File menu.\n    file_menu.add_command('Open...',\n                          lambda: open_session(graph),\n                          QtGui.QKeySequence.Open)\n    file_menu.add_command('Save...',\n                          lambda: save_session(graph),\n                          QtGui.QKeySequence.Save)\n    file_menu.add_command('Save As...',\n                          lambda: save_session_as(graph),\n                          QtGui.QKeySequence.SaveAs)\n    file_menu.add_command('Clear', lambda: clear_session(graph))\n\n    file_menu.add_separator()\n\n    file_menu.add_command('Zoom In', lambda: zoom_in(graph), '=')\n    file_menu.add_command('Zoom Out', lambda: zoom_out(graph), '-')\n    file_menu.add_command('Reset Zoom', graph.reset_zoom, 'h')\n\n    # Edit menu.\n    undo_actn = graph.undo_stack().createUndoAction(graph.viewer(), '&Undo')\n    undo_actn.setShortcuts(QtGui.QKeySequence.Undo)\n    edit_menu.add_action(undo_actn)\n\n    redo_actn = graph.undo_stack().createRedoAction(graph.viewer(), '&Redo')\n    redo_actn.setShortcuts(QtGui.QKeySequence.Redo)\n    edit_menu.add_action(redo_actn)\n\n    edit_menu.add_separator()\n\n    edit_menu.add_command('Copy', graph.copy_nodes, QtGui.QKeySequence.Copy)\n    edit_menu.add_command('Paste', graph.paste_nodes, QtGui.QKeySequence.Paste)\n    edit_menu.add_command('Delete',\n                          lambda: graph.delete_nodes(graph.selected_nodes()),\n                          QtGui.QKeySequence.Delete)\n\n    edit_menu.add_separator()\n\n    edit_menu.add_command('Select all', graph.select_all, 'Ctrl+A')\n    edit_menu.add_command('Deselect all', graph.clear_selection, 'Ctrl+Shift+A')\n    edit_menu.add_command('Enable/Disable',\n                          lambda: graph.disable_nodes(graph.selected_nodes()),\n                          'd')\n\n    edit_menu.add_command('Duplicate',\n                          lambda: graph.duplicate_nodes(graph.selected_nodes()),\n                          'Alt+c')\n    edit_menu.add_command('Center Selection',\n                          graph.fit_to_selection,\n                          'f')\n\n    edit_menu.add_separator()\n\n# --- Snippet Separator ---\n\ndef setZoomModeEnabled(self, flag=True, color=None):\n        \"\"\"Deprecated, use :meth:`setInteractiveMode` instead.\n\n        Set the zoom mode if flag is True, else item selection is enabled.\n\n        Warning: Zoom and drawing are not compatible and cannot be enabled\n        simultanelously\n\n        :param bool flag: If True, enable zoom and select mode.\n        :param color: The color to use to draw the selection area.\n                      (Default: 'black')\n        :param color: The color to use to draw the selection area\n        :type color: string (\"#RRGGBB\") or 4 column unsigned byte array or\n                     one of the predefined color names defined in Colors.py\n        \"\"\"\n        _logger.warning(\n            'setZoomModeEnabled deprecated, use setInteractiveMode instead')\n        if color is None:\n            color = 'black'\n\n        if flag:\n            self.setInteractiveMode('zoom', color=color)\n        elif self.getInteractiveMode()['mode'] == 'zoom':\n            self.setInteractiveMode('select')\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that defines a set of functions to manipulate a node graph using the nodegraphqt library. The functions should allow to zoom in, zoom out, reset zoom, set layout direction to horizontal or vertical, open, import, save, and clear a session, clear undo history, copy, cut, paste, delete, extract, clear connections of, select all, clear selection of, invert selection of, disable, duplicate, expand group of nodes, fit the zoom level to selected nodes, show undo list, set pipe style to curved, straight or angled, set background grid to none, dots or lines, auto layout nodes downstream or upstream, and toggle node search.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 129, "repo_full_name": "fortra__impacket", "instruction": "Generate code that performs a simple ICMP6 ping. The code should take source and destination IP addresses as command line arguments. It should create an IP6 packet with the source and destination addresses, and send an ICMP echo request to the destination IP. The code should then wait for an echo reply, decode the reply using the ImpactDecoder, and print the size of the reply, the destination IP, and the echo sequence number if the reply type is an echo reply. The code should continue to send echo requests and listen for replies in an infinite loop.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class APT_VCP(serial_instrument.SerialInstrument):\n    \"\"\"\n    This class handles all the basic communication with APT virtual com ports\n    \"\"\"\n    port_settings = dict(baudrate=115200,\n                         bytesize=8,\n                         parity=serial.PARITY_NONE,\n                         stopbits=1,\n                         xonxoff=0,\n                         rtscts=0,\n                         timeout=1,\n                         writeTimeout=1)\n    termination_character = \"\"  # The APT communicates via fixed length messages therefore this is not required\n    surprise_message_codes = {'MGMSG_HW_RESPONSE': 0x0080,\n                              # The message id codes for messages sent from the hardware to the device.\n                              'MGMSG_HW_RICHRESPONSE': 0x0081,\n                              # One for one line error code and one for longer error codes\n                              'Status update id': None}  # This is the satus update message id that varies for each device and therefore must be set\n\n    channel_number_to_identity = {1: 0x01, 2: 0x02, 3: 0x04, 4: 0x08}  # Sets up the channel numbers to values\n    state_conversion = {True: 0x01,\n                        False: 0x02}  # Sets up the conversion from True and False values to 1's and 2's (godknows why they havnt used 0 and 1)\n    reverse_state_conversion = {0x01: True, 0x02: False}\n    serial_num_to_device_types = {0: ['Filter flipper', 'MFF002'],\n                                  20: ['Legacy Single channel stepper driver', 'BSC001'],\n                                  25: ['Legacy single channel mini stepper driver', 'BMS001'],\n                                  27: ['K - Cube brushed DC servo driver', 'KDCT101'],\n                                  28: ['K - Cube brushless DC servo driver', 'KBD101'],\n                                  30: ['Legacy dual channel stepper driver', 'BSC002'],\n                                  35: ['Legacy dual channel mini stepper driver', 'BMS002'],\n                                  40: ['Single channel stepper driver', 'BSC101'],\n                                  60: ['OptoSTDriver(mini stepper driver)', 'OST001'],\n                                  63: ['OptoDCDriver (mini DC servo driver)', 'ODC001'],\n                                  70: ['Three channel card slot stepper driver', 'BSC103'],\n                                  80: ['Stepper Driver T-Cube', 'TST001'],\n                                  83: ['DC Driver T-Cube', 'TDC001'],\n                                  73: ['Brushless DC motherboard', 'BBD102/BBD103'],\n                                  94: ['Brushless DC motor card', 'BBD102/BBD103']}\n    command_log = deque(maxlen=20)  # stores commands sent to the device\n    timeout = 30\n    def __init__(self, port=None, source=0x01, destination=None, use_si_units=False, stay_alive=False):\n        \"\"\"\n        Set up the serial port, setting source and destinations, verbosity and hardware info.\n        \"\"\"\n        serial_instrument.SerialInstrument.__init__(self, port=port)  # this opens the port\n        self.source = source\n        if destination is None:\n            self._logger.error('destination has not been set!')\n        elif type(destination) != dict:\n            self.destination = {'1': destination}\n        else:\n            self.destination = destination\n        self.stay_alive = stay_alive\n        self.serial_number = None\n        self.model = None\n        self.number_of_channels = None\n        hrdwr_info = self.get_hardware_info()  # sets things like the serial_number, model and number_of_channels\n        self._logger.debug(hrdwr_info)\n\n    @staticmethod\n    def unpack_binary_mask(value, size=13):\n        lst = [bool(value & (1 << size - i - 1)) for i in range(size)]\n        lst.reverse()\n        return lst\n\n    @staticmethod\n    def _bit_mask_array(value, bit_mask):\n        final_mask = []\n        for mask in bit_mask:\n            final_mask += [bool(value & int(mask))]\n        return final_mask\n\n    def read(self):\n        \"\"\"Overwrite the read command with a fixed length read, check\n            for additional data stream and error codes\"\"\"\n        header = bytearray(self.ser.read(6))  # read 6 byte header\n        msgid, length, dest, source = struct.unpack('<HHBB',\n                                                    header)  # unpack the header as described by the format were a second data stream is expected\n        if msgid in list(self.surprise_message_codes.values()):  # Compare the message code to the list of suprise message codes\n            if msgid == self.surprise_message_codes['MGMSG_HW_RESPONSE']:\n                msgid, param1, param2, dest, source = struct.unpack('<HBBBB', header)\n                returned_message = {'message': msgid, 'param1': param1,\n                                    'param2': param2, 'dest': dest,\n                                    'source': source}\n                self._logger.debug(returned_message)\n                self.read()\n            elif msgid == self.surprise_message_codes['MGMSG_HW_RICHRESPONSE']:\n                data = self.ser.read(length)\n                returned_message = {'message': msgid, 'length': length,\n                                    'dest': dest, 'source': source,\n                                    'data': data}\n                self._logger.debug(returned_message)\n                self.read()\n            elif (msgid == self.surprise_message_codes['Status update id']\n                  and self.command_log[-1] == self.surprise_message_codes['Status update id']):\n                data = self.ser.read(length)\n                returned_message = {'message': msgid, 'length': length,\n                                    'dest': dest, 'source': source,\n                                    'data': data}\n                self.update_status(returned_message)\n                self.read()\n        else:\n            if self.source | 0x80 == dest:\n                data = self.ser.read(length)\n                returned_message = {'message': msgid, 'length': length,\n                                    'dest': dest, 'source': source,\n                                    'data': data}\n            elif self.source != dest:\n                if dest <= 0x80:\n                    self.source = dest\n                else:\n                    self.source = dest-128\n                data = self.ser.read(length)\n                returned_message = {'message': msgid, 'length': length,\n                                    'dest': dest, 'source': source,\n                                    'data': data}\n            else:\n                msgid, param1, param2, dest, source = struct.unpack('<HBBBB', header)\n\n                returned_message = {'message': msgid, 'param1': param1,\n                                    'param2': param2, 'dest': dest,\n                                    'source': source}\n            return returned_message\n\n    def _write(self, message_id, param1=0x00, param2=0x00, data=None, destination_id=None):\n        \"\"\"Overwrite the serial write command to combine message_id,\n            two possible parameters (set to 0 if not given)\n            with the source and destinations \"\"\"\n        if destination_id is None:\n            destination = list(self.destination.values())[0]\n        else:\n            destination = self.destination[destination_id]\n        if data is None:\n            formated_message = bytearray(struct.pack('<HBBBB', message_id, param1, param2, destination, self.source))\n        else:\n            param1 = len(data)\n            formated_message = bytearray(struct.pack('<HBBBB', message_id, param1, param2,\n                                                     destination | 0x80, self.source))\n            formated_message += data\n\n        if len(self.command_log) == self.command_log.maxlen \\\n                and 0x0492 not in self.command_log \\\n                and self.stay_alive:\n            self.command_log.append(0x0492)\n            self.staying_alive()\n        self.command_log.append(message_id)\n        self.ser.write(formated_message)\n    write = _write\n\n    def query(self, message_id, param1=0x00, param2=0x00, data=None, destination_id=None, blocking=False):\n        \"\"\"Overwrite the query command to allow the correct passing of message_ids and parameters\"\"\"\n        with self.communications_lock:\n            self.flush_input_buffer()\n            self._write(message_id, param1, param2, data=data, destination_id=destination_id)\n            time.sleep(0.1)\n            if blocking:\n                reply = self._waitForReply()\n                if reply[0]:\n                    return reply[1]\n                else:\n                    self._logger.error('No reply received for message ' + str(message_id))\n                    return reply[1]\n            else:\n                return self.read()  # question: should we strip the final newline?\n\n    # Listing General control message, not all of these can be used with every piece of equipment\n    def identify(self):\n        \"\"\"Instruct hardware unit to identify itself by flashing its LED\"\"\"\n        self.write(0x0223)\n\n    def set_channel_state(self, channel_number, new_state, destination_id=None):\n        \"\"\"Enable or disable a channel\"\"\"\n        channel_identity = self.channel_number_to_identity[channel_number]\n        new_state = self.state_conversion[new_state]\n        self.write(0x0210, param1=channel_identity, param2=new_state, destination_id=destination_id)\n\n    def get_channel_state(self, channel_number, destination_id=None):\n        \"\"\"Get the current state of a channel\"\"\"\n        message_dict = self.query(0x0211, param1=self.channel_number_to_identity[\n            channel_number], destination_id=destination_id)  # Get the entire message dictionary\n        current_state = self.reverse_state_conversion[\n            message_dict['param2']]  # pull out the current state parameter and convert it to a True/False value\n        return current_state\n\n    def disconnect(self, destination_id=None):\n        \"\"\"Disconnect the controller from the usb bus\"\"\"\n        self.write(0x002, destination_id=destination_id)\n\n    def enable_updates(self, enable_state, update_rate=10, destination_id=None):\n        \"\"\"Enable or disable hardware updates\"\"\"\n        if enable_state:\n            self.write(0x0011, param1=update_rate, destination_id=destination_id)\n        else:\n            self.write(0x0012, destination_id=destination_id)\n\n    def get_hardware_info(self, destination_id=None):\n        \"\"\"Manually get a status update\"\"\"\n        message_dict = self.query(0x0005, destination_id=destination_id)\n        serialnum, model, hwtype, swversion, notes, hwversion, modstate, nchans = struct.unpack('<I8sHI48s12xHHH',\n                                                                                                message_dict['data'])\n        if serialnum != 0 and len(str(serialnum)) != 8:\n            serialnum = int(hex(serialnum)[2:-1])\n\n        hardware_dict = {'serial_number': serialnum, 'model': str(model).replace('\\x00', ''), 'hardware_type': hwtype,\n                         'software_version': swversion, 'notes': str(notes).replace('\\x00', ''),\n                         'hardware_version': hwversion,\n                         'modstate': modstate, 'number_of_channels': nchans}\n        self.serial_number = serialnum\n\n        try:\n            self.model = self.serial_num_to_device_types[int(str(serialnum)[0:2])]\n        except KeyError:\n            self.model = ['Dummy', 'Serial number not recognised in the serial_num_to_device_types']\n            self._logger.warn('Serial number not recognised. Model set to Dummy')\n        self.number_of_channels = nchans\n        return hardware_dict\n\n    def get_status_update(self):\n        \"\"\"This need subclassing and written over as the commands and format\n        varies between devices\"\"\"\n        raise NotImplementedError\n\n    def update_status(self):\n        \"\"\"This  command should update device properties from the update message\n            however this has to be defined for every device as the status update format\n            and commands vary,\n            please implement me\n            Args:\n                The returned message from a status update request           (dict)\"\"\"\n        raise NotImplementedError\n\n    def staying_alive(self, destination_id=None):\n        \"\"\"Keeps the motor controller from thinking the Pc has crashed \"\"\"\n        if destination_id is None:\n            destination_id = list(self.destination.keys())\n        else:\n            if not hasattr(destination_id, '__iter__'):\n                destination_id = tuple(destination_id)\n        for dest in destination_id:\n            self._logger.debug(str(dest))\n            self.write(0x0492, destination_id=dest)\n        self._logger.debug(str(destination_id) + str(dest))\n\n    def _waitForReply(self):\n        reply = ''\n        t0 = time.time()\n        while len(reply) == 0:\n            try:\n                reply = self.read()\n            except struct.error:\n                reply = ''\n            time.sleep(0.1)\n            if time.time() - t0 > self.timeout:\n                return False, ''\n        return True, reply\n\n# --- Snippet Separator ---\n\ndef _handshake(self, message, full_reply, *args, **kwargs):\n        \"\"\"Checks command was executed without errors\n\n        Most times a command is sent to the streak, it replies with a message containing an error code (see ERROR_CODES)\n        and the command name. This method returns the error message if there's been an error, or tries to handshake\n        again if there was no handshake message.\n\n        :return:\n        \"\"\"\n        try:\n            split_reply = full_reply.split(',', 2)\n\n            # Some commands have a response (len = 3), others simply have a handshake (len = 2)\n            if len(split_reply) == 2:\n                split_reply += ('', )\n            error_code, command, reply = split_reply\n\n            if error_code in ['4', '5']:\n                # Some messages in the buffer simply state the streak status\n                self._logger.debug('Useless reply:\\t%s\\t%s\\t%s' % (error_code, command, reply))\n                # They are generally not useful, so we handshake again\n                full_reply = self.read(*args, **kwargs)\n                return self._handshake(message, full_reply, *args, **kwargs)\n            elif message.split('(')[0] != command:\n                self._logger.error('Comparing this: %s \\t to this: %s' % (message.split('(')[0], command))\n                raise RuntimeError('Replied command does not match')\n            elif error_code == '0':\n                self._logger.debug('Handshake worked\\t%s\\t%s' % (command, reply))\n                return reply\n            else:\n                raise StreakError(error_code, message, full_reply)\n        except Exception as e:\n            self._logger.warn('Handshake failed: %s' % e)\n\n# --- Snippet Separator ---\n\ndef aux_send(self, topic: str, request: Shareable, timeout: float, fl_ctx: FLContext) -> Shareable:\n        \"\"\"Send the request to the Server.\n\n        If reply is received, make sure to set peer_ctx into the reply shareable!\n\n        Args:\n            topic: topic of the request\n            request: request Shareable to be sent\n            timeout: number of secs to wait for reply. 0 means fire-and-forget.\n            fl_ctx: fl context\n\n        Returns: a reply.\n\n        \"\"\"\n        pass\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a simple ICMP6 ping. The code should take source and destination IP addresses as command line arguments. It should create an IP6 packet with the source and destination addresses, and send an ICMP echo request to the destination IP. The code should then wait for an echo reply, decode the reply using the ImpactDecoder, and print the size of the reply, the destination IP, and the echo sequence number if the reply type is an echo reply. The code should continue to send echo requests and listen for replies in an infinite loop.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 130, "repo_full_name": "pyomo__mpi-sppy", "instruction": "Generate code that imports necessary modules and functions from the mpi-sppy library and other necessary libraries. The code should define a function to parse arguments from the command line and set up a configuration object with various parameters. Then, it should define a main function that uses the parsed arguments to set up a scenario creator and a list of scenario names. The main function should also check if the number of scenarios is in a predefined list of available scenarios. Depending on the configuration, the main function should set up different extensions and spokes for the scenario. The main function should also create a WheelSpinner object with the hub and spokes, spin the wheel, and write the solution to a file if a solution directory is provided. Finally, the code should call the main function if the script is run as the main program.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class StreamObject(object):\n    '''For most methods, there are three stream functions to pipe computing stream:\n\n    1 ``.set_`` function to update object attributes, eg\n    ``mf = scf.RHF(mol).set(conv_tol=1e-5)`` is identical to proceed in two steps\n    ``mf = scf.RHF(mol); mf.conv_tol=1e-5``\n\n    2 ``.run`` function to execute the kenerl function (the function arguments\n    are passed to kernel function).  If keyword arguments is given, it will first\n    call ``.set`` function to update object attributes then execute the kernel\n    function.  Eg\n    ``mf = scf.RHF(mol).run(dm_init, conv_tol=1e-5)`` is identical to three steps\n    ``mf = scf.RHF(mol); mf.conv_tol=1e-5; mf.kernel(dm_init)``\n\n    3 ``.apply`` function to apply the given function/class to the current object\n    (function arguments and keyword arguments are passed to the given function).\n    Eg\n    ``mol.apply(scf.RHF).run().apply(mcscf.CASSCF, 6, 4, frozen=4)`` is identical to\n    ``mf = scf.RHF(mol); mf.kernel(); mcscf.CASSCF(mf, 6, 4, frozen=4)``\n    '''\n\n    verbose = 0\n    stdout = sys.stdout\n    _keys = set(['verbose', 'stdout'])\n\n    def kernel(self, *args, **kwargs):\n        '''\n        Kernel function is the main driver of a method.  Every method should\n        define the kernel function as the entry of the calculation.  Note the\n        return value of kernel function is not strictly defined.  It can be\n        anything related to the method (such as the energy, the wave-function,\n        the DFT mesh grids etc.).\n        '''\n        pass\n\n    def pre_kernel(self, envs):\n        '''\n        A hook to be run before the main body of kernel function is executed.\n        Internal variables are exposed to pre_kernel through the \"envs\"\n        dictionary.  Return value of pre_kernel function is not required.\n        '''\n        pass\n\n    def post_kernel(self, envs):\n        '''\n        A hook to be run after the main body of the kernel function.  Internal\n        variables are exposed to post_kernel through the \"envs\" dictionary.\n        Return value of post_kernel function is not required.\n        '''\n        pass\n\n    def run(self, *args, **kwargs):\n        '''\n        Call the kernel function of current object.  `args` will be passed\n        to kernel function.  `kwargs` will be used to update the attributes of\n        current object.  The return value of method run is the object itself.\n        This allows a series of functions/methods to be executed in pipe.\n        '''\n        self.set(**kwargs)\n        self.kernel(*args)\n        return self\n\n    def set(self, *args, **kwargs):\n        '''\n        Update the attributes of the current object.  The return value of\n        method set is the object itself.  This allows a series of\n        functions/methods to be executed in pipe.\n        '''\n        if args:\n            warnings.warn('method set() only supports keyword arguments.\\n'\n                          'Arguments %s are ignored.' % args)\n        #if getattr(self, '_keys', None):\n        #    for k,v in kwargs.items():\n        #        setattr(self, k, v)\n        #        if k not in self._keys:\n        #            sys.stderr.write('Warning: %s does not have attribute %s\\n'\n        #                             % (self.__class__, k))\n        #else:\n        for k,v in kwargs.items():\n            setattr(self, k, v)\n        return self\n\n    # An alias to .set method\n    __call__ = set\n\n    def apply(self, fn, *args, **kwargs):\n        '''\n        Apply the fn to rest arguments:  return fn(*args, **kwargs).  The\n        return value of method set is the object itself.  This allows a series\n        of functions/methods to be executed in pipe.\n        '''\n        return fn(self, *args, **kwargs)\n\n#    def _format_args(self, args, kwargs, kernel_kw_lst):\n#        args1 = [kwargs.pop(k, v) for k, v in kernel_kw_lst]\n#        return args + args1[len(args):], kwargs\n\n    def check_sanity(self):\n        '''\n        Check input of class/object attributes, check whether a class method is\n        overwritten.  It does not check the attributes which are prefixed with\n        \"_\".  The\n        return value of method set is the object itself.  This allows a series\n        of functions/methods to be executed in pipe.\n        '''\n        if (SANITY_CHECK and\n            self.verbose > 0 and  # logger.QUIET\n            getattr(self, '_keys', None)):\n            check_sanity(self, self._keys, self.stdout)\n        return self\n\n    def view(self, cls):\n        '''New view of object with the same attributes.'''\n        obj = cls.__new__(cls)\n        obj.__dict__.update(self.__dict__)\n        return obj\n\n# --- Snippet Separator ---\n\nclass MainFunction(Function):\n    \"\"\"Subclass of Function that allows multiple functions and variables to\n    be defined in a single code string. The code must contain a main() function\n    definition.\n    \"\"\"\n\n    def __init__(self, shader_type, *args, **kwargs):\n        self.shader_type = shader_type\n        self._chains = {}\n        Function.__init__(self, *args, **kwargs)\n\n    @property\n    def signature(self):\n        return ('main', [], 'void')\n\n    @property\n    def version_pragma(self):\n        \"\"\"Return version number and extra qualifiers from pragma if present.\"\"\"\n        m = re.search(parsing.re_version_pragma, self.code)\n        if m is None:\n            return None\n        return int(m.group(1)), m.group(2)\n\n    def definition(self, obj_names, version, shader):\n        code = Function.definition(self, obj_names, version, shader)\n        # strip out version pragma before returning code; this will be\n        # added to the final compiled code later.\n        code = re.sub(parsing.re_version_pragma, '', code)\n        return code\n\n    def static_names(self):\n        if self._static_vars is not None:\n            return self._static_vars\n\n        # parse static variables\n        names = Function.static_names(self)\n\n        # parse all function names + argument names\n        funcs = parsing.find_functions(self.code)\n        for f in funcs:\n            if f[0] == 'main':\n                continue\n            names.append(f[0])\n            for arg in f[1]:\n                names.append(arg[1])\n\n        self._static_vars = names\n        return names\n\n    def add_chain(self, var):\n        \"\"\"Create a new ChainFunction and attach to $var.\"\"\"\n        chain = FunctionChain(var, [])\n        self._chains[var] = chain\n        self[var] = chain\n\n    def add_callback(self, hook, func):\n        self._chains[hook].append(func)\n\n    def remove_callback(self, hook, func):\n        self._chains[hook].remove(func)\n\n# --- Snippet Separator ---\n\nclass WheelSpinner:\n\n    def __init__(self, hub_dict, list_of_spoke_dict):\n        \"\"\" top level for the hub and spoke system\n        Args:\n            hub_dict(dict): controls hub creation\n            list_of_spoke_dict(list dict): controls creation of spokes\n\n        Returns:\n            spcomm (Hub or Spoke object): the object that did the work (windowless)\n            opt_dict (dict): the dictionary that controlled creation for this rank\n\n        NOTE: the return is after termination; the objects are provided for query.\n\n        \"\"\"\n        if not haveMPI:\n            raise RuntimeError(\"spin_the_wheel called, but cannot import mpi4py\")\n        self.hub_dict = hub_dict\n        self.list_of_spoke_dict = list_of_spoke_dict\n\n        self._ran = False\n\n    def spin(self, comm_world=None):\n        return self.run(comm_world=comm_world)\n\n    def run(self, comm_world=None):\n        \"\"\" top level for the hub and spoke system\n        Args:\n            comm_world (MPI comm): the world for this hub-spoke system\n        \"\"\"\n        if self._ran:\n            raise RuntimeError(\"WheelSpinner can only be run once\")\n\n        hub_dict = self.hub_dict\n        list_of_spoke_dict = self.list_of_spoke_dict\n\n        # Confirm that the provided dictionaries specifying\n        # the hubs and spokes contain the appropriate keys\n        if \"hub_class\" not in hub_dict:\n            raise RuntimeError(\n                \"The hub_dict must contain a 'hub_class' key specifying \"\n                \"the hub class to use\"\n            )\n        if \"opt_class\" not in hub_dict:\n            raise RuntimeError(\n                \"The hub_dict must contain an 'opt_class' key specifying \"\n                \"the SPBase class to use (e.g. PHBase, etc.)\"\n            )\n        if \"hub_kwargs\" not in hub_dict:\n            hub_dict[\"hub_kwargs\"] = dict()\n        if \"opt_kwargs\" not in hub_dict:\n            hub_dict[\"opt_kwargs\"] = dict()\n        for spoke_dict in list_of_spoke_dict:\n            if \"spoke_class\" not in spoke_dict:\n                raise RuntimeError(\n                    \"Each spoke_dict must contain a 'spoke_class' key \"\n                    \"specifying the spoke class to use\"\n                )\n            if \"opt_class\" not in spoke_dict:\n                raise RuntimeError(\n                    \"Each spoke_dict must contain an 'opt_class' key \"\n                    \"specifying the SPBase class to use (e.g. PHBase, etc.)\"\n                )\n            if \"spoke_kwargs\" not in spoke_dict:\n                spoke_dict[\"spoke_kwargs\"] = dict()\n            if \"opt_kwargs\" not in spoke_dict:\n                spoke_dict[\"opt_kwargs\"] = dict()\n\n        if comm_world is None:\n            comm_world = MPI.COMM_WORLD\n        n_spokes = len(list_of_spoke_dict)\n\n        # Create the necessary communicators\n        fullcomm = comm_world\n        strata_comm, cylinder_comm = _make_comms(n_spokes, fullcomm=fullcomm)\n        strata_rank = strata_comm.Get_rank()\n        cylinder_rank = cylinder_comm.Get_rank()\n        global_rank = fullcomm.Get_rank()\n\n        # Assign hub/spokes to individual ranks\n        if strata_rank == 0: # This rank is a hub\n            sp_class = hub_dict[\"hub_class\"]\n            sp_kwargs = hub_dict[\"hub_kwargs\"]\n            opt_class = hub_dict[\"opt_class\"]\n            opt_kwargs = hub_dict[\"opt_kwargs\"]\n            opt_dict = hub_dict\n        else: # This rank is a spoke\n            spoke_dict = list_of_spoke_dict[strata_rank - 1]\n            sp_class = spoke_dict[\"spoke_class\"]\n            sp_kwargs = spoke_dict[\"spoke_kwargs\"]\n            opt_class = spoke_dict[\"opt_class\"]\n            opt_kwargs = spoke_dict[\"opt_kwargs\"]\n            opt_dict = spoke_dict\n\n        # Create the appropriate opt object locally\n        opt_kwargs[\"mpicomm\"] = cylinder_comm\n        opt = opt_class(**opt_kwargs)\n\n        # Create the SPCommunicator object (hub/spoke) with\n        # the appropriate SPBase object attached\n        if strata_rank == 0: # Hub\n            spcomm = sp_class(opt, fullcomm, strata_comm, cylinder_comm,\n                              list_of_spoke_dict, **sp_kwargs) \n        else: # Spokes\n            spcomm = sp_class(opt, fullcomm, strata_comm, cylinder_comm, **sp_kwargs) \n\n        # Create the windows, run main(), destroy the windows\n        spcomm.make_windows()\n        if strata_rank == 0:\n            spcomm.setup_hub()\n\n        global_toc(\"Starting spcomm.main()\")\n        spcomm.main()\n        if strata_rank == 0: # If this is the hub\n            spcomm.send_terminate()\n\n        # Anything that's left to do\n        spcomm.finalize()\n\n        # to ensure the messages below are True\n        cylinder_comm.Barrier()\n        global_toc(f\"Hub algorithm {opt_class.__name__} complete, waiting for spoke finalization\")\n        global_toc(f\"Spoke {sp_class.__name__} finalized\", (cylinder_rank == 0 and strata_rank != 0))\n\n        fullcomm.Barrier()\n\n        ## give the hub the chance to catch new values\n        spcomm.hub_finalize()\n\n        fullcomm.Barrier()\n\n        spcomm.free_windows()\n        global_toc(\"Windows freed\")\n\n        self.spcomm = spcomm\n        self.opt_dict = opt_dict\n        self.global_rank = global_rank\n        self.strata_rank = strata_rank\n        self.cylinder_rank = cylinder_rank\n\n        if self.strata_rank == 0:\n            self.BestInnerBound = spcomm.BestInnerBound\n            self.BestOuterBound = spcomm.BestOuterBound\n        else: # the cylinder ranks don't track the inner / outer bounds\n            self.BestInnerBound = None\n            self.BestOuterBound = None\n\n        self._ran = True\n\n    def on_hub(self):\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before finding out.\")\n        return (\"hub_class\" in self.opt_dict)\n\n    def write_first_stage_solution(self, solution_file_name,\n            first_stage_solution_writer=first_stage_nonant_writer):\n        \"\"\" Write a solution file, if a solution is available, to the solution_file_name provided\n        Args:\n            solution_file_name : filename to write the solution to\n            first_stage_solution_writer (optional) : custom first stage solution writer function\n        \"\"\"\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before querying solutions.\")\n        winner = self._determine_innerbound_winner()\n        if winner:\n            self.spcomm.opt.write_first_stage_solution(solution_file_name,first_stage_solution_writer)\n\n    def write_tree_solution(self, solution_directory_name,\n            scenario_tree_solution_writer=scenario_tree_solution_writer):\n        \"\"\" Write a tree solution directory, if available, to the solution_directory_name provided\n        Args:\n            solution_file_name : filename to write the solution to\n            scenario_tree_solution_writer (optional) : custom scenario solution writer function\n        \"\"\"\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before querying solutions.\")\n        winner = self._determine_innerbound_winner()\n        if winner:\n            self.spcomm.opt.write_tree_solution(solution_directory_name,scenario_tree_solution_writer)\n\n    def local_nonant_cache(self):\n        \"\"\" Returns a dict with non-anticipative values at each local node\n            We assume that the optimization has been done before calling this\n        \"\"\"\n        if not self._ran:\n            raise RuntimeError(\"Need to call WheelSpinner.run() before querying solutions.\")\n        local_xhats = dict()\n        for k,s in self.spcomm.opt.local_scenarios.items():\n            for node in s._mpisppy_node_list:\n                if node.name not in local_xhats:\n                    local_xhats[node.name] = [\n                        value(var) for var in node.nonant_vardata_list]\n        return local_xhats\n\n    def _determine_innerbound_winner(self):\n        if self.spcomm.global_rank == 0:\n            if self.spcomm.last_ib_idx is None:\n                best_strata_rank = -1\n                global_toc(\"No incumbent solution available to write!\")\n            else:\n                best_strata_rank = self.spcomm.last_ib_idx\n        else:\n            best_strata_rank = None\n\n        best_strata_rank = self.spcomm.fullcomm.bcast(best_strata_rank, root=0)\n        return (self.spcomm.strata_rank == best_strata_rank)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary modules and functions from the mpi-sppy library and other necessary libraries. The code should define a function to parse arguments from the command line and set up a configuration object with various parameters. Then, it should define a main function that uses the parsed arguments to set up a scenario creator and a list of scenario names. The main function should also check if the number of scenarios is in a predefined list of available scenarios. Depending on the configuration, the main function should set up different extensions and spokes for the scenario. The main function should also create a WheelSpinner object with the hub and spokes, spin the wheel, and write the solution to a file if a solution directory is provided. Finally, the code should call the main function if the script is run as the main program.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 131, "repo_full_name": "matplotlib__basemap", "instruction": "Generate code that creates a series of maps using the Basemap library. The maps should be orthographic projections centered at latitude 45 and longitude -100. The maps should have coastlines, country boundaries, and continents filled with color. The maps should also have meridians and parallels drawn every 30 degrees. \n\nThe code should plot the locations of five cities on the map, represented by filled circles, and their names should be displayed next to the circles. \n\nThe code should also generate some data on a regular lat/lon grid and contour this data over the map. \n\nThe maps should be displayed with different backgrounds: filled continent, land-sea mask, blue marble, shaded relief, etopo, and etopo with land areas transparent. \n\nFinally, the code should display all the generated maps.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class GMapOptions(HasProps):\n    \"\"\" Options for GMapPlot objects.\n\n    \"\"\"\n\n    lat = Float(help=\"\"\"\n    The latitude where the map should be centered.\n    \"\"\")\n\n    lng = Float(help=\"\"\"\n    The longitude where the map should be centered.\n    \"\"\")\n\n    zoom = Int(12, help=\"\"\"\n    The initial `zoom level`_ to use when displaying the GMapPlot.\n\n    .. _zoom level: https://developers.google.com/maps/documentation/staticmaps/#Zoomlevels\n\n    \"\"\")\n\n    map_type = Enum(MapType, help=\"\"\"\n    The `map type`_ to use for the GMapPlot.\n\n    .. _map type: https://developers.google.com/maps/documentation/staticmaps/#MapTypes\n\n    \"\"\")\n\n# --- Snippet Separator ---\n\nclass StationPlotLayout(dict):\n    r\"\"\"Make a layout to encapsulate plotting using `StationPlot`.\n\n    This class keeps a collection of offsets, plot formats, etc. for a parameter based\n    on its name. This then allows a dictionary of data (or any object that allows looking\n    up of arrays based on a name) to be passed to `plot()` to plot the data all at once.\n\n    See Also\n    --------\n    StationPlot\n\n    \"\"\"\n\n    class PlotTypes(Enum):\n        r\"\"\"Different plotting types for the layout.\n\n        Controls how items are displayed (e.g. converting values to symbols).\n        \"\"\"\n\n        value = 1\n        symbol = 2\n        text = 3\n        barb = 4\n\n    def add_value(self, location, name, fmt='.0f', units=None, **kwargs):\n        r\"\"\"Add a numeric value to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. The conversion of the data values to\n        a string is controlled by `fmt`. The units required for plotting can also\n        be passed in using `units`, which will cause the data to be converted before\n        plotting.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        fmt : str or Callable, optional\n            How to format the data as a string for plotting. If a string, it should be\n            compatible with the :func:`format` builtin. If a callable, this should take a\n            value and return a string. Defaults to '0.f'.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_text\n\n        \"\"\"\n        self[location] = (self.PlotTypes.value, name, (fmt, units, kwargs))\n\n    def add_symbol(self, location, name, symbol_mapper, **kwargs):\n        r\"\"\"Add a symbol to the station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted. Data values will be converted to glyphs\n        appropriate for MetPy's symbol font using the callable `symbol_mapper`.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple[float, float]\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        symbol_mapper : Callable\n            Controls converting data values to unicode code points for the\n            :data:`!metpy.plots.wx_symbols.wx_symbol_font` font. This should take a value and\n            return a single unicode character. See :mod:`!metpy.plots.wx_symbols` for included\n            mappers.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_text, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.symbol, name, (symbol_mapper, kwargs))\n\n    def add_text(self, location, name, **kwargs):\n        r\"\"\"Add a text field to the  station layout.\n\n        This specifies that at the offset `location`, data should be pulled from the data\n        container using the key `name` and plotted directly as text with no conversion\n        applied.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or font properties.\n\n        Parameters\n        ----------\n        location : str or tuple(float, float)\n            The offset (relative to center) to plot this value. If str, should be one of\n            'C', 'N', 'NE', 'E', 'SE', 'S', 'SW', 'W', or 'NW'. Otherwise, should be a tuple\n            specifying the number of increments in the x and y directions.\n        name : str\n            The name of the parameter, which is used as a key to pull data out of the\n            data container passed to :meth:`plot`.\n        kwargs\n            Additional keyword arguments to use for matplotlib's plotting functions.\n\n        See Also\n        --------\n        add_barb, add_symbol, add_value\n\n        \"\"\"\n        self[location] = (self.PlotTypes.text, name, kwargs)\n\n    def add_barb(self, u_name, v_name, units=None, **kwargs):\n        r\"\"\"Add a wind barb to the center of the station layout.\n\n        This specifies that u- and v-component data should be pulled from the data\n        container using the keys `u_name` and `v_name`, respectively, and plotted as\n        a wind barb at the center of the station plot. If `units` are given, both\n        components will be converted to these units.\n\n        Additional keyword arguments given will be passed onto the actual plotting\n        code; this is useful for specifying things like color or line width.\n\n        Parameters\n        ----------\n        u_name : str\n            The name of the parameter for the u-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        v_name : str\n            The name of the parameter for the v-component for `barbs`, which is used as\n            a key to pull data out of the data container passed to :meth:`plot`.\n        units : pint.Unit, optional\n            The units to use for plotting. Data will be converted to this unit before\n            conversion to a string. If not specified, no conversion is done.\n        kwargs\n            Additional keyword arguments to use for matplotlib's\n            :meth:`~matplotlib.axes.Axes.barbs` function.\n\n        See Also\n        --------\n        add_symbol, add_text, add_value\n\n        \"\"\"\n        # Not sure if putting the v_name as a plot-specific option is appropriate,\n        # but it seems simpler than making name code in plot handle tuples\n        self['barb'] = (self.PlotTypes.barb, (u_name, v_name), (units, kwargs))\n\n    def names(self):\n        \"\"\"Get the list of names used by the layout.\n\n        Returns\n        -------\n        list[str]\n            the list of names of variables used by the layout\n\n        \"\"\"\n        ret = []\n        for item in self.values():\n            if item[0] == self.PlotTypes.barb:\n                ret.extend(item[1])\n            else:\n                ret.append(item[1])\n        return ret\n\n    def plot(self, plotter, data_dict):\n        \"\"\"Plot a collection of data using this layout for a station plot.\n\n        This function iterates through the entire specified layout, pulling the fields named\n        in the layout from `data_dict` and plotting them using `plotter` as specified\n        in the layout. Fields present in the layout, but not in `data_dict`, are ignored.\n\n        Parameters\n        ----------\n        plotter : StationPlot\n            :class:`StationPlot` to use to plot the data. This controls the axes,\n            spacing, station locations, etc.\n        data_dict : dict[str, array-like]\n            Data container that maps a name to an array of data. Data from this object\n            will be used to fill out the station plot.\n\n        \"\"\"\n        def coerce_data(dat, u):\n            try:\n                return dat.to(u).magnitude\n            except AttributeError:\n                return dat\n\n        for loc, info in self.items():\n            typ, name, args = info\n            if typ == self.PlotTypes.barb:\n                # Try getting the data\n                u_name, v_name = name\n                u_data = data_dict.get(u_name)\n                v_data = data_dict.get(v_name)\n\n                # Plot if we have the data\n                if not (v_data is None or u_data is None):\n                    units, kwargs = args\n                    plotter.plot_barb(coerce_data(u_data, units), coerce_data(v_data, units),\n                                      **kwargs)\n            else:\n                # Check that we have the data for this location\n                data = data_dict.get(name)\n                if data is not None:\n                    # If we have it, hand it to the appropriate method\n                    if typ == self.PlotTypes.value:\n                        fmt, units, kwargs = args\n                        plotter.plot_parameter(loc, coerce_data(data, units), fmt, **kwargs)\n                    elif typ == self.PlotTypes.symbol:\n                        mapper, kwargs = args\n                        plotter.plot_symbol(loc, data, mapper, **kwargs)\n                    elif typ == self.PlotTypes.text:\n                        plotter.plot_text(loc, data, **args)\n\n    def __repr__(self):\n        \"\"\"Return string representation of layout.\"\"\"\n        return ('{'\n                + ', '.join(f'{loc}: ({info[0].name}, {info[1]}, ...)'\n                            for loc, info in sorted(self.items()))\n                + '}')\n\n# --- Snippet Separator ---\n\nclass HoverTool(Tool):\n    \"\"\" *toolbar icon*: |inspector_icon|\n\n    The hover tool is a passive inspector tool. It is generally on at\n    all times, but can be configured in the inspector's menu associated\n    with the *toolbar icon* shown above.\n\n    The hover tool displays informational tooltips whenever the cursor\n    is directly is over a glyph. The data to show comes from the glyph's\n    data source, and what is to be displayed is configurable through a\n    ``tooltips`` attribute that maps display names to columns in the\n    data source, or to special known variables.\n\n    Here is an example of how to configure and use the hover tool::\n\n        # Add tooltip (name, field) pairs to the tool. See below for a\n        # description of possible field values.\n        hover.tooltips = [\n            (\"index\", \"$index\"),\n            (\"(x,y)\", \"($x, $y)\"),\n            (\"radius\", \"@radius\"),\n            (\"fill color\", \"$color[hex, swatch]:fill_color\"),\n            (\"foo\", \"@foo\"),\n            (\"bar\", \"@bar\"),\n        ]\n\n    .. note::\n        Point hit testing is not currently available on all glyphs. Hover tool\n        currently does not work with line or image type glyphs.\n\n    .. |hover_icon| image:: /_images/icons/Inspector.png\n        :height: 18pt\n    \"\"\"\n\n    names = List(String, help=\"\"\"\n    A list of names to query for. If set, only renderers that\n    have a matching value for their ``name`` attribute will be used.\n    \"\"\")\n\n    renderers = List(Instance(Renderer), help=\"\"\"\n    An explicit list of renderers to hit test again. If unset,\n    defaults to all renderers on a plot.\n    \"\"\")\n\n    tooltips = List(Tuple(String, String), help=\"\"\"\n    The (name, field) pairs describing what the hover tool should\n    display when there is a hit.\n\n    Field names starting with \"@\" are interpreted as columns on the\n    data source. For instance, \"@temp\" would look up values to display\n    from the \"temp\" column of the data source.\n\n    Field names starting with \"$\" are special, known fields:\n\n    :$index: index of selected point in the data source\n    :$x: x-coordindate under the cursor in data space\n    :$y: y-coordindate under the cursor in data space\n    :$sx: x-coordindate under the cursor in screen (canvas) space\n    :$sy: y-coordindate under the cursor in screen (canvas) space\n    :$color: color data from data source, with the syntax:\n        ``$color[options]:field_name``. The available options\n        are: 'hex' (to display the color as a hex value), and\n        'swatch' to also display a small color swatch.\n\n    .. note::\n        The tooltips attribute can also be configured with a mapping type,\n        e.g. ``dict`` or ``OrderedDict``. However, if a ``dict`` is used,\n        the visual presentation order is unpecified.\n\n    \"\"\").accepts(Dict(String, String), lambda d: list(d.items()))\n\n    always_active = Bool(True, help=\"\"\"\n    Whether the hover tool must be explicitly activated.\n    \"\"\")\n\n    snap_to_data = Bool(True, help=\"\"\"\n    Whether the tooltip position should snap to the \"center\" position\n    of the associated glyph. For instance, if set to True, the tooltip\n    will point to the center of any marker (e.g., ``Circle``, `` Square``)\n    regardless of the cursor position, as long as the cursor hits the\n    glyph.\n\n    .. note::\n        Not all glyphs support this feature. Currenly all marker glyphs,\n        the ``Rect`` and ``Quad`` glyphs are supported.\n\n    \"\"\")\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a series of maps using the Basemap library. The maps should be orthographic projections centered at latitude 45 and longitude -100. The maps should have coastlines, country boundaries, and continents filled with color. The maps should also have meridians and parallels drawn every 30 degrees. \n\nThe code should plot the locations of five cities on the map, represented by filled circles, and their names should be displayed next to the circles. \n\nThe code should also generate some data on a regular lat/lon grid and contour this data over the map. \n\nThe maps should be displayed with different backgrounds: filled continent, land-sea mask, blue marble, shaded relief, etopo, and etopo with land areas transparent. \n\nFinally, the code should display all the generated maps.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 132, "repo_full_name": "iovisor__bcc", "instruction": "Generate code that uses the bcc library to create a histogram of system-wide strlen return values. The code should handle the case where strlen is an indirect function. It should define two BPF programs, one for counting the return values and another for submitting the addresses of the resolver and implementation functions. The code should also include functions to get the symbol of the indirect function, set the addresses of the resolver and implementation functions, and find the offset of the implementation function. In the main function, it should get the symbol of the indirect function, find the offset of the implementation function, and attach the counting BPF program to the implementation function. The code should then enter a loop where it sleeps for one second, prints the histogram, and clears the histogram. The loop should continue until a KeyboardInterrupt is received.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def _generate_functions(self, template):\n        self.usdt = None\n        text = b\"\"\n        if self.type == b\"p\" and not self.library:\n            functions = BPF.get_kprobe_functions(self.pattern)\n            verify_limit(len(functions))\n            for function in functions:\n                text += self._add_function(template, function)\n        elif self.type == b\"p\" and self.library:\n            # uprobes are tricky because the same function may have multiple\n            # addresses, and the same address may be mapped to multiple\n            # functions. We aren't allowed to create more than one uprobe\n            # per address, so track unique addresses and ignore functions that\n            # map to an address that we've already seen. Also ignore functions\n            # that may repeat multiple times with different addresses.\n            addresses, functions = (set(), set())\n            functions_and_addresses = BPF.get_user_functions_and_addresses(\n                                        self.library, self.pattern)\n            verify_limit(len(functions_and_addresses))\n            for function, address in functions_and_addresses:\n                if address in addresses or function in functions:\n                    continue\n                addresses.add(address)\n                functions.add(function)\n                text += self._add_function(template, function)\n        elif self.type == b\"t\":\n            tracepoints = BPF.get_tracepoints(self.pattern)\n            verify_limit(len(tracepoints))\n            for tracepoint in tracepoints:\n                text += self._add_function(template, tracepoint)\n        elif self.type == b\"u\":\n            self.usdt = USDT(path=self.library, pid=self.pid)\n            matches = []\n            for probe in self.usdt.enumerate_probes():\n                if not self.pid and (probe.bin_path != self.library):\n                    continue\n                if re.match(self.pattern, probe.name):\n                    matches.append(probe.name)\n            verify_limit(len(matches))\n            for match in matches:\n                new_func = b\"trace_count_%d\" % self.matched\n                text += self._add_function(template, match)\n                self.usdt.enable_probe(match, new_func)\n            if debug:\n                print(self.usdt.get_text())\n        return text\n\n# --- Snippet Separator ---\n\nclass Probe(object):\n    def __init__(self, pattern, use_regex=False, pid=None):\n        \"\"\"Init a new probe.\n\n        Init the probe from the pattern provided by the user. The supported\n        patterns mimic the 'trace' and 'argdist' tools, but are simpler because\n        we don't have to distinguish between probes and retprobes.\n\n            func            -- probe a kernel function\n            lib:func        -- probe a user-space function in the library 'lib'\n            /path:func      -- probe a user-space function in binary '/path'\n            p::func         -- same thing as 'func'\n            p:lib:func      -- same thing as 'lib:func'\n            t:cat:event     -- probe a kernel tracepoint\n            u:lib:probe     -- probe a USDT tracepoint\n        \"\"\"\n        parts = bytes(pattern).split(b':')\n        if len(parts) == 1:\n            parts = [b\"p\", b\"\", parts[0]]\n        elif len(parts) == 2:\n            parts = [b\"p\", parts[0], parts[1]]\n        elif len(parts) == 3:\n            if parts[0] == b\"t\":\n                parts = [b\"t\", b\"\", b\"%s:%s\" % tuple(parts[1:])]\n            if parts[0] not in [b\"p\", b\"t\", b\"u\"]:\n                raise Exception(\"Type must be 'p', 't', or 'u', but got %s\" %\n                                parts[0])\n        else:\n            raise Exception(\"Too many ':'-separated components in pattern %s\" %\n                            pattern)\n\n        (self.type, self.library, self.pattern) = parts\n        if not use_regex:\n            self.pattern = self.pattern.replace(b'*', b'.*')\n            self.pattern = b'^' + self.pattern + b'$'\n\n        if (self.type == b\"p\" and self.library) or self.type == b\"u\":\n            libpath = BPF.find_library(self.library)\n            if libpath is None:\n                # This might be an executable (e.g. 'bash')\n                libpath = BPF.find_exe(self.library)\n            if libpath is None or len(libpath) == 0:\n                raise Exception(\"unable to find library %s\" % self.library)\n            self.library = libpath\n\n        self.pid = pid\n        self.matched = 0\n        self.trace_functions = {}   # map location number to function name\n\n    def is_kernel_probe(self):\n        return self.type == b\"t\" or (self.type == b\"p\" and self.library == b\"\")\n\n    def attach(self):\n        if self.type == b\"p\" and not self.library:\n            for index, function in self.trace_functions.items():\n                self.bpf.attach_kprobe(\n                        event=function,\n                        fn_name=\"trace_count_%d\" % index)\n        elif self.type == b\"p\" and self.library:\n            for index, function in self.trace_functions.items():\n                self.bpf.attach_uprobe(\n                        name=self.library,\n                        sym=function,\n                        fn_name=\"trace_count_%d\" % index,\n                        pid=self.pid or -1)\n        elif self.type == b\"t\":\n            for index, function in self.trace_functions.items():\n                self.bpf.attach_tracepoint(\n                        tp=function,\n                        fn_name=\"trace_count_%d\" % index)\n        elif self.type == b\"u\":\n            pass    # Nothing to do -- attach already happened in `load`\n\n    def _add_function(self, template, probe_name):\n        new_func = b\"trace_count_%d\" % self.matched\n        text = template.replace(b\"PROBE_FUNCTION\", new_func)\n        text = text.replace(b\"LOCATION\", b\"%d\" % self.matched)\n        self.trace_functions[self.matched] = probe_name\n        self.matched += 1\n        return text\n\n    def _generate_functions(self, template):\n        self.usdt = None\n        text = b\"\"\n        if self.type == b\"p\" and not self.library:\n            functions = BPF.get_kprobe_functions(self.pattern)\n            verify_limit(len(functions))\n            for function in functions:\n                text += self._add_function(template, function)\n        elif self.type == b\"p\" and self.library:\n            # uprobes are tricky because the same function may have multiple\n            # addresses, and the same address may be mapped to multiple\n            # functions. We aren't allowed to create more than one uprobe\n            # per address, so track unique addresses and ignore functions that\n            # map to an address that we've already seen. Also ignore functions\n            # that may repeat multiple times with different addresses.\n            addresses, functions = (set(), set())\n            functions_and_addresses = BPF.get_user_functions_and_addresses(\n                                        self.library, self.pattern)\n            verify_limit(len(functions_and_addresses))\n            for function, address in functions_and_addresses:\n                if address in addresses or function in functions:\n                    continue\n                addresses.add(address)\n                functions.add(function)\n                text += self._add_function(template, function)\n        elif self.type == b\"t\":\n            tracepoints = BPF.get_tracepoints(self.pattern)\n            verify_limit(len(tracepoints))\n            for tracepoint in tracepoints:\n                text += self._add_function(template, tracepoint)\n        elif self.type == b\"u\":\n            self.usdt = USDT(path=self.library, pid=self.pid)\n            matches = []\n            for probe in self.usdt.enumerate_probes():\n                if not self.pid and (probe.bin_path != self.library):\n                    continue\n                if re.match(self.pattern, probe.name):\n                    matches.append(probe.name)\n            verify_limit(len(matches))\n            for match in matches:\n                new_func = b\"trace_count_%d\" % self.matched\n                text += self._add_function(template, match)\n                self.usdt.enable_probe(match, new_func)\n            if debug:\n                print(self.usdt.get_text())\n        return text\n\n    def load(self):\n        trace_count_text = b\"\"\"\nint PROBE_FUNCTION(void *ctx) {\n    FILTER\n    int loc = LOCATION;\n    u64 *val = counts.lookup(&loc);\n    if (!val) {\n        return 0;   // Should never happen, # of locations is known\n    }\n    (*val)++;\n    return 0;\n}\n        \"\"\"\n        bpf_text = b\"\"\"#include <uapi/linux/ptrace.h>\n\nBPF_ARRAY(counts, u64, NUMLOCATIONS);\n        \"\"\"\n\n        # We really mean the tgid from the kernel's perspective, which is in\n        # the top 32 bits of bpf_get_current_pid_tgid().\n        if self.pid:\n            trace_count_text = trace_count_text.replace(b'FILTER',\n                b\"\"\"u32 pid = bpf_get_current_pid_tgid() >> 32;\n                   if (pid != %d) { return 0; }\"\"\" % self.pid)\n        else:\n            trace_count_text = trace_count_text.replace(b'FILTER', b'')\n\n        bpf_text += self._generate_functions(trace_count_text)\n        bpf_text = bpf_text.replace(b\"NUMLOCATIONS\",\n                                    b\"%d\" % len(self.trace_functions))\n        if debug:\n            print(bpf_text)\n\n        if self.matched == 0:\n            raise Exception(\"No functions matched by pattern %s\" %\n                            self.pattern)\n\n        self.bpf = BPF(text=bpf_text,\n                       usdt_contexts=[self.usdt] if self.usdt else [])\n        self.clear()    # Initialize all array items to zero\n\n    def counts(self):\n        return self.bpf[\"counts\"]\n\n    def clear(self):\n        counts = self.bpf[\"counts\"]\n        for location, _ in list(self.trace_functions.items()):\n            counts[counts.Key(location)] = counts.Leaf()\n\n# --- Snippet Separator ---\n\ndef output_transformer(\n    transform_fn: TransformFn[IT, P, OT],\n) -> OutputTransformer[IT, OT, P]:\n    \"\"\"\n    Output transformer decorator\n\n    This decorator method is a convenience method to generate the appropriate types and\n    internal implementation for an overloaded renderer method. This method will provide\n    you with all the necessary types to define two different overloads: one for when the\n    decorator is called without parentheses and another for when it is called with\n    parentheses where app authors can pass in parameters to the renderer.\n\n    Transform function\n    ------------------\n\n    The output renderer's transform function (`transform_fn`) is the key building block\n    for `output_transformer`. It is a package author function that calls the app-defined\n    output value function (`value_fn`) transforms the result of type `IT` into type\n    `OT`.\n\n    The transform function is supplied meta output information, the (app-supplied) value\n    function, and any keyword arguments supplied to the output tranformer decorator:\n\n    * The first parameter to the handler function has the class\n      :class:`~shiny.render.transformer.TransformerMetadata` and is typically called\n      `_meta`. This information gives context the to the handler while trying to\n      resolve the app-supplied value function (typically called `_fn`).\n    * The second parameter is the app-defined output value function (e.g. `_fn`). It's\n      return type (`IT`) determines what types can be returned by the app-supplied\n      output value function. For example, if `_fn` has the type `ValueFn[str | None]`,\n      both the `str` and `None` types are allowed to be returned from the app-supplied\n      output value function.\n    * The remaining parameters are the keyword arguments (e.g. `alt:Optional[str] =\n      None` or `**kwargs: object`) that app authors may supply to the renderer (when the\n      renderer decorator is called with parentheses). Variadic positional parameters\n      (e.g. `*args`) are not allowed. All keyword arguments should have a type and\n      default value. No default value is needed for keyword arguments that are passed\n      through (e.g. `**kwargs: Any`).\n\n    The tranform function's return type (`OT`) determines the output type of the\n    :class:`~shiny.render.transformer.OutputRenderer`. Note that in many cases (but not\n    all!) `IT` and `OT` will be the same. The `None` type should typically be defined in\n    both `IT` and `OT`. If `IT` allows for `None` values, it (typically) signals that\n    nothing should be rendered. If `OT` allows for `None` and returns a `None` value,\n    shiny will not render the output.\n\n    Notes\n    -----\n\n    * When defining the renderer decorator overloads, if you have extra parameters of\n      `**kwargs: object`, you may get a type error about incompatible signatures. To fix\n      this, you can use `**kwargs: Any` instead or add `_fn: None = None` as the first\n      parameter in the overload containing the `**kwargs: object`.\n\n    * The `transform_fn` should be defined as an asynchronous function but should only\n      asynchronously yield (i.e. use `await` syntax) when the value function (the second\n      parameter of type `ValueFn[IT]`) is awaitable. If the value function is not\n      awaitable (i.e. it is a _synchronous_ function), then the execution of the\n      transform function should also be synchronous.\n\n\n    Parameters\n    ----------\n    transform_fn\n        Asynchronous function used to determine the app-supplied output value function\n        return type (`IT`), the transformed type (`OT`), and the keyword arguments (`P`)\n        app authors can supply to the renderer decorator.\n\n    Returns\n    -------\n    :\n        An :class:`~shiny.render.transformer.OutputTransformer` object that can be used to\n        define two overloads for your renderer function. One overload is for when the\n        renderer is called without parentheses and the other is for when the renderer is\n        called with parentheses.\n    \"\"\"\n    _assert_transformer(transform_fn)\n\n    def renderer_decorator(\n        value_fn: ValueFn[IT] | None,\n        params: TransformerParams[P],\n    ) -> OutputRenderer[OT] | OutputRendererDecorator[IT, OT]:\n        def as_value_fn(\n            fn: ValueFn[IT],\n        ) -> OutputRenderer[OT]:\n            if is_async_callable(fn):\n                return OutputRendererAsync(fn, transform_fn, params)\n            else:\n                # To avoid duplicate work just for a typeguard, we cast the function\n                fn = cast(ValueFnSync[IT], fn)\n                return OutputRendererSync(fn, transform_fn, params)\n\n        if value_fn is None:\n            return as_value_fn\n        val = as_value_fn(value_fn)\n        return val\n\n    return OutputTransformer(renderer_decorator)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the bcc library to create a histogram of system-wide strlen return values. The code should handle the case where strlen is an indirect function. It should define two BPF programs, one for counting the return values and another for submitting the addresses of the resolver and implementation functions. The code should also include functions to get the symbol of the indirect function, set the addresses of the resolver and implementation functions, and find the offset of the implementation function. In the main function, it should get the symbol of the indirect function, find the offset of the implementation function, and attach the counting BPF program to the implementation function. The code should then enter a loop where it sleeps for one second, prints the histogram, and clears the histogram. The loop should continue until a KeyboardInterrupt is received.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 133, "repo_full_name": "federatedai__fate", "instruction": "Generate code that uses the fate library to create a pipeline for a binary classification task using a heterogenous neural network (HeteroNN). The pipeline should include components for reading data, transforming data, intersecting data, and evaluating the model. The HeteroNN should be configured with specific parameters including the number of epochs, learning rate, batch size, and callback parameters. The model should also include a guest and host bottom model, an interactive layer, and a guest top model. The pipeline should be compiled and fitted, and the summary of the HeteroNN component should be printed. The code should also include a main function that accepts a configuration file as an argument.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def main():\n    # parties config\n    guest = 9999\n    host = 10000\n    arbiter = 10000\n\n    # specify input data name & namespace in database\n    guest_train_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_train_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    guest_eval_data = {\"name\": \"breast_hetero_guest\", \"namespace\": \"experiment\"}\n    host_eval_data = {\"name\": \"breast_hetero_host\", \"namespace\": \"experiment\"}\n\n    # initialize pipeline\n    pipeline = PipeLine()\n    # set job initiator\n    pipeline.set_initiator(role=\"guest\", party_id=guest)\n    # set participants information\n    pipeline.set_roles(guest=guest, host=host, arbiter=arbiter)\n\n    # define Reader components to read in data\n    reader_0 = Reader(name=\"reader_0\")\n    # configure Reader for guest\n    reader_0.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_train_data)\n    # configure Reader for host\n    reader_0.get_party_instance(role=\"host\", party_id=host).component_param(table=host_train_data)\n\n    reader_1 = Reader(name=\"reader_1\")\n    reader_1.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_1.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n\n    # define DataIO components\n    dataio_0 = DataIO(name=\"dataio_0\")\n    dataio_1 = DataIO(name=\"dataio_1\")\n\n    # get DataIO party instance of guest\n    dataio_0_guest_party_instance = dataio_0.get_party_instance(role=\"guest\", party_id=guest)\n    # configure DataIO for guest\n    dataio_0_guest_party_instance.component_param(with_label=True, output_format=\"dense\")\n    # get and configure DataIO party instance of host\n    dataio_0.get_party_instance(role=\"host\", party_id=host).component_param(with_label=False)\n\n    # define Intersection components\n    intersection_0 = Intersection(name=\"intersection_0\")\n    intersection_1 = Intersection(name=\"intersection_1\")\n\n    # define HeteroLR component\n    hetero_lr_0 = HeteroLR(name=\"hetero_lr_0\", early_stop=\"weight_diff\", learning_rate=0.15, optimizer=\"rmsprop\",\n                           max_iter=10, early_stopping_rounds=2, validation_freqs=1)\n\n    # add components to pipeline, in order of task execution\n    pipeline.add_component(reader_0)\n    pipeline.add_component(reader_1)\n    pipeline.add_component(dataio_0, data=Data(data=reader_0.output.data))\n    # set dataio_1 to replicate model from dataio_0\n    pipeline.add_component(dataio_1, data=Data(data=reader_1.output.data), model=Model(dataio_0.output.model))\n    # set data input sources of intersection components\n    pipeline.add_component(intersection_0, data=Data(data=dataio_0.output.data))\n    pipeline.add_component(intersection_1, data=Data(data=dataio_1.output.data))\n    # set train & validate data of hetero_lr_0 component\n    pipeline.add_component(\n        hetero_lr_0,\n        data=Data(\n            train_data=intersection_0.output.data,\n            validate_data=intersection_1.output.data))\n\n    # compile pipeline once finished adding modules, this step will form conf and dsl files for running job\n    pipeline.compile()\n\n    # fit model\n    pipeline.fit()\n    # query component summary\n    import json\n    print(json.dumps(pipeline.get_component(\"hetero_lr_0\").get_summary(), indent=4))\n\n    # predict\n    # deploy required components\n    pipeline.deploy_component([dataio_0, intersection_0, hetero_lr_0])\n\n    # initiate predict pipeline\n    predict_pipeline = PipeLine()\n\n    reader_2 = Reader(name=\"reader_2\")\n    reader_2.get_party_instance(role=\"guest\", party_id=guest).component_param(table=guest_eval_data)\n    reader_2.get_party_instance(role=\"host\", party_id=host).component_param(table=host_eval_data)\n    # add data reader onto predict pipeline\n    predict_pipeline.add_component(reader_2)\n    # add selected components from train pipeline onto predict pipeline\n    # specify data source\n    predict_pipeline.add_component(pipeline,\n                                   data=Data(predict_input={pipeline.dataio_0.input.data: reader_2.output.data}))\n    # run predict model\n    predict_pipeline.predict()\n\n# --- Snippet Separator ---\n\ndef add_component(self, component, data=None, model=None, cache=None):\n        if isinstance(component, PipeLine):\n            if component.is_deploy() is False:\n                raise ValueError(\"To use a training pipeline object as predict component, should deploy model first\")\n\n            if model:\n                raise ValueError(\"pipeline should not have model as input!\")\n\n            if not data:\n                raise ValueError(\"To use pipeline as a component, please set data input\")\n\n            self._stage = \"predict\"\n            self._predict_pipeline.append({\"pipeline\": component, \"data\": data.predict_input})\n\n            meta = component.get_predict_meta()\n            self.restore_roles(meta.get(\"initiator\"), meta.get(\"roles\"))\n\n            return self\n\n        if not isinstance(component, Component):\n            raise ValueError(\n                \"To add a component to pipeline, component {} should be a Component object\".format(component))\n\n        if component.name in self._components:\n            raise Warning(\"component {} is added before\".format(component.name))\n\n        self._components[component.name] = component\n\n        if data is not None:\n            if not isinstance(data, Data):\n                raise ValueError(\"data input of component {} should be passed by data object\".format(component.name))\n\n            attrs_dict = vars(data)\n            self._components_input[component.name] = {\"data\": {}}\n            for attr, val in attrs_dict.items():\n                if not attr.endswith(\"data\"):\n                    continue\n\n                if val is None:\n                    continue\n\n                data_key = attr.strip(\"_\")\n\n                if isinstance(val, list):\n                    self._components_input[component.name][\"data\"][data_key] = val\n                else:\n                    self._components_input[component.name][\"data\"][data_key] = [val]\n\n        if model is not None:\n            if not isinstance(model, Model):\n                raise ValueError(\"model input of component {} should be passed by model object\".format(component.name))\n\n            attrs_dict = vars(model)\n            for attr, val in attrs_dict.items():\n                if not attr.endswith(\"model\"):\n                    continue\n\n                if val is None:\n                    continue\n\n                if isinstance(val, list):\n                    self._components_input[component.name][attr.strip(\"_\")] = val\n                else:\n                    self._components_input[component.name][attr.strip(\"_\")] = [val]\n\n        if cache is not None:\n            if not isinstance(cache, Cache):\n                raise ValueError(\"cache input of component {} should be passed by cache object\".format(component.name))\n\n            attr = cache.cache\n            if not isinstance(attr, list):\n                attr = [attr]\n\n            self._components_input[component.name][\"cache\"] = attr\n\n        return self\n\n# --- Snippet Separator ---\n\nclass HeteroNNParam(BaseParam):\n    \"\"\"\n    Parameters used for Homo Neural Network.\n\n    Args:\n        task_type: str, task type of hetero nn model, one of 'classification', 'regression'.\n        config_type: str, accept \"keras\" only.\n        bottom_nn_define: a dict represents the structure of bottom neural network.\n        interactive_layer_define: a dict represents the structure of interactive layer.\n        interactive_layer_lr: float, the learning rate of interactive layer.\n        top_nn_define: a dict represents the structure of top neural network.\n        optimizer: optimizer method, accept following types:\n            1. a string, one of \"Adadelta\", \"Adagrad\", \"Adam\", \"Adamax\", \"Nadam\", \"RMSprop\", \"SGD\"\n            2. a dict, with a required key-value pair keyed by \"optimizer\",\n                with optional key-value pairs such as learning rate.\n            defaults to \"SGD\"\n        loss:  str, a string to define loss function used\n        early_stopping_rounds: int, default: None\n        Will stop training if one metric doesn’t improve in last early_stopping_round rounds\n        metrics: list, default: None\n            Indicate when executing evaluation during train process, which metrics will be used. If not set,\n            default metrics for specific task type will be used. As for binary classification, default metrics are\n            ['auc', 'ks'], for regression tasks, default metrics are ['root_mean_squared_error', 'mean_absolute_error'],\n            [ACCURACY, PRECISION, RECALL] for multi-classification task\n        use_first_metric_only: bool, default: False\n            Indicate whether to use the first metric in `metrics` as the only criterion for early stopping judgement.\n        epochs: int, the maximum iteration for aggregation in training.\n        batch_size : int, batch size when updating model.\n            -1 means use all data in a batch. i.e. Not to use mini-batch strategy.\n            defaults to -1.\n        early_stop : str, accept 'diff' only in this version, default: 'diff'\n            Method used to judge converge or not.\n                a)\tdiff： Use difference of loss between two iterations to judge whether converge.\n        validation_freqs: None or positive integer or container object in python. Do validation in training process or Not.\n                  if equals None, will not do validation in train process;\n                  if equals positive integer, will validate data every validation_freqs epochs passes;\n                  if container object in python, will validate data if epochs belong to this container.\n                    e.g. validation_freqs = [10, 15], will validate data when epoch equals to 10 and 15.\n                  Default: None\n                  The default value is None, 1 is suggested. You can set it to a number larger than 1 in order to\n                  speed up training by skipping validation rounds. When it is larger than 1, a number which is\n                  divisible by \"epochs\" is recommended, otherwise, you will miss the validation scores\n                  of last training epoch.\n        floating_point_precision: None or integer, if not None, means use floating_point_precision-bit to speed up calculation,\n                                   e.g.: convert an x to round(x * 2**floating_point_precision) during Paillier operation, divide\n                                          the result by 2**floating_point_precision in the end.\n        drop_out_keep_rate: float, should betweend 0 and 1, if not equals to 1.0, will enabled drop out\n    \"\"\"\n\n    def __init__(self,\n                 task_type='classification',\n                 config_type=\"keras\",\n                 bottom_nn_define=None,\n                 top_nn_define=None,\n                 interactive_layer_define=None,\n                 interactive_layer_lr=0.9,\n                 optimizer='SGD',\n                 loss=None,\n                 epochs=100,\n                 batch_size=-1,\n                 early_stop=\"diff\",\n                 tol=1e-5,\n                 encrypt_param=EncryptParam(),\n                 encrypted_mode_calculator_param=EncryptedModeCalculatorParam(),\n                 predict_param=PredictParam(),\n                 cv_param=CrossValidationParam(),\n                 validation_freqs=None,\n                 early_stopping_rounds=None,\n                 metrics=None,\n                 use_first_metric_only=True,\n                 selector_param=SelectorParam(),\n                 floating_point_precision=23,\n                 drop_out_keep_rate=1.0,\n                 callback_param=CallbackParam()):\n        super(HeteroNNParam, self).__init__()\n\n        self.task_type = task_type\n        self.config_type = config_type\n        self.bottom_nn_define = bottom_nn_define\n        self.interactive_layer_define = interactive_layer_define\n        self.interactive_layer_lr = interactive_layer_lr\n        self.top_nn_define = top_nn_define\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.early_stop = early_stop\n        self.tol = tol\n        self.optimizer = optimizer\n        self.loss = loss\n        self.validation_freqs = validation_freqs\n        self.early_stopping_rounds = early_stopping_rounds\n        self.metrics = metrics or []\n        self.use_first_metric_only = use_first_metric_only\n\n        self.encrypt_param = copy.deepcopy(encrypt_param)\n        self.encrypted_model_calculator_param = encrypted_mode_calculator_param\n        self.predict_param = copy.deepcopy(predict_param)\n        self.cv_param = copy.deepcopy(cv_param)\n\n        self.selector_param = selector_param\n        self.floating_point_precision = floating_point_precision\n\n        self.drop_out_keep_rate = drop_out_keep_rate\n\n        self.callback_param = copy.deepcopy(callback_param)\n\n    def check(self):\n        self.optimizer = self._parse_optimizer(self.optimizer)\n        supported_config_type = [\"keras\"]\n\n        if self.task_type not in [\"classification\", \"regression\"]:\n            raise ValueError(\"config_type should be classification or regression\")\n\n        if self.config_type not in supported_config_type:\n            raise ValueError(f\"config_type should be one of {supported_config_type}\")\n\n        if not isinstance(self.tol, (int, float)):\n            raise ValueError(\"tol should be numeric\")\n\n        if not isinstance(self.epochs, int) or self.epochs <= 0:\n            raise ValueError(\"epochs should be a positive integer\")\n\n        if self.bottom_nn_define and not isinstance(self.bottom_nn_define, dict):\n            raise ValueError(\"bottom_nn_define should be a dict defining the structure of neural network\")\n\n        if self.top_nn_define and not isinstance(self.top_nn_define, dict):\n            raise ValueError(\"top_nn_define should be a dict defining the structure of neural network\")\n\n        if self.interactive_layer_define is not None and not isinstance(self.interactive_layer_define, dict):\n            raise ValueError(\n                \"the interactive_layer_define should be a dict defining the structure of interactive layer\")\n\n        if self.batch_size != -1:\n            if not isinstance(self.batch_size, int) \\\n                    or self.batch_size < consts.MIN_BATCH_SIZE:\n                raise ValueError(\n                    \" {} not supported, should be larger than 10 or -1 represent for all data\".format(self.batch_size))\n\n        if self.early_stop != \"diff\":\n            raise ValueError(\"early stop should be diff in this version\")\n\n        if self.validation_freqs is None:\n            pass\n        elif isinstance(self.validation_freqs, int):\n            if self.validation_freqs < 1:\n                raise ValueError(\"validation_freqs should be larger than 0 when it's integer\")\n        elif not isinstance(self.validation_freqs, collections.Container):\n            raise ValueError(\"validation_freqs should be None or positive integer or container\")\n\n        if self.early_stopping_rounds and not isinstance(self.early_stopping_rounds, int):\n            raise ValueError(\"early stopping rounds should be None or int larger than 0\")\n        if self.early_stopping_rounds and isinstance(self.early_stopping_rounds, int):\n            if self.early_stopping_rounds < 1:\n                raise ValueError(\"early stopping should be larger than 0 when it's integer\")\n            if not self.validation_freqs:\n                raise ValueError(\"If early stopping rounds is setting, validation_freqs should not be null\")\n\n        if self.metrics is not None and not isinstance(self.metrics, list):\n            raise ValueError(\"metrics should be a list\")\n\n        if not isinstance(self.use_first_metric_only, bool):\n            raise ValueError(\"use_first_metric_only should be a boolean\")\n\n        if self.floating_point_precision is not None and \\\n                (not isinstance(self.floating_point_precision, int) or\n                 self.floating_point_precision < 0 or self.floating_point_precision > 64):\n            raise ValueError(\"floating point precision should be null or a integer between 0 and 64\")\n\n        self.encrypt_param.check()\n        self.encrypted_model_calculator_param.check()\n        self.predict_param.check()\n\n    @staticmethod\n    def _parse_optimizer(opt):\n        \"\"\"\n        Examples:\n\n            1. \"optimize\": \"SGD\"\n            2. \"optimize\": {\n                \"optimizer\": \"SGD\",\n                \"learning_rate\": 0.05\n            }\n        \"\"\"\n\n        kwargs = {}\n        if isinstance(opt, str):\n            return SimpleNamespace(optimizer=opt, kwargs=kwargs)\n        elif isinstance(opt, dict):\n            optimizer = opt.get(\"optimizer\", kwargs)\n            if not optimizer:\n                raise ValueError(f\"optimizer config: {opt} invalid\")\n            kwargs = {k: v for k, v in opt.items() if k != \"optimizer\"}\n            return SimpleNamespace(optimizer=optimizer, kwargs=kwargs)\n        elif opt is None:\n            return None\n        else:\n            raise ValueError(f\"invalid type for optimize: {type(opt)}\")\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the fate library to create a pipeline for a binary classification task using a heterogenous neural network (HeteroNN). The pipeline should include components for reading data, transforming data, intersecting data, and evaluating the model. The HeteroNN should be configured with specific parameters including the number of epochs, learning rate, batch size, and callback parameters. The model should also include a guest and host bottom model, an interactive layer, and a guest top model. The pipeline should be compiled and fitted, and the summary of the HeteroNN component should be printed. The code should also include a main function that accepts a configuration file as an argument.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 134, "repo_full_name": "lightly-ai__lightly", "instruction": "Generate code that creates a custom PyTorch model named SMoGModel. This model should inherit from nn.Module and include a backbone, a projection head, and a prediction head. The model should also have a method to cluster features using KMeans from sklearn, a method to reset group features, and a method to reset momentum weights. The forward method should return encoded and predicted values. \n\nAdditionally, the code should create an instance of the SMoGModel using a backbone derived from a ResNet18 model. It should also create a memory bank using the MemoryBankModule from the lightly library. \n\nThe code should then set up a device for computation, apply a SMoGTransform to a CIFAR10 dataset, and create a DataLoader for the transformed dataset. \n\nFinally, the code should define a CrossEntropyLoss criterion and a SGD optimizer, and then run a training loop for 10 epochs. In each epoch, the code should update the model's momentum, encode inputs, update group features, calculate loss, and update the memory bank. The average loss for each epoch should be printed out.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class BenchmarkModule(pl.LightningModule):\n    \"\"\"A PyTorch Lightning Module for automated kNN callback\n\n    At the end of every training epoch we create a feature bank by feeding the\n    `dataloader_kNN` passed to the module through the backbone.\n    At every validation step we predict features on the validation data.\n    After all predictions on validation data (validation_epoch_end) we evaluate\n    the predictions on a kNN classifier on the validation data using the\n    feature_bank features from the train data.\n\n    We can access the highest test accuracy during a kNN prediction \n    using the `max_accuracy` attribute.\n\n    Attributes:\n        backbone:\n            The backbone model used for kNN validation. Make sure that you set the\n            backbone when inheriting from `BenchmarkModule`.\n        max_accuracy:\n            Floating point number between 0.0 and 1.0 representing the maximum\n            test accuracy the benchmarked model has achieved.\n        dataloader_kNN:\n            Dataloader to be used after each training epoch to create feature bank.\n        num_classes:\n            Number of classes. E.g. for cifar10 we have 10 classes. (default: 10)\n        knn_k:\n            Number of nearest neighbors for kNN\n        knn_t:\n            Temperature parameter for kNN\n\n    Examples:\n        >>> class SimSiamModel(BenchmarkingModule):\n        >>>     def __init__(dataloader_kNN, num_classes):\n        >>>         super().__init__(dataloader_kNN, num_classes)\n        >>>         resnet = lightly.models.ResNetGenerator('resnet-18')\n        >>>         self.backbone = nn.Sequential(\n        >>>             *list(resnet.children())[:-1],\n        >>>             nn.AdaptiveAvgPool2d(1),\n        >>>         )\n        >>>         self.resnet_simsiam = \n        >>>             lightly.models.SimSiam(self.backbone, num_ftrs=512)\n        >>>         self.criterion = lightly.loss.SymNegCosineSimilarityLoss()\n        >>>\n        >>>     def forward(self, x):\n        >>>         self.resnet_simsiam(x)\n        >>>\n        >>>     def training_step(self, batch, batch_idx):\n        >>>         (x0, x1), _, _ = batch\n        >>>         x0, x1 = self.resnet_simsiam(x0, x1)\n        >>>         loss = self.criterion(x0, x1)\n        >>>         return loss\n        >>>     def configure_optimizers(self):\n        >>>         optim = torch.optim.SGD(\n        >>>             self.resnet_simsiam.parameters(), lr=6e-2, momentum=0.9\n        >>>         )\n        >>>         return [optim]\n        >>>\n        >>> model = SimSiamModel(dataloader_train_kNN)\n        >>> trainer = pl.Trainer()\n        >>> trainer.fit(\n        >>>     model,\n        >>>     train_dataloader=dataloader_train_ssl,\n        >>>     val_dataloaders=dataloader_test\n        >>> )\n        >>> # you can get the peak accuracy using\n        >>> print(model.max_accuracy)\n\n    \"\"\"\n\n    def __init__(self,\n                 dataloader_kNN: DataLoader,\n                 num_classes: int,\n                 knn_k: int=200,\n                 knn_t: float=0.1):\n        super().__init__()\n        self.backbone = nn.Module()\n        self.max_accuracy = 0.0\n        self.dataloader_kNN = dataloader_kNN\n        self.num_classes = num_classes\n        self.knn_k = knn_k\n        self.knn_t = knn_t\n\n        # create dummy param to keep track of the device the model is using\n        self.dummy_param = nn.Parameter(torch.empty(0))\n\n    def training_epoch_end(self, outputs):\n        # update feature bank at the end of each training epoch\n        self.backbone.eval()\n        self.feature_bank = []\n        self.targets_bank = []\n        with torch.no_grad():\n            for data in self.dataloader_kNN:\n                img, target, _ = data\n                img = img.to(self.dummy_param.device)\n                target = target.to(self.dummy_param.device)\n                feature = self.backbone(img).squeeze()\n                feature = F.normalize(feature, dim=1)\n                self.feature_bank.append(feature)\n                self.targets_bank.append(target)\n        self.feature_bank = torch.cat(\n            self.feature_bank, dim=0).t().contiguous()\n        self.targets_bank = torch.cat(\n            self.targets_bank, dim=0).t().contiguous()\n        self.backbone.train()\n\n    def validation_step(self, batch, batch_idx):\n        # we can only do kNN predictions once we have a feature bank\n        if hasattr(self, 'feature_bank') and hasattr(self, 'targets_bank'):\n            images, targets, _ = batch\n            feature = self.backbone(images).squeeze()\n            feature = F.normalize(feature, dim=1)\n            pred_labels = knn_predict(\n                feature,\n                self.feature_bank,\n                self.targets_bank,\n                self.num_classes,\n                self.knn_k,\n                self.knn_t\n            )\n            num = images.size()\n            top1 = (pred_labels[:, 0] == targets).float().sum()\n            return (num, top1)\n\n    def validation_epoch_end(self, outputs):\n        device = self.dummy_param.device\n        if outputs:\n            total_num = torch.Tensor([0]).to(device)\n            total_top1 = torch.Tensor([0.]).to(device)\n            for (num, top1) in outputs:\n                total_num += num[0]\n                total_top1 += top1\n\n            if dist.is_initialized() and dist.get_world_size() > 1:\n                dist.all_reduce(total_num)\n                dist.all_reduce(total_top1)\n\n            acc = float(total_top1.item() / total_num.item())\n            if acc > self.max_accuracy:\n                self.max_accuracy = acc\n            self.log('kNN_accuracy', acc * 100.0, prog_bar=True)\n\n# --- Snippet Separator ---\n\nclass MoCo(nn.Module, _MomentumEncoderMixin):\n    \"\"\"Implementation of the MoCo (Momentum Contrast)[0] architecture.\n\n    Recommended loss: :py:class:`lightly.loss.ntx_ent_loss.NTXentLoss` with \n    a memory bank.\n\n    [0] MoCo, 2020, https://arxiv.org/abs/1911.05722\n\n    Attributes:\n        backbone:\n            Backbone model to extract features from images.\n        num_ftrs:\n            Dimension of the embedding (before the projection head).\n        out_dim:\n            Dimension of the output (after the projection head).\n        m:\n            Momentum for momentum update of the key-encoder.\n\n    \"\"\"\n\n    def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 32,\n                 out_dim: int = 128,\n                 m: float = 0.999,\n                 batch_shuffle: bool = False):\n\n        super(MoCo, self).__init__()\n\n        self.backbone = backbone\n        self.projection_head = _get_moco_projection_head(num_ftrs, out_dim)\n        self.momentum_features = None\n        self.momentum_projection_head = None\n\n        self.m = m\n        self.batch_shuffle = batch_shuffle\n\n        # initialize momentum features and momentum projection head\n        self._init_momentum_encoder()\n\n    def forward(self,\n                x0: torch.Tensor,\n                x1: torch.Tensor = None,\n                return_features: bool = False):\n        \"\"\"Embeds and projects the input image.\n\n        Performs the momentum update, extracts features with the backbone and \n        applies the projection head to the output space. If both x0 and x1 are\n        not None, both will be passed through the backbone and projection head.\n        If x1 is None, only x0 will be forwarded.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output projection of x0 and (if x1 is not None) the output\n            projection of x1. If return_features is True, the output for each x\n            is a tuple (out, f) where f are the features before the projection\n            head.\n\n        Examples:\n            >>> # single input, single output\n            >>> out = model(x) \n            >>> \n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n\n        \"\"\"\n        self._momentum_update(self.m)\n\n        # forward pass of first input x0\n        f0 = self.backbone(x0).squeeze()\n        out0 = self.projection_head(f0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        # return out0 if x1 is None\n        if x1 is None:\n            return out0\n\n        # forward pass of second input x1\n        with torch.no_grad():\n\n            # shuffle for batchnorm\n            if self.batch_shuffle:\n                x1, shuffle = self._batch_shuffle(x1)\n\n            # run x1 through momentum encoder\n            f1 = self.momentum_backbone(x1).squeeze()\n            out1 = self.momentum_projection_head(f1).detach()\n\n            # unshuffle for batchnorm\n            if self.batch_shuffle:\n                f1 = self._batch_unshuffle(f1, shuffle)\n                out1 = self._batch_unshuffle(out1, shuffle)\n\n            # append features if requested\n            if return_features:\n                out1 = (out1, f1)\n\n        return out0, out1\n\n# --- Snippet Separator ---\n\nclass SimSiam(nn.Module):\n    \"\"\"Implementation of SimSiam[0] network\n\n    Recommended loss: :py:class:`lightly.loss.sym_neg_cos_sim_loss.SymNegCosineSimilarityLoss`\n\n    [0] SimSiam, 2020, https://arxiv.org/abs/2011.10566\n\n    Attributes:\n        backbone:\n            Backbone model to extract features from images.\n        num_ftrs:\n            Dimension of the embedding (before the projection head).\n        proj_hidden_dim:\n            Dimension of the hidden layer of the projection head. This should\n            be the same size as `num_ftrs`.\n        pred_hidden_dim:\n            Dimension of the hidden layer of the predicion head. This should\n            be `num_ftrs` / 4.\n        out_dim:\n            Dimension of the output (after the projection head).\n\n    \"\"\"\n\n    def __init__(self,\n                 backbone: nn.Module,\n                 num_ftrs: int = 2048,\n                 proj_hidden_dim: int = 2048,\n                 pred_hidden_dim: int = 512,\n                 out_dim: int = 2048,\n                 num_mlp_layers: int = 3):\n\n        super(SimSiam, self).__init__()\n\n        self.backbone = backbone\n        self.num_ftrs = num_ftrs\n        self.proj_hidden_dim = proj_hidden_dim\n        self.pred_hidden_dim = pred_hidden_dim\n        self.out_dim = out_dim\n\n        self.projection_mlp = \\\n            _projection_mlp(num_ftrs, proj_hidden_dim, out_dim, num_mlp_layers)\n\n        self.prediction_mlp = \\\n            _prediction_mlp(out_dim, pred_hidden_dim, out_dim)\n\n    def forward(self, \n                x0: torch.Tensor, \n                x1: torch.Tensor = None,\n                return_features: bool = False):\n        \"\"\"Forward pass through SimSiam.\n\n        Extracts features with the backbone and applies the projection\n        head and prediction head to the output space. If both x0 and x1 are not\n        None, both will be passed through the backbone, projection, and\n        prediction head. If x1 is None, only x0 will be forwarded.\n\n        Args:\n            x0:\n                Tensor of shape bsz x channels x W x H.\n            x1:\n                Tensor of shape bsz x channels x W x H.\n            return_features:\n                Whether or not to return the intermediate features backbone(x).\n\n        Returns:\n            The output prediction and projection of x0 and (if x1 is not None)\n            the output prediction and projection of x1. If return_features is\n            True, the output for each x is a tuple (out, f) where f are the\n            features before the projection head.\n\n        Examples:\n            >>> # single input, single output\n            >>> out = model(x) \n            >>> \n            >>> # single input with return_features=True\n            >>> out, f = model(x, return_features=True)\n            >>>\n            >>> # two inputs, two outputs\n            >>> out0, out1 = model(x0, x1)\n            >>>\n            >>> # two inputs, two outputs with return_features=True\n            >>> (out0, f0), (out1, f1) = model(x0, x1, return_features=True)\n        \"\"\"\n        f0 = self.backbone(x0).squeeze()\n        z0 = self.projection_mlp(f0)\n        p0 = self.prediction_mlp(z0)\n\n        out0 = (z0, p0)\n\n        # append features if requested\n        if return_features:\n            out0 = (out0, f0)\n\n        if x1 is None:\n            return out0\n\n        f1 = self.backbone(x1).squeeze()\n        z1 = self.projection_mlp(f1)\n        p1 = self.prediction_mlp(z1)\n\n        out1 = (z1, p1)\n\n        # append features if requested\n        if return_features:\n            out1 = (out1, f1)\n\n        return out0, out1\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a custom PyTorch model named SMoGModel. This model should inherit from nn.Module and include a backbone, a projection head, and a prediction head. The model should also have a method to cluster features using KMeans from sklearn, a method to reset group features, and a method to reset momentum weights. The forward method should return encoded and predicted values. \n\nAdditionally, the code should create an instance of the SMoGModel using a backbone derived from a ResNet18 model. It should also create a memory bank using the MemoryBankModule from the lightly library. \n\nThe code should then set up a device for computation, apply a SMoGTransform to a CIFAR10 dataset, and create a DataLoader for the transformed dataset. \n\nFinally, the code should define a CrossEntropyLoss criterion and a SGD optimizer, and then run a training loop for 10 epochs. In each epoch, the code should update the model's momentum, encode inputs, update group features, calculate loss, and update the memory bank. The average loss for each epoch should be printed out.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 135, "repo_full_name": "pyqtgraph__pyqtgraph", "instruction": "Generate code that creates a PyQtGraph application with multiple plots demonstrating various features. The plots should include basic array plotting, multiple curves, drawing with points, parametric plot with grid enabled, scatter plot with axis labels and log scale, an updating plot, a filled plot with axis disabled, region selection, and zoom on selected region. The plots should be arranged in a grid layout and the application window should be titled \"Basic plotting examples\". The plots should be interactive, allowing for panning and scaling. The application should also include a timer that updates one of the plots at regular intervals.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def plots(self, plot_type=None):\n        \"\"\"Return all plots of the specified type in this chart.\n\n        Parameters\n        ----------\n        plot_type : str, optional\n            The type of plots to return. Allowed types are\n            ``\"scatter\"``, ``\"line\"``, ``\"area\"``, ``\"bar\"``\n            and ``\"stack\"``.\n            If no type is provided (``None``), all plots are returned,\n            regardless of their type.\n\n        Yields\n        ------\n        plot : plotting.charts.ScatterPlot2D | plotting.charts.LinePlot2D | plotting.charts.AreaPlot | plotting.charts.BarPlot | plotting.charts.StackPlot\n            One of the plots (of the specified type) in this chart.\n\n        Examples\n        --------\n        Create a 2D chart with a line and scatter plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> scatter_plot, line_plot = chart.plot([0, 1, 2], [2, 1, 3], \"o-\")\n        >>> chart.show()\n\n        Retrieve all plots in the chart.\n\n        >>> plots = [*chart.plots()]\n        >>> scatter_plot in plots and line_plot in plots\n        True\n\n        Retrieve all line plots in the chart.\n\n        >>> line_plots = [*chart.plots(\"line\")]\n        >>> line_plot == line_plots[0]\n        True\n\n        \"\"\"\n        plot_types = self.PLOT_TYPES.keys() if plot_type is None else [plot_type]\n        for plot_type in plot_types:\n            yield from self._plots[plot_type]\n\n# --- Snippet Separator ---\n\nclass Chart2D(_vtk.vtkChartXY, _Chart):\n    \"\"\"2D chart class similar to a ``matplotlib`` figure.\n\n    Parameters\n    ----------\n    size : sequence[float], default: (1, 1)\n        Size of the chart in normalized coordinates. A size of ``(0,\n        0)`` is invisible, a size of ``(1, 1)`` occupies the whole\n        renderer's width and height.\n\n    loc : sequence[float], default: (0, 0)\n        Location of the chart (its bottom left corner) in normalized\n        coordinates. A location of ``(0, 0)`` corresponds to the\n        renderer's bottom left corner, a location of ``(1, 1)``\n        corresponds to the renderer's top right corner.\n\n    x_label : str, default: \"x\"\n        Label along the x-axis.\n\n    y_label : str, default: \"y\"\n        Label along the y-axis.\n\n    grid : bool, default: True\n        Show the background grid in the plot.\n\n    Examples\n    --------\n    Plot a simple sine wave as a scatter and line plot.\n\n    >>> import pyvista\n    >>> import numpy as np\n    >>> x = np.linspace(0, 2*np.pi, 20)\n    >>> y = np.sin(x)\n    >>> chart = pyvista.Chart2D()\n    >>> _ = chart.scatter(x, y)\n    >>> _ = chart.line(x, y, 'r')\n    >>> chart.show()\n\n    Combine multiple types of plots in the same chart.\n\n    >>> rng = np.random.default_rng(1)\n    >>> x = np.arange(1, 8)\n    >>> y = rng.integers(5, 15, 7)\n    >>> e = np.abs(rng.normal(scale=2, size=7))\n    >>> z = rng.integers(0, 5, 7)\n    >>> chart = pyvista.Chart2D()\n    >>> _ = chart.area(x, y-e, y+e, color=(0.12, 0.46, 0.71, 0.2))\n    >>> _ = chart.line(x, y, color=\"tab:blue\", style=\"--\", label=\"Scores\")\n    >>> _ = chart.scatter(x, y, color=\"tab:blue\", style=\"d\")\n    >>> _ = chart.bar(x, z, color=\"tab:orange\", label=\"Violations\")\n    >>> chart.x_axis.tick_locations = x\n    >>> chart.x_axis.tick_labels = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\",\n    ...                             \"Sat\", \"Sun\"]\n    >>> chart.x_label = \"Day of week\"\n    >>> chart.show()\n\n    \"\"\"\n\n    PLOT_TYPES = {\n        \"scatter\": ScatterPlot2D,\n        \"line\": LinePlot2D,\n        \"area\": AreaPlot,\n        \"bar\": BarPlot,\n        \"stack\": StackPlot,\n    }\n    _PLOT_CLASSES = {plot_class: plot_type for (plot_type, plot_class) in PLOT_TYPES.items()}\n    _DOC_SUBS = {\n        \"chart_name\": \"2D chart\",\n        \"chart_args\": \"\",\n        \"chart_init\": \"\"\"\n        >>> plot = chart.line([0, 1, 2], [2, 1, 3])\"\"\",\n        \"chart_set_labels\": 'plot.label = \"My awesome plot\"',\n    }\n\n    def __init__(\n        self, size=(1, 1), loc=(0, 0), x_label=\"x\", y_label=\"y\", grid=True\n    ):  # numpydoc ignore=PR01,RT01\n        \"\"\"Initialize the chart.\"\"\"\n        super().__init__(size, loc)\n        self._plots = {plot_type: [] for plot_type in self.PLOT_TYPES.keys()}\n        self.SetAutoSize(False)  # We manually set the appropriate size\n        # Overwrite custom X and Y axis using a wrapper object, as using the\n        # SetAxis method causes a crash at the end of the script's execution (nonzero exit code).\n        self._x_axis = Axis(_wrap=self.GetAxis(_vtk.vtkAxis.BOTTOM))\n        self._y_axis = Axis(_wrap=self.GetAxis(_vtk.vtkAxis.LEFT))\n        # Note: registering the axis prevents the nonzero exit code at the end, however\n        # this results in memory leaks in the plotting tests.\n        # self.SetAxis(_vtk.vtkAxis.BOTTOM, self._x_axis)\n        # self.SetAxis(_vtk.vtkAxis.LEFT, self._y_axis)\n        # self.Register(self._x_axis)\n        # self.Register(self._y_axis)\n        self.x_label = x_label\n        self.y_label = y_label\n        self.grid = grid\n        self.legend_visible = True\n\n    def _render_event(self, *args, plotter_render=False, **kwargs):\n        if plotter_render:\n            # TODO: should probably be called internally by VTK when plot data or axis behavior/logscale is changed?\n            self.RecalculateBounds()\n        super()._render_event(*args, plotter_render=plotter_render, **kwargs)\n\n    def _add_plot(self, plot_type, *args, **kwargs):\n        \"\"\"Add a plot of the given type to this chart.\"\"\"\n        plot = self.PLOT_TYPES[plot_type](self, *args, **kwargs)\n        self.AddPlot(plot)\n        self._plots[plot_type].append(plot)\n        return plot\n\n    @classmethod\n    def _parse_format(cls, fmt):\n        \"\"\"Parse a format string and separate it into a marker style, line style and color.\n\n        Parameters\n        ----------\n        fmt : str\n            Format string to parse. A format string consists of any\n            combination of a valid marker style, a valid line style\n            and parsable color. The specific order does not\n            matter. See :attr:`pyvista.ScatterPlot2D.MARKER_STYLES`\n            for a list of valid marker styles,\n            :attr:`pyvista.Pen.LINE_STYLES` for a list of valid line\n            styles and :class:`pyvista.Color` for an overview of\n            parsable colors.\n\n        Returns\n        -------\n        marker_style : str\n            Extracted marker style (empty string if no marker style\n            was present in the format string).\n\n        line_style : str\n            Extracted line style (empty string if no line style was\n            present in the format string).\n\n        color : str\n            Extracted color string (defaults to ``\"b\"`` if no color\n            was present in the format string).\n\n        Examples\n        --------\n        >>> import pyvista\n        >>> m, l, c = pyvista.Chart2D._parse_format(\"x--b\")\n\n        \"\"\"\n        marker_style = \"\"\n        line_style = \"\"\n        color = None\n        # Note: All colors, marker styles and line styles are sorted in decreasing order of length to be able to find\n        # the largest match first (e.g. find 'darkred' and '--' first instead of 'red' and '-')\n        colors = sorted(\n            itertools.chain(hexcolors.keys(), color_synonyms.keys()),\n            key=len,\n            reverse=True,\n        )\n        marker_styles = sorted(ScatterPlot2D.MARKER_STYLES.keys(), key=len, reverse=True)\n        line_styles = sorted(Pen.LINE_STYLES.keys(), key=len, reverse=True)\n        hex_pattern = \"(#|0x)[A-Fa-f0-9]{6}([A-Fa-f0-9]{2})?\"  # Match RGB(A) hex string\n        # Extract color from format string\n        match = re.search(hex_pattern, fmt)  # Start with matching hex strings\n        if match is not None:\n            color = match.group()\n        else:  # Proceed with matching color strings\n            for c in colors:\n                if c in fmt:\n                    color = c\n                    break\n        if color is not None:\n            fmt = fmt.replace(color, \"\", 1)  # Remove found color from format string\n        else:\n            color = \"b\"\n        # Extract marker style from format string\n        for style in marker_styles[:-1]:  # Last style is empty string\n            if style in fmt:\n                marker_style = style\n                fmt = fmt.replace(\n                    marker_style, \"\", 1\n                )  # Remove found marker_style from format string\n                break\n        # Extract line style from format string\n        for style in line_styles[:-1]:  # Last style is empty string\n            if style in fmt:\n                line_style = style\n                fmt = fmt.replace(line_style, \"\", 1)  # Remove found line_style from format string\n                break\n        return marker_style, line_style, color\n\n    def plot(self, x, y=None, fmt='-'):\n        \"\"\"Matplotlib like plot method.\n\n        Parameters\n        ----------\n        x : array_like\n            Values to plot on the X-axis. In case ``y`` is ``None``,\n            these are the values to plot on the Y-axis instead.\n\n        y : array_like, optional\n            Values to plot on the Y-axis.\n\n        fmt : str, default: \"-\"\n            A format string, e.g. ``'ro'`` for red circles. See the Notes\n            section for a full description of the format strings.\n\n        Returns\n        -------\n        scatter_plot : plotting.charts.ScatterPlot2D, optional\n            The created scatter plot when a valid marker style\n            was present in the format string, ``None`` otherwise.\n\n        line_plot : plotting.charts.LinePlot2D, optional\n            The created line plot when a valid line style was\n            present in the format string, ``None`` otherwise.\n\n        Notes\n        -----\n        This plot method shares many of the same plotting features as\n        the `matplotlib.pyplot.plot\n        <https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.plot.html>`_.\n        Please reference the documentation there for a full\n        description of the allowable format strings.\n\n        Examples\n        --------\n        Generate a line plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _, line_plot = chart.plot(range(10), range(10))\n        >>> chart.show()\n\n        Generate a line and scatter plot.\n\n        >>> chart = pyvista.Chart2D()\n        >>> scatter_plot, line_plot = chart.plot(range(10), fmt='o-')\n        >>> chart.show()\n\n        \"\"\"\n        if y is None:\n            y = x\n            x = np.arange(len(y))\n        elif isinstance(y, str):\n            fmt = y\n            y = x\n            x = np.arange(len(y))\n        marker_style, line_style, color = self._parse_format(fmt)\n        scatter_plot, line_plot = None, None\n        if marker_style != \"\":\n            scatter_plot = self.scatter(x, y, color, style=marker_style)\n        if line_style != \"\":\n            line_plot = self.line(x, y, color, style=line_style)\n        return scatter_plot, line_plot\n\n    def scatter(self, x, y, color=\"b\", size=10, style=\"o\", label=\"\"):\n        \"\"\"Add a scatter plot to this chart.\n\n        Parameters\n        ----------\n        x : array_like\n            X coordinates of the points to draw.\n\n        y : array_like\n            Y coordinates of the points to draw.\n\n        color : ColorLike, default: \"b\"\n            Color of the points drawn in this plot. Any color parsable\n            by :class:`pyvista.Color` is allowed.\n\n        size : float, default: 10\n            Size of the point markers drawn in this plot.\n\n        style : str, default: \"o\"\n            Style of the point markers drawn in this plot. See\n            :ref:`ScatterPlot2D.MARKER_STYLES <scatter_marker_styles>`\n            for a list of allowed marker styles.\n\n        label : str, default: \"\"\n            Label of this plot, as shown in the chart's legend.\n\n        Returns\n        -------\n        plotting.charts.ScatterPlot2D\n            The created scatter plot.\n\n        Examples\n        --------\n        Generate a scatter plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> plot = chart.scatter([0, 1, 2], [2, 1, 3])\n        >>> chart.show()\n\n        \"\"\"\n        return self._add_plot(\"scatter\", x, y, color=color, size=size, style=style, label=label)\n\n    def line(self, x, y, color=\"b\", width=1.0, style=\"-\", label=\"\"):\n        \"\"\"Add a line plot to this chart.\n\n        Parameters\n        ----------\n        x : array_like\n            X coordinates of the points through which a line should be drawn.\n\n        y : array_like\n            Y coordinates of the points through which a line should be drawn.\n\n        color : ColorLike, default: \"b\"\n            Color of the line drawn in this plot. Any color parsable\n            by :class:`pyvista.Color` is allowed.\n\n        width : float, default: 1\n            Width of the line drawn in this plot.\n\n        style : str, default: \"-\"\n            Style of the line drawn in this plot. See\n            :ref:`Pen.LINE_STYLES <pen_line_styles>` for a list of\n            allowed line styles.\n\n        label : str, default: \"\"\n            Label of this plot, as shown in the chart's legend.\n\n        Returns\n        -------\n        plotting.charts.LinePlot2D\n            The created line plot.\n\n        Examples\n        --------\n        Generate a line plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> plot = chart.line([0, 1, 2], [2, 1, 3])\n        >>> chart.show()\n\n        \"\"\"\n        return self._add_plot(\"line\", x, y, color=color, width=width, style=style, label=label)\n\n    def area(self, x, y1, y2=None, color=\"b\", label=\"\"):\n        \"\"\"Add an area plot to this chart.\n\n        Parameters\n        ----------\n        x : array_like\n            X coordinates of the points outlining the area to draw.\n\n        y1 : array_like\n            Y coordinates of the points on the first outline of the area to draw.\n\n        y2 : array_like, optional\n            Y coordinates of the points on the second outline of the\n            area to draw. Defaults to ``np.zeros_like(x)``.\n\n        color : ColorLike, default: \"b\"\n            Color of the area drawn in this plot. Any color parsable\n            by :class:`pyvista.Color` is allowed.\n\n        label : str, default: \"\"\n            Label of this plot, as shown in the chart's legend.\n\n        Returns\n        -------\n        plotting.charts.AreaPlot\n            The created area plot.\n\n        Examples\n        --------\n        Generate an area plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> plot = chart.area([0, 1, 2], [2, 1, 3])\n        >>> chart.show()\n\n        \"\"\"\n        return self._add_plot(\"area\", x, y1, y2, color=color, label=label)\n\n    def bar(self, x, y, color=None, orientation=\"V\", label=None):\n        \"\"\"Add a bar plot to this chart.\n\n        Parameters\n        ----------\n        x : array_like\n            Positions (along the x-axis for a vertical orientation,\n            along the y-axis for a horizontal orientation) of the bars\n            to draw.\n\n        y : array_like\n            Size of the bars to draw. Multiple bars can be stacked by\n            passing a sequence of sequences.\n\n        color : ColorLike, default: \"b\"\n            Color of the bars drawn in this plot. Any color parsable\n            by :class:`pyvista.Color` is allowed.\n\n        orientation : str, default: \"V\"\n            Orientation of the bars drawn in this plot. Either ``\"H\"``\n            for an horizontal orientation or ``\"V\"`` for a vertical\n            orientation.\n\n        label : str, default: \"\"\n            Label of this plot, as shown in the chart's legend.\n\n        Returns\n        -------\n        plotting.charts.BarPlot\n            The created bar plot.\n\n        Examples\n        --------\n        Generate a bar plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> plot = chart.bar([0, 1, 2], [2, 1, 3])\n        >>> chart.show()\n\n        \"\"\"\n        return self._add_plot(\"bar\", x, y, color=color, orientation=orientation, label=label)\n\n    def stack(self, x, ys, colors=None, labels=None):\n        \"\"\"Add a stack plot to this chart.\n\n        Parameters\n        ----------\n        x : array_like\n            X coordinates of the points outlining the stacks (areas) to draw.\n\n        ys : sequence[array_like]\n            Size of the stacks (areas) to draw at the corresponding X\n            coordinates. Each sequence defines the sizes of one stack\n            (area), which are stacked on top of each other.\n\n        colors : sequence[ColorLike], optional\n            Color of the stacks (areas) drawn in this plot. Any color\n            parsable by :class:`pyvista.Color` is allowed.\n\n        labels : sequence[str], default: []\n            Label for each stack (area) drawn in this plot, as shown\n            in the chart's legend.\n\n        Returns\n        -------\n        plotting.charts.StackPlot\n            The created stack plot.\n\n        Examples\n        --------\n        Generate a stack plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> plot = chart.stack([0, 1, 2], [[2, 1, 3],[1, 2, 1]])\n        >>> chart.show()\n\n        \"\"\"\n        return self._add_plot(\"stack\", x, ys, colors=colors, labels=labels)\n\n    def plots(self, plot_type=None):\n        \"\"\"Return all plots of the specified type in this chart.\n\n        Parameters\n        ----------\n        plot_type : str, optional\n            The type of plots to return. Allowed types are\n            ``\"scatter\"``, ``\"line\"``, ``\"area\"``, ``\"bar\"``\n            and ``\"stack\"``.\n            If no type is provided (``None``), all plots are returned,\n            regardless of their type.\n\n        Yields\n        ------\n        plot : plotting.charts.ScatterPlot2D | plotting.charts.LinePlot2D | plotting.charts.AreaPlot | plotting.charts.BarPlot | plotting.charts.StackPlot\n            One of the plots (of the specified type) in this chart.\n\n        Examples\n        --------\n        Create a 2D chart with a line and scatter plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> scatter_plot, line_plot = chart.plot([0, 1, 2], [2, 1, 3], \"o-\")\n        >>> chart.show()\n\n        Retrieve all plots in the chart.\n\n        >>> plots = [*chart.plots()]\n        >>> scatter_plot in plots and line_plot in plots\n        True\n\n        Retrieve all line plots in the chart.\n\n        >>> line_plots = [*chart.plots(\"line\")]\n        >>> line_plot == line_plots[0]\n        True\n\n        \"\"\"\n        plot_types = self.PLOT_TYPES.keys() if plot_type is None else [plot_type]\n        for plot_type in plot_types:\n            yield from self._plots[plot_type]\n\n    def remove_plot(self, plot):\n        \"\"\"Remove the given plot from this chart.\n\n        Parameters\n        ----------\n        plot : plotting.charts.ScatterPlot2D | plotting.charts.LinePlot2D | plotting.charts.AreaPlot | plotting.charts.BarPlot | plotting.charts.StackPlot\n            The plot to remove.\n\n        Examples\n        --------\n        Create a 2D chart with a line and scatter plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> scatter_plot, line_plot = chart.plot([0, 1, 2], [2, 1, 3], \"o-\")\n        >>> chart.show()\n\n        Remove the scatter plot from the chart.\n\n        >>> chart.remove_plot(scatter_plot)\n        >>> chart.show()\n\n        \"\"\"\n        try:\n            plot_type = self._PLOT_CLASSES[type(plot)]\n            self._plots[plot_type].remove(plot)\n            self.RemovePlotInstance(plot)\n        except (KeyError, ValueError):\n            raise ValueError(\"The given plot is not part of this chart.\")\n\n    def clear(self, plot_type=None):\n        \"\"\"Remove all plots of the specified type from this chart.\n\n        Parameters\n        ----------\n        plot_type : str, optional\n            The type of the plots to remove. Allowed types are\n            ``\"scatter\"``, ``\"line\"``, ``\"area\"``, ``\"bar\"``\n            and ``\"stack\"``.\n\n            If no type is provided (``None``), all plots are removed,\n            regardless of their type.\n\n        Examples\n        --------\n        Create a 2D chart with multiple line and scatter plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.plot([0, 1, 2], [2, 1, 3], \"o-b\")\n        >>> _ = chart.plot([-2, -1, 0], [3, 1, 2], \"d-r\")\n        >>> chart.show()\n\n        Remove all scatter plots from the chart.\n\n        >>> chart.clear(\"scatter\")\n        >>> chart.show()\n\n        \"\"\"\n        plot_types = self.PLOT_TYPES.keys() if plot_type is None else [plot_type]\n        for plot_type in plot_types:\n            # Make a copy, as this list will be modified by remove_plot\n            plots = [*self._plots[plot_type]]\n            for plot in plots:\n                self.remove_plot(plot)\n\n    @property\n    def x_axis(self):  # numpydoc ignore=RT01\n        \"\"\"Return this chart's horizontal (x) :class:`Axis <plotting.charts.Axis>`.\n\n        Examples\n        --------\n        Create a 2D plot and hide the x axis.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.line([0, 1, 2], [2, 1, 3])\n        >>> chart.x_axis.toggle()\n        >>> chart.show()\n\n        \"\"\"\n        return self._x_axis\n\n    @property\n    def y_axis(self):  # numpydoc ignore=RT01\n        \"\"\"Return this chart's vertical (y) :class:`Axis <plotting.charts.Axis>`.\n\n        Examples\n        --------\n        Create a 2D plot and hide the y axis.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.line([0, 1, 2], [2, 1, 3])\n        >>> chart.y_axis.toggle()\n        >>> chart.show()\n\n        \"\"\"\n        return self._y_axis\n\n    @property\n    def x_label(self):  # numpydoc ignore=RT01\n        \"\"\"Return or set the label of this chart's x axis.\n\n        Examples\n        --------\n        Create a 2D plot and set custom axis labels.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.line([0, 1, 2], [2, 1, 3])\n        >>> chart.x_label = \"Horizontal axis\"\n        >>> chart.y_label = \"Vertical axis\"\n        >>> chart.show()\n\n        \"\"\"\n        return self.x_axis.label\n\n    @x_label.setter\n    def x_label(self, val):  # numpydoc ignore=GL08\n        self.x_axis.label = val\n\n    @property\n    def y_label(self):  # numpydoc ignore=RT01\n        \"\"\"Return or set the label of this chart's y axis.\n\n        Examples\n        --------\n        Create a 2D plot and set custom axis labels.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.line([0, 1, 2], [2, 1, 3])\n        >>> chart.x_label = \"Horizontal axis\"\n        >>> chart.y_label = \"Vertical axis\"\n        >>> chart.show()\n\n        \"\"\"\n        return self.y_axis.label\n\n    @y_label.setter\n    def y_label(self, val):  # numpydoc ignore=GL08\n        self.y_axis.label = val\n\n    @property\n    def x_range(self):  # numpydoc ignore=RT01\n        \"\"\"Return or set the range of this chart's x axis.\n\n        Examples\n        --------\n        Create a 2D plot and set custom axis ranges.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.line([0, 1, 2], [2, 1, 3])\n        >>> chart.x_range = [-2, 2]\n        >>> chart.y_range = [0, 5]\n        >>> chart.show()\n\n        \"\"\"\n        return self.x_axis.range\n\n    @x_range.setter\n    def x_range(self, val):  # numpydoc ignore=GL08\n        self.x_axis.range = val\n\n    @property\n    def y_range(self):  # numpydoc ignore=RT01\n        \"\"\"Return or set the range of this chart's y axis.\n\n        Examples\n        --------\n        Create a 2D plot and set custom axis ranges.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.line([0, 1, 2], [2, 1, 3])\n        >>> chart.x_range = [-2, 2]\n        >>> chart.y_range = [0, 5]\n        >>> chart.show()\n\n        \"\"\"\n        return self.y_axis.range\n\n    @y_range.setter\n    def y_range(self, val):  # numpydoc ignore=GL08\n        self.y_axis.range = val\n\n    @property\n    def grid(self):  # numpydoc ignore=RT01\n        \"\"\"Enable or disable the chart grid.\n\n        Examples\n        --------\n        Create a 2D chart with the grid disabled.\n\n        >>> import pyvista\n        >>> import numpy as np\n        >>> x = np.linspace(0, 2*np.pi, 20)\n        >>> y = np.sin(x)\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.line(x, y, 'r')\n        >>> chart.grid = False\n        >>> chart.show()\n\n        Enable the grid\n\n        >>> chart.grid = True\n        >>> chart.show()\n\n        \"\"\"\n        return self.x_axis.grid and self.y_axis.grid\n\n    @grid.setter\n    def grid(self, val):  # numpydoc ignore=GL08\n        self.x_axis.grid = val\n        self.y_axis.grid = val\n\n    def hide_axes(self):\n        \"\"\"Hide the x- and y-axis of this chart.\n\n        This includes all labels, ticks and the grid.\n\n        Examples\n        --------\n        Create a 2D plot and hide the axes.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.line([0, 1, 2], [2, 1, 3])\n        >>> chart.hide_axes()\n        >>> chart.show()\n\n        \"\"\"\n        for axis in (self.x_axis, self.y_axis):\n            axis.visible = False\n            axis.label_visible = False\n            axis.ticks_visible = False\n            axis.tick_labels_visible = False\n            axis.grid = False\n\n# --- Snippet Separator ---\n\ndef clear(self, plot_type=None):\n        \"\"\"Remove all plots of the specified type from this chart.\n\n        Parameters\n        ----------\n        plot_type : str, optional\n            The type of the plots to remove. Allowed types are\n            ``\"scatter\"``, ``\"line\"``, ``\"area\"``, ``\"bar\"``\n            and ``\"stack\"``.\n\n            If no type is provided (``None``), all plots are removed,\n            regardless of their type.\n\n        Examples\n        --------\n        Create a 2D chart with multiple line and scatter plot.\n\n        >>> import pyvista\n        >>> chart = pyvista.Chart2D()\n        >>> _ = chart.plot([0, 1, 2], [2, 1, 3], \"o-b\")\n        >>> _ = chart.plot([-2, -1, 0], [3, 1, 2], \"d-r\")\n        >>> chart.show()\n\n        Remove all scatter plots from the chart.\n\n        >>> chart.clear(\"scatter\")\n        >>> chart.show()\n\n        \"\"\"\n        plot_types = self.PLOT_TYPES.keys() if plot_type is None else [plot_type]\n        for plot_type in plot_types:\n            # Make a copy, as this list will be modified by remove_plot\n            plots = [*self._plots[plot_type]]\n            for plot in plots:\n                self.remove_plot(plot)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a PyQtGraph application with multiple plots demonstrating various features. The plots should include basic array plotting, multiple curves, drawing with points, parametric plot with grid enabled, scatter plot with axis labels and log scale, an updating plot, a filled plot with axis disabled, region selection, and zoom on selected region. The plots should be arranged in a grid layout and the application window should be titled \"Basic plotting examples\". The plots should be interactive, allowing for panning and scaling. The application should also include a timer that updates one of the plots at regular intervals.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 136, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that sets up a combined fluid-kinetic simulation using the DREAM library. The simulation should include the following settings: an electric field strength of 0.6 V/m, an electron density of 5e19 m^-3, and a temperature of 1e3 eV. The simulation should include a fully ionized ion species named 'D' with a charge of 1. The hot-tail grid should be disabled and the collision frequency mode should be set to ultra-relativistic. The Dreicer and avalanche should be included with the avalanche mode set to fluid and the Dreicer rate set to neural network. The initial profile should be set to 1e15. If the runaway electron grid is enabled, it should be set with 50 radial points, 100 momentum points, and a maximum momentum of 0.5. The advection interpolation method should be set to use flux limiters and the initialization method should be isotropic. The radial grid should be set with a magnetic field strength of 5, a minor radius of 0.22, a wall radius of 0.22, and one radial point. The solver should be set to nonlinear and verbose with a relative tolerance of 1e-4 for the runaway electron current density. The simulation should include fluid effects. The time stepper should be set with a maximum time of 1e-1 and 20 time steps. The settings should be saved to an HDF5 file named 'dream_settings.h5'.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class RunawayElectrons(UnknownQuantity,PrescribedInitialParameter):\n\n    def __init__(self, settings, density=0, radius=0, avalanche=AVALANCHE_MODE_NEGLECT, dreicer=DREICER_RATE_DISABLED, compton=COMPTON_MODE_NEGLECT, Eceff=COLLQTY_ECEFF_MODE_FULL, pCutAvalanche=0, comptonPhotonFlux=0, tritium=False, hottail=HOTTAIL_MODE_DISABLED):\n        \"\"\"\n        Constructor.\n        \"\"\"\n        super().__init__(settings=settings)\n\n        self.avalanche = avalanche\n        self.dreicer   = dreicer\n        self.compton   = compton\n        self.comptonPhotonFlux = comptonPhotonFlux\n        self.Eceff     = Eceff\n        self.pCutAvalanche = pCutAvalanche\n        self.tritium   = tritium\n        self.hottail   = hottail\n        self.negative_re = False\n\n        self.advectionInterpolation = AdvectionInterpolation.AdvectionInterpolation(kinetic=False)\n        self.transport = TransportSettings(kinetic=False)\n\n        self.density = None\n        self.radius  = None\n        self.setInitialProfile(density=density, radius=radius)\n\n\n    def setInitialProfile(self, density, radius=0):\n        _data, _rad = self._setInitialData(data=density, radius=radius)\n\n        self.density = _data\n        self.radius  = _rad\n        self.verifySettingsPrescribedInitialData()\n\n\n    def setAvalanche(self, avalanche, pCutAvalanche=0):\n        \"\"\"\n        Enables/disables avalanche generation.\n        \"\"\"\n        if avalanche == False:\n            self.avalanche = AVALANCHE_MODE_NEGLECT\n        else:\n            self.avalanche = int(avalanche)\n            self.pCutAvalanche = pCutAvalanche\n\n\n    def setDreicer(self, dreicer):\n        \"\"\"\n        Specifies which model to use for calculating the\n        Dreicer runaway rate.\n        \"\"\"\n        if dreicer == False:\n            self.dreicer = DREICER_RATE_DISABLED\n        else:\n            self.dreicer = int(dreicer)\n\n\n    def setCompton(self, compton, photonFlux = None):\n        \"\"\"\n        Specifies which model to use for calculating the\n        compton runaway rate.\n        \"\"\"\n        if compton == False or compton == COMPTON_MODE_NEGLECT:\n            self.compton = COMPTON_MODE_NEGLECT\n        else:\n            if compton == COMPTON_RATE_ITER_DMS:\n                # set fluid compton source and standard ITER flux of 1e18\n                compton = COMPTON_MODE_FLUID\n                if photonFlux is None:\n                    photonFlux = ITER_PHOTON_FLUX_DENSITY\n\n            if photonFlux is None:\n                raise EquationException(\"n_re: Compton photon flux must be set.\")\n\n            self.compton = int(compton)\n            self.comptonPhotonFlux = photonFlux\n\n\n    def setEceff(self, Eceff):\n        \"\"\"\n        Specifies which model to use for calculating the\n        effective critical field (used in the avalanche formula).\n        \"\"\"\n        self.Eceff = int(Eceff)\n\n\n    def setTritium(self, tritium):\n        \"\"\"\n        Specifices whether or not to include runaway generation\n        through tritium decay as a source term.\n        \"\"\"\n        self.tritium = tritium\n\n\n    def setHottail(self, hottail):\n        \"\"\"\n        Specify which model to use for hottail runaway generation\n        \"\"\"\n        if hottail == False:\n            self.hottail = HOTTAIL_MODE_DISABLED\n        else:\n            self.hottail = hottail\n            if hottail != HOTTAIL_MODE_DISABLED:\n                self.settings.eqsys.f_hot.enableAnalyticalDistribution()\n\n\n    def setNegativeRunaways(self, negative_re=True):\n        \"\"\"\n        Introduce a density of runaway electrons with negative pitch,\n        allowing the kinetic avalanche source term to properly account for\n        large-angle collisions with runaways moving in different directions.\n        \"\"\"\n        self.negative_re = negative_re\n\n\n    def setAdvectionInterpolationMethod(self, ad_int=AD_INTERP_CENTRED,\n        ad_jac=AD_INTERP_JACOBIAN_FULL, fluxlimiterdamping=1.0):\n        \"\"\"\n        Sets the interpolation method that is used in the advection terms of\n        the transport equation.\n\n        :param int ad_int:               Interpolation method to use for the radial coordinate.\n        :param int ad_jac:               Jacobian interpolation mode to use for the radial coordinate.\n        :param float fluxlimiterdamping: Damping parameter used to under-relax the interpolation coefficients during non-linear iterations (should be between 0 and 1).\n        \"\"\"\n        self.advectionInterpolation.setMethod(ad_int=ad_int, ad_jac=ad_jac, fluxlimiterdamping=fluxlimiterdamping)\n\n\n    def fromdict(self, data):\n        \"\"\"\n        Set all options from a dictionary.\n        \"\"\"\n        self.avalanche = int(data['avalanche'])\n\n        if 'pCutAvalanche' in data:\n            self.pCutAvalanche = data['pCutAvalanche']\n\n        self.dreicer   = int(data['dreicer'])\n        self.Eceff     = int(data['Eceff'])\n        self.compton            = int(data['compton']['mode'])\n        self.comptonPhotonFlux  = data['compton']['flux']\n        self.density   = data['init']['x']\n        self.radius    = data['init']['r']\n\n        if 'adv_interp' in data:\n            self.advectionInterpolation.fromdict(data['adv_interp'])\n\n        if 'hottail' in data:\n            self.hottail = int(data['hottail'])\n\n        if 'tritium' in data:\n            self.tritium = bool(data['tritium'])\n\n        if 'negative_re' in data:\n            self.negative_re = bool(data['negative_re'])\n\n        if 'transport' in data:\n            self.transport.fromdict(data['transport'])\n\n\n    def todict(self):\n        \"\"\"\n        Returns a Python dictionary containing all settings of\n        this RunawayElectrons object.\n        \"\"\"\n        data = {\n            'avalanche': self.avalanche,\n            'dreicer': self.dreicer,\n            'Eceff': self.Eceff,\n            'pCutAvalanche': self.pCutAvalanche,\n            'transport': self.transport.todict(),\n            'tritium': self.tritium,\n            'hottail': self.hottail,\n            'negative_re': self.negative_re\n        }\n        data['compton'] = {\n            'mode': self.compton,\n            'flux': self.comptonPhotonFlux\n        }\n        data['init'] = {\n            'x': self.density,\n            'r': self.radius\n        }\n\n        # Flux limiter settings\n        data['adv_interp'] = self.advectionInterpolation.todict()\n\n        return data\n\n\n    def verifySettings(self):\n        \"\"\"\n        Verify that the settings of this unknown are correctly set.\n        \"\"\"\n        if type(self.avalanche) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'avalanche'. Expected integer.\")\n        if type(self.dreicer) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'dreicer'. Expected integer.\")\n        if type(self.compton) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'compton'. Expected integer.\")\n        if type(self.hottail) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'hottail'. Expected integer.\")\n        if type(self.Eceff) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'Eceff'. Expected integer.\")\n        if self.avalanche == AVALANCHE_MODE_KINETIC and self.pCutAvalanche == 0:\n            raise EquationException(\"n_re: Invalid value assigned to 'pCutAvalanche'. Must be set explicitly when using KINETIC avalanche.\")\n        if type(self.tritium) != bool:\n            raise EquationException(\"n_re: Invalid value assigned to 'tritium'. Expected bool.\")\n        if self.hottail != HOTTAIL_MODE_DISABLED and self.settings.eqsys.f_hot.mode == DISTRIBUTION_MODE_NUMERICAL:\n            raise EquationException(\"n_re: Invalid setting combination: when hottail is enabled, the 'mode' of f_hot cannot be NUMERICAL. Enable ANALYTICAL f_hot distribution or disable hottail.\")\n        if type(self.negative_re) != bool:\n            raise EquationException(\"n_re: Invalid value assigned to 'negative_re'. Expected bool.\")\n\n        self.advectionInterpolation.verifySettings()\n        self.transport.verifySettings()\n\n\n    def verifySettingsPrescribedInitialData(self):\n        self._verifySettingsPrescribedInitialData('n_re', data=self.density, radius=self.radius)\n\n# --- Snippet Separator ---\n\ndef setPrescribedData(self, efield, radius=0, times=0):\n        \"\"\"\n        When ``TYPE_PRESCRIBED``, sets the spatiotemporal evolution of the\n        electric field during the simulation. The parameter ``efield`` may be\n        either a scalar (in which case the electric field is taken to be\n        constant and uniform in time and radius) or a 2D array of shape\n        (nt, nr). The associated time grid ``times`` must be of size ``nt`` and\n        the radial grid must be of size ``nr``.\n\n        :param efield: Prescribed electric field.\n        :param radius: Radial grid on which the electric field is prescribed.\n        :param times:  Time grid on which the electric field is prescribed.\n        \"\"\"\n        _data, _rad, _tim = self._setPrescribedData(efield, radius, times)\n        self.efield = _data\n        self.radius = _rad\n        self.times  = _tim\n\n        self._verifySettingsPrescribedData()\n\n# --- Snippet Separator ---\n\ndef setTimeVaryingB(self, dB0dt_B0, t=0):\n        \"\"\"\n        Set the time-rate-of-change of the magnetic field strength (for the\n        approximate time varying B compression force operator). Note that the\n        specified parameter should be (1/B0)*(dB0/dt), where B0 is the on-axis\n        magnetic field strength and dB0/dt denotes the absolute\n        time-rate-of-change of B0.\n\n        :param dB0dt_B0: Normalized time-rate-of-change of the on-axis magnetic field strength.\n        :param t: Time vector corresponding to the given time-rate-of-change.\n        \"\"\"\n        self.dlnB0dt_x, self.dlnB0dt_t = self._setScalarData(data=dB0dt_B0, times=t)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that sets up a combined fluid-kinetic simulation using the DREAM library. The simulation should include the following settings: an electric field strength of 0.6 V/m, an electron density of 5e19 m^-3, and a temperature of 1e3 eV. The simulation should include a fully ionized ion species named 'D' with a charge of 1. The hot-tail grid should be disabled and the collision frequency mode should be set to ultra-relativistic. The Dreicer and avalanche should be included with the avalanche mode set to fluid and the Dreicer rate set to neural network. The initial profile should be set to 1e15. If the runaway electron grid is enabled, it should be set with 50 radial points, 100 momentum points, and a maximum momentum of 0.5. The advection interpolation method should be set to use flux limiters and the initialization method should be isotropic. The radial grid should be set with a magnetic field strength of 5, a minor radius of 0.22, a wall radius of 0.22, and one radial point. The solver should be set to nonlinear and verbose with a relative tolerance of 1e-4 for the runaway electron current density. The simulation should include fluid effects. The time stepper should be set with a maximum time of 1e-1 and 20 time steps. The settings should be saved to an HDF5 file named 'dream_settings.h5'.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 137, "repo_full_name": "pmgbergen__porepy", "instruction": "Generate code that imports necessary libraries and modules, including numpy, scipy, os, sys, and several modules from the porepy library. The code should define two functions, add_data_darcy and add_data_advection, which add data to a given grid bucket (gb) and domain with a specified tolerance (tol). The add_data_darcy function should add parameters related to Darcy's law, including permeability, source, aperture, and boundary conditions. The add_data_advection function should add parameters related to advection, including source, porosity, discharge, and boundary conditions. \n\nThe code should also append a path to the system path, import a module named soultz_grid, and set up parameters for creating a grid. It should then create a grid using the soultz_grid module, compute its geometry, coarsen it if a certain condition is met, and assign node ordering. \n\nThe code should then solve a Darcy problem using the DualVEMMixDim solver from the porepy library, add parameters to the grid bucket, compute a matrix and right-hand side vector, solve the system of equations, split the solution, extract discharge and pressure, project discharge, and compute the total flow rate. \n\nThe code should then set up parameters for a transport problem, define solvers for advection and mass matrix, add parameters to the grid bucket, compute matrices and right-hand side vectors, perform an LU factorization, initialize a solution vector, and perform a time-stepping loop to update the solution and export it at certain time steps. \n\nFinally, the code should export the solution in PVD format and save the production data to a text file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Coupler(object):\n\n    #------------------------------------------------------------------------------#\n\n    def __init__(self, discr=None, coupling=None, **kwargs):\n\n        # Consider the dofs\n        discr_ndof = kwargs.get(\"discr_ndof\")\n        if discr_ndof is None:\n            self.discr_ndof = discr.ndof\n        else:\n            self.discr_ndof = discr_ndof\n\n        # Consider the solver for each dimension\n        discr_fct = kwargs.get(\"discr_fct\")\n        if discr_fct is None:\n            self.discr_fct = discr.matrix_rhs\n        else:\n            self.discr_fct = discr_fct\n\n        # Consider the coupling between dimensions\n        coupling_fct = kwargs.get(\"coupling_fct\")\n        if coupling is None and coupling_fct is None:\n            self.coupling_fct = None\n        elif coupling_fct is not None:\n            self.coupling_fct = coupling_fct\n        else:\n            self.coupling_fct = coupling.matrix_rhs\n\n#------------------------------------------------------------------------------#\n\n    def ndof(self, gb):\n        \"\"\"\n        Store in the grid bucket the number of degrees of freedom associated to\n        the method.\n        It requires the key \"dof\" in the grid bucket as reserved.\n\n        Parameter\n        ---------\n        gb: grid bucket.\n\n        \"\"\"\n        gb.add_node_prop('dof')\n        for g, d in gb:\n            d['dof'] = self.discr_ndof(g)\n\n#------------------------------------------------------------------------------#\n\n    def matrix_rhs(self, gb, matrix_format=\"csr\"):\n        \"\"\"\n        Return the matrix and righ-hand side for a suitable discretization, where\n        a hierarchy of grids are considered. The matrices are stored in the\n        global matrix and right-hand side according to the numeration given by\n        \"node_number\".\n        It requires the key \"dof\" in the grid bucket as reserved.\n        It requires the key \"node_number\" be present in the grid bucket, see\n        GridBucket.assign_node_ordering().\n\n        Parameters\n        ----------\n        gb : grid bucket with geometry fields computed.\n        matrix_format: (optional, default is csr) format of the sparse matrix.\n\n        Return\n        ------\n        matrix: sparse matrix from the discretization.\n        rhs: array right-hand side of the problem.\n        \"\"\"\n        self.ndof(gb)\n\n        # Initialize the global matrix and rhs to store the local problems\n        matrix = np.empty((gb.size(), gb.size()), dtype=np.object)\n        rhs = np.empty(gb.size(), dtype=np.object)\n        for g_i, d_i in gb:\n            pos_i = d_i['node_number']\n            rhs[pos_i] = np.empty(d_i['dof'])\n            for g_j, d_j in gb:\n                pos_j = d_j['node_number']\n                matrix[pos_i, pos_j] = sps.coo_matrix((d_i['dof'], d_j['dof']))\n\n        # Loop over the grids and compute the problem matrix\n        for g, data in gb:\n            pos = data['node_number']\n            matrix[pos, pos], rhs[pos] = self.discr_fct(g, data)\n\n        # Handle special case of 1-element grids, that give 0-d arrays\n        rhs = np.array([np.atleast_1d(a) for a in tuple(rhs)])\n\n        # if the coupling conditions are not given fill only the diagonal part\n        if self.coupling_fct is None:\n            return sps.bmat(matrix, matrix_format), np.concatenate(tuple(rhs))\n\n        # Loop over the edges of the graph (pair of connected grids) to compute\n        # the coupling conditions\n        for e, data in gb.edges_props():\n            g_l, g_h = gb.sorted_nodes_of_edge(e)\n            pos_l, pos_h = gb.nodes_prop([g_l, g_h], 'node_number')\n            idx = np.ix_([pos_h, pos_l], [pos_h, pos_l])\n\n            data_l, data_h = gb.node_props(g_l), gb.node_props(g_h)\n            matrix[idx] += self.coupling_fct(g_h, g_l, data_h, data_l, data)\n\n        return sps.bmat(matrix, matrix_format), np.concatenate(tuple(rhs))\n\n#------------------------------------------------------------------------------#\n\n    def split(self, gb, key, values):\n        \"\"\"\n        Store in the grid bucket the vector, split in the function, solution of\n        the problem. The values are extracted from the global solution vector\n        according to the numeration given by \"node_number\".\n\n        Parameters\n        ----------\n        gb : grid bucket with geometry fields computed.\n        key: new name of the solution to be stored in the grid bucket.\n        values: array, global solution.\n\n        \"\"\"\n        dofs = self._dof_start_of_grids(gb)\n\n        gb.add_node_prop(key)\n        for g, d in gb:\n            i = d['node_number']\n            d[key] = values[slice(dofs[i], dofs[i + 1])]\n\n#------------------------------------------------------------------------------#\n    def merge(self, gb, key):\n        \"\"\"\n        Merge the stored split function stored in the grid bucket to a vector.\n        The values are put into the global  vector according to the numeration\n        given by \"node_number\".\n\n        Parameters\n        ----------\n        gb : grid bucket with geometry fields computed.\n        key: new name of the solution to be stored in the grid bucket.\n\n        Returns\n        -------\n        values: (ndarray) the values stored in the bucket as an array\n        \"\"\"\n\n        dofs = self._dof_start_of_grids(gb)\n        values = np.zeros(dofs[-1])\n\n        for g, d in gb:\n            i = d['node_number']\n            values[slice(dofs[i], dofs[i + 1])] = d[key]\n\n        return values\n#------------------------------------------------------------------------------#\n\n    def _dof_start_of_grids(self ,gb):\n        \" Helper method to get first global dof for all grids. \"\n        self.ndof(gb)\n        dofs = np.empty(gb.size(), dtype=int)\n        for _, d in gb:\n            dofs[d['node_number']] = d['dof']\n        return np.r_[0, np.cumsum(dofs)]\n#------------------------------------------------------------------------------#\n\n    def dof_of_grid(self, gb, g):\n        \"\"\" Obtain global indices of dof associated with a given grid.\n\n        Parameters:\n            gb: Grid_bucket representation of mixed-dimensional data.\n            g: Grid, one member of gb.\n\n        Returns:\n            np.array of ints: Indices of all dof for the given grid\n\n        \"\"\"\n        dof_list = self._dof_start_of_grids(gb)\n        nn = gb.node_props(g)['node_number']\n        return np.arange(dof_list[nn], dof_list[nn+1])\n\n# --- Snippet Separator ---\n\nclass EllipticModel():\n    '''\n    Class for solving an incompressible flow problem:\n    \\nabla K \\nabla p = q,\n    where K is the second order permeability tenser, p the fluid pressure\n    and q sinks and sources.\n\n    Parameters in Init:\n    gb: (Grid /GridBucket) a grid or grid bucket object. If gb = GridBucket\n        a Parameter class should be added to each grid bucket data node with\n        keyword 'param'.\n    data: (dictionary) Defaults to None. Only used if gb is a Grid. Should\n          contain a Parameter class with the keyword 'Param'\n    physics: (string): defaults to 'flow'\n\n    Functions:\n    solve(): Calls reassemble and solves the linear system.\n             Returns: the pressure p.\n             Sets attributes: self.x\n    step(): Same as solve, but without reassemble of the matrices\n    reassemble(): Assembles the lhs matrix and rhs array.\n            Returns: lhs, rhs.\n            Sets attributes: self.lhs, self.rhs\n    source_disc(): Defines the discretization of the source term.\n            Returns Source discretization object\n    flux_disc(): Defines the discretization of the flux term.\n            Returns Flux discretization object (E.g., Tpfa)\n    grid(): Returns: the Grid or GridBucket\n    data(): Returns: Data dictionary\n    split(name): Assignes the solution self.x to the data dictionary at each\n                 node in the GridBucket.\n                 Parameters:\n                    name: (string) The keyword assigned to the pressure\n    discharge(): Calls split('pressure'). Then calculate the discharges over each\n                 face in the grids and between edges in the GridBucket\n    save(): calls split('pressure'). Then export the pressure to a vtk file to the\n            folder kwargs['folder_name'] with file name\n            kwargs['file_name'], default values are 'results' for the folder and\n            physics for the file name.\n    '''\n\n    def __init__(self, gb, data=None, physics='flow', **kwargs):\n        self.physics = physics\n        self._gb = gb\n        self.is_GridBucket = isinstance(self._gb, GridBucket)\n        self._data = data\n\n        self.lhs = []\n        self.rhs = []\n        self.x = []\n\n        file_name = kwargs.get('file_name', physics)\n        folder_name = kwargs.get('folder_name', 'results')\n        mesh_kw = kwargs.get('mesh_kw', {})\n\n        tic = time.time()\n        logger.info('Create exporter')\n        self.exporter = Exporter(self._gb, file_name, folder_name, **mesh_kw)\n        logger.info('Elapsed time: ' + str(time.time() - tic))\n\n        self._flux_disc = self.flux_disc()\n        self._source_disc = self.source_disc()\n\n    def solve(self, max_direct=40000, callback=False, **kwargs):\n        \"\"\" Reassemble and solve linear system.\n\n        After the funtion has been called, the attributes lhs and rhs are\n        updated according to the parameter states. Also, the attribute x\n        gives the pressure given the current state.\n\n        TODO: Provide an option to save solver information if multiple\n        systems are to be solved with the same left hand side.\n\n        The function attempts to set up the best linear solver based on the\n        system size. The setup and parameter choices here are still\n        experimental.\n\n        Parameters:\n            max_direct (int): Maximum number of unknowns where a direct solver\n                is applied. If a direct solver can be applied this is usually\n                the most efficient option. However, if the system size is\n                too large compared to available memory, a direct solver becomes\n                extremely slow.\n            callback (boolean, optional): If True iteration information will be\n                output when an iterative solver is applied (system size larger\n                than max_direct)\n\n        Returns:\n            np.array: Pressure state.\n\n        \"\"\"\n        logger.error('Solve elliptic model')\n        # Discretize\n        tic = time.time()\n        logger.warning('Discretize')\n        self.lhs, self.rhs = self.reassemble()\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n\n        # Solve\n        tic = time.time()\n        ls = LSFactory()\n        if self.rhs.size < max_direct:\n            logger.warning('Solve linear system using direct solver')\n            self.x = ls.direct(self.lhs, self.rhs)\n        else:\n            logger.warning('Solve linear system using GMRES')\n            precond = self._setup_preconditioner()\n#            precond = ls.ilu(self.lhs)\n            slv = ls.gmres(self.lhs)\n            self.x, info = slv(self.rhs, M=precond, callback=callback,\n                               maxiter=10000, restart=1500, tol=1e-8)\n            if info == 0:\n                logger.warning('GMRES succeeded.')\n            else:\n                logger.warning('GMRES failed with status ' + str(info))\n\n        logger.warning('Done. Elapsed time ' + str(time.time() - tic))\n        return self.x\n\n    def step(self):\n        return self.solve()\n\n    def reassemble(self):\n        \"\"\"\n        reassemble matrices. This must be called between every time step to\n        update the rhs of the system.\n        \"\"\"\n        lhs_flux, rhs_flux = self._discretize(self._flux_disc)\n        lhs_source, rhs_source = self._discretize(self._source_disc)\n        assert lhs_source.nnz == 0, 'Source lhs different from zero!'\n        self.lhs = lhs_flux\n        self.rhs = rhs_flux + rhs_source\n        return self.lhs, self.rhs\n\n    def source_disc(self):\n        if self.is_GridBucket:\n            return source.IntegralMixedDim(physics=self.physics)\n        else:\n            return source.Integral(physics=self.physics)\n\n    def flux_disc(self):\n        if self.is_GridBucket:\n            return tpfa.TpfaMixedDim(physics=self.physics)\n        else:\n            return tpfa.Tpfa(physics=self.physics)\n\n    def _discretize(self, discr):\n        if self.is_GridBucket:\n            return discr.matrix_rhs(self.grid())\n        else:\n            return discr.matrix_rhs(self.grid(), self.data())\n\n    def grid(self):\n        return self._gb\n\n    def data(self):\n        return self._data\n\n    def split(self, x_name='solution'):\n        self.x_name = x_name\n        self._flux_disc.split(self.grid(), self.x_name, self.x)\n\n    def pressure(self, pressure_name='pressure'):\n        self.pressure_name = pressure_name\n        if self.is_GridBucket:\n            self.split(self.pressure_name)\n        else:\n            self._data[self.pressure_name] = self.x\n\n    def discharge(self, discharge_name='discharge'):\n        if self.is_GridBucket:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       p_name=self.pressure_name)\n        else:\n            fvutils.compute_discharges(self.grid(), self.physics,\n                                       self.pressure_name,\n                                       self._data)\n\n    def permeability(self, perm_names=['kxx', 'kyy', 'kzz']):\n        \"\"\" Assign permeability to self._data, ready for export to vtk.\n\n        For the moment, we only dump the main diagonals of the permeabliity.\n        Extensions should be trivial if needed.\n\n        Parameters:\n            perm_names (list): Which components to export. Defaults to kxx,\n                kyy and xzz.\n\n        \"\"\"\n\n        def get_ind(n):\n            if n == 'kxx':\n                return 0\n            elif n == 'kyy':\n                return 1\n            elif n == 'kzz':\n                return 2\n            else:\n                raise ValueError('Unknown perm keyword ' + n)\n\n        for n in perm_names:\n            ind = get_ind(n)\n            if self.is_GridBucket:\n                for _, d in self.grid():\n                    d[n] = d['param'].get_permeability().perm[ind, ind, :]\n            else:\n                self._data[n] = self._data['param'].get_permeability()\\\n                    .perm[ind, ind, :]\n\n    def porosity(self, poro_name='porosity'):\n        if self.is_GridBucket:\n            for _, d in self.grid():\n                d[poro_name] = d['param'].get_porosity()\n        else:\n            self._data[poro_name] = self._data['param'].get_porosity()\n\n    def save(self, variables=None, save_every=None):\n        if variables is None:\n            self.exporter.write_vtk()\n        else:\n            if not self.is_GridBucket:\n                variables = {k: self._data[k] for k in variables\n                             if k in self._data}\n            self.exporter.write_vtk(variables)\n\n    # Helper functions for linear solve below\n    def _setup_preconditioner(self):\n        solvers, ind, not_ind = self._assign_solvers()\n\n        def precond(r):\n            x = np.zeros_like(r)\n            for s, i, ni in zip(solvers, ind, not_ind):\n                x[i] += s(r[i])\n            return x\n\n        def precond_mult(r):\n            x = np.zeros_like(r)\n            A = self.lhs\n            for s, i, ni in zip(solvers, ind, not_ind):\n                r_i = r[i] - A[i, :][:, ni] * x[ni]\n                x[i] += s(r_i)\n            return x\n\n        def M(r): return precond(r)\n        return spl.LinearOperator(self.lhs.shape, M)\n\n    def _assign_solvers(self):\n        mat, ind = self._obtain_submatrix()\n        all_ind = np.arange(self.rhs.size)\n        not_ind = [np.setdiff1d(all_ind, i) for i in ind]\n\n        factory = LSFactory()\n        num_mat = len(mat)\n        solvers = np.empty(num_mat, dtype=np.object)\n        for i, A in enumerate(mat):\n            sz = A.shape[0]\n            if sz < 5000:\n                solvers[i] = factory.direct(A)\n            else:\n                # amg solver is pyamg is installed, if not ilu\n                try:\n                    solvers[i] = factory.amg(A, as_precond=True)\n                except ImportError:\n                    solvers[i] = factory.ilu(A)\n\n        return solvers, ind, not_ind\n\n    def _obtain_submatrix(self):\n\n        if isinstance(self.grid(), GridBucket):\n            gb = self.grid()\n            fd = self.flux_disc()\n            mat = []\n            sub_ind = []\n            for g, _ in self.grid():\n                ind = fd.solver.dof_of_grid(gb, g)\n                A = self.lhs[ind, :][:, ind]\n                mat.append(A)\n                sub_ind.append(ind)\n            return mat, sub_ind\n        else:\n            return [self.lhs], [np.arange(self.grid().num_cells)]\n\n# --- Snippet Separator ---\n\nclass Biot(Solver):\n\n    def __init__(self, eta=None):\n        \"\"\" Set default values for some parameters used in discretization.\n\n        \"\"\"\n        defaults = {'fluid_compr': 0,\n                    'fluid_viscosity': 1,\n                    'biot_alpha': 1\n                    }\n        self.defaults = defaults\n\n    def ndof(self, g):\n        \"\"\" Return the number of degrees of freedom associated wiht the method.\n\n        In this case, each cell has nd displacement variables, as well as a\n        pressure variable.\n\n        Parameters:\n            g: grid, or a subclass.\n\n        Returns:\n            int: Number of degrees of freedom in the grid.\n\n        \"\"\"\n        return g.num_cells * (1 + g.dim)\n\n    def matrix_rhs(self, g, data, discretize=True):\n        if discretize:\n            self.discretize(g, data)\n\n        A_biot = self.assemble_matrix(g, data)\n        rhs_bound = self.rhs(g, data)\n        return A_biot, rhs_bound\n\n#--------------------------- Helper methods for discretization ----------\n\n    def rhs(self, g, data):\n        bnd = self.rhs_bound(g, data)\n        tm = self.rhs_time(g, data)\n#        src = data['source']\n        return bnd + tm\n\n    def rhs_bound(self, g, data):\n        \"\"\" Boundary component of the right hand side.\n\n        TODO: Boundary effects of coupling terms.\n\n        Parameters:\n            g: grid, or subclass, with geometry fields computed.\n            data: dictionary to store the data terms. Must have been through a\n                call to discretize() to discretization of right hand side.\n            state: np.ndarray, solution vector from previous time step.\n\n        Returns:\n            np.ndarray: Contribution to right hand side given the current\n            state.\n\n        \"\"\"\n        d = data['param'].get_bc_val('mechanics')\n        p = data['param'].get_bc_val('flow')\n\n        div_flow = fvutils.scalar_divergence(g)\n        div_mech = fvutils.vector_divergence(g)\n\n        p_bound = -div_flow * data['bound_flux'] * p\\\n                  - data['bound_div_d'] * d\n        s_bound = -div_mech * data['bound_stress'] * d\n        return np.hstack((s_bound, p_bound))\n\n    def rhs_time(self, g, data):\n        \"\"\" Time component of the right hand side (dependency on previous time\n        step).\n\n        TODO: 1) Generalize this to allow other methods than Euler backwards?\n              2) How about time dependent boundary conditions.\n\n        Parameters:\n            g: grid, or subclass, with geometry fields computed.\n            data: dictionary to store the data terms. Must have been through a\n                call to discretize() to discretization of right hand side.\n            state: np.ndarray optional, solution vector from previous time\n                step. Defaults to zero.\n\n        Returns:\n            np.ndarray: Contribution to right hand side given the current\n            state.\n\n        \"\"\"\n        state = data.get('state', None)\n        if state is None:\n            state = np.zeros((g.dim + 1) * g.num_cells)\n\n        d = self.extractD(g, state, as_vector=True)\n        p = self.extractP(g, state)\n\n        d_scaling = data.get('displacement_scaling', 1)\n\n        div_d = np.squeeze(data['param'].biot_alpha *\n                           data['div_d'] * d * d_scaling)\n        p_cmpr = data['compr_discr'] * p\n\n        mech_rhs = np.zeros(g.dim * g.num_cells)\n\n        return np.hstack((mech_rhs, div_d + p_cmpr))\n\n    def discretize(self, g, data):\n        \"\"\" Discretize flow and mechanics equations using FV methods.\n\n        The parameters needed for the discretization are stored in the\n        dictionary data, which should contain the following mandatory keywords:\n\n            Related to flow equation:\n                perm: Second order tensor representing permeability\n                bound_flow: BoundaryCondition object for flow equation. Used in\n                    mpfa.\n\n            Related to mechanics equation:\n                stiffness: Fourth order tensor representing elastic moduli.\n                bound_mech: BoundaryCondition object for mechanics equation.\n                    Used in mpsa.\n\n        In addition, the following parameters are optional:\n\n            Related to flow equation:\n                fluid_viscosity (double). Defaults to 1.\n                fluid_compr (double): Fluid compressibility. Defaults to 0.\n\n            Related to coupling terms:\n                biot_alpha (double between 0 and 1): Biot's coefficient.\n                    Defaults to 1.\n\n            Related to numerics:\n                inverter (str): Which method to use for block inversion. See\n                    fvutils.invert_diagonal_blocks for detail, and for default\n                    options.\n                eta (double): Location of continuity point in MPSA and MPFA.\n                    Defaults to 1/3 for simplex grids, 0 otherwise.\n\n        The discretization is stored in the data dictionary, in the form of\n        several matrices representing different coupling terms. For details,\n        and how to combine these, see self.assemble_matrix()\n\n        Parameters:\n            g (grid): Grid to be discretized.\n            data (dictionary): Containing data for discretization. See above\n                for specification.\n\n        \"\"\"\n        # Discretization of elasticity / poro-mechanics\n        self._discretize_flow(g, data)\n        self._discretize_mech(g, data)\n        self._discretize_compr(g, data)\n\n    def assemble_matrix(self, g, data):\n        \"\"\" Assemble the poro-elastic system matrix.\n\n        The discretization is presumed stored in the data dictionary.\n\n        Parameters:\n            g (grid): Grid for disrcetization\n            data (dictionary): Data for discretization, as well as matrices\n                with discretization of the sub-parts of the system.\n\n        Returns:\n            scipy.sparse.bmat: Block matrix with the combined MPSA/MPFA\n                discretization.\n\n        \"\"\"\n        div_flow = fvutils.scalar_divergence(g)\n        div_mech = fvutils.vector_divergence(g)\n        param = data['param']\n\n        fluid_viscosity = param.fluid_viscosity\n        biot_alpha = param.biot_alpha\n\n        # Put together linear system\n        A_flow = div_flow * data['flux'] / fluid_viscosity\n        A_mech = div_mech * data['stress']\n\n        # Time step size\n        dt = data['dt']\n\n        d_scaling = data.get('displacement_scaling', 1)\n        # Matrix for left hand side\n        A_biot = sps.bmat([[A_mech,\n                            data['grad_p'] * biot_alpha],\n                           [data['div_d'] * biot_alpha * d_scaling,\n                            data['compr_discr']\n                            + dt * A_flow + data['stabilization']]]).tocsr()\n\n        return A_biot\n\n    def _discretize_flow(self, g, data):\n\n        # Discretiztaion using MPFA\n        md = mpfa.Mpfa(physics='flow')\n\n        md.discretize(g, data)\n        data['flux'] = data['flux']\n        data['bound_flux'] = data['bound_flux']\n\n    def _discretize_compr(self, g, data):\n        param = data['param']\n        compr = param.fluid_compr\n        poro = param.porosity\n        data['compr_discr'] = sps.dia_matrix((g.cell_volumes * compr * poro, 0),\n                                             shape=(g.num_cells, g.num_cells))\n\n    def _discretize_mech(self, g, data):\n        \"\"\"\n        Discretization of poro-elasticity by the MPSA-W method.\n\n        Implementation needs (in addition to those mentioned in mpsa function):\n            1) Fields for non-zero boundary conditions. Should be simple.\n            2) Split return value grad_p into forces and a divergence operator,\n            so that we can compute Biot forces on a face.\n\n        Parameters:\n            g (core.grids.grid): grid to be discretized\n            k (core.constit.second_order_tensor) permeability tensor\n            bound_mech: Boundary condition object for mechancis\n            bound_flow: Boundary condition object for flow.\n            constit (porepy.bc.bc.BoundaryCondition) class for boundary values\n            faces (np.ndarray) faces to be considered. Intended for partial\n                discretization, may change in the future\n            eta Location of pressure continuity point. Should be 1/3 for simplex\n                grids, 0 otherwise. On boundary faces with Dirichlet conditions,\n                eta=0 will be enforced.\n            inverter (string) Block inverter to be used, either numba (default),\n                cython or python. See fvutils.invert_diagonal_blocks for details.\n\n        Returns:\n            scipy.sparse.csr_matrix (shape num_faces * dim, num_cells * dim): stres\n                discretization, in the form of mapping from cell displacement to\n                face stresses.\n            scipy.sparse.csr_matrix (shape num_faces * dim, num_faces * dim):\n                discretization of boundary conditions. Interpreted as istresses\n                induced by the boundary condition (both Dirichlet and Neumann). For\n                Neumann, this will be the prescribed stress over the boundary face,\n                and possibly stress on faces having nodes on the boundary. For\n                Dirichlet, the values will be stresses induced by the prescribed\n                displacement.  Incorporation as a right hand side in linear system\n                by multiplication with divergence operator.\n            scipy.sparse.csr_matrix (shape num_cells * dim, num_cells): Forces from\n                the pressure gradient (I*p-term), represented as body forces.\n                TODO: Should rather be represented as forces on faces.\n            scipy.sparse.csr_matrix (shape num_cells, num_cells * dim): Trace of\n                strain matrix, cell-wise.\n            scipy.sparse.csr_matrix (shape num_cells x num_cells): Stabilization\n                term.\n\n        Example:\n            # Set up a Cartesian grid\n            g = structured.CartGrid([5, 5])\n            c = tensor.FourthOrder(g.dim, np.ones(g.num_cells))\n            k = tensor.SecondOrder(g.dim, np.ones(g.num_cells))\n\n            # Dirirchlet boundary conditions for mechanics\n            bound_faces = g.get_all_boundary_faces().ravel()\n            bnd = bc.BoundaryCondition(g, bound_faces, ['dir'] * bound_faces.size)\n\n            # Use no boundary conditions for flow, will default to homogeneous\n            # Neumann.\n\n            # Discretization\n            stress, bound_stress, grad_p, div_d, stabilization = biot(g, c, bnd)\n            flux, bound_flux = mpfa(g, k, None)\n\n            # Source in the middle of the domain\n            q_mech = np.zeros(g.num_cells * g.dim)\n\n            # Divergence operator for the grid\n            div_mech = fvutils.vector_divergence(g)\n            div_flow = fvutils.scalar_divergence(g)\n            a_mech = div_mech * stress\n            a_flow = div_flow * flux\n\n            a_biot = sps.bmat([[a_mech, grad_p], [div_d, a_flow +\n                                                           stabilization]])\n\n            # Zero boundary conditions by default.\n\n            # Injection in the middle of the domain\n            rhs = np.zeros(g.num_cells * (g.dim + 1))\n            rhs[g.num_cells * g.dim + np.ceil(g.num_cells / 2)] = 1\n            x = sps.linalg.spsolve(A, rhs)\n\n            u_x = x[0:g.num_cells * g.dim: g.dim]\n            u_y = x[1:g.num_cells * g.dim: g.dim]\n            p = x[g.num_cells * gdim:]\n\n        \"\"\"\n        param = data['param']\n        bound_mech = param.get_bc('mechanics')\n        bound_flow = param.get_bc('flow')\n        constit = param.get_tensor('mechanics')\n\n        eta = data.get('eta', 0)\n        inverter = data.get('inverter', None)\n\n        # The grid coordinates are always three-dimensional, even if the grid\n        # is really 2D. This means that there is not a 1-1 relation between the\n        # number of coordinates of a point / vector and the real dimension.\n        # This again violates some assumptions tacitly made in the\n        # discretization (in particular that the number of faces of a cell that\n        # meets in a vertex equals the grid dimension, and that this can be\n        # used to construct an index of local variables in the discretization).\n        # These issues should be possible to overcome, but for the moment, we\n        # simply force 2D grids to be proper 2D.\n        if g.dim == 2:\n            g = g.copy()\n            g.cell_centers = np.delete(g.cell_centers, (2), axis=0)\n            g.face_centers = np.delete(g.face_centers, (2), axis=0)\n            g.face_normals = np.delete(g.face_normals, (2), axis=0)\n            g.nodes = np.delete(g.nodes, (2), axis=0)\n\n            constit.c = np.delete(constit.c, (2, 5, 6, 7, 8), axis=0)\n            constit.c = np.delete(constit.c, (2, 5, 6, 7, 8), axis=1)\n        nd = g.dim\n\n        # Define subcell topology\n        subcell_topology = fvutils.SubcellTopology(g)\n        # Obtain mappings to exclude boundary faces for mechanics\n        bound_exclusion_mech = fvutils.ExcludeBoundaries(subcell_topology,\n                                                         bound_mech, nd)\n        # ... and flow\n        bound_exclusion_flow = fvutils.ExcludeBoundaries(subcell_topology,\n                                                         bound_flow, nd)\n\n        num_subhfno = subcell_topology.subhfno.size\n\n        num_nodes = np.diff(g.face_nodes.indptr)\n        sgn = g.cell_faces[subcell_topology.fno, subcell_topology.cno].A\n\n        # The pressure gradient term in the mechanics equation is discretized\n        # as a force on the faces. The right hand side is thus formed of the\n        # normal vectors.\n        def build_rhs_normals_single_dimension(dim):\n            val = g.face_normals[dim, subcell_topology.fno] \\\n                * sgn / num_nodes[subcell_topology.fno]\n            mat = sps.coo_matrix((val.squeeze(), (subcell_topology.subfno,\n                                                  subcell_topology.cno)),\n                                 shape=(subcell_topology.num_subfno,\n                                        subcell_topology.num_cno))\n            return mat\n\n        rhs_normals = build_rhs_normals_single_dimension(0)\n        for iter1 in range(1, nd):\n            this_dim = build_rhs_normals_single_dimension(iter1)\n            rhs_normals = sps.vstack([rhs_normals, this_dim])\n\n        rhs_normals = bound_exclusion_mech.exclude_dirichlet_nd(rhs_normals)\n\n        num_dir_subface = (bound_exclusion_mech.exclude_neu.shape[1] -\n                           bound_exclusion_mech.exclude_neu.shape[0]) * nd\n        # No right hand side for cell displacement equations.\n        rhs_normals_displ_var = sps.coo_matrix((nd * subcell_topology.num_subfno\n                                                - num_dir_subface,\n                                                subcell_topology.num_cno))\n\n        # Why minus?\n        rhs_normals = -sps.vstack([rhs_normals, rhs_normals_displ_var])\n        del rhs_normals_displ_var\n\n        # Call core part of MPSA\n        hook, igrad, rhs_cells, cell_node_blocks, hook_normal \\\n            = mpsa.mpsa_elasticity(g, constit, subcell_topology,\n                                   bound_exclusion_mech, eta, inverter)\n\n        # Output should be on face-level (not sub-face)\n        hf2f = fvutils.map_hf_2_f(subcell_topology.fno_unique,\n                                  subcell_topology.subfno_unique, nd)\n\n        # Stress discretization\n        stress = hf2f * hook * igrad * rhs_cells\n\n        # Right hand side for boundary discretization\n        rhs_bound = mpsa.create_bound_rhs(bound_mech, bound_exclusion_mech,\n                                          subcell_topology, g)\n        # Discretization of boundary values\n        bound_stress = hf2f * hook * igrad * rhs_bound\n\n        # Face-wise gradient operator. Used for the term grad_p in Biot's\n        # equations.\n        rows = fvutils.expand_indices_nd(subcell_topology.cno, nd)\n        cols = np.arange(num_subhfno * nd)\n        vals = np.tile(sgn, (nd, 1)).ravel('F')\n        div_gradp = sps.coo_matrix((vals, (rows, cols)),\n                                   shape=(subcell_topology.num_cno * nd,\n                                          num_subhfno * nd)).tocsr()\n\n#        del hook, rhs_bound\n        del rows, cols, vals\n\n        grad_p = div_gradp * hook_normal * igrad * rhs_normals\n        # assert np.allclose(grad_p.sum(axis=0), np.zeros(g.num_cells))\n\n        del hook_normal, div_gradp\n\n        div = self._subcell_gradient_to_cell_scalar(g, cell_node_blocks)\n\n        div_d = div * igrad * rhs_cells\n\n        # The boundary discretization of the div_d term is represented directly\n        # on the cells, instead of going via the faces.\n        bound_div_d = div * igrad * rhs_bound\n        del rhs_cells\n\n        stabilization = div * igrad * rhs_normals\n\n        data['stress'] = stress\n        data['bound_stress'] = bound_stress\n        data['grad_p'] = grad_p\n        data['div_d'] = div_d\n        data['stabilization'] = stabilization\n        data['bound_div_d'] = bound_div_d\n\n    def _face_vector_to_scalar(self, nf, nd):\n        \"\"\" Create a mapping from vector quantities on faces (stresses) to\n        scalar quantities. The mapping is intended for the boundary\n        discretization of the term div(u) (coupling term in the flow equation).\n\n        Parameters:\n            nf (int): Number of faces in the grid\n        \"\"\"\n        rows = np.tile(np.arange(nf), ((nd, 1))).reshape((1, nd * nf),\n                                                         order='F')[0]\n\n        cols = fvutils.expand_indices_nd(np.arange(nf), nd)\n        vals = np.ones(nf * nd)\n        return sps.coo_matrix((vals, (rows, cols))).tocsr()\n\n    def _subcell_gradient_to_cell_scalar(self, g, cell_node_blocks):\n        \"\"\" Create a mapping from sub-cell gradients to cell-wise traces of the\n        gradient operator. The mapping is intended for the discretization of\n        the term div(u) (coupling term in flow equation).\n        \"\"\"\n        # To pick out the trace of the strain tensor, we access elements\n        #   (2d): 0 (u_x) and 3 (u_y)\n        #   (3d): 0 (u_x), 4 (u_y), 8 (u_z)\n        nd = g.dim\n        if nd == 2:\n            trace = np.array([0, 3])\n        elif nd == 3:\n            trace = np.array([0, 4, 8])\n\n        # Sub-cell wise trace of strain tensor: One row per sub-cell\n        row, col = np.meshgrid(np.arange(cell_node_blocks.shape[1]), trace)\n        # Adjust the columns to hit each sub-cell\n        incr = np.cumsum(nd**2 * np.ones(cell_node_blocks.shape[1])) - nd**2\n        col += incr.astype('int32')\n\n        # Integrate the trace over the sub-cell, that is, distribute the cell\n        # volumes equally over the sub-cells\n        num_cell_nodes = g.num_cell_nodes()\n        cell_vol = g.cell_volumes / num_cell_nodes\n        val = np.tile(cell_vol[cell_node_blocks[0]], (nd, 1))\n        # and we have our mapping from vector to scalar values on sub-cells\n        vector_2_scalar = sps.coo_matrix((val.ravel('F'),\n                                          (row.ravel('F'),\n                                           col.ravel('F')))).tocsr()\n\n        # Mapping from sub-cells to cells\n        div_op = sps.coo_matrix((np.ones(cell_node_blocks.shape[1]),\n                                 (cell_node_blocks[0], np.arange(\n                                     cell_node_blocks.shape[1])))).tocsr()\n        # and the composed map\n        div = div_op * vector_2_scalar\n        return div\n\n#----------------------- Linear solvers -------------------------------------\n\n    def solve(self, A, solver='direct', **kwargs):\n\n        solver = solver.strip().lower()\n        if solver == 'direct':\n            def slv(b):\n                x = la.spsolve(A, b)\n                return x\n        elif solver == 'factorized':\n            slv = la.factorized(A.tocsc())\n\n        else:\n            raise ValueError('Unknown solver ' + solver)\n\n        return slv\n\n\n#----------------------- Methods for post processing -------------------------\n    def extractD(self, g, u, dims=None, as_vector=False):\n        \"\"\" Extract displacement field from solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n            dim (list of int, optional): Which dimension to extract. If None,\n                all dimensions are returned.\n        Returns:\n            list of np.ndarray: Displacement variables in the specified\n                dimensions.\n\n        \"\"\"\n        if dims is None:\n            dims = np.arange(g.dim)\n        vals = []\n\n        inds = np.arange(0, g.num_cells * g.dim, g.dim)\n\n        for d in dims:\n            vals.append(u[d + inds])\n        if as_vector:\n            vals = np.asarray(vals).reshape((-1, 1), order='F')\n            return vals\n        else:\n            return vals\n\n    def extractP(self, g, u):\n        \"\"\" Extract pressure field from solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n\n        Returns:\n            np.ndarray: Pressure part of solution vector.\n\n        \"\"\"\n        return u[g.dim * g.num_cells:]\n\n    def compute_flux(self, g, u, data):\n        \"\"\" Compute flux field corresponding to a solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n            bc_flow (np.ndarray): Flux boundary values.\n            data (dictionary): Dictionary related to grid and problem. Should\n                contain boundary discretization.\n\n        Returns:\n            np.ndarray: Flux over all faces\n\n        \"\"\"\n        flux_discr = data['flux']\n        bound_flux = data['bound_flux']\n        bound_val = data['bc_val_flow']\n        p = self.extractP(g, u)\n        flux = flux_discr * p + bound_flux * bound_val\n        return flux\n\n    def compute_stress(self, g, u, data):\n        \"\"\" Compute stress field corresponding to a solution.\n\n        Parameters:\n            g: grid, or a subclass.\n            u (np.ndarray): Solution variable, representing displacements and\n                pressure.\n            bc_flow (np.ndarray): Flux boundary values.\n            data (dictionary): Dictionary related to grid and problem. Should\n                contain boundary discretization.\n\n        Returns:\n            np.ndarray, g.dim * g.num_faces: Stress over all faces. Stored as\n                all stress values on the first face, then the second etc.\n\n        \"\"\"\n        param = data['param']\n        stress_discr = data['stress']\n        bound_stress = data['bound_stress']\n        bound_val = param.get_bc_val_mechanics()\n        d = self.extractD(g, u, as_vector=True)\n        stress = np.squeeze(stress_discr * d) + (bound_stress * bound_val)\n        return stress\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that imports necessary libraries and modules, including numpy, scipy, os, sys, and several modules from the porepy library. The code should define two functions, add_data_darcy and add_data_advection, which add data to a given grid bucket (gb) and domain with a specified tolerance (tol). The add_data_darcy function should add parameters related to Darcy's law, including permeability, source, aperture, and boundary conditions. The add_data_advection function should add parameters related to advection, including source, porosity, discharge, and boundary conditions. \n\nThe code should also append a path to the system path, import a module named soultz_grid, and set up parameters for creating a grid. It should then create a grid using the soultz_grid module, compute its geometry, coarsen it if a certain condition is met, and assign node ordering. \n\nThe code should then solve a Darcy problem using the DualVEMMixDim solver from the porepy library, add parameters to the grid bucket, compute a matrix and right-hand side vector, solve the system of equations, split the solution, extract discharge and pressure, project discharge, and compute the total flow rate. \n\nThe code should then set up parameters for a transport problem, define solvers for advection and mass matrix, add parameters to the grid bucket, compute matrices and right-hand side vectors, perform an LU factorization, initialize a solution vector, and perform a time-stepping loop to update the solution and export it at certain time steps. \n\nFinally, the code should export the solution in PVD format and save the production data to a text file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 138, "repo_full_name": "weihuayi__fealpy", "instruction": "Generate code that creates a 2D box domain for linear elasticity problems using the fealpy library. The code should define a class for the box domain with methods for initializing the mesh, defining displacement, strain, stress, source, Dirichlet and Neumann boundary conditions, and checking if a point is on the Dirichlet, Neumann, or fracture boundary. \n\nThe code should also define a class for counting iterations and a class for a fast solver for linear elasticity problems using Lagrange finite elements. The solver should include methods for preconditioning and solving the system of equations.\n\nThe main part of the code should initialize the box domain, create a mesh, define a Lagrange finite element space, and set up Dirichlet and Neumann boundary conditions. It should then create a function for the solution, compute the stiffness matrix and the linear elasticity matrix, and set up the source vector. The code should apply the boundary conditions to the system of equations and print the shape of the matrix.\n\nFinally, the code should solve the system of equations using the fast solver and print the time it took to solve the system. The code should also plot the original mesh.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class ExcludeBoundaries(object):\n    \"\"\" Wrapper class to store mapping for exclusion of equations that are\n    redundant due to the presence of boundary conditions.\n\n    The systems being set up in mpfa (and mpsa) describe continuity of flux and\n    potential (respectively stress and displacement) on all sub-faces. For\n    boundary faces, one of the two should be excluded (e.g. for a Dirichlet\n    boundary condition, there is no concept of continuity of flux/stresses).\n    The class contains mappings to eliminate the necessary fields.\n\n    \"\"\"\n\n    def __init__(self, subcell_topology, bound, nd):\n        \"\"\"\n        Define mappings to exclude boundary faces with dirichlet and neumann\n        conditions\n\n        Parameters\n        ----------\n        subcell_topology\n        bound\n\n        Returns\n        -------\n        exclude_neumann: Matrix, mapping from all faces to those having flux\n                         continuity\n        exclude_dirichlet: Matrix, mapping from all faces to those having pressure\n                           continuity\n        \"\"\"\n        self.nd = nd\n\n        # Short hand notation\n        fno = subcell_topology.fno_unique\n        num_subfno = subcell_topology.num_subfno_unique\n\n        # Define mappings to exclude boundary values\n        col_neu = np.argwhere([not it for it in bound.is_neu[fno]])\n        row_neu = np.arange(col_neu.size)\n        self.exclude_neu = sps.coo_matrix((np.ones(row_neu.size),\n                                           (row_neu, col_neu.ravel('C'))),\n                                          shape=(row_neu.size,\n                                                 num_subfno)).tocsr()\n        col_dir = np.argwhere([not it for it in bound.is_dir[fno]])\n        row_dir = np.arange(col_dir.size)\n        self.exclude_dir = sps.coo_matrix((np.ones(row_dir.size),\n                                           (row_dir, col_dir.ravel('C'))),\n                                          shape=(row_dir.size,\n                                                 num_subfno)).tocsr()\n\n    def exclude_dirichlet(self, other):\n        \"\"\" Mapping to exclude faces with Dirichlet boundary conditions from\n        local systems.\n\n        Parameters:\n            other (scipy.sparse matrix): Matrix of local equations for\n                continuity of flux and pressure.\n\n        Returns:\n            scipy.sparse matrix, with rows corresponding to faces with\n                Dirichlet conditions eliminated.\n\n        \"\"\"\n        return self.exclude_dir * other\n\n    def exclude_neumann(self, other):\n        \"\"\" Mapping to exclude faces with Neumann boundary conditions from\n        local systems.\n\n        Parameters:\n            other (scipy.sparse matrix): Matrix of local equations for\n                continuity of flux and pressure.\n\n        Returns:\n            scipy.sparse matrix, with rows corresponding to faces with\n                Neumann conditions eliminated.\n\n        \"\"\"\n        return self.exclude_neu * other\n\n    def exclude_dirichlet_nd(self, other):\n        \"\"\" Exclusion of Dirichlet conditions for vector equations (elasticity).\n        See above method without _nd suffix for description.\n\n        \"\"\"\n        exclude_dirichlet_nd = sps.kron(sps.eye(self.nd),\n                                        self.exclude_dir)\n        return exclude_dirichlet_nd * other\n\n    def exclude_neumann_nd(self, other):\n        \"\"\" Exclusion of Neumann conditions for vector equations (elasticity).\n        See above method without _nd suffix for description.\n\n        \"\"\"\n        exclude_neumann_nd = sps.kron(sps.eye(self.nd), self.exclude_neu)\n        return exclude_neumann_nd * other\n\n# --- Snippet Separator ---\n\ndef source(left, right, boundary=False):\n    \"\"\"A convinience function for creating (part of) an expression tree representing\n    a source term. This is necessary for spatial methods where the mass matrix\n    is not the identity (e.g. finite element formulation with piecwise linear\n    basis functions). The left child is the symbol representing the source term\n    and the right child is the symbol of the equation variable (currently, the\n    finite element formulation in PyBaMM assumes all functions are constructed\n    using the same basis, and the matrix here is constructed accoutning for the\n    boundary conditions of the right child). The method returns the matrix-vector\n    product of the mass matrix (adjusted to account for any Dirichlet boundary\n    conditions imposed the the right symbol) and the discretised left symbol.\n\n    Parameters\n    ----------\n\n    left : :class:`Symbol`\n        The left child node, which represents the expression for the source term.\n    right : :class:`Symbol`\n        The right child node. This is the symbol whose boundary conditions are\n        accounted for in the construction of the mass matrix.\n    boundary : bool, optional\n        If True, then the mass matrix should is assembled over the boundary,\n        corresponding to a source term which only acts on the boundary of the\n        domain. If False (default), the matrix is assembled over the entire domain,\n        corresponding to a source term in the bulk.\n\n    \"\"\"\n    # Broadcast if left is number\n    if isinstance(left, numbers.Number):\n        left = pybamm.Broadcast(left, \"current collector\")\n\n    if left.domain != [\"current collector\"] or right.domain != [\"current collector\"]:\n        raise pybamm.DomainError(\n            \"\"\"'source' only implemented in the 'current collector' domain,\n            but symbols have domains {} and {}\"\"\".format(\n                left.domain, right.domain\n            )\n        )\n    if boundary:\n        return pybamm.BoundaryMass(right) @ left\n    else:\n        return pybamm.Mass(right) @ left\n\n# --- Snippet Separator ---\n\ndef _create_bound_rhs(bnd, bound_exclusion,\n                      subcell_topology, sgn, g, num_flux, num_pr):\n    \"\"\"\n    Define rhs matrix to get basis functions for incorporates boundary\n    conditions\n\n    Parameters\n    ----------\n    bnd\n    exclude_dirichlet\n    exclude_neumann\n    fno\n    sgn : +-1, defining here and there of the faces\n    g : grid\n    num_flux : number of equations for flux continuity\n    num_pr: number of equations for pressure continuity\n\n    Returns\n    -------\n    rhs_bound: Matrix that can be multiplied with inverse block matrix to get\n               basis functions for boundary values\n    \"\"\"\n    # For primal-like discretizations like the MPFA, internal boundaries\n    # are handled by assigning Neumann conditions.\n    is_dir = np.logical_and(bnd.is_dir, np.logical_not(bnd.is_internal))\n    is_neu = np.logical_or(bnd.is_neu, bnd.is_internal)\n\n    fno = subcell_topology.fno_unique\n    num_neu = np.sum(is_neu[fno])\n    num_dir = np.sum(is_dir[fno])\n    num_bound = num_neu + num_dir\n\n    # Neumann boundary conditions\n    # Find Neumann faces, exclude Dirichlet faces (since these are excluded\n    # from the right hand side linear system), and do necessary formating.\n    neu_ind = np.argwhere(bound_exclusion.exclude_dirichlet(\n        is_neu[fno].astype('int64'))).ravel('F')\n    # We also need to map the respective Neumann and Dirichlet half-faces to\n    # the global half-face numbering (also interior faces). The latter should\n    # not have Dirichlet and Neumann excluded (respectively), and thus we need\n    # new fields\n    neu_ind_all = np.argwhere(is_neu[fno].astype('int')).ravel('F')\n    dir_ind_all = np.argwhere(is_dir[fno].astype('int')).ravel('F')\n    num_face_nodes = g.face_nodes.sum(axis=0).A.ravel(order='F')\n\n    # For the Neumann boundary conditions, we define the value as seen from\n    # the innside of the domain. E.g. outflow is defined to be positive. We\n    # therefore set the matrix indices to -1. We also have to scale it with\n    # the number of nodes per face because the flux of face is the sum of its\n    # half-faces.\n    scaled_sgn = - 1 / num_face_nodes[fno[neu_ind_all]]\n    if neu_ind.size > 0:\n        neu_cell = sps.coo_matrix((scaled_sgn,\n                                   (neu_ind, np.arange(neu_ind.size))),\n                                  shape=(num_flux, num_bound))\n    else:\n        # Special handling when no elements are found. Not sure if this is\n        # necessary, or if it is me being stupid\n        neu_cell = sps.coo_matrix((num_flux, num_bound))\n\n    # Dirichlet boundary conditions\n    dir_ind = np.argwhere(bound_exclusion.exclude_neumann(\n        is_dir[fno].astype('int64'))).ravel('F')\n    if dir_ind.size > 0:\n        dir_cell = sps.coo_matrix((sgn[dir_ind_all], (dir_ind, num_neu +\n                                                      np.arange(dir_ind.size))),\n                                  shape=(num_pr, num_bound))\n    else:\n        # Special handling when no elements are found. Not sure if this is\n        # necessary, or if it is me being stupid\n        dir_cell = sps.coo_matrix((num_pr, num_bound))\n\n    # Number of elements in neu_ind and neu_ind_all are equal, we can test with\n    # any of them. Same with dir.\n    if neu_ind.size > 0 and dir_ind.size > 0:\n        neu_dir_ind = np.hstack([neu_ind_all, dir_ind_all]).ravel('F')\n    elif neu_ind.size > 0:\n        neu_dir_ind = neu_ind_all\n    elif dir_ind.size > 0:\n        neu_dir_ind = dir_ind_all\n    else:\n        raise ValueError(\"Boundary values should be either Dirichlet or \"\n                         \"Neumann\")\n\n    num_subfno = subcell_topology.num_subfno_unique\n\n    # The columns in neu_cell, dir_cell are ordered from 0 to num_bound-1.\n    # Map these to all half-face indices\n    bnd_2_all_hf = sps.coo_matrix((np.ones(num_bound),\n                                   (np.arange(num_bound), neu_dir_ind)),\n                                  shape=(num_bound, num_subfno))\n    # The user of the discretization should now nothing about half faces,\n    # thus map from half face to face indices.\n    hf_2_f = sps.coo_matrix((np.ones(subcell_topology.subfno_unique.size),\n                             (subcell_topology.subfno_unique,\n                              subcell_topology.fno_unique)),\n                            shape=(num_subfno, g.num_faces))\n    rhs_bound = sps.vstack([neu_cell, dir_cell]) * bnd_2_all_hf * hf_2_f\n    return rhs_bound\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a 2D box domain for linear elasticity problems using the fealpy library. The code should define a class for the box domain with methods for initializing the mesh, defining displacement, strain, stress, source, Dirichlet and Neumann boundary conditions, and checking if a point is on the Dirichlet, Neumann, or fracture boundary. \n\nThe code should also define a class for counting iterations and a class for a fast solver for linear elasticity problems using Lagrange finite elements. The solver should include methods for preconditioning and solving the system of equations.\n\nThe main part of the code should initialize the box domain, create a mesh, define a Lagrange finite element space, and set up Dirichlet and Neumann boundary conditions. It should then create a function for the solution, compute the stiffness matrix and the linear elasticity matrix, and set up the source vector. The code should apply the boundary conditions to the system of equations and print the shape of the matrix.\n\nFinally, the code should solve the system of equations using the fast solver and print the time it took to solve the system. The code should also plot the original mesh.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 139, "repo_full_name": "ansys__pydpf-core", "instruction": "Generate code that demonstrates the use of the pydpf-core library for multi-stage cyclic symmetry analysis with advanced customization. The code should download a multi-stage cyclic result, create a model from it, and display the model's state. It should then verify that the model is a multi-stage model by checking the result info. The code should also go over the cyclic support, displaying the number of stages, the number of sectors in each stage, and the number of nodes in the first stage's base sector. \n\nNext, the code should expand displacement results on chosen sectors. It should create a displacement cyclic operator, select the sectors to expand on the first stage, and select the sectors to expand stage by stage. The code should then expand the displacements and get a total deformation. It should also get the expanded mesh. \n\nThe code should then plot the expanded result on the expanded mesh. It should also demonstrate how to expand only some sectors for the mesh, and plot the expanded result on the expanded mesh. \n\nFinally, the code should check results precisely. It should print the time frequency support to see the harmonic index, and verify that the displacement values are the same on all nodes.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class CyclicSupport:\n    \"\"\"Represents a cyclic support, which describes a model with cyclic symmetry.\n\n    The model has the necessary data for cyclic (and multistage) expansion.\n\n    Parameters\n    ----------\n    cyclic_support : ansys.grpc.dpf.cyclic_support_pb2.CyclicSupport message\n        Cyclic support.\n    server : DPFServer , optional\n        Server with the channel connected to the remote or local instance. The default is\n        ``None``, in which case an attempt is made to use the global server.\n\n    Examples\n    --------\n    Get a cyclic support from a model.\n\n    >>> from ansys.dpf import core as dpf\n    >>> from ansys.dpf.core import examples\n    >>> multi_stage = examples.download_multi_stage_cyclic_result()\n    >>> model = dpf.Model(multi_stage)\n    >>> result_info = model.metadata.result_info\n    >>> cyc_support = result_info.cyclic_support\n    >>> cyc_support.num_sectors()\n    6\n    >>> cyc_support.num_stages\n    2\n\n    \"\"\"\n\n    def __init__(self, cyclic_support, server=None):\n        \"\"\"Initialize time frequency support with its `TimeFreqSupport` message (if possible).\"\"\"\n        # step 1: get server\n        self._server = server_module.get_or_create_server(server)\n\n        # step 2: get api\n        self._api = self._server.get_api_for_type(\n            capi=cyclic_support_capi.CyclicSupportCAPI,\n            grpcapi=cyclic_support_grpcapi.CyclicSupportGRPCAPI)\n\n        # step3: init environment\n        self._api.init_cyclic_support_environment(self)  # creates stub when gRPC\n\n        # step4: take object instance\n        self._internal_obj = cyclic_support\n\n    def __str__(self):\n        \"\"\"Describe the entity.\n\n        Returns\n        -------\n        str\n            Description of the entity.\n        \"\"\"\n        from ansys.dpf.core.core import _description\n\n        return _description(self._internal_obj, self._server)\n\n    @property\n    def num_stages(self) -> int:\n        \"\"\"Number of cyclic stages in the model\n\n        Examples\n        --------\n        >>> from ansys.dpf.core import Model\n        >>> from ansys.dpf.core import examples\n        >>> multi_stage = examples.download_multi_stage_cyclic_result()\n        >>> cyc_support = Model(multi_stage).metadata.result_info.cyclic_support\n        >>> cyc_support.num_stages\n        2\n\n        Returns\n        -------\n        int\n            Number of cyclic stages in the model.\n        \"\"\"\n        return self._api.cyclic_support_get_num_stages(self)\n\n    def num_sectors(self, stage_num=0) -> int:\n        \"\"\"Number of sectors to expand on 360 degrees.\n\n        Parameters\n        ----------\n        stage_num : int , optional\n            Number of the stages required (from 0 to num_stages).\n\n        Returns\n        -------\n        int\n            Number of sectors to expand on 360 degrees.\n\n        Examples\n        --------\n        >>> from ansys.dpf.core import Model\n        >>> from ansys.dpf.core import examples\n        >>> multi_stage = examples.download_multi_stage_cyclic_result()\n        >>> cyc_support = Model(multi_stage).metadata.result_info.cyclic_support\n        >>> cyc_support.num_sectors(0)\n        6\n        >>> cyc_support.num_sectors(1)\n        12\n\n        \"\"\"\n        return self._api.cyclic_support_get_num_sectors(self, stage_num)\n\n    def base_nodes_scoping(self, stage_num=0) -> Scoping:\n        \"\"\"Retrieve a nodal scoping containing node IDs in the\n        base sector of the given stage.\n\n        Parameters\n        ----------\n        stage_num : int, optional\n            Number of the stage required (from 0 to num_stages).\n\n        Returns\n        -------\n        base_nodes_scoping : Scoping\n            Nodes IDs in the base sector of the given stage.\n\n        Examples\n        --------\n        >>> from ansys.dpf.core import Model\n        >>> from ansys.dpf.core import examples\n        >>> multi_stage = examples.download_multi_stage_cyclic_result()\n        >>> cyc_support = Model(multi_stage).metadata.result_info.cyclic_support\n        >>> base = cyc_support.base_nodes_scoping(0)\n\n        \"\"\"\n        base_node_scoping = self._api.cyclic_support_get_base_nodes_scoping(self, stage_num)\n        return Scoping(scoping=base_node_scoping, server=self._server)\n\n    def base_elements_scoping(self, stage_num=0) -> Scoping:\n        \"\"\"Retrieve an elemental scoping containing elements IDs in the\n        base sector of the given stage.\n\n        Parameters\n        ----------\n        stage_num : int, optional\n            Number of the stage required (from 0 to num_stages).\n\n        Returns\n        -------\n        base_elements_scoping : Scoping\n            Elements ids in the base sector of the given stage.\n\n        Examples\n        --------\n        >>> from ansys.dpf.core import Model\n        >>> from ansys.dpf.core import examples\n        >>> multi_stage = examples.download_multi_stage_cyclic_result()\n        >>> cyc_support = Model(multi_stage).metadata.result_info.cyclic_support\n        >>> base = cyc_support.base_elements_scoping(stage_num=1)\n\n        \"\"\"\n        base_element_scoping = self._api.cyclic_support_get_base_elements_scoping(self, stage_num)\n        return Scoping(scoping=base_element_scoping, server=self._server)\n\n    def sectors_set_for_expansion(self, stage_num=0) -> Scoping:\n        \"\"\"Retrieve a sector's scoping of the already expanded results\n        and mesh or the list of sectors that will be expanded by default.\n\n        A sector's scoping starts from 0, with the maximum equal to num_sectors-1.\n\n        Parameters\n        ----------\n        stage_num : int, optional\n            Number of the stage required (from 0 to num_stages).\n\n        Returns\n        -------\n        sectors_set_for_expansion : Scoping\n            List of sectors (starting from 0 to max = num_sectors-1).\n\n        Examples\n        --------\n        >>> from ansys.dpf.core import Model\n        >>> from ansys.dpf.core import examples\n        >>> multi_stage = examples.download_multi_stage_cyclic_result()\n        >>> cyc_support = Model(multi_stage).metadata.result_info.cyclic_support\n        >>> sectors_scoping = cyc_support.sectors_set_for_expansion(stage_num=1)\n        >>> print(sectors_scoping.ids)\n        [...0... 1... 2... 3... 4... 5... 6... 7... 8... 9... 10... 11]\n\n        \"\"\"\n        sectors_for_expansion = self._api.cyclic_support_get_sectors_scoping(self, stage_num)\n        return Scoping(scoping=sectors_for_expansion, server=self._server)\n\n    def expand_node_id(self, node_id, sectors=None, stage_num=0):\n        \"\"\"Retrieve the node IDs corresponding to the base sector node ID given in the input\n        after expansion.\n\n        Parameters\n        ----------\n        node_id : int\n            Base sector's node ID to expand.\n        sectors : Scoping , list of int, optional\n            List of sectors to expand (from 0 to ``num_sectors - 1``).\n            The default is ``None``, in which case all sectors are expanded.\n        stage_num : int, optional\n            Number of the stage required (from 0 to ``num_stages``).\n\n        Returns\n        -------\n        sectors_set_for_expansion : Scoping\n            List of sectors (starting from 0 to ``num_sectors - 1``).\n\n        Examples\n        --------\n        >>> from ansys.dpf.core import Model\n        >>> from ansys.dpf.core import examples\n        >>> multi_stage = examples.download_multi_stage_cyclic_result()\n        >>> cyc_support = Model(multi_stage).metadata.result_info.cyclic_support\n        >>> expanded_scoping = cyc_support.expand_node_id(1,stage_num=0)\n        >>> print(expanded_scoping.ids)\n        [...1... 3596... 5816... 8036... 10256... 12476]\n\n        \"\"\"\n        if sectors is None:\n            num_sectors = self._api.cyclic_support_get_num_sectors(self, stage_num)\n            sectors = list(range(num_sectors))\n        if isinstance(sectors, list):\n            sectors = Scoping(ids=sectors, location=\"sectors\", server=self._server)\n        expanded_ids = self._api.cyclic_support_get_expanded_node_ids(self, node_id,\n                                                                      stage_num, sectors)\n        return Scoping(scoping=expanded_ids, server=self._server)\n\n    def expand_element_id(self, element_id, sectors=None, stage_num=0):\n        \"\"\"Retrieves the element IDs corresponding to the base sector element ID given in the input\n        after expansion.\n\n        Parameters\n        ----------\n        element_id : int\n            Base sector's element ID to expand.\n        sectors : Scoping or list of int, optional\n            List of sectors to expand (from 0 to ``num_sectors - 1``).\n            The default is ``None``, in which case all sectors are expanded.\n        stage_num : int, optional\n            Number of the stage required (from 0 to ``num_stages``).\n\n        Returns\n        -------\n        sectors_set_for_expansion : Scoping\n            List of sectors (starting from 0 to ``num_sectors - 1``).\n\n        Examples\n        --------\n        >>> from ansys.dpf.core import Model\n        >>> from ansys.dpf.core import examples\n        >>> multi_stage = examples.download_multi_stage_cyclic_result()\n        >>> cyc_support = Model(multi_stage).metadata.result_info.cyclic_support\n        >>> expanded_scoping = cyc_support.expand_element_id(1,stage_num=0)\n        >>> print(expanded_scoping.ids)\n        [...1... 1558... 2533... 3508... 4483... 5458]\n\n        \"\"\"\n        if sectors is None:\n            num_sectors = self._api.cyclic_support_get_num_sectors(self, stage_num)\n            sectors = list(range(num_sectors))\n        if isinstance(sectors, list):\n            sectors = Scoping(ids=sectors, location=\"sectors\", server=self._server)\n        expanded_ids = self._api.cyclic_support_get_expanded_element_ids(self, element_id,\n                                                                         stage_num, sectors)\n        return Scoping(scoping=expanded_ids, server=self._server)\n\n    def __del__(self):\n        try:\n            self._deleter_func[0](self._deleter_func[1](self))\n        except:\n            warnings.warn(traceback.format_exc())\n\n# --- Snippet Separator ---\n\ndef sectors_set_for_expansion(self, stage_num=0) -> Scoping:\n        \"\"\"Retrieve a sector's scoping of the already expanded results\n        and mesh or the list of sectors that will be expanded by default.\n\n        A sector's scoping starts from 0, with the maximum equal to num_sectors-1.\n\n        Parameters\n        ----------\n        stage_num : int, optional\n            Number of the stage required (from 0 to num_stages).\n\n        Returns\n        -------\n        sectors_set_for_expansion : Scoping\n            List of sectors (starting from 0 to max = num_sectors-1).\n\n        Examples\n        --------\n        >>> from ansys.dpf.core import Model\n        >>> from ansys.dpf.core import examples\n        >>> multi_stage = examples.download_multi_stage_cyclic_result()\n        >>> cyc_support = Model(multi_stage).metadata.result_info.cyclic_support\n        >>> sectors_scoping = cyc_support.sectors_set_for_expansion(stage_num=1)\n        >>> print(sectors_scoping.ids)\n        [...0... 1... 2... 3... 4... 5... 6... 7... 8... 9... 10... 11]\n\n        \"\"\"\n        sectors_for_expansion = self._api.cyclic_support_get_sectors_scoping(self, stage_num)\n        return Scoping(scoping=sectors_for_expansion, server=self._server)\n\n# --- Snippet Separator ---\n\ndef expand_node_id(self, node_id, sectors=None, stage_num=0):\n        \"\"\"Retrieve the node IDs corresponding to the base sector node ID given in the input\n        after expansion.\n\n        Parameters\n        ----------\n        node_id : int\n            Base sector's node ID to expand.\n        sectors : Scoping , list of int, optional\n            List of sectors to expand (from 0 to ``num_sectors - 1``).\n            The default is ``None``, in which case all sectors are expanded.\n        stage_num : int, optional\n            Number of the stage required (from 0 to ``num_stages``).\n\n        Returns\n        -------\n        sectors_set_for_expansion : Scoping\n            List of sectors (starting from 0 to ``num_sectors - 1``).\n\n        Examples\n        --------\n        >>> from ansys.dpf.core import Model\n        >>> from ansys.dpf.core import examples\n        >>> multi_stage = examples.download_multi_stage_cyclic_result()\n        >>> cyc_support = Model(multi_stage).metadata.result_info.cyclic_support\n        >>> expanded_scoping = cyc_support.expand_node_id(1,stage_num=0)\n        >>> print(expanded_scoping.ids)\n        [...1... 3596... 5816... 8036... 10256... 12476]\n\n        \"\"\"\n        if sectors is None:\n            num_sectors = self._api.cyclic_support_get_num_sectors(self, stage_num)\n            sectors = list(range(num_sectors))\n        if isinstance(sectors, list):\n            sectors = Scoping(ids=sectors, location=\"sectors\", server=self._server)\n        expanded_ids = self._api.cyclic_support_get_expanded_node_ids(self, node_id,\n                                                                      stage_num, sectors)\n        return Scoping(scoping=expanded_ids, server=self._server)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that demonstrates the use of the pydpf-core library for multi-stage cyclic symmetry analysis with advanced customization. The code should download a multi-stage cyclic result, create a model from it, and display the model's state. It should then verify that the model is a multi-stage model by checking the result info. The code should also go over the cyclic support, displaying the number of stages, the number of sectors in each stage, and the number of nodes in the first stage's base sector. \n\nNext, the code should expand displacement results on chosen sectors. It should create a displacement cyclic operator, select the sectors to expand on the first stage, and select the sectors to expand stage by stage. The code should then expand the displacements and get a total deformation. It should also get the expanded mesh. \n\nThe code should then plot the expanded result on the expanded mesh. It should also demonstrate how to expand only some sectors for the mesh, and plot the expanded result on the expanded mesh. \n\nFinally, the code should check results precisely. It should print the time frequency support to see the harmonic index, and verify that the displacement values are the same on all nodes.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 140, "repo_full_name": "paddlepaddle__fastdeploy", "instruction": "Generate code that parses command line arguments for paths of detection, recognition, and table recognition models of PPOCR, recognition model label file, table recognition dictionary path, recognition model inference batch size, test image file path, inference device type, device ID, and inference backend type. Then, based on the parsed arguments, it should build runtime options for detection, recognition, and table recognition models. Depending on the device and backend type, it should set the appropriate backend and device for each model. If TensorRT backend is used, it should set the dynamic shape and save the TRT cache file to disk. After setting the runtime options, it should load the models and set the preprocessor and postprocessor parameters for the detection model. Then, it should create an instance of PPStructureV2Table with the loaded models and set the recognition batch size. Finally, it should read the input image, predict and print the results, visualize the results, and save the visualized image.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def parse_arguments():\n    import argparse\n    import ast\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--det_model\", required=True, help=\"Path of Detection model of PPOCR.\")\n    parser.add_argument(\n        \"--cls_model\",\n        required=True,\n        help=\"Path of Classification model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_model\",\n        required=True,\n        help=\"Path of Recognization model of PPOCR.\")\n    parser.add_argument(\n        \"--rec_label_file\",\n        required=True,\n        help=\"Path of Recognization model of PPOCR.\")\n    parser.add_argument(\n        \"--image_path\",\n        type=str,\n        required=True,\n        help=\"The directory or path or file list of the images to be predicted.\"\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        default='cpu',\n        help=\"Type of inference device, support 'cpu', 'kunlunxin' or 'gpu'.\")\n    parser.add_argument(\n        \"--backend\",\n        type=str,\n        default=\"default\",\n        help=\"Type of inference backend, support ort/trt/paddle/openvino, default 'openvino' for cpu, 'tensorrt' for gpu\"\n    )\n    parser.add_argument(\n        \"--device_id\",\n        type=int,\n        default=0,\n        help=\"Define which GPU card used to run model.\")\n    parser.add_argument(\n        \"--cpu_thread_num\",\n        type=int,\n        default=9,\n        help=\"Number of threads while inference on CPU.\")\n    parser.add_argument(\n        \"--cls_bs\",\n        type=int,\n        default=1,\n        help=\"Classification model inference batch size.\")\n    parser.add_argument(\n        \"--rec_bs\",\n        type=int,\n        default=6,\n        help=\"Recognition model inference batch size\")\n    parser.add_argument(\"--thread_num\", type=int, default=1, help=\"thread num\")\n    parser.add_argument(\n        \"--use_multi_process\",\n        type=ast.literal_eval,\n        default=False,\n        help=\"Wether to use multi process.\")\n    parser.add_argument(\n        \"--process_num\", type=int, default=1, help=\"process num\")\n    return parser.parse_args()\n\n# --- Snippet Separator ---\n\nclass Recognizer(FastDeployModel):\n    def __init__(self,\n                 model_file=\"\",\n                 params_file=\"\",\n                 label_path=\"\",\n                 runtime_option=None,\n                 model_format=ModelFormat.PADDLE):\n        \"\"\"Load OCR recognition model provided by PaddleOCR\n\n        :param model_file: (str)Path of model file, e.g ./ch_PP-OCRv3_rec_infer/model.pdmodel.\n        :param params_file: (str)Path of parameter file, e.g ./ch_PP-OCRv3_rec_infer/model.pdiparams, if the model format is ONNX, this parameter will be ignored.\n        :param label_path: (str)Path of label file used by OCR recognition model. e.g ./ppocr_keys_v1.txt\n        :param runtime_option: (fastdeploy.RuntimeOption)RuntimeOption for inference this model, if it's None, will use the default backend on CPU.\n        :param model_format: (fastdeploy.ModelForamt)Model format of the loaded model.\n        \"\"\"\n        super(Recognizer, self).__init__(runtime_option)\n\n        if (len(model_file) == 0):\n            self._model = C.vision.ocr.Recognizer()\n            self._runnable = False\n        else:\n            self._model = C.vision.ocr.Recognizer(\n                model_file, params_file, label_path, self._runtime_option,\n                model_format)\n            assert self.initialized, \"Recognizer initialize failed.\"\n            self._runnable = True\n\n    def clone(self):\n        \"\"\"Clone OCR recognition model object\n        :return: a new OCR recognition model object\n        \"\"\"\n\n        class RecognizerClone(Recognizer):\n            def __init__(self, model):\n                self._model = model\n\n        clone_model = RecognizerClone(self._model.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: rec_text, rec_score\n        \"\"\"\n        if self._runnable:\n            return self._model.predict(input_image)\n        return False\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: list of rec_text, list of rec_score\n        \"\"\"\n        if self._runnable:\n            return self._model.batch_predict(images)\n        return False\n\n    @property\n    def preprocessor(self):\n        return self._model.preprocessor\n\n    @preprocessor.setter\n    def preprocessor(self, value):\n        self._model.preprocessor = value\n\n    @property\n    def postprocessor(self):\n        return self._model.postprocessor\n\n    @postprocessor.setter\n    def postprocessor(self, value):\n        self._model.postprocessor = value\n\n    @property\n    def static_shape_infer(self):\n        return self._model.preprocessor.static_shape_infer\n\n    @static_shape_infer.setter\n    def static_shape_infer(self, value):\n        assert isinstance(\n            value,\n            bool), \"The value to set `static_shape_infer` must be type of bool.\"\n        self._model.preprocessor.static_shape_infer = value\n\n    @property\n    def rec_image_shape(self):\n        return self._model.preprocessor.rec_image_shape\n\n    @rec_image_shape.setter\n    def rec_image_shape(self, value):\n        assert isinstance(\n            value,\n            list), \"The value to set `rec_image_shape` must be type of list.\"\n        self._model.preprocessor.rec_image_shape = value\n\n# --- Snippet Separator ---\n\nclass PPStructureV2Table(FastDeployModel):\n    def __init__(self, det_model=None, rec_model=None, table_model=None):\n        \"\"\"Consruct a pipeline with text detector, text recognizer and table recognizer models\n\n        :param det_model: (FastDeployModel) The detection model object created by fastdeploy.vision.ocr.DBDetector.\n        :param rec_model: (FastDeployModel) The recognition model object created by fastdeploy.vision.ocr.Recognizer.\n        :param table_model: (FastDeployModel) The table recognition model object created by fastdeploy.vision.ocr.Table.\n        \"\"\"\n        assert det_model is not None and rec_model is not None and table_model is not None, \"The det_model, rec_model and table_model cannot be None.\"\n        self.system_ = C.vision.ocr.PPStructureV2Table(\n            det_model._model,\n            rec_model._model,\n            table_model._model, )\n\n    def clone(self):\n        \"\"\"Clone PPStructureV2Table pipeline object\n        :return: a new PPStructureV2Table pipeline object\n        \"\"\"\n\n        class PPStructureV2TableClone(PPStructureV2Table):\n            def __init__(self, system):\n                self.system_ = system\n\n        clone_model = PPStructureV2TableClone(self.system_.clone())\n        return clone_model\n\n    def predict(self, input_image):\n        \"\"\"Predict an input image\n\n        :param input_image: (numpy.ndarray)The input image data, 3-D array with layout HWC, BGR format\n        :return: OCRResult\n        \"\"\"\n        return self.system_.predict(input_image)\n\n    def batch_predict(self, images):\n        \"\"\"Predict a batch of input image\n        :param images: (list of numpy.ndarray) The input image list, each element is a 3-D array with layout HWC, BGR format\n        :return: OCRBatchResult\n        \"\"\"\n\n        return self.system_.batch_predict(images)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that parses command line arguments for paths of detection, recognition, and table recognition models of PPOCR, recognition model label file, table recognition dictionary path, recognition model inference batch size, test image file path, inference device type, device ID, and inference backend type. Then, based on the parsed arguments, it should build runtime options for detection, recognition, and table recognition models. Depending on the device and backend type, it should set the appropriate backend and device for each model. If TensorRT backend is used, it should set the dynamic shape and save the TRT cache file to disk. After setting the runtime options, it should load the models and set the preprocessor and postprocessor parameters for the detection model. Then, it should create an instance of PPStructureV2Table with the loaded models and set the recognition batch size. Finally, it should read the input image, predict and print the results, visualize the results, and save the visualized image.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 141, "repo_full_name": "pyscf__pyscf", "instruction": "Generate code that calculates the force from Quantum Mechanics (QM) region acting on the background Molecular Mechanics (MM) particles. The code should define a molecule using the pyscf library, generate random coordinates and charges for MM particles, and define a function to calculate the force. The force calculation should include the interaction between QM atoms and MM particles, and the interaction between electron density and MM particles. The code should then calculate the force from Hartree-Fock (HF) electron density and verify it. \n\nNext, the code should consider the response of HF orbitals in the analytical gradients for post-HF methods. As an example, it should use MP2 gradients to demonstrate how to include the orbital response effects in the force for MM particles. The code should define a function to make the reduced density matrix (rdm1) with orbital response, calculate the force from MP2 electron density (including orbital response), and verify it.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def add_mm_charges(scf_method, atoms_or_coords, charges, unit=None):\n    '''Embedding the one-electron (non-relativistic) potential generated by MM\n    point charges into QM Hamiltonian.\n\n    The total energy includes the regular QM energy, the interaction between\n    the nuclei in QM region and the MM charges, and the static Coulomb\n    interaction between the electron density and the MM charges. It does not\n    include the static Coulomb interactions of the MM point charges, the MM\n    energy, the vdw interaction or other bonding/non-bonding effects between\n    QM region and MM particles.\n\n    Args:\n        scf_method : a HF or DFT object\n\n        atoms_or_coords : 2D array, shape (N,3)\n            MM particle coordinates\n        charges : 1D array\n            MM particle charges\n    Kwargs:\n        unit : str\n            Bohr, AU, Ang (case insensitive). Default is the same to mol.unit\n\n    Returns:\n        Same method object as the input scf_method with modified 1e Hamiltonian\n\n    Note:\n        1. if MM charge and X2C correction are used together, function mm_charge\n        needs to be applied after X2C decoration (.x2c method), eg\n        mf = mm_charge(scf.RHF(mol).x2c()), [(0.5,0.6,0.8)], [-0.5]).\n        2. Once mm_charge function is applied on the SCF object, it\n        affects all the post-HF calculations eg MP2, CCSD, MCSCF etc\n\n    Examples:\n\n    >>> mol = gto.M(atom='H 0 0 0; F 0 0 1', basis='ccpvdz', verbose=0)\n    >>> mf = mm_charge(dft.RKS(mol), [(0.5,0.6,0.8)], [-0.3])\n    >>> mf.kernel()\n    -101.940495711284\n    '''\n    mol = scf_method.mol\n    if unit is None:\n        unit = mol.unit\n    mm_mol = mm_mole.create_mm_mol(atoms_or_coords, charges, unit)\n    return qmmm_for_scf(scf_method, mm_mol)\n\n# --- Snippet Separator ---\n\ndef add_mm_charges_grad(scf_grad, atoms_or_coords, charges, unit=None):\n    '''Apply the MM charges in the QM gradients' method.  It affects both the\n    electronic and nuclear parts of the QM fragment.\n\n    Args:\n        scf_grad : a HF or DFT gradient object (grad.HF or grad.RKS etc)\n            Once the add_mm_charges_grad was applied, it affects all post-HF\n            calculations eg MP2, CCSD, MCSCF etc\n        coords : 2D array, shape (N,3)\n            MM particle coordinates\n        charges : 1D array\n            MM particle charges\n    Kwargs:\n        unit : str\n            Bohr, AU, Ang (case insensitive). Default is the same to mol.unit\n\n    Returns:\n        Same gradeints method object as the input scf_grad method\n\n    Examples:\n\n    >>> from pyscf import gto, scf, grad\n    >>> mol = gto.M(atom='H 0 0 0; F 0 0 1', basis='ccpvdz', verbose=0)\n    >>> mf = mm_charge(scf.RHF(mol), [(0.5,0.6,0.8)], [-0.3])\n    >>> mf.kernel()\n    -101.940495711284\n    >>> hfg = mm_charge_grad(grad.hf.RHF(mf), coords, charges)\n    >>> hfg.kernel()\n    [[-0.25912357 -0.29235976 -0.38245077]\n     [-1.70497052 -1.89423883  1.2794798 ]]\n    '''\n    assert(isinstance(scf_grad, grad.rhf.Gradients))\n    mol = scf_grad.mol\n    if unit is None:\n        unit = mol.unit\n    mm_mol = mm_mole.create_mm_mol(atoms_or_coords, charges, unit)\n    mm_grad = qmmm_grad_for_scf(scf_grad)\n    mm_grad.base.mm_mol = mm_mol\n    return mm_grad\n\n# --- Snippet Separator ---\n\ndef qmmm_for_scf(scf_method, mm_mol):\n    '''Add the potential of MM particles to SCF (HF and DFT) method or CASCI\n    method then generate the corresponding QM/MM method for the QM system.\n\n    Args:\n        mm_mol : MM Mole object\n    '''\n    assert(isinstance(scf_method, (scf.hf.SCF, mcscf.casci.CASCI)))\n\n    if isinstance(scf_method, scf.hf.SCF):\n        # Avoid to initialize _QMMM twice\n        if isinstance(scf_method, _QMMM):\n            scf_method.mm_mol = mm_mol\n            return scf_method\n\n        method_class = scf_method.__class__\n\n    else:\n        if isinstance(scf_method._scf, _QMMM):\n            scf_method._scf.mm_mol = mm_mol\n            return scf_method\n\n        method_class = scf_method._scf.__class__\n\n    class QMMM(_QMMM, method_class):\n        def __init__(self, scf_method, mm_mol):\n            self.__dict__.update(scf_method.__dict__)\n            self.mm_mol = mm_mol\n            self._keys.update(['mm_mol'])\n\n        def dump_flags(self, verbose=None):\n            method_class.dump_flags(self, verbose)\n            logger.info(self, '** Add background charges for %s **',\n                        method_class)\n            if self.verbose >= logger.DEBUG:\n                logger.debug(self, 'Charge      Location')\n                coords = self.mm_mol.atom_coords()\n                charges = self.mm_mol.atom_charges()\n                for i, z in enumerate(charges):\n                    logger.debug(self, '%.9g    %s', z, coords[i])\n            return self\n\n        def get_hcore(self, mol=None):\n            if mol is None: mol = self.mol\n            if getattr(method_class, 'get_hcore', None):\n                h1e = method_class.get_hcore(self, mol)\n            else:  # DO NOT modify post-HF objects to avoid the MM charges applied twice\n                raise RuntimeError('mm_charge function cannot be applied on post-HF methods')\n\n            coords = self.mm_mol.atom_coords()\n            charges = self.mm_mol.atom_charges()\n            if pyscf.DEBUG:\n                v = 0\n                for i,q in enumerate(charges):\n                    mol.set_rinv_origin(coords[i])\n                    v += mol.intor('int1e_rinv') * -q\n            else:\n                if mol.cart:\n                    intor = 'int3c2e_cart'\n                else:\n                    intor = 'int3c2e_sph'\n                nao = mol.nao\n                max_memory = self.max_memory - lib.current_memory()[0]\n                blksize = int(min(max_memory*1e6/8/nao**2, 200))\n                if max_memory <= 0:\n                    blksize = 1\n                    logger.warn(self, 'Memory estimate for reading point charges is negative. '\n                                'Trying to read point charges one by one.')\n                cintopt = gto.moleintor.make_cintopt(mol._atm, mol._bas,\n                                                     mol._env, intor)\n                v = 0\n                for i0, i1 in lib.prange(0, charges.size, blksize):\n                    fakemol = gto.fakemol_for_charges(coords[i0:i1])\n                    j3c = df.incore.aux_e2(mol, fakemol, intor=intor,\n                                           aosym='s2ij', cintopt=cintopt)\n                    v += numpy.einsum('xk,k->x', j3c, -charges[i0:i1])\n                v = lib.unpack_tril(v)\n            return h1e + v\n\n        def energy_nuc(self):\n            # interactions between QM nuclei and MM particles\n            nuc = self.mol.energy_nuc()\n            coords = self.mm_mol.atom_coords()\n            charges = self.mm_mol.atom_charges()\n            for j in range(self.mol.natm):\n                q2, r2 = self.mol.atom_charge(j), self.mol.atom_coord(j)\n                r = lib.norm(r2-coords, axis=1)\n                nuc += q2*(charges/r).sum()\n            return nuc\n\n        def nuc_grad_method(self):\n            scf_grad = method_class.nuc_grad_method(self)\n            return qmmm_grad_for_scf(scf_grad)\n        Gradients = nuc_grad_method\n\n    if isinstance(scf_method, scf.hf.SCF):\n        return QMMM(scf_method, mm_mol)\n    else:  # post-HF methods\n        scf_method._scf = QMMM(scf_method._scf, mm_mol).run()\n        scf_method.mo_coeff = scf_method._scf.mo_coeff\n        scf_method.mo_energy = scf_method._scf.mo_energy\n        return scf_method\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that calculates the force from Quantum Mechanics (QM) region acting on the background Molecular Mechanics (MM) particles. The code should define a molecule using the pyscf library, generate random coordinates and charges for MM particles, and define a function to calculate the force. The force calculation should include the interaction between QM atoms and MM particles, and the interaction between electron density and MM particles. The code should then calculate the force from Hartree-Fock (HF) electron density and verify it. \n\nNext, the code should consider the response of HF orbitals in the analytical gradients for post-HF methods. As an example, it should use MP2 gradients to demonstrate how to include the orbital response effects in the force for MM particles. The code should define a function to make the reduced density matrix (rdm1) with orbital response, calculate the force from MP2 electron density (including orbital response), and verify it.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 142, "repo_full_name": "urwid__urwid", "instruction": "Generate code that creates a text editor using the urwid library in Python. The text editor should be able to lazily load text files, handle keypresses for saving and quitting the application, deleting and backspacing at the end and beginning of lines respectively, starting new lines, and navigating left and right. It should also be able to combine and split lines of text, and save the edited text back to the original file. The text editor should have a custom list walker for lazily loading the text file and a display that includes a list box for the text and a footer with instructions. The main function should take a filename as an argument and instantiate the text editor with that file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def strip_newsgroup_footer(text):\n    \"\"\"\n    Given text in \"news\" format, attempt to remove a signature block.\n\n    As a rough heuristic, we assume that signatures are set apart by either\n    a blank line or a line made of hyphens, and that it is the last such line\n    in the file (disregarding blank lines at the end).\n\n    Parameters\n    ----------\n    text : str\n        The text from which to remove the signature block.\n    \"\"\"\n    lines = text.strip().split(\"\\n\")\n    for line_num in range(len(lines) - 1, -1, -1):\n        line = lines[line_num]\n        if line.strip().strip(\"-\") == \"\":\n            break\n\n    if line_num > 0:\n        return \"\\n\".join(lines[:line_num])\n    else:\n        return text\n\n# --- Snippet Separator ---\n\ndef enhance_info_description(info, line_length=79):\n    ''' Using the info['description'], add fields to info.\n\n    info['files'] is the source filename and any filenames referenced by the\n    magic words in the description, e.g. 'the file xxx.py' or\n    'The image this.png'. These are as written in the description, do\n    not allow ../dir notation, and are relative to the source directory.\n\n    info['enhanced_description'] is the description, as an array of\n    paragraphs where each paragraph is an array of lines wrapped to width\n    line_length. This enhanced description include the rst links to\n    the files of info['files'].\n    '''\n\n    # make text a set of long lines, one per paragraph.\n    paragraphs = info['description'].split('\\n\\n')\n    lines = [\n        paragraph.replace('\\n', '$newline$')\n        for paragraph in paragraphs\n    ]\n    text = '\\n'.join(lines)\n\n    info['files'] = [info['file'] + '.' + info['ext']]\n    regex = r'[tT]he (?:file|image) ([\\w\\/]+\\.\\w+)'\n    for name in re.findall(regex, text):\n        if name not in info['files']:\n            info['files'].append(name)\n\n    # add links where the files are referenced\n    folder = '_'.join(info['source'].split(sep)[:-1]) + '_'\n    text = re.sub(r'([tT]he (?:file|image) )([\\w\\/]+\\.\\w+)',\n                  r'\\1:ref:`\\2 <$folder$\\2>`', text)\n    text = text.replace('$folder$', folder)\n\n    # now break up text into array of paragraphs, each an array of lines.\n    lines = [line.replace('$newline$', '\\n') for line in text.split('\\n')]\n    paragraphs = [\n        textwrap.wrap(line, line_length)\n        # ignore wrapping if .. note:: or similar block\n        if not line.startswith(' ') else [line]\n        for line in lines\n    ]\n    info['enhanced_description'] = paragraphs\n\n# --- Snippet Separator ---\n\ndef credits1(creditfile,width,stretch=30,color='white',\n                 stroke_color='black', stroke_width=2,\n                 font='Impact-Normal',fontsize=60):\n    \"\"\"\n\n\n    Parameters\n    -----------\n\n    creditfile\n      A text file whose content must be as follows: ::\n\n        # This is a comment\n        # The next line says : leave 4 blank lines\n        .blank 4\n\n        ..Executive Story Editor\n        MARCEL DURAND\n\n        ..Associate Producers\n        MARTIN MARCEL\n        DIDIER MARTIN\n\n        ..Music Supervisor\n        JEAN DIDIER\n\n    width\n      Total width of the credits text in pixels\n\n    gap\n      Gap in pixels between the jobs and the names.\n\n    **txt_kw\n      Additional argument passed to TextClip (font, colors, etc.)\n\n\n\n\n    Returns\n    ---------\n\n    image\n       An ImageClip instance that looks like this and can be scrolled\n       to make some credits :\n\n        Executive Story Editor    MARCEL DURAND\n           Associate Producers    MARTIN MARCEL\n                                  DIDIER MARTIN\n              Music Supervisor    JEAN DIDIER\n\n    \"\"\"\n\n\n    # PARSE THE TXT FILE\n\n    with open(creditfile) as f:\n        lines = f.readlines()\n\n    lines = filter(lambda x:not x.startswith('\\n'),lines) \n    texts = []\n    oneline=True\n    for l in  lines:\n        if not l.startswith('#'):\n            if l.startswith('.blank'):\n                for i in range(int(l.split(' ')[1])):\n                    texts.append(['\\n','\\n'])\n            elif  l.startswith('..'):\n                texts.append([l[2:],''])\n                oneline=True\n            else:\n                if oneline:\n                    texts.append(['',l])\n                    oneline=False\n                else:\n                    texts.append(['\\n',l])\n\n    left,right = [ \"\".join(l) for l in zip(*texts)]\n\n    # MAKE TWO COLUMNS FOR THE CREDITS\n\n    left,right =  [TextClip(txt,color=color,stroke_color=stroke_color,\n                                stroke_width=stroke_width,font=font,\n                                fontsize=fontsize,align=al)\n               for txt,al in [(left,'East'),(right,'West')]]\n\n\n    cc = CompositeVideoClip( [left, right.set_pos((left.w+gap,0))],\n                             size = (left.w+right.w+gap,right.h),\n                             transparent=True)\n\n    # SCALE TO THE REQUIRED SIZE\n\n    scaled = cc.fx(resize , width=width)\n\n    # TRANSFORM THE WHOLE CREDIT CLIP INTO AN ImageCLip\n\n    imclip = ImageClip(scaled.get_frame(0))\n    amask = ImageClip(scaled.mask.get_frame(0),ismask=True)\n\n    return imclip.set_mask(amask)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a text editor using the urwid library in Python. The text editor should be able to lazily load text files, handle keypresses for saving and quitting the application, deleting and backspacing at the end and beginning of lines respectively, starting new lines, and navigating left and right. It should also be able to combine and split lines of text, and save the edited text back to the original file. The text editor should have a custom list walker for lazily loading the text file and a display that includes a list box for the text and a footer with instructions. The main function should take a filename as an argument and instantiate the text editor with that file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 143, "repo_full_name": "rstudio__py-shiny", "instruction": "Generate code that creates a web application using the py-shiny library. The application should have a user interface with three columns. The first column should contain two inputs that control the other inputs on the page. The second column should contain a set of inputs that are controlled by the first two inputs. The third column should contain a set of inputs and a tabset. The server function should update the inputs in the second and third columns based on the values of the first two inputs. The application should be run in debug mode.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Setup(CodesSetup):\n    \"\"\"\n    Setup task for a plasmod solver.\n\n    On run, this task writes a plasmod input file using the input values\n    defined in this class.\n\n    Parameters\n    ----------\n    params:\n        The bluemira parameters for the task. Note that this task does\n        not apply any mappings to the ParameterFrame, so they should\n        already be set. Most likely by a solver.\n    problem_settings:\n        Any non-bluemira parameters that should be passed to plasmod.\n    plasmod_input_file:\n        The path where the plasmod input file should be written.\n    \"\"\"\n\n    params: PlasmodSolverParams\n\n    def __init__(\n        self,\n        params: PlasmodSolverParams,\n        problem_settings: Dict[str, Any],\n        plasmod_input_file: str,\n    ):\n        super().__init__(params, PLASMOD_NAME)\n\n        self.inputs = PlasmodInputs()\n        self.plasmod_input_file = plasmod_input_file\n        self.update_inputs(problem_settings)\n\n    def run(self):\n        \"\"\"\n        Run plasmod setup.\n        \"\"\"\n        self._write_input()\n\n    def mock(self):\n        \"\"\"\n        Run plasmod setup in mock mode.\n\n        No need to generate an input file as results will be mocked.\n        \"\"\"\n        pass\n\n    def read(self):\n        \"\"\"\n        Run plasmod setup in read mode.\n\n        No need to generate an input file as results will be read from\n        file.\n        \"\"\"\n        pass\n\n    def update_inputs(\n        self, new_inputs: Optional[Dict[str, Union[float, enum.Enum]]] = None\n    ):\n        \"\"\"\n        Update plasmod inputs using the given values.\n\n        This also pulls input values from the task's ParameterFrame and\n        uses them to update the inputs attributes. The inputs to this\n        method take precedence over inputs in the ParameterFrame.\n\n        Parameters\n        ----------\n        new_inputs:\n            The new inputs to update with.\n\n        Notes\n        -----\n        Updates this class's :code:`inputs` attribute.\n        \"\"\"\n        new_inputs = {} if new_inputs is None else new_inputs\n        new_inputs = self._remove_non_plasmod_inputs(new_inputs)\n        new = self._get_new_inputs()\n        new.update(new_inputs)\n        # Create a new PlasmodInputs object so we still benefit from\n        # the __post_init__ processing (converts models to enums)\n        self.inputs = PlasmodInputs(**new)\n\n    @staticmethod\n    def _remove_non_plasmod_inputs(_inputs: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"\n        Remove non-plasmod inputs from a dictionary. Warn that the\n        removed inputs will be ignored.\n\n        This copies the original dictionary, the input dictionary is not\n        modified.\n        \"\"\"\n        inputs = copy.deepcopy(_inputs)\n        fields = set(field.name for field in dataclasses.fields(PlasmodInputs))\n        # Convert to list to copy the keys, as we are changing the size\n        # of the dict during iteration.\n        for input_name in list(inputs.keys()):\n            if input_name not in fields:\n                bluemira_warn(f\"Ignoring unknown plasmod input '{input_name}'.\")\n                inputs.pop(input_name)\n        return inputs\n\n    def _write_input(self):\n        \"\"\"\n        Write inputs to file to be read by plasmod.\n        \"\"\"\n        try:\n            with open(self.plasmod_input_file, \"w\") as io_stream:\n                self.inputs.write(io_stream)\n        except OSError as os_error:\n            raise CodesError(\n                f\"Could not write plasmod input file: '{self.plasmod_input_file}': {os_error}\"\n            ) from os_error\n\n# --- Snippet Separator ---\n\ndef check_paired_arrays(X, Y):\n    \"\"\"Set X and Y appropriately and checks inputs for paired distances.\n\n    All paired distance metrics should use this function first to assert that\n    the given parameters are correct and safe to use.\n\n    Specifically, this function first ensures that both X and Y are arrays,\n    then checks that they are at least two dimensional while ensuring that\n    their elements are floats. Finally, the function checks that the size\n    of the dimensions of the two arrays are equal.\n\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n\n    Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n\n    Returns\n    -------\n    safe_X : {array-like, sparse matrix} of shape (n_samples_X, n_features)\n        An array equal to X, guaranteed to be a numpy array.\n\n    safe_Y : {array-like, sparse matrix} of shape (n_samples_Y, n_features)\n        An array equal to Y if Y was not None, guaranteed to be a numpy array.\n        If Y was None, safe_Y will be a pointer to X.\n    \"\"\"\n    X, Y = check_pairwise_arrays(X, Y)\n    if X.shape != Y.shape:\n        raise ValueError(\n            \"X and Y should be of same shape. They were respectively %r and %r long.\"\n            % (X.shape, Y.shape)\n        )\n    return X, Y\n\n# --- Snippet Separator ---\n\nclass GenericJaxModel:\n    \"\"\"An interface for models to follow in order to train or predict. A model\n    implementing this interface will need to contain not only the forward\n    model but also loss and update fn. Some examples can be found in\n    https://github.com/funkelab/funlib.learn.jax\n\n    Args:\n\n        is_training (``bool``):\n\n            Indicating whether the model will be used for training\n            or inferencing.\n    \"\"\"\n\n    def __init__(self, is_training):\n        pass\n\n    def initialize(self, rng_key, inputs):\n        \"\"\"Initialize parameters for training.\n\n        Args:\n\n            rng_key (jax.random.PRNGKey):\n\n                Seed for parameter initialization\n\n            inputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of inputs, provided to initialize parameters\n                with the correct dimensions.\n\n        Return:\n\n            params (Any):\n\n                Function should return an object encapsulating different\n                parameters of the model.\n        \"\"\"\n        raise RuntimeError(\"Unimplemented\")\n\n    def forward(self, params, inputs):\n        \"\"\"Run the forward model.\n\n        Args:\n\n            params (Any):\n\n                Model parameters.\n\n            inputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of inputs.\n\n        Return:\n\n            outputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of outputs.\n        \"\"\"\n        raise RuntimeError(\"Unimplemented\")\n\n    def train_step(self, params, inputs, pmapped):\n        \"\"\"Run one iteration of training on the model.\n\n        Args:\n\n            params (Any):\n\n                Model parameters.\n\n            inputs (``dict``, ``string`` -> jnp.ndarray):\n\n                Dictionary of inputs.\n\n            pmapped (``bool``):\n\n                Whether the function is run with `jax.pmap` or not.\n                If pmapped across devices, the function should take care to\n                synchronize gradients during the train step.\n                The `axis_name` is set to the ``string`` \"num_devices\"\n\n        Return:\n\n            Tuple(new_params, outputs, loss)\n\n                new_params (Any):\n\n                    Updated model parameters.\n\n                outputs (``dict``, ``string`` -> jnp.ndarray):\n\n                    Dictionary of outputs.\n\n                loss (Union[``float``, (``dict``, ``string`` -> ``float``)]):\n\n                    Loss value of this iteration. Value can either be a single\n                    ``float`` or a dictionary of multiple losses.\n        \"\"\"\n        raise RuntimeError(\"Unimplemented\")\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that creates a web application using the py-shiny library. The application should have a user interface with three columns. The first column should contain two inputs that control the other inputs on the page. The second column should contain a set of inputs that are controlled by the first two inputs. The third column should contain a set of inputs and a tabset. The server function should update the inputs in the second and third columns based on the values of the first two inputs. The application should be run in debug mode.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 144, "repo_full_name": "silnrsi__pysilfont", "instruction": "Generate code that uses the pysilfont library to create an FTML (Font Test Markup Language) document from a UFO (Unified Font Object) and a glyph data CSV file. The script should accept various command line arguments to customize the output, such as input UFO, output file, glyph info CSV file, font code, log file name, list of BCP47 language tags, right-to-left feature enabling, rendering check disabling, test name, font source, text scaling, anchor points regular expression, total width of all string column, and XSL stylesheet. The script should read the input CSV, initialize the FTML document, and add encoded characters, unencoded specials and ligatures, Lam-Alef data, and diacritic attachment data to the document based on the provided arguments. Finally, the script should write the output FTML file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def doit(args):\n    font = args.ifont\n    incsv = args.input\n    logger = args.logger\n    indent = ' '*args.indent\n\n    if not (args.quiet or 'scrlevel' in args.paramsobj.sets['command line']):\n        logger.raisescrlevel('W')  # Raise level to W if not already W or higher\n\n    def csvWarning(msg, exception=None):\n        m = f'glyph_data line {incsv.line_num}: {msg}'\n        if exception is not None:\n            m += '; ' + exception.message\n        logger.log(m, 'W')\n\n    # Get glyph names and encoding from input file\n    glyphFromCSVuid = {}\n    uidsFromCSVglyph = {}\n\n    # Identify file format (plain text or csv) from first line\n    # If csv file, it must have headers for \"glyph_name\" and \"USV\"\n    fl = incsv.firstline\n    if fl is None: logger.log('Empty input file', 'S')\n    numfields = len(fl)\n    incsv.numfields = numfields\n    usvCol = None  # Use this as a flag later to determine whether to check USV inventory\n    if numfields > 1:  # More than 1 column, so must have headers\n        # Required columns:\n        try:\n            nameCol = fl.index('glyph_name');\n        except ValueError as e:\n            logger.log('Missing csv input field: ' + e.message, 'S')\n        except Exception as e:\n            logger.log('Error reading csv input field: ' + e.message, 'S')\n        # Optional columns:\n        usvCol = fl.index('USV') if 'USV' in fl else None\n\n        next(incsv.reader, None)  # Skip first line with headers in\n\n        glyphList = set()\n        for line in incsv:\n            gname = line[nameCol]\n            if len(gname) == 0 or line[0].strip().startswith('#'):\n                continue    # No need to include cases where name is blank or comment\n            if gname in glyphList:\n                csvWarning(f'glyph name {gname} previously seen; ignored')\n                continue\n            glyphList.add(gname)\n\n            if usvCol:\n                # Process USV field, which can be:\n                #   empty string -- unencoded glyph\n                #   single USV -- encoded glyph\n                #   USVs connected by '_' -- ligature (in glyph_data for test generation, not glyph encoding)\n                #   space-separated list of the above, where presence of multiple USVs indicates multiply-encoded glyph\n                for usv in line[usvCol].split():\n                    if '_' in usv:\n                        # ignore ligatures -- these are for test generation, not encoding\n                        continue\n                    try:\n                        uid = int(usv, 16)\n                    except Exception as e:\n                        csvWarning(\"invalid USV '%s' (%s); ignored: \" % (usv, e.message))\n\n                    if uid in glyphFromCSVuid:\n                        csvWarning('USV %04X previously seen; ignored' % uid)\n                    else:\n                        # Remember this glyph encoding\n                        glyphFromCSVuid[uid] = gname\n                        uidsFromCSVglyph.setdefault(gname, set()).add(uid)\n    elif numfields == 1:   # Simple text file.\n        glyphList = set(line[0] for line in incsv)\n    else:\n        logger.log('Invalid csv file', 'S')\n\n    # Get the list of glyphs in the UFO\n    ufoList = set(font.deflayer.keys())\n\n    notInUFO = glyphList - ufoList\n    notInGlyphData = ufoList - glyphList\n\n    if len(notInUFO):\n        logger.log('Glyphs present in glyph_data but missing from UFO:\\n' + '\\n'.join(indent + g for g in sorted(notInUFO)), 'W')\n\n    if len(notInGlyphData):\n        logger.log('Glyphs present in UFO but missing from glyph_data:\\n' + '\\n'.join(indent + g for g in sorted(notInGlyphData)), 'W')\n\n    if len(notInUFO) == 0 and len(notInGlyphData) == 0:\n        logger.log('No glyph inventory differences found', 'P')\n\n    if usvCol:\n        # We can check encoding of glyphs in common\n        inBoth = glyphList & ufoList   # Glyphs we want to examine\n\n        csvEncodings = set(f'{gname}|{uid:04X}' for gname in filter(lambda x: x in uidsFromCSVglyph, inBoth) for uid in uidsFromCSVglyph[gname] )\n        ufoEncodings = set(f'{gname}|{int(u.hex, 16):04X}' for gname in inBoth for u in font.deflayer[gname]['unicode'])\n\n        notInUFO = csvEncodings - ufoEncodings\n        notInGlyphData = ufoEncodings - csvEncodings\n\n        if len(notInUFO):\n            logger.log('Encodings present in glyph_data but missing from UFO:\\n' + '\\n'.join(indent + g for g in sorted(notInUFO)), 'W')\n\n        if len(notInGlyphData):\n            logger.log('Encodings present in UFO but missing from glyph_data:\\n' + '\\n'.join(indent + g for g in sorted(notInGlyphData)), 'W')\n\n        if len(notInUFO) == 0 and len(notInGlyphData) == 0:\n            logger.log('No glyph encoding differences found', 'P')\n\n    else:\n        logger.log('Glyph encodings not compared', 'P')\n\n# --- Snippet Separator ---\n\ndef doit(args) :\n    font = args.ifont\n    logger = args.logger\n\n    # Process duplicates csv file into a dictionary structure\n    args.input.numfields = 2\n    duplicates = {}\n    for line in args.input :\n        duplicates[line[0]] = line[1]\n\n    # Iterate through dictionary (unsorted)\n    for source, target in duplicates.items() :\n        # Check if source glyph is in font\n        if source in font.keys() :\n            # Give warning if target is already in font, but overwrite anyway\n            if target in font.keys() :\n                logger.log(\"Warning: \" + target + \" already in font and will be replaced\")\n            sourceglyph = font[source]\n            # Make a copy of source into a new glyph object\n            newglyph = sourceglyph.copy()\n            # Modify that glyph object\n            newglyph.unicodes = []\n            # Add the new glyph object to the font with name target\n            font.__setitem__(target,newglyph)\n            logger.log(source + \" duplicated to \" + target)\n        else :\n            logger.log(\"Warning: \" + source + \" not in font\")\n\n    return font\n\n# --- Snippet Separator ---\n\ndef doit(args):\n    font = args.ifont\n    logger = args.logger\n    incsv = args.input\n    gname = args.gname\n    removemissing = args.removemissing\n\n    glyphlist = list(font.deflayer.keys())  # List to check every glyph has a psname supplied\n\n    # Identify file format from first line\n    fl = incsv.firstline\n    if fl is None: logger.log(\"Empty input file\", \"S\")\n    numfields = len(fl)\n    incsv.numfields = numfields\n    if numfields == 2:\n        glyphnpos = 0\n        psnamepos = 1    # Default for plain csv\n    elif numfields > 2:  # More than 2 columns, so must have standard headers\n        if gname in fl:\n            glyphnpos = fl.index(gname)\n        else:\n            logger.log(\"No \" + gname + \" field in csv headers\", \"S\")\n        if \"ps_name\" in fl:\n            psnamepos = fl.index(\"ps_name\")\n        else:\n            logger.log(\"No ps_name field in csv headers\", \"S\")\n        next(incsv.reader, None)  # Skip first line with headers in\n    else:\n        logger.log(\"Invalid csv file\", \"S\")\n\n    # Now process the data\n    dict = ET.Element(\"dict\")\n    for line in incsv:\n        glyphn = line[glyphnpos]\n        psname = line[psnamepos]\n        if len(psname) == 0 or glyphn == psname:\n            continue\t# No need to include cases where production name is blank or same as working name\n        # Check if in font\n        infont = False\n        if glyphn in glyphlist:\n            glyphlist.remove(glyphn)\n            infont = True\n        else:\n            if not removemissing: logger.log(\"No glyph in font for \" + glyphn + \" on line \" + str(incsv.line_num), \"I\")\n        if not removemissing or infont:\n            # Add to dict\n            sub = ET.SubElement(dict, \"key\")\n            sub.text = glyphn\n            sub = ET.SubElement(dict, \"string\")\n            sub.text = psname\n    # Add to lib.plist\n    if len(dict) > 0:\n        if \"lib\" not in font.__dict__: font.addfile(\"lib\")\n        font.lib.setelem(\"public.postscriptNames\", dict)\n    else:\n        if \"lib\" in font.__dict__ and \"public.postscriptNames\" in font.lib:\n            font.lib.remove(\"public.postscriptNames\")\n\n    for glyphn in sorted(glyphlist): logger.log(\"No PS name in input file for font glyph \" + glyphn, \"I\")\n\n    return font\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the pysilfont library to create an FTML (Font Test Markup Language) document from a UFO (Unified Font Object) and a glyph data CSV file. The script should accept various command line arguments to customize the output, such as input UFO, output file, glyph info CSV file, font code, log file name, list of BCP47 language tags, right-to-left feature enabling, rendering check disabling, test name, font source, text scaling, anchor points regular expression, total width of all string column, and XSL stylesheet. The script should read the input CSV, initialize the FTML document, and add encoded characters, unencoded specials and ligatures, Lam-Alef data, and diacritic attachment data to the document based on the provided arguments. Finally, the script should write the output FTML file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 145, "repo_full_name": "chalmersplasmatheory__dream", "instruction": "Generate code that performs a combined fluid-kinetic simulation using the DREAM library. The simulation should include both the hot-tail and runaway electron grids. Set the electric field strength, electron density, and temperature to specific values. Define the momentum grid and set up initial hot electron Maxwellian. Include Dreicer and avalanche in the equation system. Set up the radial grid and disable the runaway grid. Set the Svensson transport coefficients and use the nonlinear solver. Finally, set the time stepper and save the settings to an HDF5 file.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def __init__(self, name, enabled=True, ttype=TYPE_PXI, np=0, nxi=0, pmax=None):\n        \"\"\"\n        Constructor.\n\n        name:    Name, indicating what type of grid this is (hot-tail or runaway).\n        enabled: If 'True', enables the hot-tail grid in the simulation.\n        ttype:   Type of momentum grid (p/xi or ppar/pperp).\n        np:      Number of momentum grid points.\n        nxi:     Number of pitch grid points.\n        pmax:    Maximum momentum on grid.\n\n        \"\"\"\n        self.name = name\n\n        self.set(enabled=enabled, ttype=ttype, np=np, nxi=nxi, pmax=pmax)\n\n# --- Snippet Separator ---\n\nclass RunawayElectrons(UnknownQuantity,PrescribedInitialParameter):\n\n    def __init__(self, settings, density=0, radius=0, avalanche=AVALANCHE_MODE_NEGLECT, dreicer=DREICER_RATE_DISABLED, compton=COMPTON_MODE_NEGLECT, Eceff=COLLQTY_ECEFF_MODE_FULL, pCutAvalanche=0, comptonPhotonFlux=0, tritium=False, hottail=HOTTAIL_MODE_DISABLED):\n        \"\"\"\n        Constructor.\n        \"\"\"\n        super().__init__(settings=settings)\n\n        self.avalanche = avalanche\n        self.dreicer   = dreicer\n        self.compton   = compton\n        self.comptonPhotonFlux = comptonPhotonFlux\n        self.Eceff     = Eceff\n        self.pCutAvalanche = pCutAvalanche\n        self.tritium   = tritium\n        self.hottail   = hottail\n        self.negative_re = False\n\n        self.advectionInterpolation = AdvectionInterpolation.AdvectionInterpolation(kinetic=False)\n        self.transport = TransportSettings(kinetic=False)\n\n        self.density = None\n        self.radius  = None\n        self.setInitialProfile(density=density, radius=radius)\n\n\n    def setInitialProfile(self, density, radius=0):\n        _data, _rad = self._setInitialData(data=density, radius=radius)\n\n        self.density = _data\n        self.radius  = _rad\n        self.verifySettingsPrescribedInitialData()\n\n\n    def setAvalanche(self, avalanche, pCutAvalanche=0):\n        \"\"\"\n        Enables/disables avalanche generation.\n        \"\"\"\n        if avalanche == False:\n            self.avalanche = AVALANCHE_MODE_NEGLECT\n        else:\n            self.avalanche = int(avalanche)\n            self.pCutAvalanche = pCutAvalanche\n\n\n    def setDreicer(self, dreicer):\n        \"\"\"\n        Specifies which model to use for calculating the\n        Dreicer runaway rate.\n        \"\"\"\n        if dreicer == False:\n            self.dreicer = DREICER_RATE_DISABLED\n        else:\n            self.dreicer = int(dreicer)\n\n\n    def setCompton(self, compton, photonFlux = None):\n        \"\"\"\n        Specifies which model to use for calculating the\n        compton runaway rate.\n        \"\"\"\n        if compton == False or compton == COMPTON_MODE_NEGLECT:\n            self.compton = COMPTON_MODE_NEGLECT\n        else:\n            if compton == COMPTON_RATE_ITER_DMS:\n                # set fluid compton source and standard ITER flux of 1e18\n                compton = COMPTON_MODE_FLUID\n                if photonFlux is None:\n                    photonFlux = ITER_PHOTON_FLUX_DENSITY\n\n            if photonFlux is None:\n                raise EquationException(\"n_re: Compton photon flux must be set.\")\n\n            self.compton = int(compton)\n            self.comptonPhotonFlux = photonFlux\n\n\n    def setEceff(self, Eceff):\n        \"\"\"\n        Specifies which model to use for calculating the\n        effective critical field (used in the avalanche formula).\n        \"\"\"\n        self.Eceff = int(Eceff)\n\n\n    def setTritium(self, tritium):\n        \"\"\"\n        Specifices whether or not to include runaway generation\n        through tritium decay as a source term.\n        \"\"\"\n        self.tritium = tritium\n\n\n    def setHottail(self, hottail):\n        \"\"\"\n        Specify which model to use for hottail runaway generation\n        \"\"\"\n        if hottail == False:\n            self.hottail = HOTTAIL_MODE_DISABLED\n        else:\n            self.hottail = hottail\n            if hottail != HOTTAIL_MODE_DISABLED:\n                self.settings.eqsys.f_hot.enableAnalyticalDistribution()\n\n\n    def setNegativeRunaways(self, negative_re=True):\n        \"\"\"\n        Introduce a density of runaway electrons with negative pitch,\n        allowing the kinetic avalanche source term to properly account for\n        large-angle collisions with runaways moving in different directions.\n        \"\"\"\n        self.negative_re = negative_re\n\n\n    def setAdvectionInterpolationMethod(self, ad_int=AD_INTERP_CENTRED,\n        ad_jac=AD_INTERP_JACOBIAN_FULL, fluxlimiterdamping=1.0):\n        \"\"\"\n        Sets the interpolation method that is used in the advection terms of\n        the transport equation.\n\n        :param int ad_int:               Interpolation method to use for the radial coordinate.\n        :param int ad_jac:               Jacobian interpolation mode to use for the radial coordinate.\n        :param float fluxlimiterdamping: Damping parameter used to under-relax the interpolation coefficients during non-linear iterations (should be between 0 and 1).\n        \"\"\"\n        self.advectionInterpolation.setMethod(ad_int=ad_int, ad_jac=ad_jac, fluxlimiterdamping=fluxlimiterdamping)\n\n\n    def fromdict(self, data):\n        \"\"\"\n        Set all options from a dictionary.\n        \"\"\"\n        self.avalanche = int(data['avalanche'])\n\n        if 'pCutAvalanche' in data:\n            self.pCutAvalanche = data['pCutAvalanche']\n\n        self.dreicer   = int(data['dreicer'])\n        self.Eceff     = int(data['Eceff'])\n        self.compton            = int(data['compton']['mode'])\n        self.comptonPhotonFlux  = data['compton']['flux']\n        self.density   = data['init']['x']\n        self.radius    = data['init']['r']\n\n        if 'adv_interp' in data:\n            self.advectionInterpolation.fromdict(data['adv_interp'])\n\n        if 'hottail' in data:\n            self.hottail = int(data['hottail'])\n\n        if 'tritium' in data:\n            self.tritium = bool(data['tritium'])\n\n        if 'negative_re' in data:\n            self.negative_re = bool(data['negative_re'])\n\n        if 'transport' in data:\n            self.transport.fromdict(data['transport'])\n\n\n    def todict(self):\n        \"\"\"\n        Returns a Python dictionary containing all settings of\n        this RunawayElectrons object.\n        \"\"\"\n        data = {\n            'avalanche': self.avalanche,\n            'dreicer': self.dreicer,\n            'Eceff': self.Eceff,\n            'pCutAvalanche': self.pCutAvalanche,\n            'transport': self.transport.todict(),\n            'tritium': self.tritium,\n            'hottail': self.hottail,\n            'negative_re': self.negative_re\n        }\n        data['compton'] = {\n            'mode': self.compton,\n            'flux': self.comptonPhotonFlux\n        }\n        data['init'] = {\n            'x': self.density,\n            'r': self.radius\n        }\n\n        # Flux limiter settings\n        data['adv_interp'] = self.advectionInterpolation.todict()\n\n        return data\n\n\n    def verifySettings(self):\n        \"\"\"\n        Verify that the settings of this unknown are correctly set.\n        \"\"\"\n        if type(self.avalanche) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'avalanche'. Expected integer.\")\n        if type(self.dreicer) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'dreicer'. Expected integer.\")\n        if type(self.compton) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'compton'. Expected integer.\")\n        if type(self.hottail) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'hottail'. Expected integer.\")\n        if type(self.Eceff) != int:\n            raise EquationException(\"n_re: Invalid value assigned to 'Eceff'. Expected integer.\")\n        if self.avalanche == AVALANCHE_MODE_KINETIC and self.pCutAvalanche == 0:\n            raise EquationException(\"n_re: Invalid value assigned to 'pCutAvalanche'. Must be set explicitly when using KINETIC avalanche.\")\n        if type(self.tritium) != bool:\n            raise EquationException(\"n_re: Invalid value assigned to 'tritium'. Expected bool.\")\n        if self.hottail != HOTTAIL_MODE_DISABLED and self.settings.eqsys.f_hot.mode == DISTRIBUTION_MODE_NUMERICAL:\n            raise EquationException(\"n_re: Invalid setting combination: when hottail is enabled, the 'mode' of f_hot cannot be NUMERICAL. Enable ANALYTICAL f_hot distribution or disable hottail.\")\n        if type(self.negative_re) != bool:\n            raise EquationException(\"n_re: Invalid value assigned to 'negative_re'. Expected bool.\")\n\n        self.advectionInterpolation.verifySettings()\n        self.transport.verifySettings()\n\n\n    def verifySettingsPrescribedInitialData(self):\n        self._verifySettingsPrescribedInitialData('n_re', data=self.density, radius=self.radius)\n\n# --- Snippet Separator ---\n\nclass ElectricField(FluidQuantity):\n\n\n    def __init__(self, name, data, grid, output, attr=list()):\n        \"\"\"\n        Constructor.\n        \"\"\"\n        super().__init__(name=name, data=data, attr=attr, grid=grid, output=output)\n\n\n    def getNormEfield(self, field, r=None, t=None):\n        \"\"\"\n        Returns an electric field from the other quantities by name.\n        This routine is intended as a uniform interface for fetching\n        quantities such as Ec, Eceff, ED etc.\n        \"\"\"\n        # List of supported quantities (to avoid user error)\n        nrm = ['Eceff', 'Ecfree', 'Ectot', 'Ec', 'ED', 'EDreic']\n        if field == 'Ec': field = 'Ectot'\n        elif field == 'ED': field = 'EDreic'\n\n        if 'fluid' not in self.output.other:\n            raise OutputException('No \"other\" fluid quantities saved in output. Normalizing electric fields are thus not available.')\n        if field not in nrm:\n            raise OutputException(\"Cannot normalize to '{}': This seems to not make sense.\".format(field))\n        if field not in self.output.other.fluid:\n            raise OutputException(\"Cannot normalize to '{}': quantity not saved to output after simulation.\".format(field))\n\n        return self.output.other.fluid[field].get(r=r, t=t)\n\n\n    def maxEnergy(self, t=-1):\n        r\"\"\"\n        Evaluates the maximum attainable runaway kinetic energy (in normalized\n        units) at time ``t``. This energy is obtained by integrating the\n        equation of motion:\n\n        .. math::\n\n            \\frac{\\mathrm{d}p}{\\mathrm{d}t} = eE \\quad\\implies\\quad\n            p = \\int_0^t eE(t)\\,\\mathrm{d}t',\\\\\n            W = mc^2(\\sqrt{p^2+1}-1),\n\n        where :math:`e` is the elementary charge and :math:`p` is the electron\n        momentum.\n\n        :param int t: Index of time to calculate transferred momentum until.\n        \"\"\"\n        p = self.maxMomentum(t=t)\n        return np.sqrt(p**2 + 1)-1\n\n\n    def maxMomentum(self, t=-1):\n        r\"\"\"\n        Evaluates the maximum attainable runaway momentum (in normalized units)\n        at time ``t``. This momentum is obtained by integrating the equation of\n        motion:\n\n        .. math::\n\n            \\frac{\\mathrm{d}p}{\\mathrm{d}t} = eE \\quad\\implies\\quad\n            p = \\int_0^t eE(t)\\,\\mathrm{d}t',\n\n        where :math:`e` is the elementary charge and :math:`p` is the electron\n        momentum.\n\n        :param int t: Index of time to calculate transferred momentum until.\n        \"\"\"\n        if np.isscalar(t):\n            p = np.trapz(self[:t], self.grid.t[:t], axis=0)\n        else:\n            p = []\n            t = np.asarray(t)\n\n            if t.ndim != 1:\n                raise OutputException(\"Unrecognized dimensions of time index: {}.\".format(t.ndim))\n\n            for time in t:\n                p.append(np.trapz(self[:time], self.grid.t[:time], axis=0))\n\n            p = np.array(p)\n\n        p *= scipy.constants.e / (scipy.constants.m_e * scipy.constants.c)\n        return p\n\n\n    def norm(self, to='Ec'):\n        \"\"\"\n        Return the value of this quantity normalized to the\n        quantity specified by 'to'.\n\n        to: Name of quantity to normalize electric field to.\n            Possible values:\n               Eceff   Effective critical electric field (as defined by Hesslow et al)\n               Ecfree  Connor-Hastie threshold field (calculated with n=n_free)\n               Ectot   Connor-Hastie threshold field (calculated with n=n_tot)\n               Ec      (alias for 'Ectot')\n               ED      Dreicer field\n            Note that the quantity with which to normalize to\n            must be saved as an 'OtherQuantity'.\n        \"\"\"\n        return self.normalize(to=to)\n\n\n    def normalize(self, to='Ec'):\n        \"\"\"\n        Return the value of this quantity normalized to the\n        quantity specified by 'to'.\n\n        to: Name of quantity to normalize electric field to.\n            Possible values:\n               Eceff   Effective critical electric field (as defined by Hesslow et al)\n               Ecfree  Connor-Hastie threshold field (calculated with n=n_free)\n               Ectot   Connor-Hastie threshold field (calculated with n=n_tot)\n               Ec      (alias for 'Ectot')\n               EDreic  Dreicer field\n               ED      (alias for 'EDreic')\n            Note that the quantity with which to normalize to\n            must be saved as an 'OtherQuantity'.\n        \"\"\"\n        # OtherQuantity's are not defined at t=0, so we extend them\n        # arbitrarily here (in order for the resulting FluidQuantity to\n        # be plottable on the regular time grid)\n        Enorm = np.zeros(self.data.shape)\n        Enorm[1:,:] = self.getNormEfield(field=to)\n        Enorm[0,:]  = Enorm[1,:]\n\n        data = self.data[:] / Enorm\n        return FluidQuantity(name='E / {}'.format(to), data=data, grid=self.grid, output=self.output)\n\n\n    def plot(self, norm=None, **kwargs):\n        \"\"\"\n        Wrapper for FluidQuantity.plot(), adding the 'norm' argument\n        (for directly plotting a normalized electric field)\n        \"\"\"\n        if norm is None:\n            return super().plot(**kwargs)\n        else:\n            _E = self.normalize(to=norm)\n            return _E.plot(**kwargs)\n\n\n    def plotRadialProfile(self, norm=None, **kwargs):\n        \"\"\"\n        Wrapper for FluidQuantity.plotRadialProfile(), adding the 'norm'\n        argument (for directly plotting a normalized electric field)\n        \"\"\"\n        if norm is None:\n            return super().plotRadialProfile(**kwargs)\n        else:\n            _E = self.normalize(to=norm)\n            return _E.plotRadialProfile(**kwargs)\n\n\n    def plotTimeProfile(self, norm=None, **kwargs):\n        \"\"\"\n        Wrapper for FluidQuantity.plotTimeProfile(), adding the 'norm'\n        argument (for directly plotting a normalized electric field)\n        \"\"\"\n        if norm is None:\n            return super().plotTimeProfile(**kwargs)\n        else:\n            _E = self.normalize(to=norm)\n            return _E.plotTimeProfile(**kwargs)\n\n\n    def plotIntegral(self, norm=None, **kwargs):\n        \"\"\"\n        Wrapper for FluidQuantity.plotTimeProfile(), adding the 'norm'\n        argument (for directly plotting a normalized electric field)\n        \"\"\"\n        if norm is None:\n            return super().plotIntegral(**kwargs)\n        else:\n            _E = self.normalize(to=norm)\n            return _E.plotIntegral(**kwargs)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a combined fluid-kinetic simulation using the DREAM library. The simulation should include both the hot-tail and runaway electron grids. Set the electric field strength, electron density, and temperature to specific values. Define the momentum grid and set up initial hot electron Maxwellian. Include Dreicer and avalanche in the equation system. Set up the radial grid and disable the runaway grid. Set the Svensson transport coefficients and use the nonlinear solver. Finally, set the time stepper and save the settings to an HDF5 file.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 146, "repo_full_name": "kubernetes-client__python", "instruction": "Generate code that uses the Kubernetes Python client to create a deployment, a service, and an ingress in a Kubernetes cluster. The deployment should use a container with a specific image and expose a certain port. The service should be associated with the deployment and expose the same port. The ingress should allow external network access to the service on a specified host and path. The code should also include a main function that loads the local Kubernetes configuration and calls the functions to create the deployment, service, and ingress.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def connect(host, port, service=VoidService, config={}, ipv6=False, keepalive=False):\n    \"\"\"\n    creates a socket-connection to the given host and port\n\n    :param host: the hostname to connect to\n    :param port: the TCP port to use\n    :param service: the local service to expose (defaults to Void)\n    :param config: configuration dict\n    :param ipv6: whether to create an IPv6 socket (defaults to ``False``)\n    :param keepalive: whether to set TCP keepalive on the socket (defaults to ``False``)\n\n    :returns: an RPyC connection\n    \"\"\"\n    s = SocketStream.connect(host, port, ipv6=ipv6, keepalive=keepalive)\n    return connect_stream(s, service, config)\n\n# --- Snippet Separator ---\n\ndef connect_thread(service=VoidService, config={}, remote_service=VoidService, remote_config={}):\n    \"\"\"starts an rpyc_ipy server on a new thread, bound to an arbitrary port,\n    and connects to it over a socket.\n\n    :param service: the local service to expose (defaults to Void)\n    :param config: configuration dict\n    :param remote_service: the remote service to expose (of the server; defaults to Void)\n    :param remote_config: remote configuration dict (of the server)\n    \"\"\"\n    listener = socket.socket()\n    listener.bind((\"localhost\", 0))\n    listener.listen(1)\n    remote_server = partial(_server, listener, remote_service, remote_config)\n    spawn(remote_server)\n    host, port = listener.getsockname()\n    return connect(host, port, service=service, config=config)\n\n# --- Snippet Separator ---\n\ndef connect_multiprocess(service=VoidService, config={}, remote_service=VoidService, remote_config={}, args={}):\n    \"\"\"starts an rpyc_ipy server on a new process, bound to an arbitrary port,\n    and connects to it over a socket. Basically a copy of connect_thread().\n    However if args is used and if these are shared memory then changes\n    will be bi-directional. That is we now have access to shared memory.\n\n    :param service: the local service to expose (defaults to Void)\n    :param config: configuration dict\n    :param remote_service: the remote service to expose (of the server; defaults to Void)\n    :param remote_config: remote configuration dict (of the server)\n    :param args: dict of local vars to pass to new connection, form {'name':var}\n\n    Contributed by *@tvanzyl*\n    \"\"\"\n    from multiprocessing import Process\n\n    listener = socket.socket()\n    listener.bind((\"localhost\", 0))\n    listener.listen(1)\n    remote_server = partial(_server, listener, remote_service, remote_config, args)\n    t = Process(target=remote_server)\n    t.start()\n    host, port = listener.getsockname()\n    return connect(host, port, service=service, config=config)\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that uses the Kubernetes Python client to create a deployment, a service, and an ingress in a Kubernetes cluster. The deployment should use a container with a specific image and expose a certain port. The service should be associated with the deployment and expose the same port. The ingress should allow external network access to the service on a specified host and path. The code should also include a main function that loads the local Kubernetes configuration and calls the functions to create the deployment, service, and ingress.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 147, "repo_full_name": "mne-tools__mne-python", "instruction": "Generate code that performs a full pipeline on the SPM Faces dataset using the MNE-Python library. The pipeline should include artifact removal, averaging epochs, forward model computation, and source reconstruction using dSPM on the contrast \"faces - scrambled\". The code should load and filter data, set up epochs, fit ICA, find and remove major artifacts, compute and visualize the contrast, estimate noise covariance, visualize fields on MEG helmet, compute forward and inverse solutions, and finally plot the contrast in 3D.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        def lcmv_epochs(epochs, forward, noise_cov, data_cov, reg=0.01, label=None,\n                pick_ori=None, return_generator=False, picks=None, rank=None,\n                verbose=None):\n    \"\"\"Linearly Constrained Minimum Variance (LCMV) beamformer.\n\n    Compute Linearly Constrained Minimum Variance (LCMV) beamformer\n    on single trial data.\n\n    NOTE : This implementation has not been heavily tested so please\n    report any issue or suggestions.\n\n    Parameters\n    ----------\n    epochs : Epochs\n        Single trial epochs.\n    forward : dict\n        Forward operator.\n    noise_cov : Covariance\n        The noise covariance.\n    data_cov : Covariance\n        The data covariance.\n    reg : float\n        The regularization for the whitened data covariance.\n    label : Label\n        Restricts the LCMV solution to a given label.\n    pick_ori : None | 'normal' | 'max-power'\n        If 'normal', rather than pooling the orientations by taking the norm,\n        only the radial component is kept. If 'max-power', the source\n        orientation that maximizes output source power is chosen.\n    return_generator : bool\n        Return a generator object instead of a list. This allows iterating\n        over the stcs without having to keep them all in memory.\n    picks : array-like of int\n        Channel indices to use for beamforming (if None all channels\n        are used except bad channels).\n    rank : None | int | dict\n        Specified rank of the noise covariance matrix. If None, the rank is\n        detected automatically. If int, the rank is specified for the MEG\n        channels. A dictionary with entries 'eeg' and/or 'meg' can be used\n        to specify the rank for each modality.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see mne.verbose).\n\n    Returns\n    -------\n    stc: list | generator of (SourceEstimate | VolSourceEstimate)\n        The source estimates for all epochs\n\n    See Also\n    --------\n    lcmv_raw, lcmv\n\n    Notes\n    -----\n    The original reference is:\n    Van Veen et al. Localization of brain electrical activity via linearly\n    constrained minimum variance spatial filtering.\n    Biomedical Engineering (1997) vol. 44 (9) pp. 867--880\n\n    The reference for finding the max-power orientation is:\n    Sekihara et al. Asymptotic SNR of scalar and vector minimum-variance\n    beamformers for neuromagnetic source reconstruction.\n    Biomedical Engineering (2004) vol. 51 (10) pp. 1726--34\n    \"\"\"\n    _check_reference(epochs)\n\n    info = epochs.info\n    tmin = epochs.times[0]\n\n    picks = _setup_picks(picks, info, forward, noise_cov)\n\n    data = epochs.get_data()[:, picks, :]\n    stcs = _apply_lcmv(\n        data=data, info=info, tmin=tmin, forward=forward, noise_cov=noise_cov,\n        data_cov=data_cov, reg=reg, label=label, picks=picks, rank=rank,\n        pick_ori=pick_ori)\n\n    if not return_generator:\n        stcs = [s for s in stcs]\n\n    return stcs\n\n# --- Snippet Separator ---\n\ndef rap_music(evoked, forward, noise_cov, n_dipoles=5, return_residual=False,\n              picks=None, verbose=None):\n    \"\"\"RAP-MUSIC source localization method.\n\n    Compute Recursively Applied and Projected MUltiple SIgnal Classification\n    (RAP-MUSIC) on evoked data.\n\n    Parameters\n    ----------\n    evoked : instance of Evoked\n        Evoked data to localize.\n    forward : instance of Forward\n        Forward operator.\n    noise_cov : instance of Covariance\n        The noise covariance.\n    n_dipoles : int\n        The number of dipoles to look for. The default value is 5.\n    return_residual : bool\n        If True, the residual is returned as an Evoked instance.\n    picks : array-like of int | None\n        Indices (in info) of data channels. If None, MEG and EEG data channels\n        (without bad channels) will be used.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see mne.verbose).\n\n    Returns\n    -------\n    dipoles : list of instance of Dipole\n        The dipole fits.\n    residual : instance of Evoked\n        The residual a.k.a. data not explained by the dipoles.\n        Only returned if return_residual is True.\n\n    See Also\n    --------\n    mne.fit_dipole\n\n    Notes\n    -----\n    The references are:\n\n        J.C. Mosher and R.M. Leahy. 1999. Source localization using recursively\n        applied and projected (RAP) MUSIC. Signal Processing, IEEE Trans. 47, 2\n        (February 1999), 332-340.\n        DOI=10.1109/78.740118 http://dx.doi.org/10.1109/78.740118\n\n        Mosher, J.C.; Leahy, R.M., EEG and MEG source localization using\n        recursively applied (RAP) MUSIC, Signals, Systems and Computers, 1996.\n        pp.1201,1207 vol.2, 3-6 Nov. 1996\n        doi: 10.1109/ACSSC.1996.599135\n\n    .. versionadded:: 0.9.0\n    \"\"\"\n\n    info = evoked.info\n    data = evoked.data\n    times = evoked.times\n\n    picks = _setup_picks(picks, info, forward, noise_cov)\n\n    data = data[picks]\n\n    dipoles, explained_data = _apply_rap_music(data, info, times, forward,\n                                               noise_cov, n_dipoles,\n                                               picks, return_residual)\n\n    if return_residual:\n        residual = evoked.copy()\n        selection = [info['ch_names'][p] for p in picks]\n\n        residual = pick_channels_evoked(residual,\n                                        include=selection)\n        residual.data -= explained_data\n        active_projs = [p for p in residual.info['projs'] if p['active']]\n        for p in active_projs:\n            p['active'] = False\n        residual.add_proj(active_projs, remove_existing=True)\n        residual.apply_proj()\n        return dipoles, residual\n    else:\n        return dipoles\n\n# --- Snippet Separator ---\n\ndef _apply_lcmv(data, info, tmin, forward, noise_cov, data_cov, reg,\n                label=None, picks=None, pick_ori=None, rank=None,\n                verbose=None):\n    \"\"\" LCMV beamformer for evoked data, single epochs, and raw data\n\n    Parameters\n    ----------\n    data : array or list / iterable\n        Sensor space data. If data.ndim == 2 a single observation is assumed\n        and a single stc is returned. If data.ndim == 3 or if data is\n        a list / iterable, a list of stc's is returned.\n    info : dict\n        Measurement info.\n    tmin : float\n        Time of first sample.\n    forward : dict\n        Forward operator.\n    noise_cov : Covariance\n        The noise covariance.\n    data_cov : Covariance\n        The data covariance.\n    reg : float\n        The regularization for the whitened data covariance.\n    label : Label\n        Restricts the LCMV solution to a given label.\n    picks : array-like of int | None\n        Indices (in info) of data channels. If None, MEG and EEG data channels\n        (without bad channels) will be used.\n    pick_ori : None | 'normal' | 'max-power'\n        If 'normal', rather than pooling the orientations by taking the norm,\n        only the radial component is kept. If 'max-power', the source\n        orientation that maximizes output source power is chosen.\n    rank : None | int | dict\n        Specified rank of the noise covariance matrix. If None, the rank is\n        detected automatically. If int, the rank is specified for the MEG\n        channels. A dictionary with entries 'eeg' and/or 'meg' can be used\n        to specify the rank for each modality.\n    verbose : bool, str, int, or None\n        If not None, override default verbose level (see mne.verbose).\n\n    Returns\n    -------\n    stc : SourceEstimate | VolSourceEstimate (or list of thereof)\n        Source time courses.\n    \"\"\"\n    is_free_ori, ch_names, proj, vertno, G = \\\n        _prepare_beamformer_input(info, forward, label, picks, pick_ori)\n\n    # Handle whitening + data covariance\n    whitener, _ = compute_whitener(noise_cov, info, picks, rank=rank)\n\n    # whiten the leadfield\n    G = np.dot(whitener, G)\n\n    # Apply SSPs + whitener to data covariance\n    data_cov = pick_channels_cov(data_cov, include=ch_names)\n    Cm = data_cov['data']\n    if info['projs']:\n        Cm = np.dot(proj, np.dot(Cm, proj.T))\n    Cm = np.dot(whitener, np.dot(Cm, whitener.T))\n\n    # Calculating regularized inverse, equivalent to an inverse operation after\n    # the following regularization:\n    # Cm += reg * np.trace(Cm) / len(Cm) * np.eye(len(Cm))\n    Cm_inv = linalg.pinv(Cm, reg)\n\n    # Compute spatial filters\n    W = np.dot(G.T, Cm_inv)\n    n_orient = 3 if is_free_ori else 1\n    n_sources = G.shape[1] // n_orient\n    for k in range(n_sources):\n        Wk = W[n_orient * k: n_orient * k + n_orient]\n        Gk = G[:, n_orient * k: n_orient * k + n_orient]\n        Ck = np.dot(Wk, Gk)\n\n        # Find source orientation maximizing output source power\n        if pick_ori == 'max-power':\n            eig_vals, eig_vecs = linalg.eigh(Ck)\n\n            # Choosing the eigenvector associated with the middle eigenvalue.\n            # The middle and not the minimal eigenvalue is used because MEG is\n            # insensitive to one (radial) of the three dipole orientations and\n            # therefore the smallest eigenvalue reflects mostly noise.\n            for i in range(3):\n                if i != eig_vals.argmax() and i != eig_vals.argmin():\n                    idx_middle = i\n\n            # TODO: The eigenvector associated with the smallest eigenvalue\n            # should probably be used when using combined EEG and MEG data\n            max_ori = eig_vecs[:, idx_middle]\n\n            Wk[:] = np.dot(max_ori, Wk)\n            Ck = np.dot(max_ori, np.dot(Ck, max_ori))\n            is_free_ori = False\n\n        if is_free_ori:\n            # Free source orientation\n            Wk[:] = np.dot(linalg.pinv(Ck, 0.1), Wk)\n        else:\n            # Fixed source orientation\n            Wk /= Ck\n\n    # Pick source orientation maximizing output source power\n    if pick_ori == 'max-power':\n        W = W[0::3]\n\n    # Preparing noise normalization\n    noise_norm = np.sum(W ** 2, axis=1)\n    if is_free_ori:\n        noise_norm = np.sum(np.reshape(noise_norm, (-1, 3)), axis=1)\n    noise_norm = np.sqrt(noise_norm)\n\n    # Pick source orientation normal to cortical surface\n    if pick_ori == 'normal':\n        W = W[2::3]\n        is_free_ori = False\n\n    # Applying noise normalization\n    if not is_free_ori:\n        W /= noise_norm[:, None]\n\n    if isinstance(data, np.ndarray) and data.ndim == 2:\n        data = [data]\n        return_single = True\n    else:\n        return_single = False\n\n    subject = _subject_from_forward(forward)\n    for i, M in enumerate(data):\n        if len(M) != len(picks):\n            raise ValueError('data and picks must have the same length')\n\n        if not return_single:\n            logger.info(\"Processing epoch : %d\" % (i + 1))\n\n        # SSP and whitening\n        if info['projs']:\n            M = np.dot(proj, M)\n        M = np.dot(whitener, M)\n\n        # project to source space using beamformer weights\n\n        if is_free_ori:\n            sol = np.dot(W, M)\n            logger.info('combining the current components...')\n            sol = combine_xyz(sol)\n            sol /= noise_norm[:, None]\n        else:\n            # Linear inverse: do computation here or delayed\n            if M.shape[0] < W.shape[0] and pick_ori != 'max-power':\n                sol = (W, M)\n            else:\n                sol = np.dot(W, M)\n            if pick_ori == 'max-power':\n                sol = np.abs(sol)\n\n        tstep = 1.0 / info['sfreq']\n        yield _make_stc(sol, vertices=vertno, tmin=tmin, tstep=tstep,\n                        subject=subject)\n\n    logger.info('[done]')\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a full pipeline on the SPM Faces dataset using the MNE-Python library. The pipeline should include artifact removal, averaging epochs, forward model computation, and source reconstruction using dSPM on the contrast \"faces - scrambled\". The code should load and filter data, set up epochs, fit ICA, find and remove major artifacts, compute and visualize the contrast, estimate noise covariance, visualize fields on MEG helmet, compute forward and inverse solutions, and finally plot the contrast in 3D.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 148, "repo_full_name": "ansys__pyaedt", "instruction": "Generate code that performs a multiphysics analysis using the PyAEDT library. The code should import the necessary libraries and set the graphical mode. It should then download and open a project, and start HFSS with a specified version. The code should also start a Circuit and add the HFSS dynamic link component to it. It should set up dynamic link options and create ports and excitations. The code should then create a setup and solve the circuit, pushing excitations to the HFSS model. It should start Mechanical and copy bodies from the HFSS project. The code should get losses from HFSS and assign the convection to Mechanical. Finally, the code should plot the model, solve and plot the thermal results, and release AEDT.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class Mechanical(FieldAnalysis3D, object):\n    \"\"\"Provides the Mechanical application interface.\n\n    Parameters\n    ----------\n    projectname : str, optional\n        Name of the project to select or the full path to the project\n        or AEDTZ archive to open.  The default is ``None``, in which\n        case an attempt is made to get an active project. If no\n        projects are present, an empty project is created.\n    designname : str, optional\n        Name of the design to select. The default is ``None``, in\n        which case an attempt is made to get an active design. If no\n        designs are present, an empty design is created.\n    solution_type : str, optional\n        Solution type to apply to the design. The default is\n        ``None``, in which case the default type is applied.\n    setup_name : str, optional\n        Name of the setup to use as the nominal. The default is\n        ``None``, in which case the active setup is used or\n        nothing is used.\n    specified_version : str, optional\n        Version of AEDT to use. The default is ``None``, in which case\n        the active version or latest installed version is used.\n        This parameter is ignored when Script is launched within AEDT.\n    non_graphical : bool, optional\n        Whether to launch AEDT in the non-graphical mode. The default\n        is ``False``, in which case AEDT is launched in the graphical mode.\n        This parameter is ignored when a script is launched within AEDT.\n    new_desktop_session : bool, optional\n        Whether to launch an instance of AEDT in a new thread, even if\n        another instance of the ``specified_version`` is active on the\n        machine. The default is ``True``. This parameter is ignored when Script is launched within AEDT.\n    close_on_exit : bool, optional\n        Whether to release AEDT on exit.\n    student_version : bool, optional\n        Whether to open the AEDT student version. The default is ``False``.\n        This parameter is ignored when Script is launched within AEDT.\n    machine : str, optional\n        Machine name to which connect the oDesktop Session. Works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n        If machine is `\"localhost\"` the server will also start if not present.\n    port : int, optional\n        Port number of which start the oDesktop communication on already existing server.\n        This parameter is ignored in new server creation. It works only on 2022R2.\n        Remote Server must be up and running with command `\"ansysedt.exe -grpcsrv portnum\"`.\n    aedt_process_id : int, optional\n        Only used when ``new_desktop_session = False``, specifies by process ID which instance\n        of Electronics Desktop to point PyAEDT at.\n\n    Examples\n    --------\n    Create an instance of Mechanical and connect to an existing\n    HFSS design or create a new HFSS design if one does not exist.\n\n    >>> from pyaedt import Mechanical\n    >>> aedtapp = Mechanical()\n\n    Create an instance of Mechanical and link to a project named\n    ``\"projectname\"``. If this project does not exist, create one with\n    this name.\n\n    >>> aedtapp = Mechanical(projectname)\n\n    Create an instance of Mechanical and link to a design named\n    ``\"designname\"`` in a project named ``\"projectname\"``.\n\n    >>> aedtapp = Mechanical(projectname,designame)\n\n    Create an instance of Mechanical and open the specified\n    project, which is named ``\"myfile.aedt\"``.\n\n    >>> aedtapp = Mechanical(\"myfile.aedt\")\n\n    Create a ``Desktop on 2021R2`` object and then create an\n    ``Mechanical`` object and open the specified project, which is\n    named ``\"myfile.aedt\"``.\n\n    >>> aedtapp = Mechanical(specified_version=\"2021.2\", projectname=\"myfile.aedt\")\n\n    \"\"\"\n\n    def __init__(\n        self,\n        projectname=None,\n        designname=None,\n        solution_type=None,\n        setup_name=None,\n        specified_version=None,\n        non_graphical=False,\n        new_desktop_session=False,\n        close_on_exit=False,\n        student_version=False,\n        machine=\"\",\n        port=0,\n        aedt_process_id=None,\n    ):\n\n        FieldAnalysis3D.__init__(\n            self,\n            \"Mechanical\",\n            projectname,\n            designname,\n            solution_type,\n            setup_name,\n            specified_version,\n            non_graphical,\n            new_desktop_session,\n            close_on_exit,\n            student_version,\n            machine,\n            port,\n            aedt_process_id,\n        )\n\n    def __enter__(self):\n        return self\n\n    @pyaedt_function_handler()\n    def assign_em_losses(\n        self,\n        designname=\"HFSSDesign1\",\n        setupname=\"Setup1\",\n        sweepname=\"LastAdaptive\",\n        map_frequency=None,\n        surface_objects=[],\n        source_project_name=None,\n        paramlist=[],\n        object_list=[],\n    ):\n        \"\"\"Map EM losses to a Mechanical design.\n\n        Parameters\n        ----------\n        designname : str, optional\n            Name of the design of the source mapping. The default is ``\"HFSSDesign1\"``.\n        setupname : str, optional\n            Name of the EM setup. The default is ``\"Setup1\"``.\n        sweepname : str, optional\n            Name of the EM sweep to use for the mapping. The default is ``\"LastAdaptive\"``.\n        map_frequency : str, optional\n            Frequency to map. The default is ``None``. The value must be ``None`` for\n            Eigenmode analysis.\n        surface_objects : list, optional\n            List objects in the source that are metals. The default is ``[]``.\n        source_project_name : str, optional\n            Name of the source project. The default is ``None``, in which case\n            the source from the same project is used.\n        paramlist : list, optional\n            List of all parameters in the EM to map. The default is ``[]``.\n        object_list : list, optional\n             The default is ``[]``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n\n        References\n        ----------\n\n        >>> oModule.AssignEMLoss\n        \"\"\"\n        assert self.solution_type == \"Thermal\", \"This method works only in a Mechanical structural analysis.\"\n\n        self.logger.info(\"Mapping HFSS EM Lossess\")\n        oName = self.project_name\n        if oName == source_project_name or source_project_name is None:\n            projname = \"This Project*\"\n        else:\n            projname = source_project_name + \".aedt\"\n        #\n        # Generate a list of model objects from the lists made previously and use to map the HFSS losses into Icepak.\n        #\n        if not object_list:\n            allObjects = self.modeler.object_names\n        else:\n            allObjects = object_list[:]\n        surfaces = surface_objects\n        if map_frequency:\n            intr = [map_frequency]\n        else:\n            intr = []\n\n        argparam = OrderedDict({})\n        for el in self.available_variations.nominal_w_values_dict:\n            argparam[el] = self.available_variations.nominal_w_values_dict[el]\n\n        for el in paramlist:\n            argparam[el] = el\n\n        props = OrderedDict(\n            {\n                \"Objects\": allObjects,\n                \"allObjects\": False,\n                \"Project\": projname,\n                \"projname\": \"ElectronicsDesktop\",\n                \"Design\": designname,\n                \"Soln\": setupname + \" : \" + sweepname,\n                \"Params\": argparam,\n                \"ForceSourceToSolve\": True,\n                \"PreservePartnerSoln\": True,\n                \"PathRelativeTo\": \"TargetProject\",\n            }\n        )\n        if intr:\n            props[\"Intrinsics\"] = intr\n            props[\"SurfaceOnly\"] = surfaces\n\n        name = generate_unique_name(\"EMLoss\")\n        bound = BoundaryObject(self, name, props, \"EMLoss\")\n        if bound.create():\n            self.boundaries.append(bound)\n            self.logger.info(\"EM losses mapped from design %s.\", designname)\n            return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_thermal_map(\n        self,\n        object_list,\n        designname=\"IcepakDesign1\",\n        setupname=\"Setup1\",\n        sweepname=\"SteadyState\",\n        source_project_name=None,\n        paramlist=[],\n    ):\n        \"\"\"Map thermal losses to a Mechanical design.\n\n        .. note::\n           This method works in 2021 R2 only when coupled with Icepak.\n\n        Parameters\n        ----------\n        object_list : list\n\n        designname : str, optional\n            Name of the design with the source mapping. The default is ``\"IcepakDesign1\"``.\n        setupname : str, optional\n            Name of the EM setup. The default is ``\"Setup1\"``.\n        sweepname :str, optional\n            Name of the EM sweep to use for the mapping. The default is ``\"SteadyState\"``.\n        source_project_name : str, optional\n            Name of the source project. The default is ``None``, in which case the\n            source from the same project is used.\n        paramlist : list, optional\n            List of all parameters in the EM to map. The default is ``[]``.\n\n        Returns\n        -------\n        :class:`aedt.modules.Boundary.Boundary object`\n            Boundary object.\n\n        References\n        ----------\n\n        >>> oModule.AssignThermalCondition\n        \"\"\"\n\n        assert self.solution_type == \"Structural\", \"This method works only in a Mechanical structural analysis.\"\n\n        self.logger.info(\"Mapping HFSS EM Lossess\")\n        oName = self.project_name\n        if oName == source_project_name or source_project_name is None:\n            projname = \"This Project*\"\n        else:\n            projname = source_project_name + \".aedt\"\n        #\n        # Generate a list of model objects from the lists made previously and use to map the HFSS losses into Icepak.\n        #\n        object_list = self.modeler.convert_to_selections(object_list, True)\n        if not object_list:\n            allObjects = self.modeler.object_names\n        else:\n            allObjects = object_list[:]\n        argparam = OrderedDict({})\n        for el in self.available_variations.nominal_w_values_dict:\n            argparam[el] = self.available_variations.nominal_w_values_dict[el]\n\n        for el in paramlist:\n            argparam[el] = el\n\n        props = OrderedDict(\n            {\n                \"Objects\": allObjects,\n                \"Uniform\": False,\n                \"Project\": projname,\n                \"Product\": \"ElectronicsDesktop\",\n                \"Design\": designname,\n                \"Soln\": setupname + \" : \" + sweepname,\n                \"Params\": argparam,\n                \"ForceSourceToSolve\": True,\n                \"PreservePartnerSoln\": True,\n                \"PathRelativeTo\": \"TargetProject\",\n            }\n        )\n\n        name = generate_unique_name(\"ThermalLink\")\n        bound = BoundaryObject(self, name, props, \"ThermalCondition\")\n        if bound.create():\n            self.boundaries.append(bound)\n            self.logger.info(\"Thermal conditions are mapped from design %s.\", designname)\n            return bound\n\n        return True\n\n    @pyaedt_function_handler()\n    def assign_uniform_convection(\n        self, objects_list, convection_value, convection_unit=\"w_per_m2kel\", temperature=\"AmbientTemp\", boundary_name=\"\"\n    ):\n        \"\"\"Assign a uniform convection to the face list.\n\n        Parameters\n        ----------\n        objects_list : list\n            List of objects, faces, or both.\n        convection_value : float\n            Convection value.\n        convection_unit : str, optional\n            Units for the convection value. The default is ``\"w_per_m2kel\"``.\n        temperature : str, optional\n            Temperature. The default is ``\"AmbientTemp\"``.\n        boundary_name : str, optional\n            Name of the boundary. The default is ``\"\"``.\n\n        Returns\n        -------\n        :class:`aedt.modules.Boundary.Boundary object`\n            Boundary object.\n\n        References\n        ----------\n\n        >>> oModule.AssignConvection\n        \"\"\"\n        assert self.solution_type == \"Thermal\", \"This method works only in a Mechanical structural analysis.\"\n\n        props = {}\n        objects_list = self.modeler.convert_to_selections(objects_list, True)\n\n        if type(objects_list) is list:\n            if type(objects_list[0]) is str:\n                props[\"Objects\"] = objects_list\n            else:\n                props[\"Faces\"] = objects_list\n\n        props[\"Temperature\"] = temperature\n        props[\"Uniform\"] = True\n        props[\"FilmCoeff\"] = str(convection_value) + convection_unit\n\n        if not boundary_name:\n            boundary_name = generate_unique_name(\"Convection\")\n        bound = BoundaryObject(self, boundary_name, props, \"Convection\")\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_uniform_temperature(self, objects_list, temperature=\"AmbientTemp\", boundary_name=\"\"):\n        \"\"\"Assign a uniform temperature boundary.\n\n        .. note::\n            This method works only in a Mechanical thermal analysis.\n\n        Parameters\n        ----------\n        objects_list : list\n            List of objects, faces, or both.\n        temperature : str, optional.\n            Type of the temperature. The default is ``\"AmbientTemp\"``.\n        boundary_name : str, optional\n            Name of the boundary. The default is ``\"\"``.\n\n        Returns\n        -------\n        :class:`aedt.modules.Boundary.Boundary object`\n            Boundary object.\n\n        References\n        ----------\n\n        >>> oModule.AssignTemperature\n        \"\"\"\n        assert self.solution_type == \"Thermal\", \"This method works only in a Mechanical structural analysis.\"\n\n        props = {}\n        objects_list = self.modeler.convert_to_selections(objects_list, True)\n\n        if type(objects_list) is list:\n            if type(objects_list[0]) is str:\n                props[\"Objects\"] = objects_list\n            else:\n                props[\"Faces\"] = objects_list\n\n        props[\"Temperature\"] = temperature\n\n        if not boundary_name:\n            boundary_name = generate_unique_name(\"Temp\")\n        bound = BoundaryObject(self, boundary_name, props, \"Temperature\")\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_frictionless_support(self, objects_list, boundary_name=\"\"):\n        \"\"\"Assign a Mechanical frictionless support.\n\n        .. note::\n            This method works only in a Mechanical structural analysis.\n\n        Parameters\n        ----------\n        objects_list : list\n            List of faces to apply to the frictionless support.\n        boundary_name : str, optional\n            Name of the boundary. The default is ``\"\"``.\n\n        Returns\n        -------\n        :class:`aedt.modules.Boundary.Boundary object`\n            Boundary object.\n\n        References\n        ----------\n\n        >>> oModule.AssignFrictionlessSupport\n        \"\"\"\n\n        if not (self.solution_type == \"Structural\" or \"Modal\" in self.solution_type):\n            self.logger.error(\"This method works only in Mechanical atructural analysis.\")\n            return False\n        props = {}\n        objects_list = self.modeler.convert_to_selections(objects_list, True)\n\n        if type(objects_list) is list:\n            if type(objects_list[0]) is str:\n                props[\"Objects\"] = objects_list\n            else:\n                props[\"Faces\"] = objects_list\n\n        if not boundary_name:\n            boundary_name = generate_unique_name(\"Temp\")\n        bound = BoundaryObject(self, boundary_name, props, \"Frictionless\")\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n    @pyaedt_function_handler()\n    def assign_fixed_support(self, objects_list, boundary_name=\"\"):\n        \"\"\"Assign a Mechanical fixed support.\n\n        .. note::\n           This method works only in a Mechanical structural analysis.\n\n        Parameters\n        ----------\n        objects_list : list\n            List of faces to apply to the fixed support.\n        boundary_name : str, optional\n            Name of the boundary. The default is ``\"\"``.\n\n        Returns\n        -------\n        aedt.modules.Boundary.Boundary\n            Boundary object.\n\n        References\n        ----------\n\n        >>> oModule.AssignFixedSupport\n        \"\"\"\n        if not (self.solution_type == \"Structural\" or \"Modal\" in self.solution_type):\n            self.logger.error(\"This method works only in a Mechanical structural analysis.\")\n            return False\n        props = {}\n        objects_list = self.modeler.convert_to_selections(objects_list, True)\n\n        if type(objects_list) is list:\n            props[\"Faces\"] = objects_list\n\n        if not boundary_name:\n            boundary_name = generate_unique_name(\"Temp\")\n        bound = BoundaryObject(self, boundary_name, props, \"FixedSupport\")\n        if bound.create():\n            self.boundaries.append(bound)\n            return bound\n        return False\n\n    @property\n    def existing_analysis_sweeps(self):\n        \"\"\"Existing analysis sweeps in the design.\n\n        Returns\n        -------\n        list\n            List of existing analysis sweeps.\n\n        References\n        ----------\n\n        >>> oModule.GetSetups\n        \"\"\"\n        setup_list = self.existing_analysis_setups\n        sweep_list = []\n        for el in setup_list:\n            sweep_list.append(el + \" : Solution\")\n        return sweep_list\n\n# --- Snippet Separator ---\n\ndef assign_voltage_source_to_sheet(self, sheet_name, axisdir=0, sourcename=None):\n        \"\"\"Create a voltage source taking one sheet.\n\n        Parameters\n        ----------\n        sheet_name : str\n            Name of the sheet to apply the boundary to.\n        axisdir : int or :class:`pyaedt.application.Analysis.Analysis.AxisDir`, optional\n            Position of the port. It should be one of the values for ``Application.AxisDir``,\n            which are: ``XNeg``, ``YNeg``, ``ZNeg``, ``XPos``, ``YPos``, and ``ZPos``.\n            The default is ``Application.AxisDir.XNeg``.\n        sourcename : str, optional\n            Name of the source. The default is ``None``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Boundary object.\n\n        References\n        ----------\n\n        >>> oModule.AssignVoltage\n\n        Examples\n        --------\n\n        Create a sheet and assign to it some voltage.\n\n        >>> sheet = hfss.modeler.create_rectangle(hfss.PLANE.XY,\n        ...                                                  [0, 0, -70], [10, 2], name=\"VoltageSheet\",\n        ...                                                  matname=\"copper\")\n        >>> v1 = hfss.assign_voltage_source_to_sheet(sheet.name, hfss.AxisDir.XNeg, \"VoltageSheetExample\")\n\n        \"\"\"\n\n        if self.solution_type in [\"Modal\", \"Terminal\", \"Transient Network\"]:\n            point0, point1 = self.modeler.get_mid_points_on_dir(sheet_name, axisdir)\n            sourcename = self._get_unique_source_name(sourcename, \"Voltage\")\n            return self.create_source_excitation(sheet_name, point0, point1, sourcename, sourcetype=\"Voltage\")\n        return False\n\n# --- Snippet Separator ---\n\ndef assign_current_source_to_sheet(self, sheet_name, axisdir=0, sourcename=None):\n        \"\"\"Create a current source taking one sheet.\n\n        Parameters\n        ----------\n        sheet_name : str\n            Name of the sheet to apply the boundary to.\n        axisdir : int or :class:`pyaedt.application.Analysis.Analysis.AxisDir`, optional\n            Position of the port. It should be one of the values for ``Application.AxisDir``,\n            which are: ``XNeg``, ``YNeg``, ``ZNeg``, ``XPos``, ``YPos``, and ``ZPos``.\n            The default is ``Application.AxisDir.XNeg``.\n        sourcename : str, optional\n            Name of the source. The default is ``None``.\n\n        Returns\n        -------\n        :class:`pyaedt.modules.Boundary.BoundaryObject`\n            Boundary object.\n\n        References\n        ----------\n\n        >>> oModule.AssignCurrent\n\n        Examples\n        --------\n\n        Create a sheet and assign some current to it.\n\n        >>> sheet = hfss.modeler.create_rectangle(hfss.PLANE.XY, [0, 0, -50],\n        ...                                                  [5, 1], name=\"CurrentSheet\", matname=\"copper\")\n        >>> hfss.assign_current_source_to_sheet(sheet.name, hfss.AxisDir.XNeg, \"CurrentSheetExample\")\n        'CurrentSheetExample'\n\n        \"\"\"\n\n        if self.solution_type in [\"Modal\", \"Terminal\", \"Transient Network\"]:\n            point0, point1 = self.modeler.get_mid_points_on_dir(sheet_name, axisdir)\n            sourcename = self._get_unique_source_name(sourcename, \"Current\")\n            return self.create_source_excitation(sheet_name, point0, point1, sourcename, sourcetype=\"Current\")\n        return False\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that performs a multiphysics analysis using the PyAEDT library. The code should import the necessary libraries and set the graphical mode. It should then download and open a project, and start HFSS with a specified version. The code should also start a Circuit and add the HFSS dynamic link component to it. It should set up dynamic link options and create ports and excitations. The code should then create a setup and solve the circuit, pushing excitations to the HFSS model. It should start Mechanical and copy bodies from the HFSS project. The code should get losses from HFSS and assign the convection to Mechanical. Finally, the code should plot the model, solve and plot the thermal results, and release AEDT.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}, {"idx": 149, "repo_full_name": "hiddensymmetries__simsopt", "instruction": "Generate code that solves a stage-II coil optimization problem using the simsopt library. The goal is to find coils that generate a specific target normal field on a given surface. The target equilibrium is a W7-X configuration with an average beta of 4%. The code should use a virtual casing calculation to compute the target B_{External}·n. The objective function is given by J = (1/2) ∫ |B_{BiotSavart}·n - B_{External}·n|^2 ds + LENGTH_PENALTY * Σ ½(CurveLength - L0)^2. The code should also include a Taylor test and run the optimization using the L-BFGS-B method. The results should be saved in the \"output\" directory.", "prompt": "        ### Long Code Arena · Library-Based Code Generation (with Snippets)\n\n        You are an elite Python 3 engineer.\n\n        **Goal** Generate a **single, runnable `.py` file** that fulfils the\n        **Task Description** below.  \n        *Everything you need* – target library name, expected behaviour, I/O\n        format – is stated inside the task itself.\n\n        ── Retrieved Library Snippets ──────────────────────────────────────\n        The fenced block contains **real fragments** from the target library.\n        • Treat them as *read-only* context.  \n        • Copy verbatim **only** trivial boiler-plate (e.g. imports, constant\n          tables).  For everything else, write original code inspired by them.  \n        • Ignore snippets that do not help the task.\n\n        ```python\n        class SquaredFlux(Optimizable):\n\n    r\"\"\"\n    Objective representing quadratic-flux-like quantities, useful for stage-2\n    coil optimization. Several variations are available, which can be selected\n    using the ``definition`` argument. For ``definition=\"quadratic flux\"`` \n    (the default), the objective is defined as\n\n    .. math::\n        J = \\frac12 \\int_{S} (\\mathbf{B}\\cdot \\mathbf{n} - B_T)^2 ds,\n\n    where :math:`\\mathbf{n}` is the surface unit normal vector and\n    :math:`B_T` is an optional (zero by default) target value for the\n    magnetic field. Also :math:`\\int_{S} ds` indicates a surface integral.\n    For ``definition=\"normalized\"``, the objective is defined as\n\n    .. math::\n        J = \\frac12 \\frac{\\int_{S} (\\mathbf{B}\\cdot \\mathbf{n} - B_T)^2 ds}\n                         {\\int_{S} |\\mathbf{B}|^2 ds}.\n\n    For ``definition=\"local\"``, the objective is defined as\n\n    .. math::\n        J = \\frac12 \\int_{S} \\frac{(\\mathbf{B}\\cdot \\mathbf{n} - B_T)^2}{|\\mathbf{B}|^2} ds.\n\n    The definition ``\"quadratic flux\"`` has the advantage of simplicity, and it\n    is used in other contexts such as REGCOIL. However for stage-2 optimization,\n    the optimizer can \"cheat\", lowering this objective by reducing the magnitude\n    of the field. The definitions ``\"normalized\"`` and ``\"local\"`` close this loophole.\n\n    Args:\n        surface: A :obj:`simsopt.geo.surface.Surface` object on which to compute the flux\n        field: A :obj:`simsopt.field.magneticfield.MagneticField` for which to compute the flux.\n        target: A ``nphi x ntheta`` numpy array containing target values for the flux. Here \n          ``nphi`` and ``ntheta`` correspond to the number of quadrature points on `surface` \n          in ``phi`` and ``theta`` direction.\n        definition: A string to select among the definitions above. The\n          available options are ``\"quadratic flux\"``, ``\"normalized\"``, and ``\"local\"``.\n    \"\"\"\n\n    def __init__(self, surface, field, target=None, definition=\"quadratic flux\"):\n        self.surface = surface\n        if target is not None:\n            self.target = np.ascontiguousarray(target)\n        else:\n            self.target = np.zeros(self.surface.normal().shape[:2])\n        self.field = field\n        xyz = self.surface.gamma()\n        self.field.set_points(xyz.reshape((-1, 3)))\n        if definition not in [\"quadratic flux\", \"normalized\", \"local\"]:\n            raise ValueError(\"Unrecognized option for 'definition'.\")\n        self.definition = definition\n        Optimizable.__init__(self, x0=np.asarray([]), depends_on=[field])\n\n    def J(self):\n        n = self.surface.normal()\n        Bcoil = self.field.B().reshape(n.shape)\n        return sopp.integral_BdotN(Bcoil, self.target, n, self.definition)\n\n    @derivative_dec\n    def dJ(self):\n        n = self.surface.normal()\n        absn = np.linalg.norm(n, axis=2)\n        unitn = n * (1. / absn)[:, :, None]\n        Bcoil = self.field.B().reshape(n.shape)\n        Bcoil_n = np.sum(Bcoil * unitn, axis=2)\n        if self.target is not None:\n            B_n = (Bcoil_n - self.target)\n        else:\n            B_n = Bcoil_n\n\n        if self.definition == \"quadratic flux\":\n            dJdB = (B_n[..., None] * unitn * absn[..., None]) / absn.size\n            dJdB = dJdB.reshape((-1, 3))\n\n        elif self.definition == \"local\":\n            mod_Bcoil = np.linalg.norm(Bcoil, axis=2)\n            dJdB = ((\n                (B_n/mod_Bcoil)[..., None] * (\n                    unitn / mod_Bcoil[..., None] - (B_n / mod_Bcoil**3)[..., None] * Bcoil\n                )) * absn[..., None]) / absn.size\n\n        elif self.definition == \"normalized\":\n            mod_Bcoil = np.linalg.norm(Bcoil, axis=2)\n            num = np.mean(B_n**2 * absn)\n            denom = np.mean(mod_Bcoil**2 * absn)\n\n            dnum = 2 * (B_n[..., None] * unitn * absn[..., None]) / absn.size\n            ddenom = 2 * (Bcoil * absn[..., None]) / absn.size\n            dJdB = 0.5 * (dnum / denom - num * ddenom / denom**2)\n\n        else:\n            raise ValueError(\"Should never get here\")\n\n        dJdB = dJdB.reshape((-1, 3))\n        return self.field.B_vjp(dJdB)\n\n# --- Snippet Separator ---\n\nclass VirtualCasing:\n    r\"\"\"\n    Use the virtual casing principle to compute the contribution to\n    the total magnetic field due to current outside a bounded surface.\n\n    Usually, an instance of this class is created using the\n    :func:`from_vmec()` class method, which also drives the\n    computationally demanding part of the calculation (solving the\n    integral equation).  In the future, interfaces to other\n    equilibrium codes may be added.\n\n    In the standard 2-stage approach to stellarator optimization, the\n    virtual casing calculation is run once, at the end of stage 1\n    (optimizing the plasma shape), with the result provided as input\n    to stage 2 (optimizing the coil shapes). In this case, you can use\n    the :func:`save()` function or the ``filename`` argument of\n    :func:`from_vmec()` to save the results of the virtual casing\n    calculation.  These saved results can then be loaded in later\n    using the :func:`load()` class method, when needed for solving the\n    stage-2 problem.\n\n    In order to compute the external field accurately, one requires\n    fairly high grid resolution. For the stage-2 problem however, a\n    lower resolution is often sufficient. To deal with this, we\n    consider two grids, one denoted by the prefix ``src_`` and the\n    other one denoted by ``trgt_``. ``src_nphi`` and ``src_ntheta``\n    refer to the resolution of the grid for the input data, i.e. the\n    total magnetic field and shape of the surface. ``trgt_nphi`` and\n    ``trgt_ntheta`` refer to the resolution of the grid where the\n    external field is computed, i.e. the output of the virtual casing\n    calculation that is provided as input to the stage 2 problem).\n\n    To set the grid resolutions ``src_nphi`` and ``src_ntheta``, it can be\n    convenient to use the function\n    :func:`simsopt.geo.surface.best_nphi_over_ntheta`.\n\n    An instance of this class has the following attributes. For all\n    vector quantites, Cartesian coordinates are used, corresponding to\n    array dimensions of size 3:\n\n    - ``src_nphi``: The number of grid points in the toroidal angle :math:`\\phi`, for a half field period (or a full field period if use_stellsym=False)\n    - ``src_ntheta``: The number of grid points in the poloidal angle :math:`\\theta`.\n    - ``src_phi``: An array of size ``(src_nphi,)`` with the grid points of :math:`\\phi`.\n    - ``src_theta``: An array of size ``(src_ntheta,)`` with the grid points of :math:`\\theta`.\n    - ``trgt_nphi``: The number of grid points in the toroidal angle :math:`\\phi`, for a half field period (or a full field period if use_stellsym=False)\n    - ``trgt_ntheta``: The number of grid points in the poloidal angle :math:`\\theta`.\n    - ``trgt_phi``: An array of size ``(trgt_nphi,)`` with the grid points of :math:`\\phi`.\n    - ``trgt_theta``: An array of size ``(trgt_ntheta,)`` with the grid points of :math:`\\theta`.\n\n    - ``gamma``: An array of size ``(src_nphi, src_ntheta, 3)`` with the position vector on the surface.\n    - ``B_total``: An array of size ``(src_nphi, src_ntheta, 3)`` with the total magnetic field vector on the surface.\n    - ``unit_normal``: An array of size ``(trgt_nphi, trgt_ntheta, 3)`` with the unit normal vector on the surface.\n    - ``B_external``: An array of size ``(trgt_nphi, trgt_ntheta, 3)`` with the contribution\n      to the magnetic field due to current outside the surface.\n    - ``B_external_normal``: An array of size ``(trgt_nphi, trgt_ntheta)`` with the contribution\n      to the magnetic field due to current outside the surface, taking just the component\n      normal to the surface.\n\n    The :math:`\\phi` and :math:`\\theta` grids for these data are both\n    uniformly spaced, and are the same as for\n    :obj:`~simsopt.geo.surface.Surface` classes with ``range=\"half\n    period\"`` or ``range=\"full period\"``, for the case of\n    stellarator-symmetry or non-stellarator-symmetry respectively.\n    (See the description of the ``range`` parameter in the\n    documentation on :ref:`surfaces`.)  For the usual case of\n    stellarator symmetry, all the virtual casing data are given on\n    half a field period. There is no grid point at :math:`\\phi=0`,\n    rather the grid is shifted in :math:`\\phi` by half the grid\n    spacing. Thus, the ``src_phi`` grid is ``np.linspace(1 / (2 * nfp\n    * src_nphi), (src_nphi - 0.5) / (src_nphi * nfp), src_nphi)``\n    (recalling the simsopt convention that :math:`\\phi` and :math:`\\theta` have period 1,\n    not :math:`2\\pi`). For a non-stellarator-symmetric calculation,\n    the ``src_phi`` grid is ``np.linspace(0, 1 / nfp, src_nphi,\n    endpoint=False)``.  The ``trgt_phi`` grid follows the same logic as\n    the ``src_phi`` grid.  Note that for stellarator symmetry, if\n    ``src_nphi != trgt_nphi``, then the shift (i.e. first grid point)\n    in ``src_phi`` and ``trgt_phi`` will be different. For both\n    stellarator symmetry and non-stellarator-symmetry, the\n    ``src_theta`` grid is ``np.linspace(0, 1, src_ntheta,\n    endpoint=False)``, and the ``trgt_theta`` grid is the same but with\n    ``trgt_ntheta``.\n\n    In particular, ``B_external_normal`` is given on the grid that\n    would be naturally used for stage-2 coil optimization, so no\n    resampling is required.\n    \"\"\"\n\n    @classmethod\n    def from_vmec(cls, vmec, src_nphi, src_ntheta=None, trgt_nphi=None, trgt_ntheta=None, use_stellsym=True, digits=6, filename=\"auto\"):\n        \"\"\"\n        Given a :obj:`~simsopt.mhd.vmec.Vmec` object, compute the contribution\n        to the total magnetic field due to currents outside the plasma.\n\n        This function requires the python ``virtual_casing`` package to be\n        installed.\n\n        The argument ``src_nphi`` refers to the number of points around a half\n        field period if stellarator symmetry is exploited, or a full field\n        period if not.\n\n        To set the grid resolutions ``src_nphi`` and ``src_ntheta``, it can be\n        convenient to use the function\n        :func:`simsopt.geo.surface.best_nphi_over_ntheta`. This is\n        done automatically if you omit the ``src_ntheta`` argument.\n\n        For now, this routine only works for stellarator symmetry.\n\n        Args:\n            vmec: Either an instance of :obj:`simsopt.mhd.vmec.Vmec`, or the name of a\n              Vmec ``input.*`` or ``wout*`` file.\n            src_nphi: Number of grid points toroidally for the input of the calculation.\n            src_ntheta: Number of grid points poloidally for the input of the calculation. If ``None``,\n              the number of grid points will be calculated automatically using\n              :func:`simsopt.geo.surface.best_nphi_over_ntheta()` to minimize\n              the grid anisotropy, given the specified ``nphi``.\n            trgt_nphi: Number of grid points toroidally for the output of the calculation.\n              If unspecified, ``src_nphi`` will be used.\n            trgt_ntheta: Number of grid points poloidally for the output of the calculation.\n              If unspecified, ``src_ntheta`` will be used.\n            use_stellsym: whether to exploit stellarator symmetry in the calculation.\n            digits: Approximate number of digits of precision for the calculation.\n            filename: If not ``None``, the results of the virtual casing calculation\n              will be saved in this file. For the default value of ``\"auto\"``, the\n              filename will automatically be set to ``\"vcasing_<extension>.nc\"``\n              where ``<extension>`` is the string associated with Vmec input and output\n              files, analogous to the Vmec output file ``\"wout_<extension>.nc\"``.\n        \"\"\"\n        import virtual_casing as vc_module\n\n        if not isinstance(vmec, Vmec):\n            vmec = Vmec(vmec)\n\n        vmec.run()\n        nfp = vmec.wout.nfp\n        stellsym = (not bool(vmec.wout.lasym)) and use_stellsym\n        if vmec.wout.lasym:\n            raise RuntimeError('virtual casing presently only works for stellarator symmetry')\n\n        if src_ntheta is None:\n            src_ntheta = int((1+int(stellsym)) * nfp * src_nphi / best_nphi_over_ntheta(vmec.boundary))\n            logger.info(f'new src_ntheta: {src_ntheta}')\n\n        # The requested nphi and ntheta may not match the quadrature\n        # points in vmec.boundary, and the range may not be \"full torus\",\n        # so generate a SurfaceRZFourier with the desired resolution:\n        if stellsym:\n            ran = \"half period\"\n        else:\n            ran = \"field period\"\n        surf = SurfaceRZFourier.from_nphi_ntheta(mpol=vmec.wout.mpol, ntor=vmec.wout.ntor, nfp=nfp,\n                                                 nphi=src_nphi, ntheta=src_ntheta, range=ran)\n        for jmn in range(vmec.wout.mnmax):\n            surf.set_rc(int(vmec.wout.xm[jmn]), int(vmec.wout.xn[jmn] / nfp), vmec.wout.rmnc[jmn, -1])\n            surf.set_zs(int(vmec.wout.xm[jmn]), int(vmec.wout.xn[jmn] / nfp), vmec.wout.zmns[jmn, -1])\n        Bxyz = B_cartesian(vmec, nphi=src_nphi, ntheta=src_ntheta, range=ran)\n        gamma = surf.gamma()\n        logger.debug(f'gamma.shape: {gamma.shape}')\n        logger.debug(f'Bxyz[0].shape: {Bxyz[0].shape}')\n\n        if trgt_nphi is None:\n            trgt_nphi = src_nphi\n        if trgt_ntheta is None:\n            trgt_ntheta = src_ntheta\n        trgt_surf = SurfaceRZFourier.from_nphi_ntheta(mpol=vmec.wout.mpol, ntor=vmec.wout.ntor, nfp=nfp,\n                                                      nphi=trgt_nphi, ntheta=trgt_ntheta, range=ran)\n        trgt_surf.x = surf.x\n\n        unit_normal = trgt_surf.unitnormal()\n        logger.debug(f'unit_normal.shape: {unit_normal.shape}')\n\n        # virtual_casing wants all input arrays to be 1D. The order is\n        # {x11, x12, ..., x1Np, x21, x22, ... , xNtNp, y11, ... , z11, ...}\n        # where Nt is toroidal (not theta!) and Np is poloidal (not phi!)\n        gamma1d = np.zeros(src_nphi * src_ntheta * 3)\n        B1d = np.zeros(src_nphi * src_ntheta * 3)\n        B3d = np.zeros((src_nphi, src_ntheta, 3))\n        for jxyz in range(3):\n            gamma1d[jxyz * src_nphi * src_ntheta: (jxyz + 1) * src_nphi * src_ntheta] = gamma[:, :, jxyz].flatten(order='C')\n            B1d[jxyz * src_nphi * src_ntheta: (jxyz + 1) * src_nphi * src_ntheta] = Bxyz[jxyz].flatten(order='C')\n            B3d[:, :, jxyz] = Bxyz[jxyz]\n\n        \"\"\"\n        # Check order:\n        index = 0\n        for jxyz in range(3):\n            for jphi in range(src_nphi):\n                for jtheta in range(src_ntheta):\n                    np.testing.assert_allclose(gamma1d[index], gamma[jphi, jtheta, jxyz])\n                    np.testing.assert_allclose(B1d[index], Bxyz[jxyz][jphi, jtheta])\n                    index += 1\n        \"\"\"\n\n        vcasing = vc_module.VirtualCasing()\n        vcasing.setup(\n            digits, nfp, stellsym,\n            src_nphi, src_ntheta, gamma1d,\n            src_nphi, src_ntheta,\n            trgt_nphi, trgt_ntheta)\n        # This next line launches the main computation:\n        Bexternal1d = np.array(vcasing.compute_external_B(B1d))\n\n        # Unpack 1D array results:\n        Bexternal3d = np.zeros((trgt_nphi, trgt_ntheta, 3))\n        for jxyz in range(3):\n            Bexternal3d[:, :, jxyz] = Bexternal1d[jxyz * trgt_nphi * trgt_ntheta: (jxyz + 1) * trgt_nphi * trgt_ntheta].reshape((trgt_nphi, trgt_ntheta), order='C')\n\n        \"\"\"\n        # Check order:\n        index = 0\n        for jxyz in range(3):\n            for jphi in range(trgt_nphi):\n                for jtheta in range(trgt_ntheta):\n                    np.testing.assert_allclose(Bexternal1d[index], Bexternal3d[jphi, jtheta, jxyz])\n                    index += 1\n        \"\"\"\n\n        Bexternal_normal = np.sum(Bexternal3d * unit_normal, axis=2)\n\n        vc = cls()\n        vc.src_ntheta = src_ntheta\n        vc.src_nphi = src_nphi\n        vc.src_theta = surf.quadpoints_theta\n        vc.src_phi = surf.quadpoints_phi\n\n        vc.trgt_ntheta = trgt_ntheta\n        vc.trgt_nphi = trgt_nphi\n        vc.trgt_theta = trgt_surf.quadpoints_theta\n        vc.trgt_phi = trgt_surf.quadpoints_phi\n\n        vc.nfp = nfp\n        vc.B_total = B3d\n        vc.gamma = gamma\n        vc.unit_normal = unit_normal\n        vc.B_external = Bexternal3d\n        vc.B_external_normal = Bexternal_normal\n\n        if filename is not None:\n            if filename == 'auto':\n                directory, basefile = os.path.split(vmec.output_file)\n                filename = os.path.join(directory, 'vcasing' + basefile[4:])\n                logger.debug(f'New filename: {filename}')\n            vc.save(filename)\n\n        return vc\n\n    def save(self, filename=\"vcasing.nc\"):\n        \"\"\"\n        Save the results of a virtual casing calculation in a NetCDF file.\n\n        Args:\n            filename: Name of the file to create.\n        \"\"\"\n        with netcdf_file(filename, 'w') as f:\n            f.history = 'This file created by simsopt on ' + datetime.now().strftime(\"%B %d %Y, %H:%M:%S\")\n            f.createDimension('src_ntheta', self.src_ntheta)\n            f.createDimension('src_nphi', self.src_nphi)\n            f.createDimension('trgt_ntheta', self.trgt_ntheta)\n            f.createDimension('trgt_nphi', self.trgt_nphi)\n            f.createDimension('xyz', 3)\n\n            src_ntheta = f.createVariable('src_ntheta', 'i', tuple())\n            src_ntheta.assignValue(self.src_ntheta)\n            src_ntheta.description = 'Number of grid points in the poloidal angle theta for source B field and surface shape'\n            src_ntheta.units = 'Dimensionless'\n\n            trgt_ntheta = f.createVariable('trgt_ntheta', 'i', tuple())\n            trgt_ntheta.assignValue(self.trgt_ntheta)\n            trgt_ntheta.description = 'Number of grid points in the poloidal angle theta for resulting B_external'\n            trgt_ntheta.units = 'Dimensionless'\n\n            src_nphi = f.createVariable('src_nphi', 'i', tuple())\n            src_nphi.assignValue(self.src_nphi)\n            src_nphi.description = 'Number of grid points in the toroidal angle phi for source B field and surface shape'\n            src_nphi.units = 'Dimensionless'\n\n            trgt_nphi = f.createVariable('trgt_nphi', 'i', tuple())\n            trgt_nphi.assignValue(self.trgt_nphi)\n            trgt_nphi.description = 'Number of grid points in the toroidal angle phi for resulting B_external'\n            trgt_nphi.units = 'Dimensionless'\n\n            nfp = f.createVariable('nfp', 'i', tuple())\n            nfp.assignValue(self.nfp)\n            nfp.description = 'Periodicity in toroidal direction'\n            nfp.units = 'Dimensionless'\n\n            src_theta = f.createVariable('src_theta', 'd', ('src_ntheta',))\n            src_theta[:] = self.src_theta\n            src_theta.description = 'Grid points in the poloidal angle theta for source B field and surface shape. Note that theta extends over [0, 1) not [0, 2pi).'\n            src_theta.units = 'Dimensionless'\n\n            trgt_theta = f.createVariable('trgt_theta', 'd', ('trgt_ntheta',))\n            trgt_theta[:] = self.trgt_theta\n            trgt_theta.description = 'Grid points in the poloidal angle theta for resulting B_external. Note that theta extends over [0, 1) not [0, 2pi).'\n            trgt_theta.units = 'Dimensionless'\n\n            src_phi = f.createVariable('src_phi', 'd', ('src_nphi',))\n            src_phi[:] = self.src_phi\n            src_phi.description = 'Grid points in the toroidal angle phi for source B field and surface shape. Note that phi extends over [0, 1) not [0, 2pi).'\n            src_phi.units = 'Dimensionless'\n\n            trgt_phi = f.createVariable('trgt_phi', 'd', ('trgt_nphi',))\n            trgt_phi[:] = self.trgt_phi\n            trgt_phi.description = 'Grid points in the toroidal angle phi for resulting B_external. Note that phi extends over [0, 1) not [0, 2pi).'\n            trgt_phi.units = 'Dimensionless'\n\n            gamma = f.createVariable('gamma', 'd', ('src_nphi', 'src_ntheta', 'xyz'))\n            gamma[:, :, :] = self.gamma\n            gamma.description = 'Position vector on the boundary surface'\n            gamma.units = 'meter'\n\n            unit_normal = f.createVariable('unit_normal', 'd', ('trgt_nphi', 'trgt_ntheta', 'xyz'))\n            unit_normal[:, :, :] = self.unit_normal\n            unit_normal.description = 'Unit-length normal vector on the boundary surface'\n            unit_normal.units = 'Dimensionless'\n\n            B_total = f.createVariable('B_total', 'd', ('src_nphi', 'src_ntheta', 'xyz'))\n            B_total[:, :, :] = self.B_total\n            B_total.description = 'Total magnetic field vector on the surface, including currents both inside and outside of the surface'\n            B_total.units = 'Tesla'\n\n            B_external = f.createVariable('B_external', 'd', ('trgt_nphi', 'trgt_ntheta', 'xyz'))\n            B_external[:, :, :] = self.B_external\n            B_external.description = 'Contribution to the magnetic field vector on the surface due only to currents outside the surface'\n            B_external.units = 'Tesla'\n\n            B_external_normal = f.createVariable('B_external_normal', 'd', ('trgt_nphi', 'trgt_ntheta'))\n            B_external_normal[:, :] = self.B_external_normal\n            B_external_normal.description = 'Component of B_external normal to the surface'\n            B_external_normal.units = 'Tesla'\n\n    @classmethod\n    def load(cls, filename):\n        \"\"\"\n        Load in the results of a previous virtual casing calculation,\n        previously saved in NetCDF format.\n\n        Args:\n            filename: Name of the file to load.\n        \"\"\"\n        vc = cls()\n        with netcdf_file(filename, mmap=False) as f:\n            for key, val in f.variables.items():\n                val2 = val[()]  # Convert to numpy array\n                vc.__setattr__(key, val2)\n        return vc\n\n    def plot(self, ax=None, show=True):\n        \"\"\"\n        Plot ``B_external_normal``, the component normal to the surface of\n        the magnetic field generated by currents outside the surface.\n        This routine requires ``matplotlib``.\n\n        Args:\n            ax: The axis object on which to plot. This argument is useful when plotting multiple\n              objects on the same axes. If equal to the default ``None``, a new axis will be created.\n            show: Whether to call matplotlib's ``show()`` function.\n\n        Returns:\n            An axis which could be passed to a further call to matplotlib if desired.\n        \"\"\"\n        import matplotlib.pyplot as plt\n        if ax is None:\n            fig, ax = plt.subplots()\n        else:\n            fig = plt.gcf()\n        contours = ax.contourf(self.trgt_phi, self.trgt_theta, self.B_external_normal.T, 25)\n        ax.set_xlabel(r'$\\phi$')\n        ax.set_ylabel(r'$\\theta$')\n        ax.set_title('B_external_normal [Tesla]')\n        fig.colorbar(contours)\n        fig.tight_layout()\n        if show:\n            plt.show()\n        return ax\n\n# --- Snippet Separator ---\n\ndef boozer_surface_residual_dB(surface, iota, G, biotsavart):\n    r\"\"\"\n    For a given surface with points x on it, this function computes the\n    differentiated residual\n\n    .. math::\n        \\frac{d}{dB_{i,j,k}}[ G B_{i,j,k} - \\|\\mathbf{B}_{i,j}\\|^2  (\\mathbf{x}_{\\varphi} + \\iota  \\mathbf{x}_{\\theta}) ]\n\n    where :math:`B_{i,j,k}` is the kth component of the magnetic field :math:`\\mathbf B_{i,j}` at quadrature point :math:`(i,j)` \n    as well as the derivatives of this residual with respect to surface dofs,\n    :math:`\\iota`, and :math:`G`. :math:`G` is known for exact boozer surfaces, so if \n    G=None is passed, then that value is used instead.\n    \"\"\"\n\n    user_provided_G = G is not None\n    if not user_provided_G:\n        G = 2. * np.pi * np.sum(np.abs(biotsavart.coil_currents)) * (4 * np.pi * 10**(-7) / (2 * np.pi))\n\n    x = surface.gamma()\n    xphi = surface.gammadash1()\n    xtheta = surface.gammadash2()\n    nphi = x.shape[0]\n    ntheta = x.shape[1]\n\n    xsemiflat = x.reshape((x.size//3, 3)).copy()\n\n    biotsavart.set_points(xsemiflat)\n\n    B = biotsavart.B().reshape((nphi, ntheta, 3))\n\n    tang = xphi + iota * xtheta\n    residual = G*B - np.sum(B**2, axis=2)[..., None] * tang\n\n    GI = np.eye(3, 3) * G\n    dresidual_dB = GI[None, None, :, :] - 2. * tang[:, :, :, None] * B[:, :, None, :]\n\n    residual_flattened = residual.reshape((nphi*ntheta*3, ))\n    dresidual_dB_flattened = dresidual_dB.reshape((nphi*ntheta*3, 3))\n    r = residual_flattened\n    dr_dB = dresidual_dB_flattened\n\n    return r, dr_dB\n        ```\n\n        ── Task Description ────────────────────────────────────────────────\n        Generate code that solves a stage-II coil optimization problem using the simsopt library. The goal is to find coils that generate a specific target normal field on a given surface. The target equilibrium is a W7-X configuration with an average beta of 4%. The code should use a virtual casing calculation to compute the target B_{External}·n. The objective function is given by J = (1/2) ∫ |B_{BiotSavart}·n - B_{External}·n|^2 ds + LENGTH_PENALTY * Σ ½(CurveLength - L0)^2. The code should also include a Taylor test and run the optimization using the L-BFGS-B method. The results should be saved in the \"output\" directory.\n\n        ── Implementation Rules (strict) ───────────────────────────────────\n        1. Use **only** public APIs of the target library + Python ≥ 3.9 stdlib.  \n        2. Write production-ready code **without extra comments** (docstrings\n           are optional but allowed).  This maximises evaluation similarity.  \n        3. Replicate import aliases that you see in the snippets where useful\n           (helps API-Recall).  \n        4. No third-party deps beyond the target library.  \n        5. If you define reusable functions/classes, add a minimal\n           ``if __name__ == \"__main__\":`` demo that illustrates usage.  \n        6. Keep runtime ≤ 30 s and RAM ≤ 2 GiB.  \n        7. **Do not** reveal chain-of-thought; keep reasoning internal.\n\n        ── Output Protocol ─────────────────────────────────────────────────\n        • Reply with **only** the Python code, nothing else.  \n        • Start your code right after the marker below and end with a matching\n          fence.  The first executable line must be an import or module docstring.\n\n        ```python\n", "top_k": 3, "num_snippets_used": 3}]}