{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available?: True\n",
      "Number of GPUs: 2\n",
      "GPU Name: NVIDIA GeForce RTX 3080 Ti\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(\"Is GPU available?:\", gpu_available)\n",
    "\n",
    "# If yes, print GPU details\n",
    "if gpu_available:\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "else:\n",
    "    print(\"Using CPU only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNpkTgRYJckA",
    "tags": []
   },
   "source": [
    "## Section 1: Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNe6W1jUJo2g",
    "outputId": "26d2f2cf-df97-4c72-dd8f-07088ce76392",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "C extension: pandas.compat._constants not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/__init__.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     27\u001b[0m         is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev,  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/compat/__init__.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_constants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     IS64,\n\u001b[1;32m     19\u001b[0m     ISMUSL,\n\u001b[1;32m     20\u001b[0m     PY310,\n\u001b[1;32m     21\u001b[0m     PY311,\n\u001b[1;32m     22\u001b[0m     PY312,\n\u001b[1;32m     23\u001b[0m     PYPY,\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompressors\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ISMUSL' from 'pandas.compat._constants' (/opt/conda/lib/python3.10/site-packages/pandas/compat/_constants.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# from typing import List, Optional # For type hinting (optional)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# from transformers import PreTrainedTokenizerBase # For type hinting (optional)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DownloadMode\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EntryNotFoundError\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/__init__.py:31\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m _err:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     _module \u001b[38;5;241m=\u001b[39m _err\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC extension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_module\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not built. If you want to import \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas from the source directory, you may need to run \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython setup.py build_ext\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to build the C extensions first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m_err\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m     get_option,\n\u001b[1;32m     39\u001b[0m     set_option,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     options,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: C extension: pandas.compat._constants not built. If you want to import pandas from the source directory, you may need to run 'python setup.py build_ext' to build the C extensions first."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time # for the delay before nvidia-smi\n",
    "import warnings # for non-critical warnings\n",
    "import shutil\n",
    "import tarfile # for .tar archives\n",
    "import ast\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import textwrap # for snipper preview\n",
    "import traceback\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "import requests\n",
    "# import torch # already imported\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# from typing import List, Optional # For type hinting (optional)\n",
    "# from transformers import PreTrainedTokenizerBase # For type hinting (optional)\n",
    "from datasets import load_dataset, DownloadMode\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub.utils import EntryNotFoundError\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import importlib.metadata as md # For version checking after install\n",
    "\n",
    "print(\"Libraries correctly imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYTrT48hEbrS",
    "tags": []
   },
   "source": [
    "### **Importing Prompt Templates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from prompts import (\n",
    "    build_baseline_prompt_v1,\n",
    "    build_rag_prompt_v1,\n",
    "    build_baseline_prompt_v2,\n",
    "    build_rag_prompt_v2,\n",
    "    build_baseline_prompt_v3,\n",
    "    build_rag_prompt_v3,\n",
    "    build_baseline_prompt_v4,\n",
    "    build_rag_prompt_v4,\n",
    "    build_baseline_prompt_v5,\n",
    "    build_rag_prompt_v5,\n",
    "    truncate_to_n_tokens,\n",
    "    build_baseline_prompt_v6,\n",
    "    build_rag_prompt_v6,\n",
    "    build_baseline_prompt_v6_2,\n",
    "    build_rag_prompt_v6_2,\n",
    "    build_baseline_prompt_v6_3,\n",
    "    build_rag_prompt_v6_3,\n",
    "    build_baseline_prompt_v7,\n",
    "    build_rag_prompt_v7,\n",
    "    build_baseline_prompt_v8,\n",
    "    build_rag_prompt_v8,\n",
    "    build_baseline_prompt_v9,\n",
    "    build_rag_prompt_v9,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Other Useful Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils import _qcfg_to_dict , download_github_raw_json, robust_code_tokenizer_for_s5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kqbp1sqaJzkH",
    "tags": []
   },
   "source": [
    "## Section 2: LLM &Tokenizer Loading with 4-bit Quantization Model + Save in cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **LLM & Tokenizer Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PEZfMT3QJxJU",
    "outputId": "31cb4747-c06f-4a67-86aa-894f9c477195",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting trust_remote_code=False for deepseek-ai/deepseek-coder-7b-instruct-v1.5\n",
      "Model cache will be saved in: ./cache/deepseek-ai_deepseek-coder-7b-instruct-v1.5_4bit_nf4\n"
     ]
    }
   ],
   "source": [
    "# check that there is only one selected model\n",
    "\n",
    "# --- Gemma Series (Google) ---\n",
    "# model_name = \"google/codegemma-7b\"\n",
    "# model_name = \"google/codegemma-7b-it\"\n",
    "\n",
    "# --- Qwen Series (Alibaba) ---\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-3B-Instruct\"\n",
    "# model_name = \"Qwen/Qwen2.5-Coder-1.5B-Instruct\"\n",
    "\n",
    "# --- Deepseek Coder Series (Deepseek AI) ---\n",
    "model_name = \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\"\n",
    "#model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\"\n",
    "# model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "#model_name = \"deepseek-ai/deepseek-coder-1.3b-base\"\n",
    "\n",
    "# --- Code Llama Series (Meta) ---\n",
    "# model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "\n",
    "# --- Phi Series (Microsoft) ---\n",
    "# model_name = \"microsoft/Phi-4-mini-instruct\"\n",
    "# model_name = \"microsoft/Phi-4-multimodal-instruct\"\n",
    "\n",
    "TRUST_REMOTE_CODE_MODELS = [\"microsoft/Phi-\",\"Qwen/\",]\n",
    "trust_code = any(model_name.startswith(prefix) for prefix in TRUST_REMOTE_CODE_MODELS)\n",
    "print(f\"Setting trust_remote_code={trust_code} for {model_name}\")\n",
    "if trust_code:\n",
    "    print(\"WARNING: trust_remote_code=True will execute Python code from the model's Hugging Face repo\")\n",
    "\n",
    "# Definisci dove salvare la cache in locale (nella cartella 'cache' del progetto)\n",
    "CACHE_ROOT = \"./cache\"\n",
    "\n",
    "# Sottocartella per ogni modello e configurazione\n",
    "CACHE_DIR = os.path.join( CACHE_ROOT,model_name.replace(\"/\", \"_\") + \"_4bit_nf4\")\n",
    "META_FILE = os.path.join(CACHE_DIR, \"metadata.json\")\n",
    "\n",
    "# Mostra dove verrà salvata la cache\n",
    "print(f\"Model cache will be saved in: {CACHE_DIR}\")\n",
    "\n",
    "# == Build the 4‐bit config (for GPU only) ===================\n",
    "# (use bfloat16 on bf16‐capable GPUs, else float16)\n",
    "compute_dtype = (torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16)\n",
    "\n",
    "QUANT_CFG = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "\n",
    "REQ_META = { \"model_name\": model_name, \"quant_cfg\":  _qcfg_to_dict(QUANT_CFG)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409,
     "referenced_widgets": [
      "f4ee30d84024494db3b9ea9d1c1f2f85",
      "ee0bf3c1f5f648fc81f9df2e2ba7bba5",
      "9a9a4ba023f54118883534728d328c44",
      "4bdc077612f04c0ca506febfacbb5403",
      "1dfc2b303feb4c6da474749cf4862d2e",
      "2ba51afe6897494bb5b644730fa7580a",
      "068a344ed8fd4b88a3c46829a831f4e1",
      "63b0ab0483d043fcb20d700080deea9c",
      "e8f3e8d0ac49416891538845d9aa162e",
      "cee3f670b6284a049cfbe3b398f67b3d",
      "284bb83d8cfc497b84986d71002ef4ab",
      "ce8c9952ee7642e38eaa921a7cd02268",
      "267570e14e5045298267cb1d37872d95",
      "e2ec2396e421442f95cf4af84a6e88ec",
      "65331f835f524151ae560466c6bf4655",
      "64240b75cad84dea83ce189a43405668",
      "6edd17a05ba74f6b9c40415e76e76edb",
      "1c1f106b607540efb29dc036f3a734d9",
      "762edaea5c844c75aa521dc3b2e457f5",
      "172ce2b591a44aec9d915cd51e1e2475",
      "c24f30967965427baf4d2061b9883339",
      "96d2db5db33f4ee7abf73e81aeac4876",
      "02229e3be13145feb476833c0a010e9b",
      "1ed7852ba9164b1290eadfcce48989e7",
      "6dafa92adc684ac6a7839cbe16086f31",
      "6fe28078f0104b65b52f2eb2c3b50f06",
      "e136773141a84282bc7738cd6ef5182c",
      "cf4871b8e77a4663bce7eb33f61ae488",
      "20ca0aa7ea094bc1868b2d6375539dc8",
      "026b6657b8a14e7294e98d6ffb416428",
      "3d2c4bc1094142d28b78064211489abc",
      "4ca50826a66b467f96132b94b04139a3",
      "0c05f3641a9842008fcf1cb24c8201f4",
      "9d0252f73031468b922be378b48d468f",
      "08756a2d862c4a23bbde86d5e5925503",
      "01c12387bfe44e8a885faa0519ec54dc",
      "d94ced3cf1c244a3b917116d26605f8c",
      "ab4b9cd4479c4b48918b3b4645c7e54f",
      "4cb9b2c5352c4379b3509b0a1258cb52",
      "d969f62f700846d2bfcb8639daed0eeb",
      "066a8ac1f0ca408587f2c31c9a4a729a",
      "f2bf430ae1d044c7a8231839df32643b",
      "eec0c25deea944a2a5fc266a8a42841b",
      "53b13d341dd14a4e9c3c5621a1be7ae0",
      "dd2e8df7c52c477585cad40927a6b99e",
      "966496cb09c34c97b9babf5322b280c3",
      "581af374236a4d43bb86da7adee2935b",
      "5e0ab05077c04d4a91ed387c1da927f5",
      "d1b9209d040641a9aebc419bfced5a04",
      "b4e469ab5b744732876285bbe263cca1",
      "6780e440c66e429ea03a78b07a37d055",
      "ba7c44a3158d4260825a9bd09a5e7be6",
      "340132d4270843be8263c2395f2507d1",
      "98c36be12d654abcb37c449ff8bfbc28",
      "1bc262f3a6684312927bdcf23013660f",
      "6cbc77aeefe4413b90f4aa451d670b8a",
      "c3070aef55354d3e9ff249720344ab46",
      "e88c9d72d238456c9c88e2b13cf23ad3",
      "4a57ba83748f4d26a80d23dba269f78a",
      "bf9fdb62bb96432683174a2fd62a4761",
      "461f713a4655448d860d4703292dc2ac",
      "0e6f207cf944448cbb2d0ce6802eaf9a",
      "1e7572793db84015901449b3692ab724",
      "9a34345411584027975e13b46ae4cfba",
      "ef998139bff54cab8ee1b1bd38192049",
      "93c2e5556e434296a451947047016705",
      "3fc9fb45d8c2481d88855889e2d98009",
      "c577e5f2988b48e09c8cb38035ac0fab",
      "a97ea78ece504c579708f2c1143fc878",
      "42f2591b35a141e09a84fcae0856e255",
      "c1697bf367b34cfb92e47f4e493f52a1",
      "cf40b8459da8451298d382dac19b8227",
      "7312f9e7a84c41609554f83e80302549",
      "ba3f200d65984c17ad00dc9da172ed3c",
      "fc7aa020b1054776bd421a07e266bfa5",
      "b206161a19da4d5eadb73297fdbccb7d",
      "57b283a87dd2453aae003c23b2fc4cbc",
      "a9dd9236d47e47478fde14621fab685d",
      "a6f29cfb2ac445a2a86de57484a5c755",
      "157d2afd234243ac9c3a4769d578c166",
      "6a0eaee7c1044ac891d5d2085bb10059",
      "f238997f5118458ba47572c5cdfe9228",
      "30463b09c6e044029710b9773264eb17",
      "dd2dc35a0df5432180b47563c423a782",
      "50ac78d2cec04adcaf987960fcb987d6",
      "f888b9d356d94d228b613ba1c2111ace",
      "e4d93681fc384839af27b86ce39bfd81",
      "3ec7392d77814607a7474bdc00eb34e1",
      "411523e423fa4db0b07ec8c4b8477d6e",
      "ca403f7e1bbc4b1d9b249500a6f344d8",
      "3c9c72bf76e44338b861b8c181336a26",
      "d10571e7bca74ea9aba536eeb3e532fd",
      "2d7d05b5081740368bd938203423f159",
      "7bf836b4b2ba4a66a3a7801561cf3010",
      "9c53da4af20b486ab78df9ae4230c53d",
      "ea65ba42e7e8495aace282926cb14113",
      "c002ab1fb2f64243876e2b9abf279cc3",
      "5dcad31a9d404c4481d569957322cc66",
      "8f3a9a0f60b44df18efd9747d1528570"
     ]
    },
    "id": "-seMg9q8MKYR",
    "outputId": "73a6513c-0e6c-4683-f252-e9b773ca1315",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No valid cache. CUDA available? True\n",
      " Quantising 4-bit… this will happen once\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9807ac318a014ae7a6a7ffdafde3c117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model/tokenizer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2258/2649707255.py\", line 39, in <module>\n",
      "    model = AutoModelForCausalLM.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3960, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 4434, in _load_pretrained_model\n",
      "    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 961, in _load_state_dict_into_meta_model\n",
      "    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py\", line 337, in set_module_tensor_to_device\n",
      "    new_value = value.to(device)\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 800.00 MiB. GPU \u0001 has a total capacity of 11.67 GiB of which 700.25 MiB is free. Process 160571 has 7.16 GiB memory in use. Process 315811 has 1.90 GiB memory in use. Process 320021 has 1.90 GiB memory in use. Of the allocated memory 1.44 GiB is allocated by PyTorch, and 181.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 800.00 MiB. GPU \u0001 has a total capacity of 11.67 GiB of which 700.25 MiB is free. Process 160571 has 7.16 GiB memory in use. Process 315811 has 1.90 GiB memory in use. Process 320021 has 1.90 GiB memory in use. Of the allocated memory 1.44 GiB is allocated by PyTorch, and 181.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# ─── model ───────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_4bit:\n\u001b[0;32m---> 39\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQUANT_CFG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m     model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     48\u001b[0m         model_name,\n\u001b[1;32m     49\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mcompute_dtype,\n\u001b[1;32m     50\u001b[0m         device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     51\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_code,\n\u001b[1;32m     52\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    570\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3960\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3950\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3951\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3953\u001b[0m     (\n\u001b[1;32m   3954\u001b[0m         model,\n\u001b[1;32m   3955\u001b[0m         missing_keys,\n\u001b[1;32m   3956\u001b[0m         unexpected_keys,\n\u001b[1;32m   3957\u001b[0m         mismatched_keys,\n\u001b[1;32m   3958\u001b[0m         offload_index,\n\u001b[1;32m   3959\u001b[0m         error_msgs,\n\u001b[0;32m-> 3960\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3961\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3967\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3968\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3971\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3972\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgguf_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgguf_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3977\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3979\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3980\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:4434\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path)\u001b[0m\n\u001b[1;32m   4430\u001b[0m                 set_module_tensor_to_device(\n\u001b[1;32m   4431\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   4432\u001b[0m                 )\n\u001b[1;32m   4433\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4434\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4436\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4437\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4438\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4439\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4441\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4442\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4443\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4444\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4445\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4446\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4447\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4448\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4449\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4450\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4451\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   4452\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4453\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:961\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[1;32m    950\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, state_dict_folder, state_dict_index)\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    952\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m is_quantized\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    959\u001b[0m ):\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 961\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    963\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/utils/modeling.py:337\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    335\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 337\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 800.00 MiB. GPU \u0001 has a total capacity of 11.67 GiB of which 700.25 MiB is free. Process 160571 has 7.16 GiB memory in use. Process 315811 has 1.90 GiB memory in use. Process 320021 has 1.90 GiB memory in use. Of the allocated memory 1.44 GiB is allocated by PyTorch, and 181.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# == Check for existing cache & metadata ====================\n",
    "use_cache = False\n",
    "if os.path.isfile(META_FILE):\n",
    "    try:\n",
    "        saved = json.load(open(META_FILE))\n",
    "        use_cache = saved == REQ_META\n",
    "        print(\" Cache metadata match:\", use_cache)\n",
    "    except Exception:\n",
    "        print(\" Could not parse metadata.json; ignoring cache.\")\n",
    "\n",
    "# == 7.  Load tokenizer & model (fast or slow path) ==============\n",
    "trust_code = model_name.startswith((\"microsoft/Phi-\", \"Qwen/\"))\n",
    "\n",
    "try:\n",
    "    if use_cache:\n",
    "        print(\" Loading from cache…\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(CACHE_DIR, local_files_only=True, trust_remote_code=trust_code)\n",
    "        model     = AutoModelForCausalLM.from_pretrained(\n",
    "            CACHE_DIR,\n",
    "            device_map=\"auto\",\n",
    "            low_cpu_mem_usage=True,\n",
    "            trust_remote_code=trust_code,\n",
    "        )\n",
    "    else:\n",
    "        # decide whether we *can* do 4-bit quant:\n",
    "        do_4bit = torch.cuda.is_available()\n",
    "        print(f\" No valid cache. CUDA available? {do_4bit}\")\n",
    "        print(f\" {'Quantising 4-bit…' if do_4bit else 'Loading fp16…'} this will happen once\")\n",
    "\n",
    "        # ─── tokenizer ───────────────────────────────────────────\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=trust_code)\n",
    "        if tokenizer.pad_token_id is None:\n",
    "            tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "            if tokenizer.pad_token is None:\n",
    "                tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "\n",
    "        # ─── model ───────────────────────────────────────────────\n",
    "        if do_4bit:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=QUANT_CFG,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=trust_code,\n",
    "                low_cpu_mem_usage=True,\n",
    "            )\n",
    "        else:\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=compute_dtype,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=trust_code,\n",
    "            )\n",
    "\n",
    "        # ─── Save cache for next time ───────────────────────────\n",
    "        print(\"Saving to cache…\")\n",
    "        os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "        tokenizer.save_pretrained(CACHE_DIR)\n",
    "        model.save_pretrained(CACHE_DIR)\n",
    "        with open(META_FILE, \"w\") as f:\n",
    "            json.dump(REQ_META, f)\n",
    "        print(\"Cache written at\", CACHE_DIR)\n",
    "\n",
    "    # ensure model.pad_token_id\n",
    "    if getattr(model, \"config\", None) and model.config.pad_token_id is None:\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    print(\"Model & tokenizer ready!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error loading model/tokenizer:\")\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0G8BVCNQFTg",
    "tags": []
   },
   "source": [
    "## Section 3: Dataset Preparation and Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LW9aGepXMMHf",
    "outputId": "ead666c1-cb46-444d-db9b-e06d8f432efa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scegli una directory locale dove vuoi salvare risultati e output\n",
    "save_path = './results/'  # puoi cambiare il nome se preferisci\n",
    "\n",
    "# Verifica/crea la directory\n",
    "try:\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    print(f\"Directory disponibile per i risultati: {save_path}\")\n",
    "except OSError as e:\n",
    "    print(f\"Warning: non posso creare o verificare la directory: {save_path}. Dettagli: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272,
     "referenced_widgets": [
      "57e96516642d4e77a15b71e6ee226306",
      "e222f7b7569244b8b8b8d1bf6314498c",
      "1054aad8e7f74b898398b6c887f13432",
      "0d6df7a3326546c3a1162737168f4f45",
      "c0bdf6bafca34fb2ad5e912780af468c",
      "e95ee4a871534d8b93154783169cf752",
      "0555a2e0d3a04ad789c516c6564c4dbf",
      "9de2f3538db94de384d05fb86a41058d",
      "a859333b703648b7a88711246a8cc46e",
      "ab1151ab30104706ab5dc65da7c0dd23",
      "8abdfa5a467340f8a5f34b4eee9f88a4",
      "d2fb6731d14b4286bb942d06ace7ad53",
      "58191cfc5fc3498faaf0afd1006cdbdb",
      "4d26233c6ea94099864362810e6b61ca",
      "8b1cc7b109a24ab49ba68e9be47582e0",
      "df003250270b4525a904d81279d3e0eb",
      "ddd680ab7beb47a2b03d492ac063af0f",
      "5477f02797ca4a168b129c3c67c115f3",
      "240a4244def74e7687ea9de628dc7a7f",
      "099da97063be4ae884c7e212a75798c0",
      "a401381eee884b4b91ccec23680a954a",
      "a9b41ec9bd424e659750259cab16cafd",
      "34293dc64fef47b2b9890826c3841160",
      "944adee829e84bd4a58d93de0757c2a3",
      "d9999cb78a8c417a8a7f1e749a92fa6a",
      "75dbf677bb9c467daca6a3100a8590b5",
      "24b1f5a8a80b45b5beeba95133d3e750",
      "99516291a8d74c878c890b05680f1fff",
      "a24646b489744fc4b0d475ff0108c84f",
      "f3c430bdef7a45159393b738e346f5ae",
      "1d4fc7b2663e429eaf8aa4aa8424bbe6",
      "3e63d227c86f44739555219525946285",
      "6b004d6c247241559144fa7fc7470328"
     ]
    },
    "id": "FphZD_qGQTVT",
    "outputId": "6ee05beb-0da4-4124-d009-6a642cd064c7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DownloadMode\n",
    "\n",
    "dataset_name = \"JetBrains-Research/lca-library-based-code-generation\"\n",
    "data_split   = \"test\"\n",
    "\n",
    "print(f\"\\\\n  Loading dataset '{dataset_name}' (split='{data_split}')…\")\n",
    "\n",
    "try:\n",
    "    # To work with all libraries\n",
    "    lca_dataset_all_libraries = load_dataset( # Renamed for clarity\n",
    "        dataset_name,\n",
    "        split=data_split,\n",
    "    )\n",
    "    lca_dataset_split = lca_dataset_all_libraries\n",
    "    print(f\"Dataset loaded with {len(lca_dataset_all_libraries)} examples across all libraries.\")\n",
    "\n",
    "    # To work with only specific libraries for testing:\n",
    "    # target_repos = [\"seed-emulator\", \"another-repo\"] # Examples\n",
    "    # lca_dataset_split = lca_dataset_all_libraries.filter(\n",
    "    #     lambda ex: ex[\"repo_name\"] in target_repos\n",
    "    # )\n",
    "    # print(f\"Filtered to {len(lca_dataset_split)} examples in {target_repos}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" ERROR loading or filtering dataset: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4KHU4SrReZ1",
    "tags": []
   },
   "source": [
    "## Section 4: GitHub Knowledge Base Access Setup\n",
    "\n",
    "This section configures access to pre-built Knowledge Bases (KBs) hosted on a GitHub repository.\n",
    "\n",
    "It performs two main tasks:\n",
    "1.  Defines a helper function (`download_github_raw_json`) to fetch KB JSON files from GitHub.\n",
    "2.  Sets essential GitHub repository parameters (username, repo name, branch, KB folder path) to construct the base URL for downloading KBs. It also specifies a local temporary directory for these downloads.\n",
    "\n",
    "This setup enables subsequent sections to dynamically load specific KBs from the designated GitHub source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y_yE_xRMG2Gi",
    "outputId": "d46f1967-4aa3-4457-bcca-e69f63b552d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "GITHUB_USERNAME = \"PatrizioAcquadro\"\n",
    "GITHUB_REPO_NAME = \"RAG_Project_SE2\"\n",
    "GITHUB_BRANCH = \"main\"\n",
    "GITHUB_KBS_FOLDER_PATH = \"knowledge_bases_prod\"\n",
    "\n",
    "# Base URL per contenuti raw di GitHub\n",
    "GITHUB_RAW_CONTENT_BASE_URL = f\"https://raw.githubusercontent.com/{GITHUB_USERNAME}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}/{GITHUB_KBS_FOLDER_PATH}\"\n",
    "\n",
    "# Directory locale dove salvare temporaneamente le KB scaricate\n",
    "LOCAL_TEMP_KB_DOWNLOAD_DIR = \"./temp_downloaded_kbs\"\n",
    "os.makedirs(LOCAL_TEMP_KB_DOWNLOAD_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"--- RAG Knowledge Base Configuration (GitHub) ---\")\n",
    "print(f\"  KBs will be downloaded from GitHub base URL: {GITHUB_RAW_CONTENT_BASE_URL}/kb_LIBRARY_KEY.json\")\n",
    "print(f\"  Downloaded KBs will be temporarily stored in: {LOCAL_TEMP_KB_DOWNLOAD_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "\n",
    "# GitHub configuration (reuse your variables)\n",
    "GITHUB_USERNAME = \"PatrizioAcquadro\"\n",
    "GITHUB_REPO_NAME = \"RAG_Project_SE2\"\n",
    "GITHUB_BRANCH = \"main\"\n",
    "GITHUB_KBS_FOLDER_PATH = \"knowledge_bases_prod\"\n",
    "GITHUB_API_URL = f\"https://api.github.com/repos/{GITHUB_USERNAME}/{GITHUB_REPO_NAME}/contents/{GITHUB_KBS_FOLDER_PATH}?ref={GITHUB_BRANCH}\"\n",
    "GITHUB_RAW_CONTENT_BASE_URL = f\"https://raw.githubusercontent.com/{GITHUB_USERNAME}/{GITHUB_REPO_NAME}/{GITHUB_BRANCH}/{GITHUB_KBS_FOLDER_PATH}\"\n",
    "\n",
    "print(\"Fetching KB files from GitHub...\")\n",
    "\n",
    "try:\n",
    "    response = requests.get(GITHUB_API_URL)\n",
    "    response.raise_for_status()\n",
    "    kb_files = response.json()\n",
    "\n",
    "    kb_files = [f for f in kb_files if f['name'].endswith('.json')]\n",
    "    print(f\"\\nFound {len(kb_files)} KB files:\\n\")\n",
    "\n",
    "    for file in kb_files:\n",
    "        kb_name = file[\"name\"]\n",
    "        raw_url = f\"{GITHUB_RAW_CONTENT_BASE_URL}/{kb_name}\"\n",
    "        print(f\"- {kb_name}\")\n",
    "        \n",
    "        try:\n",
    "            kb_resp = requests.get(raw_url)\n",
    "            kb_resp.raise_for_status()\n",
    "            kb_json = kb_resp.json()\n",
    "\n",
    "            if isinstance(kb_json, list):\n",
    "                print(f\"  Entries: {len(kb_json)} (list)\")\n",
    "            elif isinstance(kb_json, dict):\n",
    "                print(f\"  Top-level keys: {list(kb_json.keys())}\")\n",
    "            else:\n",
    "                print(\"  Unknown KB format\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Failed to load: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching KB metadata from GitHub: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGMR7ZvsRz9p",
    "tags": []
   },
   "source": [
    "## Section 5: BM25 Retrieval Analysis\n",
    "\n",
    "This section is dedicated to analyzing the BM25 retrieval process for a selected code generation sample. It is divided into two main parts:\n",
    "1.  **Configuration:** Defining all parameters for the retrieval analysis (The sample to inspect, BM25 settings, and the tokenization strategy).\n",
    "2.  **Execution & Display:** Loading the relevant KB, performing BM25 retrieval based on the configurations, and displaying the results.\n",
    "\n",
    "This allows for easy experimentation with different retrieval settings before full-scale evaluation.\n",
    "\n",
    "It uses pre-built KBs from `DRIVE_KBS_ROOT_PATH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f20m3cZmR2bM",
    "outputId": "8984559d-3400-46f1-cca3-2f591b70c5af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Rep & Sample Selection ---\n",
    "ANALYSIS_TARGET_REPO_FULL_NAME = \"pyscf__pyscf\"\n",
    "ANALYSIS_SAMPLE_INDEX_WITHIN_REPO = 0 # If line above = \"None\", it's chosen randomly from all repos\n",
    "\n",
    "# --- 2. BM25 Algorithm & Retrieval Parameters ---\n",
    "ANALYSIS_BM25_K1 = 1.5\n",
    "ANALYSIS_BM25_B = 0.75\n",
    "ANALYSIS_TOP_K_SNIPPETS = 1\n",
    "\n",
    "# --- 3. BM25 Tokenizer Selection ---\n",
    "# Define tokenizer functions here. The selected one will be used by BM25 in Cell 5.2.\n",
    "def robust_code_tokenizer_for_s5(text_input):\n",
    "    if not isinstance(text_input, str): return []\n",
    "    text = text_input.lower()\n",
    "    raw_tokens = re.split(r'[^a-z0-9_]+', text) # Keep alphanumeric and underscore\n",
    "    # Filter out empty strings, single characters (often noise), and pure numbers\n",
    "    return [token for token in raw_tokens if token and len(token) > 1 and not token.isdigit()]\n",
    "\n",
    "# Select the tokenizer function to be used:\n",
    "ANALYSIS_BM25_TOKENIZER = robust_code_tokenizer_for_s5\n",
    "\n",
    "# --- 4. Display Options for Analysis Cell (Cell 5.2) ---\n",
    "ANALYSIS_SHOW_QUERY_TOKENS = True\n",
    "ANALYSIS_HIGHLIGHT_KEYWORDS = True # In retrieved snippets\n",
    "\n",
    "print(\"  Configuration for BM25 Retrieval Analysis (Section 5) is set:\")\n",
    "\n",
    "if ANALYSIS_TARGET_REPO_FULL_NAME:\n",
    "    print(f\"    Target Library for Analysis: '{ANALYSIS_TARGET_REPO_FULL_NAME}'\")\n",
    "    print(f\"    Instruction Index within this library's samples: {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO}\")\n",
    "else:\n",
    "    print(f\"    Sample Index from lca_dataset_split for Analysis: {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO}\")\n",
    "\n",
    "print(f\"    BM25 Params: k1={ANALYSIS_BM25_K1}, b={ANALYSIS_BM25_B}\")\n",
    "print(f\"    Number of Snippets to Retrieve (Top-K): {ANALYSIS_TOP_K_SNIPPETS}\")\n",
    "print(f\"    Tokenizer for BM25: {ANALYSIS_BM25_TOKENIZER.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "870d6c9eec114a27976a87b6be6c84fd",
      "7d801e6efd734e1d95622a5d21f054cd",
      "6e1712852726449dbe7232700998bc6e",
      "9dbaa7cb6c7d47e182968a45898c1a3d",
      "75b07acb3d98493d862c953aaff52099",
      "0083e0d64c0f4ac7a163678190a9c2ff",
      "7d3280993b5d4973baee8f4ff3f85996",
      "f983de53b3bb4a2da8a88008744c7b21",
      "5437d0aa677f44709eeef7de74a6ad95",
      "b7693f7a52864c97a73844f988f4da63",
      "a36c01c200b641a0bbca604db7f39b4d"
     ]
    },
    "id": "pxosTnFxSC9Y",
    "outputId": "336a3a3c-b61a-4a53-f3fb-5f9699adafc5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 5.2: Retrieval Analysis - Execution and Display (GitHub KBs, Simplified top_k)\n",
    "\n",
    "# Needed for highlighting if not imported globally for ANALYSIS_BM25_TOKENIZER\n",
    "\n",
    "# --- 1. Ensure Configurations from Cell 5.1 & Globals are available ---\n",
    "config_vars_s5_2_final_github = [\n",
    "    'ANALYSIS_SAMPLE_INDEX_WITHIN_REPO', 'ANALYSIS_TARGET_REPO_FULL_NAME',\n",
    "    'ANALYSIS_BM25_K1', 'ANALYSIS_BM25_B', 'ANALYSIS_TOP_K_SNIPPETS',\n",
    "    'ANALYSIS_BM25_TOKENIZER', 'ANALYSIS_SHOW_QUERY_TOKENS', 'ANALYSIS_HIGHLIGHT_KEYWORDS'\n",
    "]\n",
    "if any(var_name not in globals() for var_name in config_vars_s5_2_final_github):\n",
    "    raise NameError(\"One or more configuration variables from Cell 5.1 are not defined. Run Cell 5.1 first.\")\n",
    "\n",
    "global_vars_s5_2_final_github = ['lca_dataset_split', 'GITHUB_RAW_CONTENT_BASE_URL',\n",
    "                                 'LOCAL_TEMP_KB_DOWNLOAD_DIR', 'download_github_raw_json']\n",
    "if any(var_name not in globals() for var_name in global_vars_s5_2_final_github):\n",
    "    raise NameError(\"One or more global prerequisite variables (dataset, GitHub config, download helper) are not defined.\")\n",
    "\n",
    "print(f\"--- Section 5.2: Executing BM25 Retrieval Analysis (from GitHub KBs) ---\")\n",
    "\n",
    "# --- 2. Select Sample Data based on Configuration from Cell 5.1 ---\n",
    "s5_instruction_to_s6 = None\n",
    "s5_snippets_to_s6 = [] # Initialize for output to Section 6\n",
    "library_key_for_kb_file = None # This will be the 'repo_full_name' from dataset\n",
    "\n",
    "try:\n",
    "    if ANALYSIS_TARGET_REPO_FULL_NAME: # As defined in Cell 5.1\n",
    "        library_key_for_kb_file = ANALYSIS_TARGET_REPO_FULL_NAME\n",
    "        library_samples = lca_dataset_split.filter(lambda ex: ex['repo_full_name'] == ANALYSIS_TARGET_REPO_FULL_NAME)\n",
    "        if not library_samples:\n",
    "            raise ValueError(f\"No samples found for specified library (repo_full_name): '{ANALYSIS_TARGET_REPO_FULL_NAME}'.\")\n",
    "        if not (0 <= ANALYSIS_SAMPLE_INDEX_WITHIN_REPO < len(library_samples)): # ANALYSIS_SAMPLE_INDEX_WITHIN_REPO from Cell 5.1\n",
    "            raise IndexError(f\"ANALYSIS_SAMPLE_INDEX_WITHIN_REPO {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO} is out of bounds for library '{ANALYSIS_TARGET_REPO_FULL_NAME}'.\")\n",
    "        target_sample_data = library_samples[ANALYSIS_SAMPLE_INDEX_WITHIN_REPO]\n",
    "        print(f\"  Analyzing instruction #{ANALYSIS_SAMPLE_INDEX_WITHIN_REPO} from library: '{library_key_for_kb_file}'\")\n",
    "    else:\n",
    "        if not (0 <= ANALYSIS_SAMPLE_INDEX_WITHIN_REPO < len(lca_dataset_split)):\n",
    "            raise IndexError(f\"ANALYSIS_SAMPLE_INDEX_WITHIN_REPO {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO} is out of bounds for the full lca_dataset_split.\")\n",
    "        target_sample_data = lca_dataset_split[ANALYSIS_SAMPLE_INDEX_WITHIN_REPO]\n",
    "        library_key_for_kb_file = target_sample_data.get('repo_full_name') # Derive from sample\n",
    "        print(f\"  Analyzing sample at global index {ANALYSIS_SAMPLE_INDEX_WITHIN_REPO}. Derived Library Key: '{library_key_for_kb_file}'\")\n",
    "\n",
    "    s5_instruction_to_s6 = target_sample_data.get('instruction')\n",
    "    if not s5_instruction_to_s6 or not library_key_for_kb_file:\n",
    "        raise ValueError(\"Selected sample missing 'instruction' or 'repo_full_name' could not be determined.\")\n",
    "\n",
    "    print(f\"\\n  Target Library Key for KB: '{library_key_for_kb_file}'\") # This is the 'repo_full_name'\n",
    "    print(f\"  Instruction Text:\\n    {textwrap.fill(s5_instruction_to_s6, width=100, initial_indent='    ', subsequent_indent='    ')}\")\n",
    "\n",
    "    query_tokens = ANALYSIS_BM25_TOKENIZER(s5_instruction_to_s6) # ANALYSIS_BM25_TOKENIZER from Cell 5.1\n",
    "    if ANALYSIS_SHOW_QUERY_TOKENS: # From Cell 5.1\n",
    "        print(f\"\\n  Query Tokens (using '{ANALYSIS_BM25_TOKENIZER.__name__}'):\\n    {query_tokens}\")\n",
    "\n",
    "except Exception as e_sample_select:\n",
    "    print(f\"  ERROR during sample selection: {type(e_sample_select).__name__}: {e_sample_select}\")\n",
    "    library_key_for_kb_file = None # Prevent further processing if sample selection fails\n",
    "\n",
    "# --- 3. Load KB from GitHub, Build Index, and Perform BM25 Retrieval ---\n",
    "if library_key_for_kb_file: # Proceed only if library key was successfully determined\n",
    "    kb_filename_on_github = f\"kb_{library_key_for_kb_file}.json\"\n",
    "    raw_kb_url_s5 = f\"{GITHUB_RAW_CONTENT_BASE_URL}/{kb_filename_on_github}\" # GITHUB_RAW_CONTENT_BASE_URL from Sec4.Cell1 (GitHub config)\n",
    "\n",
    "    # Download to a subfolder within LOCAL_TEMP_KB_DOWNLOAD_DIR\n",
    "    temp_save_subdir_for_kb_s5 = os.path.join(LOCAL_TEMP_KB_DOWNLOAD_DIR, library_key_for_kb_file) # LOCAL_TEMP_KB_DOWNLOAD_DIR from Sec4.Cell1\n",
    "\n",
    "    print(f\"\\n  Attempting to download/load KB for '{library_key_for_kb_file}' from GitHub...\")\n",
    "    kb_data_from_git = download_github_raw_json( # download_github_raw_json helper function\n",
    "        raw_kb_url_s5,\n",
    "        temp_save_subdir_for_kb_s5,\n",
    "        kb_filename_on_github,\n",
    "        overwrite=True # For analysis, always get fresh from GitHub, or False to use local Colab cache\n",
    "    )\n",
    "\n",
    "    if not kb_data_from_git:\n",
    "        print(f\"  ERROR: Failed to download or parse KB for '{library_key_for_kb_file}' from GitHub.\")\n",
    "    else:\n",
    "        print(f\"  Successfully loaded KB for '{library_key_for_kb_file}' from GitHub ({len(kb_data_from_git)} snippets).\")\n",
    "        valid_kb_docs = [str(doc) for doc in kb_data_from_git if isinstance(doc, str) and str(doc).strip()]\n",
    "\n",
    "        if not valid_kb_docs:\n",
    "            print(\"  No valid string snippets in loaded KB for BM25 indexing.\")\n",
    "        else:\n",
    "            tokenized_corpus = [ANALYSIS_BM25_TOKENIZER(doc) for doc in valid_kb_docs]\n",
    "            final_bm25_corpus_docs, map_idx_bm25_to_valid_docs = [], []\n",
    "            for i, tokens in enumerate(tokenized_corpus):\n",
    "                if tokens:\n",
    "                    final_bm25_corpus_docs.append(tokens)\n",
    "                    map_idx_bm25_to_valid_docs.append(i)\n",
    "\n",
    "            if not final_bm25_corpus_docs:\n",
    "                print(\"  Tokenized KB is empty after filtering. BM25 index not built.\")\n",
    "            else:\n",
    "                print(f\"  Creating BM25 index from {len(final_bm25_corpus_docs)} processable documents...\")\n",
    "                # ANALYSIS_BM25_K1 and ANALYSIS_BM25_B are from Cell 5.1\n",
    "                bm25_index = BM25Okapi(final_bm25_corpus_docs, k1=ANALYSIS_BM25_K1, b=ANALYSIS_BM25_B)\n",
    "                print(\"  BM25 index built.\")\n",
    "\n",
    "                # ANALYSIS_TOP_K_SNIPPETS is from Cell 5.1\n",
    "                if query_tokens and ANALYSIS_TOP_K_SNIPPETS > 0:\n",
    "                    print(f\"\\n  --- Retrieving and Displaying Top {ANALYSIS_TOP_K_SNIPPETS} Snippets ---\")\n",
    "                    num_docs = len(final_bm25_corpus_docs)\n",
    "                    top_indices = bm25_index.get_top_n(\n",
    "                        query_tokens, list(range(num_docs)),\n",
    "                        n=min(ANALYSIS_TOP_K_SNIPPETS, num_docs)\n",
    "                    )\n",
    "                    s5_snippets_to_s6 = [valid_kb_docs[map_idx_bm25_to_valid_docs[i]] for i in top_indices]\n",
    "\n",
    "                    if not s5_snippets_to_s6:\n",
    "                        print(f\"    No snippets retrieved for top_k = {ANALYSIS_TOP_K_SNIPPETS}.\")\n",
    "                    else:\n",
    "                        for i_snip, snip_content in enumerate(s5_snippets_to_s6):\n",
    "                            print(f\"\\n    Snippet {i_snip+1}/{len(s5_snippets_to_s6)} (Length: {len(snip_content)} chars):\")\n",
    "                            if ANALYSIS_HIGHLIGHT_KEYWORDS: # From Cell 5.1\n",
    "                                hl_content = snip_content\n",
    "                                unique_qt = sorted(list(set(query_tokens)), key=len, reverse=True)\n",
    "                                for i_t, tkn in enumerate(unique_qt):\n",
    "                                    ph = f\"__HL_S5_{i_t}__\" # More specific placeholder\n",
    "                                    hl_content = re.sub(f\"\\\\b({re.escape(tkn)})\\\\b\", ph, hl_content, flags=re.IGNORECASE)\n",
    "                                for i_t, tkn in enumerate(unique_qt):\n",
    "                                    ph = f\"__HL_S5_{i_t}__\"\n",
    "                                    hl_content = hl_content.replace(ph, f\"<b style='background-color:#FFFACD; color:black; font-weight:bold;'>{tkn}</b>\")\n",
    "                                display(HTML(f\"<pre style='white-space:pre-wrap; word-wrap:break-word; border:1px dashed #ccc; padding:6px; margin-left:20px;'>{hl_content}</pre>\"))\n",
    "                            else:\n",
    "                                print(textwrap.indent(textwrap.fill(snip_content, width=100, subsequent_indent='      '), '      '))\n",
    "                    print(f\"\\n    Stored {len(s5_snippets_to_s6)} snippets in 's5_snippets_to_s6' for Section 6.\")\n",
    "                elif not query_tokens: print(\"  Query tokens empty. BM25 retrieval skipped.\")\n",
    "                else: print(f\"  ANALYSIS_TOP_K_SNIPPETS ({ANALYSIS_TOP_K_SNIPPETS}) is not positive. No snippets retrieved.\")\n",
    "else: # library_key_for_kb_file was None due to sample selection error\n",
    "    print(\"\\n  Sample selection failed earlier. Skipping KB loading and BM25 retrieval.\")\n",
    "\n",
    "if not s5_snippets_to_s6 and library_key_for_kb_file and ('kb_data_from_git' in locals() and kb_data_from_git is not None):\n",
    "    print(\"  INFO: No snippets were ultimately stored for Section 6 from this analysis.\")\n",
    "\n",
    "print(f\"\\n--- Section 5.2: Retrieval Analysis Execution Complete ---\")\n",
    "# Variables `s5_instruction_to_s6` and `s5_snippets_to_s6` are now populated for Section 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eltpUcq5U6mU"
   },
   "source": [
    "## Section 6: RAG Prompt Assembly for Demo Sample\n",
    "\n",
    "This section assembles the final RAG prompt for the LLM, using the instruction and retrieved snippets from the Section 5 analysis.\n",
    "\n",
    "It first sets up by selecting the desired `build_rag_prompt` function (from the globally defined prompt templates) and checks for its dependencies, like the LLM `tokenizer` if needed for snippet truncation.\n",
    "\n",
    "Then, it takes the instruction and retrieved context (snippets) provided by Section 5, formats the snippets into a text block, and calls the chosen `build_rag_prompt` function. The resulting complete prompt string is stored in `s6_final_rag_prompt_output` and a preview is displayed, making it ready for the LLM generation step in Section 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHHG4wb4R4K2",
    "outputId": "203ddda4-7159-4640-a231-6d7f40d7158e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "# --- 1. Ensure Prerequisite variables from Section 5 (5.2) are defined ---\n",
    "if 's5_instruction_to_s6' not in locals() or \\\n",
    "   's5_snippets_to_s6' not in locals():\n",
    "    raise NameError(\"Variables 's5_instruction_to_s6' or 's5_snippets_to_s6' not found. \"\n",
    "                    \"Ensure Section 5 (BM25 Retrieval Analysis - Execution and Display) has been run successfully.\")\n",
    "\n",
    "# --- 2. Select the RAG Prompt Builder Function ---\n",
    "build_rag_prompt_to_use_in_s6 = build_rag_prompt_v6_3 # Defaulting to v6 as an example\n",
    "\n",
    "# Verify that the chosen function is actually defined\n",
    "if 'build_rag_prompt_to_use_in_s6' not in locals() or not callable(build_rag_prompt_to_use_in_s6):\n",
    "    raise NameError(f\"The function assigned to 'build_rag_prompt_to_use_in_s6' is not defined or not callable. \"\n",
    "                    f\"Please check its definition and assignment in this cell.\")\n",
    "\n",
    "print(f\"INFO: Using '{build_rag_prompt_to_use_in_s6.__name__}' for RAG prompt assembly in this section.\")\n",
    "\n",
    "\n",
    "# --- 3. Check for Tokenizer Dependency if needed by the chosen prompt builder ---\n",
    "# Important if the chosen prompt builder (v5/v6) uses a helper like `truncate_to_n_tokens` which itself requires the LLM's tokenizer.\n",
    "TOKENIZER_NEEDED_BY_SELECTED_PROMPT_BUILDER = False\n",
    "# Heuristic: Check if 'truncate_to_n_tokens' is called by the selected builder.\n",
    "# This assumes 'truncate_to_n_tokens' is the name of the helper that needs the tokenizer.\n",
    "try:\n",
    "    func_code_object = getattr(build_rag_prompt_to_use_in_s6, '__code__', None)\n",
    "    if func_code_object and \"truncate_to_n_tokens\" in func_code_object.co_names:\n",
    "        TOKENIZER_NEEDED_BY_SELECTED_PROMPT_BUILDER = True\n",
    "        # Also, ensure 'truncate_to_n_tokens' itself is defined globally if it's called.\n",
    "        if \"truncate_to_n_tokens\" not in globals():\n",
    "            raise NameError(f\"'truncate_to_n_tokens' function is called by '{build_rag_prompt_to_use_in_s6.__name__}' \"\n",
    "                            \"but 'truncate_to_n_tokens' itself is not defined globally.\")\n",
    "except AttributeError:\n",
    "    # Could happen if build_rag_prompt_to_use_in_s6 is a lambda or other non-standard callable\n",
    "    # For simplicity, we'll assume if we can't inspect, it might not need it, or it will fail at runtime if it does and tokenizer is missing.\n",
    "    pass\n",
    "\n",
    "if TOKENIZER_NEEDED_BY_SELECTED_PROMPT_BUILDER:\n",
    "    if 'tokenizer' not in globals():\n",
    "        raise NameError(f\"LLM 'tokenizer' not defined globally, but it is required by the selected \"\n",
    "                        f\"prompt builder '{build_rag_prompt_to_use_in_s6.__name__}' (likely for snippet truncation). \"\n",
    "                        \"Please ensure the tokenizer is loaded in an earlier section (e.g., Section 2).\")\n",
    "    else:\n",
    "        print(f\"  INFO: Selected prompt builder '{build_rag_prompt_to_use_in_s6.__name__}' may use the global 'tokenizer'. Ensure 'tokenizer' is correctly loaded.\")\n",
    "else:\n",
    "    print(f\"  INFO: Selected prompt builder '{build_rag_prompt_to_use_in_s6.__name__}' does not appear to directly require the global 'tokenizer' for truncation via 'truncate_to_n_tokens'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eG0YOrVwuVvi",
    "outputId": "8535d59e-2b2f-45fe-f4fc-e1aa9bc45572",
    "tags": []
   },
   "outputs": [],
   "source": [
    "s6_final_rag_prompt_output = None # Initialize the output of this section\n",
    "\n",
    "# Prepare the prompt with the instruction\n",
    "if s5_instruction_to_s6 is None:\n",
    "    print(\"  ERROR: No instruction ('s5_instruction_to_s6') available from Section 5. Cannot assemble prompt.\")\n",
    "else:\n",
    "    print(f\"  Using instruction: '{s5_instruction_to_s6[:100]}...'\")\n",
    "    print(f\"  Using {len(s5_snippets_to_s6)} retrieved snippets from 's5_snippets_to_s6'.\")\n",
    "\n",
    "\n",
    "    # Prepare the prompt with the retrieved snippets (`build_rag_prompt` likely expects a single string, so they are joined by separator)\n",
    "    retrieved_snippets_as_text_block = \"\\n\\n# --- Snippet Separator ---\\n\\n\".join(s5_snippets_to_s6) \\\n",
    "                                       if s5_snippets_to_s6 else \"\"\n",
    "    if not s5_snippets_to_s6:\n",
    "        print(\"  INFO: No snippets were provided from Section 5 ('s5_snippets_to_s6' is empty). \"\n",
    "              \"The RAG prompt will be assembled with an empty retrieved context.\")\n",
    "\n",
    "    # --- Build the Prompt ---\n",
    "    try:\n",
    "        s6_final_rag_prompt_output = build_rag_prompt_to_use_in_s6(\n",
    "            s5_instruction_to_s6,\n",
    "            retrieved_snippets_as_text_block\n",
    "        )\n",
    "\n",
    "        if s6_final_rag_prompt_output:\n",
    "            print(\"\\n  Final RAG Prompt Assembled (First 700 Characters):\")\n",
    "            print(textwrap.shorten(s6_final_rag_prompt_output, width=700, placeholder=\"... (prompt truncated) ...\"))\n",
    "            # For the full prompt if needed for debugging:\n",
    "            # print(s6_final_rag_prompt_output)\n",
    "        else:\n",
    "             print(\"  ERROR: Prompt assembly using your 'build_rag_prompt' function \"\n",
    "                   \"resulted in an empty or None prompt. Please check the function's logic.\")\n",
    "    except Exception as e_build_prompt:\n",
    "        print(f\"  ERROR during prompt assembly with '{build_rag_prompt_to_use_in_s6.__name__}': {e_build_prompt}\")\n",
    "        s6_final_rag_prompt_output = None\n",
    "\n",
    "\n",
    "# The variable `s6_final_rag_prompt_output` now holds the complete RAG prompt string.\n",
    "if s6_final_rag_prompt_output:\n",
    "    print(f\"\\n--- Section 6: RAG Prompt Assembly Complete. Output in 's6_final_rag_prompt_output' (Length: {len(s6_final_rag_prompt_output)} chars) ---\")\n",
    "else:\n",
    "    print(f\"\\n--- Section 6: RAG Prompt Assembly Failed or Produced No Output ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3RCwduHR6iR"
   },
   "source": [
    "## Section 7: LLM Generation for RAG Demo Prompt\n",
    "\n",
    "This section generates code using the LLM for the demo RAG prompt assembled in Section 6.\n",
    "\n",
    "It first **configures LLM generation settings**, including global inference parameters and a custom stopping criteria class (`EosAndCodeEndStoppingCriteria`) to manage output length. This setup is done once.\n",
    "\n",
    "Subsequently, it **executes the generation**:\n",
    "*   Takes the RAG prompt from Section 6.\n",
    "*   Uses the pre-set configurations to call `model.generate()`.\n",
    "*   Decodes, cleans, and displays the LLM-generated code, storing the result in `s7_generated_code_rag_demo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GpVd94pKAtGr",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "import time\n",
    "import re\n",
    "import textwrap\n",
    "\n",
    "def generate_llm_code_and_clean(\n",
    "    prompt_text: str,\n",
    "    llm_model,\n",
    "    llm_tokenizer,\n",
    "    max_new_tokens_gen,\n",
    "    do_sample_gen,\n",
    "    temperature_gen,\n",
    "    top_p_gen,\n",
    "    top_k_gen,\n",
    "    repetition_penalty_gen,\n",
    "    stopping_criteria_list_gen, # Can be None\n",
    "    prompt_name: str = \"Prompt\" # For logging\n",
    "    ) -> str | None:\n",
    "    cleaned_generated_code = None\n",
    "    print(f\"  Starting LLM generation for: {prompt_name}\")\n",
    "    try:\n",
    "        inputs = llm_tokenizer(\n",
    "            prompt_text, return_tensors=\"pt\"\n",
    "        ).to(llm_model.device)\n",
    "        prompt_len = inputs['input_ids'].shape[1]\n",
    "        # print(f\"    Tokenized prompt length: {prompt_len} tokens.\") # Optional\n",
    "\n",
    "        gen_args = {\n",
    "            \"input_ids\": inputs['input_ids'], \"attention_mask\": inputs['attention_mask'],\n",
    "            \"max_new_tokens\": max_new_tokens_gen, \"pad_token_id\": llm_tokenizer.eos_token_id,\n",
    "            \"repetition_penalty\": repetition_penalty_gen, \"stopping_criteria\": stopping_criteria_list_gen\n",
    "        }\n",
    "        if do_sample_gen:\n",
    "            gen_args.update({\n",
    "                \"temperature\": temperature_gen, \"top_p\": top_p_gen,\n",
    "                \"top_k\": top_k_gen, \"do_sample\": True\n",
    "            })\n",
    "        else:\n",
    "            gen_args[\"do_sample\"] = False\n",
    "\n",
    "        gen_start = time.time()\n",
    "        with torch.no_grad(): output_ids = llm_model.generate(**gen_args)\n",
    "        print(f\"    LLM generation for '{prompt_name}' finished in {time.time() - gen_start:.2f}s.\")\n",
    "\n",
    "        generated_ids_part = output_ids[0, prompt_len:]\n",
    "        raw_output = llm_tokenizer.decode(generated_ids_part, skip_special_tokens=True)\n",
    "        # print(f\"\\n    Raw LLM Output for '{prompt_name}' (first 300 chars):\\n{textwrap.shorten(raw_output, 300, placeholder='...')}\") # Optional\n",
    "\n",
    "        # Using your preferred cleaning logic (can be the external function if defined)\n",
    "        # For simplicity, embedding it here. If you defined `extract_code_from_llm_output` earlier, call that.\n",
    "        match = re.search(r\"```python\\n(.*?)(?:\\n```|\\Z)\", raw_output, re.DOTALL | re.IGNORECASE)\n",
    "        if match: cleaned_generated_code = match.group(1).strip()\n",
    "        elif \"\\n```\" in raw_output: cleaned_generated_code = raw_output.split(\"\\n```\")[0].strip()\n",
    "        else: cleaned_generated_code = raw_output.strip()\n",
    "\n",
    "        # print(f\"    Cleaned code for '{prompt_name}' (first 300 chars):\\n{textwrap.shorten(cleaned_generated_code, 300, placeholder='...') if cleaned_generated_code else '[No code extracted]'}\")\n",
    "\n",
    "    except torch.cuda.OutOfMemoryError as e: cleaned_generated_code = None; print(f\"  ❌ OOM ERROR during '{prompt_name}' generation: {e}\")\n",
    "    except Exception as e: cleaned_generated_code = None; print(f\"  ❌ ERROR during '{prompt_name}' generation: {type(e).__name__}: {e}\")\n",
    "\n",
    "    return cleaned_generated_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqOSW9JBR6Fc",
    "outputId": "8873004b-2f68-4ea5-ce98-87e07eca57fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = globals().get('MAX_NEW_TOKENS', 384)\n",
    "TEMPERATURE = globals().get('TEMPERATURE', 0.6)\n",
    "TOP_P = globals().get('TOP_P', 0.95)\n",
    "TOP_K = globals().get('TOP_K', 50)\n",
    "REPETITION_PENALTY = globals().get('REPETITION_PENALTY', 1.1)\n",
    "DO_SAMPLE = globals().get('DO_SAMPLE', True)\n",
    "STOP_ON_EOS = globals().get('STOP_ON_EOS', True)           # For custom stopping criteria\n",
    "STOP_ON_CODE_END = globals().get('STOP_ON_CODE_END', True) # For custom stopping criteria\n",
    "\n",
    "print(f\"  Global LLM Generation Parameters now set/confirmed:\")\n",
    "print(f\"    MAX_NEW_TOKENS={MAX_NEW_TOKENS}, TEMPERATURE={TEMPERATURE if DO_SAMPLE else 'N/A (Greedy)'}\")\n",
    "print(f\"    TOP_P={TOP_P if DO_SAMPLE else 'N/A'}, TOP_K={TOP_K if DO_SAMPLE else 'N/A'}\")\n",
    "print(f\"    REPETITION_PENALTY={REPETITION_PENALTY}, DO_SAMPLE={DO_SAMPLE}\")\n",
    "print(f\"    Custom Stopping: STOP_ON_EOS={STOP_ON_EOS}, STOP_ON_CODE_END={STOP_ON_CODE_END}\")\n",
    "\n",
    "# --- 2. Define Custom Stopping Criteria Class (if not already globally defined) ---\n",
    "if 'EosAndCodeEndStoppingCriteria' not in globals(): # Define only if not already defined\n",
    "    class EosAndCodeEndStoppingCriteria(StoppingCriteria):\n",
    "        \"\"\"Stops generation on EOS token or a specific code-ending sequence.\"\"\"\n",
    "        def __init__(self, tokenizer_instance, stop_on_eos_token=True, code_end_sequence=\"\\n```\\n\"):\n",
    "            self.tokenizer = tokenizer_instance\n",
    "            self.stop_on_eos = stop_on_eos_token\n",
    "            self.code_end_sequence_str = code_end_sequence\n",
    "            self.code_end_sequence_ids = []\n",
    "            if self.code_end_sequence_str and self.tokenizer: # Ensure tokenizer is valid\n",
    "                try:\n",
    "                    self.code_end_sequence_ids = self.tokenizer.encode(\n",
    "                        self.code_end_sequence_str,\n",
    "                        add_special_tokens=False\n",
    "                    )\n",
    "                except Exception as e_encode:\n",
    "                    print(f\"    WARNING: Failed to encode stop sequence '{self.code_end_sequence_str}': {e_encode}\")\n",
    "                    self.code_end_sequence_ids = [] # Ensure it's empty on failure\n",
    "\n",
    "        def __call__(self, current_ids: torch.LongTensor, current_scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "            if self.stop_on_eos and self.tokenizer and hasattr(self.tokenizer, 'eos_token_id') and self.tokenizer.eos_token_id is not None:\n",
    "                if current_ids[0, -1] == self.tokenizer.eos_token_id:\n",
    "                    return True\n",
    "            if self.code_end_sequence_ids: # Only check if sequence IDs were successfully created\n",
    "                seq_len = len(self.code_end_sequence_ids)\n",
    "                if current_ids.shape[1] >= seq_len:\n",
    "                    if torch.equal(current_ids[0, -seq_len:], torch.tensor(self.code_end_sequence_ids).to(current_ids.device)):\n",
    "                        return True\n",
    "            return False\n",
    "    print(\"  Custom 'EosAndCodeEndStoppingCriteria' class defined.\")\n",
    "else:\n",
    "    print(\"  Custom 'EosAndCodeEndStoppingCriteria' class already defined.\")\n",
    "\n",
    "\n",
    "# --- 3. Instantiate Global Stopping Criteria List ---\n",
    "# This `llm_stopping_criteria_global` will be used by all generation calls needing these criteria.\n",
    "llm_stopping_criteria_global = None\n",
    "if 'tokenizer' not in globals() or tokenizer is None: # Check if global 'tokenizer' is loaded\n",
    "    print(\"  WARNING (Cell 7.1): Global 'tokenizer' not found or is None. \"\n",
    "          \"Custom stopping criteria cannot be created. LLM will use default stopping.\")\n",
    "elif STOP_ON_EOS or STOP_ON_CODE_END: # Only create if flags are true\n",
    "    try:\n",
    "        # Ensure EosAndCodeEndStoppingCriteria is defined before calling it\n",
    "        if 'EosAndCodeEndStoppingCriteria' not in globals():\n",
    "             raise NameError(\"EosAndCodeEndStoppingCriteria class not defined prior to instantiation.\")\n",
    "\n",
    "        eos_code_ender_criteria_instance = EosAndCodeEndStoppingCriteria(\n",
    "            tokenizer, # Use the globally loaded LLM tokenizer\n",
    "            stop_on_eos_token=STOP_ON_EOS,\n",
    "            code_end_sequence=\"\\n```\\n\" if STOP_ON_CODE_END else None\n",
    "        )\n",
    "        llm_stopping_criteria_global = StoppingCriteriaList([eos_code_ender_criteria_instance])\n",
    "        print(\"  Global 'llm_stopping_criteria_global' (for EOS/code end) created successfully.\")\n",
    "    except Exception as e_stop_crit_create:\n",
    "        print(f\"  WARNING (Cell 7.1): Failed to create global custom stopping criteria: {type(e_stop_crit_create).__name__}: {e_stop_crit_create}\")\n",
    "        llm_stopping_criteria_global = None # Ensure it's None on failure\n",
    "else:\n",
    "    print(\"  Global custom stopping criteria (EOS/code end) are disabled by configuration flags (STOP_ON_EOS/STOP_ON_CODE_END).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HiiNEkPX9nRT",
    "outputId": "ca6f8afc-63c3-4b36-c18e-4ae4c58be816",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 's6_final_rag_prompt_output' not in locals() or not s6_final_rag_prompt_output:\n",
    "    raise NameError(\"Input prompt 's6_final_rag_prompt_output' from Section 6 not found. Run Section 6 first.\")\n",
    "if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "    raise NameError(\"LLM 'model' or 'tokenizer' not defined. Ensure Section 2 has run.\")\n",
    "if 'generate_llm_code_and_clean' not in locals(): # Helper defined before 7.1\n",
    "    raise NameError(\"Helper function 'generate_llm_code_and_clean' not defined. Ensure it's defined before Section 7.1.\")\n",
    "\n",
    "# Ensure global generation params and llm_stopping_criteria_global are available from Cell 7.1\n",
    "required_globals_for_7_2 = [\n",
    "    'MAX_NEW_TOKENS', 'DO_SAMPLE', 'TEMPERATURE', 'TOP_P', 'TOP_K',\n",
    "    'REPETITION_PENALTY', 'llm_stopping_criteria_global' # Note: llm_stopping_criteria_global can be None\n",
    "]\n",
    "if any(p not in globals() for p in required_globals_for_7_2):\n",
    "    raise NameError(f\"One or more global LLM generation parameters/criteria from Cell 7.1 are missing.\")\n",
    "\n",
    "print(f\"  Using RAG prompt (length: {len(s6_final_rag_prompt_output)} chars) from Section 6.\")\n",
    "\n",
    "# --- 2. Call Reusable LLM Generation Function ---\n",
    "s7_generated_code_rag_demo = generate_llm_code_and_clean(\n",
    "    prompt_text=s6_final_rag_prompt_output,\n",
    "    llm_model=model,\n",
    "    llm_tokenizer=tokenizer,\n",
    "    max_new_tokens_gen=MAX_NEW_TOKENS,         # Global param\n",
    "    do_sample_gen=DO_SAMPLE,                  # Global param\n",
    "    temperature_gen=TEMPERATURE,              # Global param\n",
    "    top_p_gen=TOP_P,                          # Global param\n",
    "    top_k_gen=TOP_K,                          # Global param\n",
    "    repetition_penalty_gen=REPETITION_PENALTY,# Global param\n",
    "    stopping_criteria_list_gen=llm_stopping_criteria_global, # From Cell 7.1 (can be None)\n",
    "    prompt_name=\"RAG Demo Prompt (S7.2)\"      # For logging within the helper\n",
    ")\n",
    "\n",
    "# --- 3. Display Result ---\n",
    "if s7_generated_code_rag_demo is not None: # Check if helper returned code (not None for error)\n",
    "    print(\"\\n  --- Cleaned Generated RAG Code (Demo) ---\")\n",
    "    # Displaying a significant portion for review\n",
    "    # print(textwrap.shorten(s7_generated_code_rag_demo, width=1000, placeholder=\"... (cleaned code truncated for display) ...\"))\n",
    "    # To print the entire generated code if needed:\n",
    "    print(\"\\n  Full Cleaned Generated RAG Code (Demo):\\n\", s7_generated_code_rag_demo)\n",
    "    print(f\"\\n--- RAG Demo Generation Complete. Result in 's7_generated_code_rag_demo'. ---\")\n",
    "else:\n",
    "    print(f\"\\n--- RAG Demo Generation Failed or Produced No Valid Code Output. 's7_generated_code_rag_demo' is None. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0TPBQavR-OM"
   },
   "source": [
    "## Section 8: Baseline LLM Code Generation\n",
    "\n",
    "This section generates code using the LLM for the **baseline prompt** corresponding to the same demo sample instruction analyzed in Section 5 (`s5_instruction_to_s6`). It does **not** use any retrieved RAG context.\n",
    "\n",
    "**Workflow:**\n",
    "1.  **Prompt Builder Selection:** Chooses the appropriate `build_baseline_prompt_vX` function.\n",
    "2.  **Baseline Prompt Construction:** Creates the baseline prompt using the demo instruction.\n",
    "3.  **LLM Generation:** Calls the reusable `generate_llm_code_and_clean` helper function with the baseline prompt. It uses the same global LLM generation parameters and stopping criteria as defined in Section 7.1 for consistency in comparing RAG vs. Baseline outputs.\n",
    "4.  **Output:** Displays the cleaned baseline-generated code and stores it in `s8_generated_code_baseline_demo`.\n",
    "\n",
    "This allows for a direct comparison between the RAG-augmented output (from Section 7) and the LLM's output with only the instruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oWWgseLgR_Xd",
    "outputId": "3ea7c782-6c76-4e32-ebeb-200284e8c6d8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Ensure Prerequisite variables and functions are defined ---\n",
    "# From Section 5:\n",
    "if 's5_instruction_to_s6' not in locals() or not s5_instruction_to_s6:\n",
    "    raise NameError(\"Instruction 's5_instruction_to_s6' from Section 5 not found. Run Section 5 first.\")\n",
    "# From earlier sections (or Section 7.1):\n",
    "if 'model' not in locals() or 'tokenizer' not in locals():\n",
    "    raise NameError(\"LLM 'model' or 'tokenizer' not defined.\")\n",
    "if 'generate_llm_code_and_clean' not in locals():\n",
    "    raise NameError(\"Helper function 'generate_llm_code_and_clean' not defined.\")\n",
    "# Ensure global generation params (MAX_NEW_TOKENS, etc.) and llm_stopping_criteria_global are set (typically in Sec 7.1)\n",
    "required_gen_params_s8 = ['MAX_NEW_TOKENS', 'REPETITION_PENALTY', 'DO_SAMPLE', 'TEMPERATURE', 'TOP_P', 'TOP_K', 'llm_stopping_criteria_global']\n",
    "if any(p not in globals() for p in required_gen_params_s8):\n",
    "    raise NameError(f\"One or more global LLM generation parameters/criteria needed for baseline are missing. Run Section 7.1.\")\n",
    "\n",
    "# --- 2. Select the Baseline Prompt Builder Function ---\n",
    "CHOSEN_BASELINE_PROMPT_BUILDER = build_baseline_prompt_v6_3\n",
    "\n",
    "if 'CHOSEN_BASELINE_PROMPT_BUILDER' not in locals() or not callable(CHOSEN_BASELINE_PROMPT_BUILDER):\n",
    "    if 'build_baseline_prompt_v1' in globals(): # Generic name\n",
    "        CHOSEN_BASELINE_PROMPT_BUILDER = build_baseline_prompt_v1\n",
    "    elif 'build_baseline_prompt_v6' in globals(): # Specific version\n",
    "        CHOSEN_BASELINE_PROMPT_BUILDER = build_baseline_prompt_v6\n",
    "    else:\n",
    "        raise NameError(\"A suitable 'build_baseline_prompt_vX' or 'build_baseline_prompt_v1' function is not defined.\")\n",
    "print(f\"INFO: Using Baseline Prompt Builder: {CHOSEN_BASELINE_PROMPT_BUILDER.__name__}\")\n",
    "\n",
    "# --- 3. Construct Baseline Prompt ---\n",
    "s8_generated_code_baseline_demo = None # Initialize output\n",
    "\n",
    "# Using s5_instruction_to_s6 (the instruction for the demo sample from Section 5)\n",
    "baseline_prompt_s8 = CHOSEN_BASELINE_PROMPT_BUILDER(s5_instruction_to_s6)\n",
    "\n",
    "# print(\"\\n  Baseline Prompt (first 500 chars):\")\n",
    "# print(textwrap.shorten(baseline_prompt_s8, width=500, placeholder=\"...\"))\n",
    "\n",
    "# --- 4. Call Reusable LLM Generation Function ---\n",
    "s8_generated_code_baseline_demo = generate_llm_code_and_clean(\n",
    "    prompt_text=baseline_prompt_s8,\n",
    "    llm_model=model,\n",
    "    llm_tokenizer=tokenizer,\n",
    "    max_new_tokens_gen=MAX_NEW_TOKENS, # Global param\n",
    "    do_sample_gen=DO_SAMPLE,           # Global param\n",
    "    temperature_gen=TEMPERATURE,       # Global param\n",
    "    top_p_gen=TOP_P,                   # Global param\n",
    "    top_k_gen=TOP_K,                   # Global param\n",
    "    repetition_penalty_gen=REPETITION_PENALTY, # Global param\n",
    "    stopping_criteria_list_gen=llm_stopping_criteria_global, # From Sec 7.1\n",
    "    prompt_name=\"Baseline Demo Prompt\"\n",
    ")\n",
    "\n",
    "if s8_generated_code_baseline_demo is not None:\n",
    "    print(\"\\n  --- Cleaned Generated Baseline Code (Demo) ---\")\n",
    "    # print(textwrap.shorten(s8_generated_code_baseline_demo, width=700, placeholder=\"... (code truncated) ...\"))\n",
    "    print(\"\\n  Full Cleaned Generated RAG Code (Demo):\\n\", s8_generated_code_baseline_demo)\n",
    "    print(f\"\\n--- Baseline Demo Generation Complete. Result in 's8_generated_code_baseline_demo'. ---\")\n",
    "else:\n",
    "    print(f\"\\n--- Baseline Demo Generation Failed or No Output. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLbYVy06SGS5"
   },
   "source": [
    "## Section 9 · Metrics Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dubtt5LHdYHJ",
    "outputId": "bb7bb953-0a1a-4cd6-82ab-a6115cab3eae",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re # For the tokenizer if defined here again\n",
    "\n",
    "# --- 1. Number of Samples for Evaluation ---\n",
    "# NUM_EXAMPLES_TO_EVALUATE = len(lca_dataset_split) if 'lca_dataset_split' in globals() else 10 # Evaluate all (or by defaul 10)\n",
    "NUM_EXAMPLES_TO_EVALUATE = 3  # Subset for a quicker test run\n",
    "\n",
    "# --- 2. Select Prompt Builders for Evaluation (Defined Globally) ---\n",
    "EVAL_RAG_PROMPT_BUILDER = build_rag_prompt_v6 # RAG Prompt\n",
    "EVAL_BASELINE_PROMPT_BUILDER = build_baseline_prompt_v6 # Baseline Prompt\n",
    "\n",
    "print(f\"  Using RAG Prompt Builder for Evaluation: {EVAL_RAG_PROMPT_BUILDER.__name__}\")\n",
    "print(f\"  Using Baseline Prompt Builder for Evaluation: {EVAL_BASELINE_PROMPT_BUILDER.__name__}\")\n",
    "\n",
    "\n",
    "# --- 3. BM25 Parameters for Evaluation ---\n",
    "EVAL_BM25_K1 = 1.5\n",
    "EVAL_BM25_B = 0.75\n",
    "EVAL_TOP_K_SNIPPETS_FOR_PROMPT = 3 # Adjust this to select the number of snippets to retrieve\n",
    "\n",
    "# Define/Select the BM25 tokenizer for evaluation\n",
    "def eval_robust_code_tokenizer_s9(text_input: str) -> list[str]:\n",
    "    if not isinstance(text_input, str): \n",
    "        return []\n",
    "    \n",
    "    text = text_input.lower()\n",
    "    raw_tokens = re.split(r'[^a-z0-9_.]+', text)\n",
    "    \n",
    "    processed_tokens = []\n",
    "    for token in raw_tokens:\n",
    "        if not token:\n",
    "            continue\n",
    "        \n",
    "        sub_tokens = token.split('.')\n",
    "        for sub_token in sub_tokens:\n",
    "            if not sub_token:\n",
    "                continue\n",
    "            if len(sub_token) > 1 and not sub_token.isdigit():\n",
    "                processed_tokens.append(sub_token)\n",
    "            elif len(sub_token) == 1 and sub_token.isalpha():\n",
    "                 processed_tokens.append(sub_token)\n",
    "                \n",
    "    return processed_tokens\n",
    "\n",
    "EVAL_BM25_TOKENIZER = eval_robust_code_tokenizer_s9\n",
    "\n",
    "print(f\"  BM25 Params for Evaluation: K1={EVAL_BM25_K1}, B={EVAL_BM25_B}, Top-K for prompt={EVAL_TOP_K_SNIPPETS_FOR_PROMPT}\")\n",
    "print(f\"  BM25 Tokenizer for Evaluation: {EVAL_BM25_TOKENIZER.__name__}\")\n",
    "\n",
    "\n",
    "# --- 4. LLM Generation Parameters for Evaluation ---\n",
    "# LLM parameters\n",
    "EVAL_MAX_NEW_TOKENS = 524\n",
    "EVAL_TEMPERATURE = 0.5  # Potentially more deterministic for evaluation\n",
    "EVAL_TOP_P = 0.95\n",
    "EVAL_TOP_K = 50\n",
    "EVAL_REPETITION_PENALTY = 1.1\n",
    "EVAL_DO_SAMPLE = True    # Set to False for deterministic greedy decoding during evaluation if preferred\n",
    "\n",
    "# Stopping criteria\n",
    "EVAL_STOP_ON_EOS = True\n",
    "EVAL_STOP_ON_CODE_END = True\n",
    "\n",
    "eval_llm_stopping_criteria = None\n",
    "if 'tokenizer' not in globals() or tokenizer is None:\n",
    "    print(\"  WARNING: Global 'tokenizer' not found. Custom stopping criteria for evaluation cannot be created.\")\n",
    "elif 'EosAndCodeEndStoppingCriteria' not in globals():\n",
    "    print(\"  WARNING: 'EosAndCodeEndStoppingCriteria' class not defined (expected from Sec 7.1). Cannot create custom stopping criteria.\")\n",
    "elif EVAL_STOP_ON_EOS or EVAL_STOP_ON_CODE_END:\n",
    "    try:\n",
    "        eval_stopper_instance = EosAndCodeEndStoppingCriteria( # Class from 7.1\n",
    "            tokenizer, # Global tokenizer\n",
    "            stop_on_eos_token=EVAL_STOP_ON_EOS,\n",
    "            code_end_sequence=\"\\n```\\n\" if EVAL_STOP_ON_CODE_END else None\n",
    "        )\n",
    "        eval_llm_stopping_criteria = StoppingCriteriaList([eval_stopper_instance])\n",
    "        print(\"  Custom 'eval_llm_stopping_criteria' for evaluation run created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  WARNING: Failed to create custom stopping criteria for evaluation: {e}\")\n",
    "else:\n",
    "    print(\"  Custom stopping criteria (EOS/code end) for evaluation are disabled by EVAL_ flags.\")\n",
    "\n",
    "print(f\"  LLM Config: MaxNew={EVAL_MAX_NEW_TOKENS}, Temp={EVAL_TEMPERATURE if EVAL_DO_SAMPLE else 'N/A'}, DoSample={EVAL_DO_SAMPLE}, StopCriteriaIsSet={'Yes' if eval_llm_stopping_criteria else 'No'}\")\n",
    "\n",
    "\n",
    "# --- 5. Verify other critical dependencies for Cell 9.2 ---\n",
    "critical_deps_for_9_2 = [\n",
    "    'model', 'generate_llm_code_and_clean',\n",
    "    'GITHUB_RAW_CONTENT_BASE_URL', 'LOCAL_TEMP_KB_DOWNLOAD_DIR', 'download_github_raw_json'\n",
    "]\n",
    "if any(dep not in globals() for dep in critical_deps_for_9_2):\n",
    "    raise NameError(f\"One or more critical global dependencies for Cell 9.2 are missing: {critical_deps_for_9_2}. \"\n",
    "                    \"Ensure all prior setup sections (LLM loading, GitHub config, helper functions) have run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "323930aba6d248c4bc368c8c26e015f1",
      "8a1cbf1636da45729290e8e280b2078f",
      "08a3c06eb1244d878fea4140143dcfaf",
      "2436bc8979ca400082a11aa867802522",
      "a2c796d746954971899bd64255ff68bc",
      "b166cc1fb249432688cefb8480548a2e",
      "2bb2241e8c7a4b3c9b68713b1bb1b8db",
      "f7c9c0e335424ea98e6096f67cfa5825",
      "69fbf1d625be4b1c9bd667afce922e44",
      "40e6d4b7b07e4afc8b5ac70c7495aaa7",
      "95b45e18f9744582a4f27017a95be7be"
     ]
    },
    "id": "Mu7JiE23FMNp",
    "outputId": "87f4d5a4-dd2e-47e2-dcf8-a951a30ce248",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- 1. Verify Essential Configurations & Dependencies (Quick Check) ---\n",
    "required_globals_for_eval = [\n",
    "    'lca_dataset_split', 'EVAL_RAG_PROMPT_BUILDER', 'EVAL_BASELINE_PROMPT_BUILDER',\n",
    "    'EVAL_BM25_TOKENIZER', 'EVAL_BM25_K1', 'EVAL_BM25_B', 'EVAL_TOP_K_SNIPPETS_FOR_PROMPT',\n",
    "    'GITHUB_RAW_CONTENT_BASE_URL', 'LOCAL_TEMP_KB_DOWNLOAD_DIR', 'download_github_raw_json',\n",
    "    'model', 'tokenizer', 'generate_llm_code_and_clean', 'NUM_EXAMPLES_TO_EVALUATE',\n",
    "    'EVAL_MAX_NEW_TOKENS', 'EVAL_TEMPERATURE', 'EVAL_TOP_P', 'EVAL_TOP_K',\n",
    "    'EVAL_REPETITION_PENALTY', 'EVAL_DO_SAMPLE', 'eval_llm_stopping_criteria' # Can be None\n",
    "]\n",
    "if any(var not in globals() for var in required_globals_for_eval):\n",
    "    raise NameError(\"One or more critical configurations or dependencies for the evaluation loop are missing. \"\n",
    "                    \"Ensure Cell 9.1 and all preceding setup cells (dataset, model, helpers) have been run.\")\n",
    "\n",
    "\n",
    "print(f\"\\n--- Starting Main Evaluation Loop, targeting {NUM_EXAMPLES_TO_EVALUATE} samples. ---\")\n",
    "# --- 2. Initialize Output Lists & BM25 Cache ---\n",
    "eval_all_baseline_outputs, eval_all_rag_outputs = [], []\n",
    "eval_all_references, eval_all_reference_apis    = [], []\n",
    "eval_bm25_index_cache = {} # {library_key: (bm25_index, valid_docs_list, map_bm25_to_valid_idx_list)}\n",
    "\n",
    "# --- 3. Prepare Dataset View ---\n",
    "global dataset_for_eval\n",
    "dataset_for_eval = lca_dataset_split.select(\n",
    "    range(min(NUM_EXAMPLES_TO_EVALUATE, len(lca_dataset_split)))\n",
    ")\n",
    "num_samples_to_process = len(dataset_for_eval)\n",
    "\n",
    "# --- 4. Main Evaluation Loop ---\n",
    "for idx, sample_data in enumerate(tqdm(dataset_for_eval, desc=\"⚙️ Evaluating\", unit=\"sample\")):\n",
    "    instruction = sample_data.get(\"instruction\")\n",
    "    repo_key = sample_data.get('repo_full_name') # Used for KB lookup and logging\n",
    "\n",
    "    # Initialize per-sample outputs to ensure lists stay synchronized on error\n",
    "    baseline_code = \"\"\n",
    "    rag_code = \"\"\n",
    "\n",
    "    if not instruction or not repo_key:\n",
    "        print(f\"  Skipping sample {idx} due to missing instruction or repo_key.\")\n",
    "    else:\n",
    "        # --- A. Baseline Generation ---\n",
    "        try:\n",
    "            baseline_prompt = EVAL_BASELINE_PROMPT_BUILDER(instruction)\n",
    "            baseline_code = generate_llm_code_and_clean(\n",
    "                prompt_text=baseline_prompt, llm_model=model, llm_tokenizer=tokenizer,\n",
    "                max_new_tokens_gen=EVAL_MAX_NEW_TOKENS, do_sample_gen=EVAL_DO_SAMPLE,\n",
    "                temperature_gen=EVAL_TEMPERATURE, top_p_gen=EVAL_TOP_P, top_k_gen=EVAL_TOP_K,\n",
    "                repetition_penalty_gen=EVAL_REPETITION_PENALTY,\n",
    "                stopping_criteria_list_gen=eval_llm_stopping_criteria,\n",
    "                prompt_name=f\"Base_{idx}_{repo_key}\"\n",
    "            )\n",
    "        except Exception as e_base:\n",
    "            print(f\"  Error during Baseline for sample {idx} ({repo_key}): {type(e_base).__name__}\")\n",
    "\n",
    "        # --- B. RAG Generation ---\n",
    "        retrieved_context_str = \"\"\n",
    "        try:\n",
    "            if repo_key in eval_bm25_index_cache:\n",
    "                bm25_idx, valid_docs, map_to_valid = eval_bm25_index_cache[repo_key]\n",
    "            else:\n",
    "                kb_file = f\"kb_{repo_key}.json\"\n",
    "                kb_url = f\"{GITHUB_RAW_CONTENT_BASE_URL}/{kb_file}\"\n",
    "                temp_kb_dir = os.path.join(LOCAL_TEMP_KB_DOWNLOAD_DIR, f\"eval_kb_{repo_key}\")\n",
    "                kb_json = download_github_raw_json(kb_url, temp_kb_dir, kb_file, overwrite=False)\n",
    "\n",
    "                bm25_idx, valid_docs, map_to_valid, tokenized_corpus_for_index = (None, [], [], []) # Defaults\n",
    "                if kb_json and isinstance(kb_json, list):\n",
    "                    valid_docs = [str(d) for d in kb_json if isinstance(d, str) and str(d).strip()]\n",
    "                    if valid_docs:\n",
    "                        tokenized_kb = [EVAL_BM25_TOKENIZER(doc) for doc in valid_docs]\n",
    "                        for i_map, toks in enumerate(tokenized_kb):\n",
    "                            if toks: tokenized_corpus_for_index.append(toks); map_to_valid.append(i_map)\n",
    "                        if tokenized_corpus_for_index:\n",
    "                            bm25_idx = BM25Okapi(tokenized_corpus_for_index, k1=EVAL_BM25_K1, b=EVAL_BM25_B)\n",
    "                            eval_bm25_index_cache[repo_key] = (bm25_idx, valid_docs, map_to_valid, tokenized_corpus_for_index)\n",
    "\n",
    "            if bm25_idx:\n",
    "                query_toks = EVAL_BM25_TOKENIZER(instruction)\n",
    "                if query_toks:\n",
    "                    num_idx_docs = len(bm25_idx.doc_len)\n",
    "                    top_idxs = bm25_idx.get_top_n(query_toks, list(range(num_idx_docs)),\n",
    "                                                  n=min(EVAL_TOP_K_SNIPPETS_FOR_PROMPT, num_idx_docs))\n",
    "                    retrieved_list = [valid_docs[map_to_valid[i]] for i in top_idxs]\n",
    "                    retrieved_context_str = \"\\n\\n# --- Snippet ---\\n\\n\".join(retrieved_list)\n",
    "\n",
    "            # --- Assemble RAG Prompt ---\n",
    "            if 'tokenizer' not in globals() and hasattr(EVAL_RAG_PROMPT_BUILDER, '__code__') and \"truncate_to_n_tokens\" in EVAL_RAG_PROMPT_BUILDER.__code__.co_names:\n",
    "                 print(f\"  WARNING: Tokenizer needed by {EVAL_RAG_PROMPT_BUILDER.__name__} but not global. Truncation may fail.\")\n",
    "\n",
    "            rag_prompt_text = EVAL_RAG_PROMPT_BUILDER(instruction, retrieved_context_str)\n",
    "\n",
    "            rag_code = generate_llm_code_and_clean(\n",
    "                prompt_text=rag_prompt_text, llm_model=model, llm_tokenizer=tokenizer,\n",
    "                max_new_tokens_gen=EVAL_MAX_NEW_TOKENS, do_sample_gen=EVAL_DO_SAMPLE,\n",
    "                temperature_gen=EVAL_TEMPERATURE, top_p_gen=EVAL_TOP_P, top_k_gen=EVAL_TOP_K,\n",
    "                repetition_penalty_gen=EVAL_REPETITION_PENALTY,\n",
    "                stopping_criteria_list_gen=eval_llm_stopping_criteria,\n",
    "                prompt_name=f\"RAG_Eval_{idx}_{repo_key}\"\n",
    "            )\n",
    "        except Exception as e_rag:\n",
    "            print(f\"  Error during RAG for sample {idx} ({repo_key}): {type(e_rag).__name__}\")\n",
    "\n",
    "    # --- Append results for this sample ---\n",
    "    eval_all_baseline_outputs.append(baseline_code or \"\")\n",
    "    eval_all_rag_outputs.append(rag_code or \"\")\n",
    "    eval_all_references.append(sample_data.get(\"clean_reference\", \"\"))\n",
    "    eval_all_reference_apis.append(sample_data.get(\"unique_apis\", []))\n",
    "\n",
    "# --- Final Sanity Check of List Lengths ---\n",
    "if not (len(eval_all_baseline_outputs) == len(eval_all_rag_outputs) == \\\n",
    "        len(eval_all_references) == len(eval_all_reference_apis) == num_samples_to_process):\n",
    "    print(\"\\n❌ CRITICAL ERROR: Length mismatch in final evaluation output lists!\")\n",
    "else:\n",
    "    print(f\"\\n✅ Successfully collected all outputs for {num_samples_to_process} evaluation examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZSe7GvrtkcD",
    "outputId": "5a2fde2c-66ec-4071-a268-c1190561014b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sacrebleu # Ensure sacrebleu is imported if not already globally\n",
    "import re\n",
    "import warnings\n",
    "import importlib.metadata as md # For version checking\n",
    "import traceback # For more detailed error info\n",
    "\n",
    "# --- Initialize Sacrebleu & CodeBLEU ---\n",
    "_HAS_CODEBLEU_METRIC_UTIL = False\n",
    "try:\n",
    "    sbleu_version_util = md.version(\"sacrebleu\")\n",
    "    print(f\" Sacrebleu imported successfully for utilities (Version: {sbleu_version_util}).\")\n",
    "except md.PackageNotFoundError:\n",
    "    print(\"Sacrebleu not found via importlib.metadata. Ensure it's installed.\")\n",
    "    sbleu_version_util = \"n/a\"\n",
    "\n",
    "try:\n",
    "    from codebleu import calc_codebleu\n",
    "    cb_version_util = md.version(\"codebleu\")\n",
    "    print(f\" CodeBLEU imported successfully for utilities (Version: {cb_version_util}).\")\n",
    "    _HAS_CODEBLEU_METRIC_UTIL = True\n",
    "except ImportError:\n",
    "    print(\" CodeBLEU import failed in utilities. CodeBLEU scores will be skipped or result in None/0.\")\n",
    "    def calc_codebleu(*args, **kwargs): # Dummy function\n",
    "        warnings.warn(\"calc_codebleu called, but CodeBLEU library is not available or failed to import.\")\n",
    "        return {}\n",
    "except md.PackageNotFoundError:\n",
    "    print(\"CodeBLEU seems imported but version not found via importlib.metadata.\")\n",
    "    _HAS_CODEBLEU_METRIC_UTIL = True # Assume import was fine\n",
    "\n",
    "# --- Globals for CodeBLEU Key Detection ---\n",
    "_codebleu_score_key_cache_util = None\n",
    "\n",
    "def _get_and_cache_codebleu_score_key(pred_for_key_detect: str, ref_for_key_detect: str):\n",
    "    global _codebleu_score_key_cache_util\n",
    "    if not _HAS_CODEBLEU_METRIC_UTIL or _codebleu_score_key_cache_util is not None:\n",
    "        return _codebleu_score_key_cache_util\n",
    "\n",
    "    safe_pred_for_key = pred_for_key_detect if isinstance(pred_for_key_detect, str) and pred_for_key_detect.strip() else \"def example_pred(): pass\"\n",
    "    safe_ref_for_key = ref_for_key_detect if isinstance(ref_for_key_detect, str) and ref_for_key_detect.strip() else \"def example_ref(): pass\"\n",
    "\n",
    "    try:\n",
    "        result_dict_for_key = calc_codebleu(\n",
    "            references=[[safe_ref_for_key]],\n",
    "            predictions=[safe_pred_for_key],\n",
    "            lang=\"python\",\n",
    "            weights=(0.25,0.25,0.25,0.25)\n",
    "        )\n",
    "        for key_name in result_dict_for_key:\n",
    "            if \"codebleu\" in key_name.lower():\n",
    "                _codebleu_score_key_cache_util = key_name\n",
    "                break\n",
    "        if not _codebleu_score_key_cache_util:\n",
    "             warnings.warn(\"Could not auto-detect CodeBLEU score key from calc_codebleu output.\")\n",
    "    except Exception as e_cb_key_detect:\n",
    "        warnings.warn(f\"Error during CodeBLEU key detection: {type(e_cb_key_detect).__name__}: {e_cb_key_detect}.\")\n",
    "        traceback.print_exc()\n",
    "    return _codebleu_score_key_cache_util\n",
    "\n",
    "# --- Metric Calculation Functions ---\n",
    "def calculate_chrf_score_metric(prediction: str, reference: str) -> float | None:\n",
    "    if not (isinstance(prediction, str) and isinstance(reference, str)): return None\n",
    "    if not prediction.strip() or not reference.strip(): return 0.0\n",
    "    try: return sacrebleu.corpus_chrf([prediction], [[reference]]).score\n",
    "    except Exception as e: warnings.warn(f\"ChrF failed: {e}\"); return None\n",
    "\n",
    "def calculate_codebleu_score_metric(prediction: str, reference: str) -> float | None:\n",
    "    if not _HAS_CODEBLEU_METRIC_UTIL: return None\n",
    "    if not (isinstance(prediction, str) and isinstance(reference, str)): return None\n",
    "    if not prediction.strip() or not reference.strip(): return 0.0\n",
    "\n",
    "    global _codebleu_score_key_cache_util\n",
    "    if _codebleu_score_key_cache_util is None: # Should be pre-cached by evaluate_rag_vs_baseline\n",
    "        _get_and_cache_codebleu_score_key(prediction, reference) # Attempt to cache if not already\n",
    "        if _codebleu_score_key_cache_util is None:\n",
    "            warnings.warn(\"CodeBLEU key not available for calculation; returning None for CodeBLEU.\")\n",
    "            return None\n",
    "    try:\n",
    "        res_cb = calc_codebleu(references=[[reference]], predictions=[prediction], lang=\"python\", weights=(0.25,0.25,0.25,0.25))\n",
    "        return float(res_cb.get(_codebleu_score_key_cache_util, 0.0))\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"CodeBLEU calc failed for sample: {type(e).__name__} - {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_api_recall_score_metric(generated_code: str, ref_apis: list) -> float:\n",
    "    if not isinstance(generated_code, str) or not isinstance(ref_apis, list): return 0.0\n",
    "    valid_apis = [api for api in ref_apis if isinstance(api, str) and api.strip()]\n",
    "    if not generated_code.strip() or not valid_apis: return 0.0\n",
    "    # CORRECTED LINE: Removed the trailing backslash in the regex f-string\n",
    "    hits = sum(bool(re.search(rf\"\\b{re.escape(api)}\\b\", generated_code)) for api in valid_apis)\n",
    "    return hits / len(valid_apis) if valid_apis else 0.0\n",
    "\n",
    "# --- Helper for Averaging Metrics ---\n",
    "def _calculate_mean_score_metric(metric_func, predictions_list: list, ground_truths_list: list) -> float:\n",
    "    scores = [metric_func(p, gt) for p, gt in zip(predictions_list, ground_truths_list)]\n",
    "    valid_scores = [s for s in scores if s is not None]\n",
    "    return np.mean(valid_scores) if valid_scores else 0.0\n",
    "\n",
    "# --- Main Evaluation Function ---\n",
    "def evaluate_rag_vs_baseline(baseline_preds: list, rag_preds: list, refs: list, ref_api_lists: list) -> dict:\n",
    "    global _codebleu_score_key_cache_util\n",
    "\n",
    "    if _HAS_CODEBLEU_METRIC_UTIL and _codebleu_score_key_cache_util is None:\n",
    "        first_p, first_r = (None, None)\n",
    "        for p_list_to_check in [baseline_preds, rag_preds]:\n",
    "            for p_item, r_item in zip(p_list_to_check, refs):\n",
    "                if isinstance(p_item, str) and p_item.strip() and isinstance(r_item, str) and r_item.strip():\n",
    "                    first_p, first_r = p_item, r_item; break\n",
    "            if first_p: break\n",
    "        if first_p and first_r: _get_and_cache_codebleu_score_key(first_p, first_r)\n",
    "        else: warnings.warn(\"evaluate_rag_vs_baseline: No valid data pair to detect CodeBLEU key initially.\")\n",
    "\n",
    "    metrics_data = {\n",
    "        \"API Recall\": (_calculate_mean_score_metric(calculate_api_recall_score_metric, baseline_preds, ref_api_lists),\n",
    "                       _calculate_mean_score_metric(calculate_api_recall_score_metric, rag_preds, ref_api_lists)),\n",
    "        \"ChrF Score\": (_calculate_mean_score_metric(calculate_chrf_score_metric, baseline_preds, refs),\n",
    "                       _calculate_mean_score_metric(calculate_chrf_score_metric, rag_preds, refs)),\n",
    "        \"CodeBLEU Score\": (_calculate_mean_score_metric(calculate_codebleu_score_metric, baseline_preds, refs),\n",
    "                           _calculate_mean_score_metric(calculate_codebleu_score_metric, rag_preds, refs))\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- Automatic Evaluation Metrics ---\")\n",
    "    print(f\"| Metric         | Baseline |   RAG   | Δ (RAG - Base) |\")\n",
    "    print(f\"|----------------|----------|---------|----------------|\")\n",
    "    for name, (base_val, rag_val) in metrics_data.items():\n",
    "        delta_val = rag_val - base_val\n",
    "        format_str = \"{:8.4f}\" if name == \"API Recall\" else \"{:8.2f}\"\n",
    "        print(f\"| {name:<14} | {format_str.format(base_val)} | {format_str.format(rag_val):>7} | {delta_val:+14.4f} |\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "\n",
    "    return {\n",
    "        metric_name: {\"baseline\": scores[0], \"rag\": scores[1], \"delta\": scores[1] - scores[0]}\n",
    "        for metric_name, scores in metrics_data.items()\n",
    "    }\n",
    "\n",
    "print(\"Metric calculation utilities (including 'evaluate_rag_vs_baseline') defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VE_YkPNasTM3",
    "outputId": "08e77b20-85bc-448b-e7c2-6ee22886b32c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 1. Essential Prerequisite Checks ---\n",
    "if 'evaluate_rag_vs_baseline' not in globals() or not callable(globals()['evaluate_rag_vs_baseline']):\n",
    "    raise NameError(\"The main evaluation function ('evaluate_rag_vs_baseline') is not defined. \"\n",
    "                    \"Ensure the 'Metric Calculation Utilities' cell (e.g., 9.2.5) has been run.\")\n",
    "\n",
    "result_lists_for_metrics_final = [\n",
    "    'eval_all_baseline_outputs', 'eval_all_rag_outputs',\n",
    "    'eval_all_references', 'eval_all_reference_apis'\n",
    "]\n",
    "for required_list_name_final in result_lists_for_metrics_final:\n",
    "    if required_list_name_final not in globals() or not isinstance(globals()[required_list_name_final], list):\n",
    "        raise NameError(f\"Result list '{required_list_name_final}' from Cell 9.2 is missing or not a list. \"\n",
    "                        \"Ensure Cell 9.2 (Main Evaluation Loop) completed successfully.\")\n",
    "\n",
    "# --- 2. Proceed with Metrics Calculation if Data is Available ---\n",
    "if not eval_all_baseline_outputs:\n",
    "    print(\"  INFO: Evaluation output lists are empty. Skipping metrics calculation.\")\n",
    "else:\n",
    "    try:\n",
    "        final_metrics_results_dict_output = evaluate_rag_vs_baseline(\n",
    "            eval_all_baseline_outputs,\n",
    "            eval_all_rag_outputs,\n",
    "            eval_all_references,\n",
    "            eval_all_reference_apis\n",
    "        )\n",
    "\n",
    "    except Exception as e_metric_calc_final_run:\n",
    "        print(f\"\\n   ERROR during metrics calculation via 'evaluate_rag_vs_baseline':\")\n",
    "        print(f\"    Error: {type(e_metric_calc_final_run).__name__}: {e_metric_calc_final_run}\")\n",
    "        import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKFt9Dn9f_vJ",
    "outputId": "8d619ca9-9ef9-4346-89f0-be03423f6f37",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 9.4: Qualitative Analysis Sample Viewer\n",
    "import textwrap\n",
    "from rank_bm25 import BM25Okapi\n",
    "import os\n",
    "\n",
    "# --- Configuration for this Analysis Cell ---\n",
    "INDICES_TO_ANALYZE = [0, 1, 2] # Specify the indices from your evaluation run\n",
    "MAX_SNIPPET_DISPLAY_LENGTH = 500\n",
    "# If you want to ensure you're using the exact BM25 index built during eval:\n",
    "# USE_CACHED_BM25_INDEX = True\n",
    "# For simplicity now, let's rebuild on the fly.\n",
    "\n",
    "print(f\"--- Qualitative Analysis for Selected Samples ---\")\n",
    "\n",
    "# --- Prerequisite Checks ---\n",
    "# Check for dataset_for_eval explicitly\n",
    "if 'dataset_for_eval' not in globals():\n",
    "    raise NameError(\"Global variable 'dataset_for_eval' not found. Ensure Cell 9.2 (with 'global dataset_for_eval') has been run.\")\n",
    "if not hasattr(globals()['dataset_for_eval'], '__getitem__') or not hasattr(globals()['dataset_for_eval'], '__len__'):\n",
    "    raise TypeError(f\"'dataset_for_eval' is not a list-like object (e.g., a Hugging Face Dataset). Type is: {type(globals()['dataset_for_eval'])}\")\n",
    "if len(globals()['dataset_for_eval']) == 0 and len(INDICES_TO_ANALYZE) > 0 and INDICES_TO_ANALYZE[0] >=0 : # Check if empty only if we intend to access it\n",
    "     # Allow empty if INDICES_TO_ANALYZE is empty or all negative (though that's unusual)\n",
    "    if any(i >= 0 for i in INDICES_TO_ANALYZE): # Only raise if we actually try to access a positive index\n",
    "        raise ValueError(\"'dataset_for_eval' is empty. Ensure Cell 9.2 processed samples and NUM_EXAMPLES_TO_EVALUATE > 0.\")\n",
    "\n",
    "\n",
    "# Verify other prerequisite lists from Cell 9.2\n",
    "other_required_lists = ['eval_all_baseline_outputs', 'eval_all_rag_outputs', 'eval_all_references']\n",
    "for lst_name in other_required_lists:\n",
    "    if lst_name not in globals() or not isinstance(globals()[lst_name], list):\n",
    "        raise NameError(f\"Required list '{lst_name}' not found or not a list. Run Cell 9.2 first.\")\n",
    "    # Check if these lists are non-empty if we actually plan to use them based on INDICES_TO_ANALYZE\n",
    "    if any(i >= 0 and i < len(globals()[lst_name]) for i in INDICES_TO_ANALYZE) and not globals()[lst_name]:\n",
    "         raise ValueError(f\"Required list '{lst_name}' is empty, but INDICES_TO_ANALYZE expects to access it.\")\n",
    "    # Check if indices are valid for these lists\n",
    "    for sample_idx_to_check in INDICES_TO_ANALYZE:\n",
    "        if sample_idx_to_check >= 0 and sample_idx_to_check >= len(globals()[lst_name]):\n",
    "            raise IndexError(f\"Index {sample_idx_to_check} is out of bounds for list '{lst_name}' (size {len(globals()[lst_name])}).\")\n",
    "\n",
    "\n",
    "# Verify BM25 and GitHub config\n",
    "bm25_config_vars = ['EVAL_BM25_TOKENIZER', 'EVAL_BM25_K1', 'EVAL_BM25_B', 'EVAL_TOP_K_SNIPPETS_FOR_PROMPT']\n",
    "github_vars = ['GITHUB_RAW_CONTENT_BASE_URL', 'LOCAL_TEMP_KB_DOWNLOAD_DIR', 'download_github_raw_json']\n",
    "if any(v not in globals() for v in bm25_config_vars + github_vars):\n",
    "    raise NameError(\"BM25 or GitHub configuration variables from Cell 9.1 are missing.\")\n",
    "\n",
    "\n",
    "# --- Main Analysis Loop ---\n",
    "for sample_idx_in_eval_run in INDICES_TO_ANALYZE:\n",
    "    # We've already checked bounds for dataset_for_eval implicitly by checking other lists\n",
    "    # but an explicit check against dataset_for_eval's length is good.\n",
    "    if not (0 <= sample_idx_in_eval_run < len(globals()['dataset_for_eval'])):\n",
    "        print(f\"\\n Warning: Index {sample_idx_in_eval_run} is out of bounds for 'dataset_for_eval' (size {len(globals()['dataset_for_eval'])}). Skipping.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n\\n======= Analyzing Sample Index (from eval run): {sample_idx_in_eval_run} =======\")\n",
    "\n",
    "    # 1. Get data from the evaluation run\n",
    "    # Access dataset_for_eval via globals() for this check, then use directly\n",
    "    sample_data_original = globals()['dataset_for_eval'][sample_idx_in_eval_run]\n",
    "    instruction = sample_data_original.get(\"instruction\")\n",
    "    repo_key = sample_data_original.get('repo_full_name')\n",
    "    reference_code = eval_all_references[sample_idx_in_eval_run]\n",
    "    baseline_output = eval_all_baseline_outputs[sample_idx_in_eval_run]\n",
    "    rag_output = eval_all_rag_outputs[sample_idx_in_eval_run]\n",
    "\n",
    "    print(f\"Repo Key: {repo_key}\")\n",
    "    print(\"\\n--- INSTRUCTION ---\")\n",
    "    print(textwrap.fill(instruction, width=100))\n",
    "\n",
    "    print(\"\\n--- REFERENCE CODE ---\")\n",
    "    print(reference_code if reference_code.strip() else \"[No Reference Code]\")\n",
    "\n",
    "    # 2. Re-perform BM25 retrieval for this sample to get the snippets\n",
    "    print(\"\\n--- RETRIEVED SNIPPETS (for RAG) ---\")\n",
    "    retrieved_snippets_for_display = []\n",
    "    if not repo_key:\n",
    "        print(\"  No repo_key for this sample, cannot retrieve snippets.\")\n",
    "    else:\n",
    "        kb_filename_on_github = f\"kb_{repo_key}.json\"\n",
    "        raw_kb_url = f\"{GITHUB_RAW_CONTENT_BASE_URL}/{kb_filename_on_github}\"\n",
    "        temp_save_subdir_for_kb = os.path.join(LOCAL_TEMP_KB_DOWNLOAD_DIR, f\"qa_kb_{repo_key}\")\n",
    "        kb_data = download_github_raw_json(raw_kb_url, temp_save_subdir_for_kb, kb_filename_on_github, overwrite=False)\n",
    "\n",
    "        if not kb_data or not isinstance(kb_data, list):\n",
    "            print(f\"  Could not load or parse KB for '{repo_key}'.\")\n",
    "        else:\n",
    "            valid_kb_docs = [str(doc) for doc in kb_data if isinstance(doc, str) and str(doc).strip()]\n",
    "            if not valid_kb_docs:\n",
    "                print(f\"  No valid string snippets in KB for '{repo_key}'.\")\n",
    "            else:\n",
    "                tokenized_kb_for_bm25 = [EVAL_BM25_TOKENIZER(doc) for doc in valid_kb_docs]\n",
    "                final_bm25_corpus_tokenized = []\n",
    "                map_idx_bm25_to_original_valid_docs = []\n",
    "\n",
    "                for i, tokens in enumerate(tokenized_kb_for_bm25):\n",
    "                    if tokens:\n",
    "                        final_bm25_corpus_tokenized.append(tokens)\n",
    "                        map_idx_bm25_to_original_valid_docs.append(i)\n",
    "\n",
    "                if not final_bm25_corpus_tokenized:\n",
    "                    print(\"  Tokenized KB is empty after filtering. BM25 index not built.\")\n",
    "                else:\n",
    "                    bm25_index_for_sample = BM25Okapi(final_bm25_corpus_tokenized, k1=EVAL_BM25_K1, b=EVAL_BM25_B)\n",
    "                    query_tokens_for_sample = EVAL_BM25_TOKENIZER(instruction)\n",
    "\n",
    "                    if not query_tokens_for_sample:\n",
    "                        print(\"  Instruction tokenized to empty list, no snippets retrieved.\")\n",
    "                    else:\n",
    "                        num_docs_in_bm25_index = len(final_bm25_corpus_tokenized)\n",
    "                        top_n_indices_in_bm25_corpus = bm25_index_for_sample.get_top_n(\n",
    "                            query_tokens_for_sample,\n",
    "                            list(range(num_docs_in_bm25_index)),\n",
    "                            n=min(EVAL_TOP_K_SNIPPETS_FOR_PROMPT, num_docs_in_bm25_index)\n",
    "                        )\n",
    "                        retrieved_original_indices = [map_idx_bm25_to_original_valid_docs[i] for i in top_n_indices_in_bm25_corpus]\n",
    "                        retrieved_snippets_for_display = [valid_kb_docs[i] for i in retrieved_original_indices]\n",
    "        if retrieved_snippets_for_display:\n",
    "            for i_snip, snippet_text in enumerate(retrieved_snippets_for_display):\n",
    "                print(f\"\\n  Snippet {i_snip+1}/{len(retrieved_snippets_for_display)} (Length: {len(snippet_text)} chars):\")\n",
    "                print(textwrap.shorten(snippet_text, width=MAX_SNIPPET_DISPLAY_LENGTH, placeholder=\"... (snippet truncated) ...\"))\n",
    "        else:\n",
    "            print(\"  No snippets were retrieved for this sample.\")\n",
    "\n",
    "    print(\"\\n--- BASELINE OUTPUT ---\")\n",
    "    print(baseline_output if baseline_output.strip() else \"[No Baseline Output]\")\n",
    "\n",
    "    print(\"\\n--- RAG OUTPUT ---\")\n",
    "    print(rag_output if rag_output.strip() else \"[No RAG Output]\")\n",
    "\n",
    "    print(f\"======= End of Analysis for Sample Index: {sample_idx_in_eval_run} =======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 10: RAG Performance vs. Baseline with Varying Top-K Retrieved Snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # For the tokenizer if defined here again\n",
    "\n",
    "# --- 1. Number of Samples for Section 10 Evaluation ---\n",
    "NUM_EXAMPLES_TO_EVALUATE_S10 = 3\n",
    "\n",
    "# --- 2. Top-K Values for Snippet Retrieval ---\n",
    "TOP_K_VALUES_FOR_S10_EVAL = [1, 3, 5, 10]\n",
    "\n",
    "# --- 3. Select Prompt Builders for Section 10 Evaluation (Reusing from Section 9) ---\n",
    "# Ensure EVAL_RAG_PROMPT_BUILDER and EVAL_BASELINE_PROMPT_BUILDER are defined in Section 9.1\n",
    "if 'EVAL_RAG_PROMPT_BUILDER' not in globals() or 'EVAL_BASELINE_PROMPT_BUILDER' not in globals():\n",
    "    raise NameError(\"EVAL_RAG_PROMPT_BUILDER or EVAL_BASELINE_PROMPT_BUILDER not defined. Run Section 9.1 first.\")\n",
    "S10_EVAL_RAG_PROMPT_BUILDER = EVAL_RAG_PROMPT_BUILDER\n",
    "S10_EVAL_BASELINE_PROMPT_BUILDER = EVAL_BASELINE_PROMPT_BUILDER\n",
    "\n",
    "# --- 4. BM25 Parameters for Section 10 Evaluation (Reusing from Section 9) ---\n",
    "# Ensure EVAL_BM25_K1, EVAL_BM25_B, EVAL_BM25_TOKENIZER are defined in Section 9.1\n",
    "s9_bm25_params_check = ['EVAL_BM25_K1', 'EVAL_BM25_B', 'EVAL_BM25_TOKENIZER']\n",
    "if any(p not in globals() for p in s9_bm25_params_check):\n",
    "    raise NameError(f\"One or more BM25 parameters ({s9_bm25_params_check}) from Section 9.1 are missing.\")\n",
    "S10_EVAL_BM25_K1 = EVAL_BM25_K1\n",
    "S10_EVAL_BM25_B = EVAL_BM25_B\n",
    "S10_EVAL_BM25_TOKENIZER = EVAL_BM25_TOKENIZER\n",
    "\n",
    "# --- 5. LLM Generation Parameters for Section 10 Evaluation (Reusing from Section 9) ---\n",
    "# Ensure EVAL_MAX_NEW_TOKENS, EVAL_TEMPERATURE, etc., and eval_llm_stopping_criteria are set in Section 9.1\n",
    "s9_llm_params_check = [\n",
    "    'EVAL_MAX_NEW_TOKENS', 'EVAL_TEMPERATURE', 'EVAL_TOP_P', 'EVAL_TOP_K',\n",
    "    'EVAL_REPETITION_PENALTY', 'EVAL_DO_SAMPLE', 'eval_llm_stopping_criteria'\n",
    "]\n",
    "if any(p not in globals() for p in s9_llm_params_check):\n",
    "    raise NameError(f\"One or more LLM generation parameters from Section 9.1 are missing.\")\n",
    "\n",
    "# --- 6. Verify other critical dependencies for Cell 10.2 ---\n",
    "critical_deps_for_s10_loop = [\n",
    "    'lca_dataset_split', 'model', 'tokenizer', 'generate_llm_code_and_clean',\n",
    "    'GITHUB_RAW_CONTENT_BASE_URL', 'LOCAL_TEMP_KB_DOWNLOAD_DIR', 'download_github_raw_json',\n",
    "    'BM25Okapi' # Added BM25Okapi here for explicitness, it's imported in S9.2\n",
    "]\n",
    "if any(dep not in globals() for dep in critical_deps_for_s10_loop):\n",
    "    missing_deps = [dep for dep in critical_deps_for_s10_loop if dep not in globals()]\n",
    "    raise NameError(f\"Critical S10 dependencies missing: {missing_deps}. Ensure prior setup sections ran.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- 1. Initialize Output Storage ---\n",
    "s10_evaluation_results = {}\n",
    "for top_k in TOP_K_VALUES_FOR_S10_EVAL:\n",
    "    s10_evaluation_results[top_k] = {\n",
    "        'baseline_outputs': [],\n",
    "        'rag_outputs': [],\n",
    "        'references': [],\n",
    "        'reference_apis': []\n",
    "    }\n",
    "\n",
    "eval_bm25_index_cache_s10 = {} # Cache for BM25 indexes to speed up if multiple samples use same repo\n",
    "\n",
    "# --- 2. Prepare Dataset View for Section 10 ---\n",
    "dataset_for_s10_eval = lca_dataset_split.select(\n",
    "    range(min(NUM_EXAMPLES_TO_EVALUATE_S10, len(lca_dataset_split)))\n",
    ")\n",
    "num_samples_to_process_s10 = len(dataset_for_s10_eval)\n",
    "\n",
    "# --- 3. Main Evaluation Loop ---\n",
    "for idx, sample_data in enumerate(tqdm(dataset_for_s10_eval, desc=\"⚙️ S10 Evaluating\", unit=\"sample\")):\n",
    "    instruction = sample_data.get(\"instruction\")\n",
    "    repo_key = sample_data.get('repo_full_name')\n",
    "    reference_code_s10 = sample_data.get(\"clean_reference\", \"\")\n",
    "    reference_apis_s10 = sample_data.get(\"unique_apis\", [])\n",
    "\n",
    "    baseline_code_s10 = \"\"\n",
    "    if not instruction or not repo_key:\n",
    "        print(f\"  Skipping S10 sample {idx} ({repo_key}) due to missing instruction or repo_key.\")\n",
    "        # Append empty strings to keep list lengths consistent for all top_k\n",
    "        for top_k_val in TOP_K_VALUES_FOR_S10_EVAL:\n",
    "            s10_evaluation_results[top_k_val]['baseline_outputs'].append(\"\")\n",
    "            s10_evaluation_results[top_k_val]['rag_outputs'].append(\"\")\n",
    "            s10_evaluation_results[top_k_val]['references'].append(reference_code_s10)\n",
    "            s10_evaluation_results[top_k_val]['reference_apis'].append(reference_apis_s10)\n",
    "        continue\n",
    "\n",
    "    # --- A. Baseline Generation (once per sample) ---\n",
    "    try:\n",
    "        baseline_prompt_s10 = S10_EVAL_BASELINE_PROMPT_BUILDER(instruction)\n",
    "        baseline_code_s10 = generate_llm_code_and_clean(\n",
    "            prompt_text=baseline_prompt_s10, llm_model=model, llm_tokenizer=tokenizer,\n",
    "            max_new_tokens_gen=EVAL_MAX_NEW_TOKENS, do_sample_gen=EVAL_DO_SAMPLE,\n",
    "            temperature_gen=EVAL_TEMPERATURE, top_p_gen=EVAL_TOP_P, top_k_gen=EVAL_TOP_K,\n",
    "            repetition_penalty_gen=EVAL_REPETITION_PENALTY,\n",
    "            stopping_criteria_list_gen=eval_llm_stopping_criteria, # from S9.1\n",
    "            prompt_name=f\"S10_Base_{idx}_{repo_key}\"\n",
    "        )\n",
    "    except Exception as e_base:\n",
    "        print(f\"  Error during S10 Baseline for sample {idx} ({repo_key}): {type(e_base).__name__}: {e_base}\")\n",
    "        baseline_code_s10 = \"\" # Ensure it's an empty string on error\n",
    "\n",
    "    # --- B. RAG Generation (iterating through TOP_K_VALUES_FOR_S10_EVAL) ---\n",
    "    for top_k_val in TOP_K_VALUES_FOR_S10_EVAL:\n",
    "        s10_evaluation_results[top_k_val]['baseline_outputs'].append(baseline_code_s10 or \"\")\n",
    "        s10_evaluation_results[top_k_val]['references'].append(reference_code_s10)\n",
    "        s10_evaluation_results[top_k_val]['reference_apis'].append(reference_apis_s10)\n",
    "\n",
    "        rag_code_s10 = \"\"\n",
    "        retrieved_context_str_s10 = \"\"\n",
    "        try:\n",
    "            # BM25 Retrieval (similar to Section 9.2)\n",
    "            if repo_key in eval_bm25_index_cache_s10:\n",
    "                bm25_idx, valid_docs, map_to_valid = eval_bm25_index_cache_s10[repo_key]\n",
    "            else:\n",
    "                kb_file = f\"kb_{repo_key}.json\"\n",
    "                kb_url = f\"{GITHUB_RAW_CONTENT_BASE_URL}/{kb_file}\"\n",
    "                temp_kb_dir = os.path.join(LOCAL_TEMP_KB_DOWNLOAD_DIR, f\"s10_eval_kb_{repo_key}\")\n",
    "                kb_json = download_github_raw_json(kb_url, temp_kb_dir, kb_file, overwrite=False) # Use cache if available\n",
    "\n",
    "                bm25_idx, valid_docs, map_to_valid, tokenized_corpus_for_index = (None, [], [], [])\n",
    "                if kb_json and isinstance(kb_json, list):\n",
    "                    valid_docs = [str(d) for d in kb_json if isinstance(d, str) and str(d).strip()]\n",
    "                    if valid_docs:\n",
    "                        tokenized_kb = [S10_EVAL_BM25_TOKENIZER(doc) for doc in valid_docs]\n",
    "                        for i_map, toks in enumerate(tokenized_kb):\n",
    "                            if toks: tokenized_corpus_for_index.append(toks); map_to_valid.append(i_map)\n",
    "                        if tokenized_corpus_for_index:\n",
    "                            bm25_idx = BM25Okapi(tokenized_corpus_for_index, k1=S10_EVAL_BM25_K1, b=S10_EVAL_BM25_B)\n",
    "                            eval_bm25_index_cache_s10[repo_key] = (bm25_idx, valid_docs, map_to_valid)\n",
    "\n",
    "            if bm25_idx:\n",
    "                query_toks = S10_EVAL_BM25_TOKENIZER(instruction)\n",
    "                if query_toks:\n",
    "                    num_idx_docs = len(bm25_idx.doc_len) # Access doc_len from BM25Okapi instance\n",
    "                    top_idxs = bm25_idx.get_top_n(query_toks, list(range(num_idx_docs)),\n",
    "                                                  n=min(top_k_val, num_idx_docs))\n",
    "                    retrieved_list = [valid_docs[map_to_valid[i]] for i in top_idxs]\n",
    "                    retrieved_context_str_s10 = \"\\\\n\\\\n# --- Snippet ---\\\\n\\\\n\".join(retrieved_list)\n",
    "\n",
    "            # Assemble RAG Prompt\n",
    "            rag_prompt_text_s10 = S10_EVAL_RAG_PROMPT_BUILDER(instruction, retrieved_context_str_s10)\n",
    "\n",
    "            rag_code_s10 = generate_llm_code_and_clean(\n",
    "                prompt_text=rag_prompt_text_s10, llm_model=model, llm_tokenizer=tokenizer,\n",
    "                max_new_tokens_gen=EVAL_MAX_NEW_TOKENS, do_sample_gen=EVAL_DO_SAMPLE,\n",
    "                temperature_gen=EVAL_TEMPERATURE, top_p_gen=EVAL_TOP_P, top_k_gen=EVAL_TOP_K,\n",
    "                repetition_penalty_gen=EVAL_REPETITION_PENALTY,\n",
    "                stopping_criteria_list_gen=eval_llm_stopping_criteria, # from S9.1\n",
    "                prompt_name=f\"S10_RAG_TopK{top_k_val}_{idx}_{repo_key}\"\n",
    "            )\n",
    "        except Exception as e_rag:\n",
    "            print(f\"  Error during S10 RAG (TopK={top_k_val}) for sample {idx} ({repo_key}): {type(e_rag).__name__}: {e_rag}\")\n",
    "            rag_code_s10 = \"\" # Ensure it's an empty string on error\n",
    "        s10_evaluation_results[top_k_val]['rag_outputs'].append(rag_code_s10 or \"\")\n",
    "\n",
    "# --- 4. Final Sanity Check of List Lengths ---\n",
    "for tk_val, results_dict in s10_evaluation_results.items():\n",
    "    if not (len(results_dict['baseline_outputs']) == len(results_dict['rag_outputs']) == \\\n",
    "            len(results_dict['references']) == len(results_dict['reference_apis']) == num_samples_to_process_s10):\n",
    "        print(f\"\\nCRITICAL ERROR: Length mismatch for TopK={tk_val} in S10 final output lists!\")\n",
    "    else:\n",
    "        print(f\"\\nSUCCESS: Collected all S10 outputs for TopK={tk_val} for {num_samples_to_process_s10} evaluation examples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Essential Prerequisite Checks ---\n",
    "if 'evaluate_rag_vs_baseline' not in globals() or not callable(globals()['evaluate_rag_vs_baseline']):\n",
    "    raise NameError(\"Function 'evaluate_rag_vs_baseline' from S9.3 is not defined.\")\n",
    "if 's10_evaluation_results' not in globals() or not isinstance(s10_evaluation_results, dict):\n",
    "    raise NameError(\"S10 evaluation results ('s10_evaluation_results') not found or not a dict. Run Cell 10.2.\")\n",
    "\n",
    "# --- 2. Proceed with Metrics Calculation ---\n",
    "if not s10_evaluation_results or not any(d.get('rag_outputs') for d in s10_evaluation_results.values()):\n",
    "    print(\"  INFO: Section 10 evaluation output lists are empty or no RAG outputs. Skipping metrics calculation.\")\n",
    "else:\n",
    "    # Store all metrics for potential summary later, though evaluate_rag_vs_baseline already prints tables\n",
    "    all_final_metrics_s10_by_top_k = {}\n",
    "\n",
    "    for top_k_value in sorted(s10_evaluation_results.keys()):\n",
    "        results_for_this_top_k = s10_evaluation_results[top_k_value]\n",
    "        print(f\"\\n======= METRICS FOR RAG (Top-K = {top_k_value}) vs BASELINE =======\")\n",
    "\n",
    "        # Check if there's anything to evaluate for this top_k\n",
    "        if not results_for_this_top_k.get('rag_outputs') or not results_for_this_top_k.get('baseline_outputs'):\n",
    "            print(f\"  Skipping Top-K={top_k_value} due to empty RAG or Baseline outputs.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            final_metrics_dict_output_s10 = evaluate_rag_vs_baseline(\n",
    "                results_for_this_top_k['baseline_outputs'],\n",
    "                results_for_this_top_k['rag_outputs'],\n",
    "                results_for_this_top_k['references'],\n",
    "                results_for_this_top_k['reference_apis']\n",
    "            )\n",
    "            all_final_metrics_s10_by_top_k[top_k_value] = final_metrics_dict_output_s10\n",
    "        except Exception as e_metric_calc_s10:\n",
    "            print(f\"\\n   ERROR during S10 metrics calculation for Top-K = {top_k_value}: {type(e_metric_calc_s10).__name__}: {e_metric_calc_s10}\")\n",
    "            import traceback; traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0083e0d64c0f4ac7a163678190a9c2ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "01c12387bfe44e8a885faa0519ec54dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_066a8ac1f0ca408587f2c31c9a4a729a",
      "max": 28090,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f2bf430ae1d044c7a8231839df32643b",
      "value": 28090
     }
    },
    "02229e3be13145feb476833c0a010e9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1ed7852ba9164b1290eadfcce48989e7",
       "IPY_MODEL_6dafa92adc684ac6a7839cbe16086f31",
       "IPY_MODEL_6fe28078f0104b65b52f2eb2c3b50f06"
      ],
      "layout": "IPY_MODEL_e136773141a84282bc7738cd6ef5182c"
     }
    },
    "026b6657b8a14e7294e98d6ffb416428": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0555a2e0d3a04ad789c516c6564c4dbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "066a8ac1f0ca408587f2c31c9a4a729a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "068a344ed8fd4b88a3c46829a831f4e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08756a2d862c4a23bbde86d5e5925503": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4cb9b2c5352c4379b3509b0a1258cb52",
      "placeholder": "​",
      "style": "IPY_MODEL_d969f62f700846d2bfcb8639daed0eeb",
      "value": "model.safetensors.index.json: 100%"
     }
    },
    "08a3c06eb1244d878fea4140143dcfaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f7c9c0e335424ea98e6096f67cfa5825",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_69fbf1d625be4b1c9bd667afce922e44",
      "value": 50
     }
    },
    "099da97063be4ae884c7e212a75798c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0c05f3641a9842008fcf1cb24c8201f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0d6df7a3326546c3a1162737168f4f45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab1151ab30104706ab5dc65da7c0dd23",
      "placeholder": "​",
      "style": "IPY_MODEL_8abdfa5a467340f8a5f34b4eee9f88a4",
      "value": " 5.21k/5.21k [00:00&lt;00:00, 551kB/s]"
     }
    },
    "0e6f207cf944448cbb2d0ce6802eaf9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1054aad8e7f74b898398b6c887f13432": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9de2f3538db94de384d05fb86a41058d",
      "max": 5212,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a859333b703648b7a88711246a8cc46e",
      "value": 5212
     }
    },
    "157d2afd234243ac9c3a4769d578c166": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_50ac78d2cec04adcaf987960fcb987d6",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f888b9d356d94d228b613ba1c2111ace",
      "value": 2
     }
    },
    "172ce2b591a44aec9d915cd51e1e2475": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1bc262f3a6684312927bdcf23013660f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1c1f106b607540efb29dc036f3a734d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1d4fc7b2663e429eaf8aa4aa8424bbe6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1dfc2b303feb4c6da474749cf4862d2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e7572793db84015901449b3692ab724": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ed7852ba9164b1290eadfcce48989e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf4871b8e77a4663bce7eb33f61ae488",
      "placeholder": "​",
      "style": "IPY_MODEL_20ca0aa7ea094bc1868b2d6375539dc8",
      "value": "config.json: 100%"
     }
    },
    "20ca0aa7ea094bc1868b2d6375539dc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "240a4244def74e7687ea9de628dc7a7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2436bc8979ca400082a11aa867802522": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_40e6d4b7b07e4afc8b5ac70c7495aaa7",
      "placeholder": "​",
      "style": "IPY_MODEL_95b45e18f9744582a4f27017a95be7be",
      "value": " 50/50 [49:30&lt;00:00, 38.53s/sample]"
     }
    },
    "24b1f5a8a80b45b5beeba95133d3e750": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "267570e14e5045298267cb1d37872d95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6edd17a05ba74f6b9c40415e76e76edb",
      "placeholder": "​",
      "style": "IPY_MODEL_1c1f106b607540efb29dc036f3a734d9",
      "value": "tokenizer.json: 100%"
     }
    },
    "284bb83d8cfc497b84986d71002ef4ab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2ba51afe6897494bb5b644730fa7580a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bb2241e8c7a4b3c9b68713b1bb1b8db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2d7d05b5081740368bd938203423f159": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30463b09c6e044029710b9773264eb17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "323930aba6d248c4bc368c8c26e015f1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8a1cbf1636da45729290e8e280b2078f",
       "IPY_MODEL_08a3c06eb1244d878fea4140143dcfaf",
       "IPY_MODEL_2436bc8979ca400082a11aa867802522"
      ],
      "layout": "IPY_MODEL_a2c796d746954971899bd64255ff68bc"
     }
    },
    "340132d4270843be8263c2395f2507d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "34293dc64fef47b2b9890826c3841160": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_944adee829e84bd4a58d93de0757c2a3",
       "IPY_MODEL_d9999cb78a8c417a8a7f1e749a92fa6a",
       "IPY_MODEL_75dbf677bb9c467daca6a3100a8590b5"
      ],
      "layout": "IPY_MODEL_24b1f5a8a80b45b5beeba95133d3e750"
     }
    },
    "3c9c72bf76e44338b861b8c181336a26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea65ba42e7e8495aace282926cb14113",
      "max": 181,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c002ab1fb2f64243876e2b9abf279cc3",
      "value": 181
     }
    },
    "3d2c4bc1094142d28b78064211489abc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3e63d227c86f44739555219525946285": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ec7392d77814607a7474bdc00eb34e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3fc9fb45d8c2481d88855889e2d98009": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c577e5f2988b48e09c8cb38035ac0fab",
       "IPY_MODEL_a97ea78ece504c579708f2c1143fc878",
       "IPY_MODEL_42f2591b35a141e09a84fcae0856e255"
      ],
      "layout": "IPY_MODEL_c1697bf367b34cfb92e47f4e493f52a1"
     }
    },
    "40e6d4b7b07e4afc8b5ac70c7495aaa7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "411523e423fa4db0b07ec8c4b8477d6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ca403f7e1bbc4b1d9b249500a6f344d8",
       "IPY_MODEL_3c9c72bf76e44338b861b8c181336a26",
       "IPY_MODEL_d10571e7bca74ea9aba536eeb3e532fd"
      ],
      "layout": "IPY_MODEL_2d7d05b5081740368bd938203423f159"
     }
    },
    "42f2591b35a141e09a84fcae0856e255": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b206161a19da4d5eadb73297fdbccb7d",
      "placeholder": "​",
      "style": "IPY_MODEL_57b283a87dd2453aae003c23b2fc4cbc",
      "value": " 8.61G/8.61G [01:27&lt;00:00, 90.3MB/s]"
     }
    },
    "461f713a4655448d860d4703292dc2ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a57ba83748f4d26a80d23dba269f78a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ef998139bff54cab8ee1b1bd38192049",
      "placeholder": "​",
      "style": "IPY_MODEL_93c2e5556e434296a451947047016705",
      "value": " 6.62G/6.62G [01:17&lt;00:00, 38.5MB/s]"
     }
    },
    "4bdc077612f04c0ca506febfacbb5403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cee3f670b6284a049cfbe3b398f67b3d",
      "placeholder": "​",
      "style": "IPY_MODEL_284bb83d8cfc497b84986d71002ef4ab",
      "value": " 3.07k/3.07k [00:00&lt;00:00, 228kB/s]"
     }
    },
    "4ca50826a66b467f96132b94b04139a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4cb9b2c5352c4379b3509b0a1258cb52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d26233c6ea94099864362810e6b61ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_240a4244def74e7687ea9de628dc7a7f",
      "max": 4577126,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_099da97063be4ae884c7e212a75798c0",
      "value": 4577126
     }
    },
    "50ac78d2cec04adcaf987960fcb987d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "53b13d341dd14a4e9c3c5621a1be7ae0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5437d0aa677f44709eeef7de74a6ad95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5477f02797ca4a168b129c3c67c115f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57b283a87dd2453aae003c23b2fc4cbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "57e96516642d4e77a15b71e6ee226306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e222f7b7569244b8b8b8d1bf6314498c",
       "IPY_MODEL_1054aad8e7f74b898398b6c887f13432",
       "IPY_MODEL_0d6df7a3326546c3a1162737168f4f45"
      ],
      "layout": "IPY_MODEL_c0bdf6bafca34fb2ad5e912780af468c"
     }
    },
    "58191cfc5fc3498faaf0afd1006cdbdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ddd680ab7beb47a2b03d492ac063af0f",
      "placeholder": "​",
      "style": "IPY_MODEL_5477f02797ca4a168b129c3c67c115f3",
      "value": "(…)-00000-of-00001-518ed46ecbe35ff9.parquet: 100%"
     }
    },
    "581af374236a4d43bb86da7adee2935b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba7c44a3158d4260825a9bd09a5e7be6",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_340132d4270843be8263c2395f2507d1",
      "value": 2
     }
    },
    "5dcad31a9d404c4481d569957322cc66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e0ab05077c04d4a91ed387c1da927f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98c36be12d654abcb37c449ff8bfbc28",
      "placeholder": "​",
      "style": "IPY_MODEL_1bc262f3a6684312927bdcf23013660f",
      "value": " 2/2 [01:28&lt;00:00, 88.03s/it]"
     }
    },
    "63b0ab0483d043fcb20d700080deea9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "64240b75cad84dea83ce189a43405668": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65331f835f524151ae560466c6bf4655": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c24f30967965427baf4d2061b9883339",
      "placeholder": "​",
      "style": "IPY_MODEL_96d2db5db33f4ee7abf73e81aeac4876",
      "value": " 7.03M/7.03M [00:03&lt;00:00, 2.20MB/s]"
     }
    },
    "6780e440c66e429ea03a78b07a37d055": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69fbf1d625be4b1c9bd667afce922e44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6a0eaee7c1044ac891d5d2085bb10059": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e4d93681fc384839af27b86ce39bfd81",
      "placeholder": "​",
      "style": "IPY_MODEL_3ec7392d77814607a7474bdc00eb34e1",
      "value": " 2/2 [01:12&lt;00:00, 35.52s/it]"
     }
    },
    "6b004d6c247241559144fa7fc7470328": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6cbc77aeefe4413b90f4aa451d670b8a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c3070aef55354d3e9ff249720344ab46",
       "IPY_MODEL_e88c9d72d238456c9c88e2b13cf23ad3",
       "IPY_MODEL_4a57ba83748f4d26a80d23dba269f78a"
      ],
      "layout": "IPY_MODEL_bf9fdb62bb96432683174a2fd62a4761"
     }
    },
    "6dafa92adc684ac6a7839cbe16086f31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_026b6657b8a14e7294e98d6ffb416428",
      "max": 680,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3d2c4bc1094142d28b78064211489abc",
      "value": 680
     }
    },
    "6e1712852726449dbe7232700998bc6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f983de53b3bb4a2da8a88008744c7b21",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5437d0aa677f44709eeef7de74a6ad95",
      "value": 150
     }
    },
    "6edd17a05ba74f6b9c40415e76e76edb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6fe28078f0104b65b52f2eb2c3b50f06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ca50826a66b467f96132b94b04139a3",
      "placeholder": "​",
      "style": "IPY_MODEL_0c05f3641a9842008fcf1cb24c8201f4",
      "value": " 680/680 [00:00&lt;00:00, 51.5kB/s]"
     }
    },
    "7312f9e7a84c41609554f83e80302549": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75b07acb3d98493d862c953aaff52099": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75dbf677bb9c467daca6a3100a8590b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3e63d227c86f44739555219525946285",
      "placeholder": "​",
      "style": "IPY_MODEL_6b004d6c247241559144fa7fc7470328",
      "value": " 150/150 [00:00&lt;00:00, 949.71 examples/s]"
     }
    },
    "762edaea5c844c75aa521dc3b2e457f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7bf836b4b2ba4a66a3a7801561cf3010": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d3280993b5d4973baee8f4ff3f85996": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7d801e6efd734e1d95622a5d21f054cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0083e0d64c0f4ac7a163678190a9c2ff",
      "placeholder": "​",
      "style": "IPY_MODEL_7d3280993b5d4973baee8f4ff3f85996",
      "value": "Filter: 100%"
     }
    },
    "870d6c9eec114a27976a87b6be6c84fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7d801e6efd734e1d95622a5d21f054cd",
       "IPY_MODEL_6e1712852726449dbe7232700998bc6e",
       "IPY_MODEL_9dbaa7cb6c7d47e182968a45898c1a3d"
      ],
      "layout": "IPY_MODEL_75b07acb3d98493d862c953aaff52099"
     }
    },
    "8a1cbf1636da45729290e8e280b2078f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b166cc1fb249432688cefb8480548a2e",
      "placeholder": "​",
      "style": "IPY_MODEL_2bb2241e8c7a4b3c9b68713b1bb1b8db",
      "value": "⚙️ Evaluating: 100%"
     }
    },
    "8abdfa5a467340f8a5f34b4eee9f88a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8b1cc7b109a24ab49ba68e9be47582e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a401381eee884b4b91ccec23680a954a",
      "placeholder": "​",
      "style": "IPY_MODEL_a9b41ec9bd424e659750259cab16cafd",
      "value": " 4.58M/4.58M [00:00&lt;00:00, 5.29MB/s]"
     }
    },
    "8f3a9a0f60b44df18efd9747d1528570": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93c2e5556e434296a451947047016705": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "944adee829e84bd4a58d93de0757c2a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99516291a8d74c878c890b05680f1fff",
      "placeholder": "​",
      "style": "IPY_MODEL_a24646b489744fc4b0d475ff0108c84f",
      "value": "Generating test split: 100%"
     }
    },
    "95b45e18f9744582a4f27017a95be7be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "966496cb09c34c97b9babf5322b280c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b4e469ab5b744732876285bbe263cca1",
      "placeholder": "​",
      "style": "IPY_MODEL_6780e440c66e429ea03a78b07a37d055",
      "value": "Fetching 2 files: 100%"
     }
    },
    "96d2db5db33f4ee7abf73e81aeac4876": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "98c36be12d654abcb37c449ff8bfbc28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99516291a8d74c878c890b05680f1fff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a34345411584027975e13b46ae4cfba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9a9a4ba023f54118883534728d328c44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_63b0ab0483d043fcb20d700080deea9c",
      "max": 3071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e8f3e8d0ac49416891538845d9aa162e",
      "value": 3071
     }
    },
    "9c53da4af20b486ab78df9ae4230c53d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d0252f73031468b922be378b48d468f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08756a2d862c4a23bbde86d5e5925503",
       "IPY_MODEL_01c12387bfe44e8a885faa0519ec54dc",
       "IPY_MODEL_d94ced3cf1c244a3b917116d26605f8c"
      ],
      "layout": "IPY_MODEL_ab4b9cd4479c4b48918b3b4645c7e54f"
     }
    },
    "9dbaa7cb6c7d47e182968a45898c1a3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b7693f7a52864c97a73844f988f4da63",
      "placeholder": "​",
      "style": "IPY_MODEL_a36c01c200b641a0bbca604db7f39b4d",
      "value": " 150/150 [00:00&lt;00:00, 238.13 examples/s]"
     }
    },
    "9de2f3538db94de384d05fb86a41058d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a24646b489744fc4b0d475ff0108c84f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a2c796d746954971899bd64255ff68bc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a36c01c200b641a0bbca604db7f39b4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a401381eee884b4b91ccec23680a954a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6f29cfb2ac445a2a86de57484a5c755": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30463b09c6e044029710b9773264eb17",
      "placeholder": "​",
      "style": "IPY_MODEL_dd2dc35a0df5432180b47563c423a782",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "a859333b703648b7a88711246a8cc46e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a97ea78ece504c579708f2c1143fc878": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba3f200d65984c17ad00dc9da172ed3c",
      "max": 8606596466,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fc7aa020b1054776bd421a07e266bfa5",
      "value": 8606596466
     }
    },
    "a9b41ec9bd424e659750259cab16cafd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a9dd9236d47e47478fde14621fab685d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a6f29cfb2ac445a2a86de57484a5c755",
       "IPY_MODEL_157d2afd234243ac9c3a4769d578c166",
       "IPY_MODEL_6a0eaee7c1044ac891d5d2085bb10059"
      ],
      "layout": "IPY_MODEL_f238997f5118458ba47572c5cdfe9228"
     }
    },
    "ab1151ab30104706ab5dc65da7c0dd23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ab4b9cd4479c4b48918b3b4645c7e54f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b166cc1fb249432688cefb8480548a2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b206161a19da4d5eadb73297fdbccb7d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b4e469ab5b744732876285bbe263cca1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7693f7a52864c97a73844f988f4da63": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba3f200d65984c17ad00dc9da172ed3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba7c44a3158d4260825a9bd09a5e7be6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf9fdb62bb96432683174a2fd62a4761": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c002ab1fb2f64243876e2b9abf279cc3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c0bdf6bafca34fb2ad5e912780af468c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1697bf367b34cfb92e47f4e493f52a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c24f30967965427baf4d2061b9883339": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3070aef55354d3e9ff249720344ab46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_461f713a4655448d860d4703292dc2ac",
      "placeholder": "​",
      "style": "IPY_MODEL_0e6f207cf944448cbb2d0ce6802eaf9a",
      "value": "model-00002-of-000002.safetensors: 100%"
     }
    },
    "c577e5f2988b48e09c8cb38035ac0fab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf40b8459da8451298d382dac19b8227",
      "placeholder": "​",
      "style": "IPY_MODEL_7312f9e7a84c41609554f83e80302549",
      "value": "model-00001-of-000002.safetensors: 100%"
     }
    },
    "ca403f7e1bbc4b1d9b249500a6f344d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7bf836b4b2ba4a66a3a7801561cf3010",
      "placeholder": "​",
      "style": "IPY_MODEL_9c53da4af20b486ab78df9ae4230c53d",
      "value": "generation_config.json: 100%"
     }
    },
    "ce8c9952ee7642e38eaa921a7cd02268": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_267570e14e5045298267cb1d37872d95",
       "IPY_MODEL_e2ec2396e421442f95cf4af84a6e88ec",
       "IPY_MODEL_65331f835f524151ae560466c6bf4655"
      ],
      "layout": "IPY_MODEL_64240b75cad84dea83ce189a43405668"
     }
    },
    "cee3f670b6284a049cfbe3b398f67b3d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf40b8459da8451298d382dac19b8227": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf4871b8e77a4663bce7eb33f61ae488": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d10571e7bca74ea9aba536eeb3e532fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dcad31a9d404c4481d569957322cc66",
      "placeholder": "​",
      "style": "IPY_MODEL_8f3a9a0f60b44df18efd9747d1528570",
      "value": " 181/181 [00:00&lt;00:00, 19.2kB/s]"
     }
    },
    "d1b9209d040641a9aebc419bfced5a04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2fb6731d14b4286bb942d06ace7ad53": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58191cfc5fc3498faaf0afd1006cdbdb",
       "IPY_MODEL_4d26233c6ea94099864362810e6b61ca",
       "IPY_MODEL_8b1cc7b109a24ab49ba68e9be47582e0"
      ],
      "layout": "IPY_MODEL_df003250270b4525a904d81279d3e0eb"
     }
    },
    "d94ced3cf1c244a3b917116d26605f8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eec0c25deea944a2a5fc266a8a42841b",
      "placeholder": "​",
      "style": "IPY_MODEL_53b13d341dd14a4e9c3c5621a1be7ae0",
      "value": " 28.1k/28.1k [00:00&lt;00:00, 3.02MB/s]"
     }
    },
    "d969f62f700846d2bfcb8639daed0eeb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9999cb78a8c417a8a7f1e749a92fa6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3c430bdef7a45159393b738e346f5ae",
      "max": 150,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1d4fc7b2663e429eaf8aa4aa8424bbe6",
      "value": 150
     }
    },
    "dd2dc35a0df5432180b47563c423a782": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dd2e8df7c52c477585cad40927a6b99e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_966496cb09c34c97b9babf5322b280c3",
       "IPY_MODEL_581af374236a4d43bb86da7adee2935b",
       "IPY_MODEL_5e0ab05077c04d4a91ed387c1da927f5"
      ],
      "layout": "IPY_MODEL_d1b9209d040641a9aebc419bfced5a04"
     }
    },
    "ddd680ab7beb47a2b03d492ac063af0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df003250270b4525a904d81279d3e0eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e136773141a84282bc7738cd6ef5182c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e222f7b7569244b8b8b8d1bf6314498c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e95ee4a871534d8b93154783169cf752",
      "placeholder": "​",
      "style": "IPY_MODEL_0555a2e0d3a04ad789c516c6564c4dbf",
      "value": "README.md: 100%"
     }
    },
    "e2ec2396e421442f95cf4af84a6e88ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_762edaea5c844c75aa521dc3b2e457f5",
      "max": 7031660,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_172ce2b591a44aec9d915cd51e1e2475",
      "value": 7031660
     }
    },
    "e4d93681fc384839af27b86ce39bfd81": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e88c9d72d238456c9c88e2b13cf23ad3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e7572793db84015901449b3692ab724",
      "max": 6624675384,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9a34345411584027975e13b46ae4cfba",
      "value": 6624675384
     }
    },
    "e8f3e8d0ac49416891538845d9aa162e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e95ee4a871534d8b93154783169cf752": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea65ba42e7e8495aace282926cb14113": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee0bf3c1f5f648fc81f9df2e2ba7bba5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ba51afe6897494bb5b644730fa7580a",
      "placeholder": "​",
      "style": "IPY_MODEL_068a344ed8fd4b88a3c46829a831f4e1",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "eec0c25deea944a2a5fc266a8a42841b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ef998139bff54cab8ee1b1bd38192049": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f238997f5118458ba47572c5cdfe9228": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2bf430ae1d044c7a8231839df32643b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3c430bdef7a45159393b738e346f5ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4ee30d84024494db3b9ea9d1c1f2f85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ee0bf3c1f5f648fc81f9df2e2ba7bba5",
       "IPY_MODEL_9a9a4ba023f54118883534728d328c44",
       "IPY_MODEL_4bdc077612f04c0ca506febfacbb5403"
      ],
      "layout": "IPY_MODEL_1dfc2b303feb4c6da474749cf4862d2e"
     }
    },
    "f7c9c0e335424ea98e6096f67cfa5825": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f888b9d356d94d228b613ba1c2111ace": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f983de53b3bb4a2da8a88008744c7b21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc7aa020b1054776bd421a07e266bfa5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
