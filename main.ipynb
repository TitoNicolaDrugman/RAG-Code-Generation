{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f31912e-d6cd-4f1b-bbcc-939c4f3e7138",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RP 7.1 LLMs for Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b9cf0-18d0-4227-83cd-ab6c53d94381",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 0: Check the presence of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5058addd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "GPU count: 2\n",
      "Current device: 0\n",
      "GPU name: NVIDIA GeForce RTX 5060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU count:\", torch.cuda.device_count())\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "bd6abdd2-a693-4281-8f51-0bd26d4ec672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available?: True\n",
      "Number of GPUs: 2\n",
      "GPU Name: NVIDIA GeForce RTX 5060 Ti\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "from utils.hardware import check_gpu\n",
    "gpu_info = check_gpu(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0fbee62b-616c-40ca-b1a8-3db96f18b173",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: c:\\Users\\drugm\\anaconda3\\envs\\rp\\python.exe\n",
      "torch: 2.8.0+cu128\n"
     ]
    }
   ],
   "source": [
    "#pip install python-dotenv\n",
    "#!pip install google-generativeai\n",
    "import sys, torch\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"torch:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cfa1b1-4f98-44c6-990a-cb9213f7821c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 1: Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a22990-b739-4671-a6b3-c16f30859c8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 -> Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "804d62ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "392562ab-53f7-41e4-9051-8d5cb5377c20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking required libraries...\n",
      "torch ✓ (version 2.8.0+cu128)\n",
      "transformers ✓ (version 4.52.3)\n",
      "datasets ✓ (version 3.6.0)\n",
      "huggingface_hub ✓ (version 0.32.2)\n",
      "tqdm ✓ (version 4.67.1)\n",
      "rank-bm25 ✓ (version 0.2.2)\n",
      "numpy ✓ (version 2.3.1)\n",
      "requests ✓ (version 2.32.5)\n",
      "ipython ✓ (version 9.2.0)\n"
     ]
    }
   ],
   "source": [
    "from utils.check_imports import check_libraries\n",
    "print(\"Checking required libraries...\")\n",
    "lib_status = check_libraries()\n",
    "from utils import (\n",
    "    _qcfg_to_dict,\n",
    "    download_github_raw_json,\n",
    "    robust_code_tokenizer_for_s5,\n",
    "    extract_code_units_from_file,\n",
    "    generate_kb_for_library_sources\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa942e9-c6ee-4736-954c-83eeeb66c306",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 -> Import of prompts (Baseline & RAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a244a-3f2b-43ef-87d7-8a5284ad1b44",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "| Versione | Builder Baseline | Builder RAG | Idea chiave | Gestione snippet | Protocollo di output |\n",
    "|----------|------------------|--------------|-------------|------------------|-----------------------|\n",
    "| **v1**   | `build_baseline_prompt_v1` | `build_rag_prompt_v1` | Struttura minimale: “Task → Code” | RAG: sezione “Retrieved Examples” in testa (grezza) | Codice dopo `### Code:` (testuale, semplice) |\n",
    "| **v2**   | `build_baseline_prompt_v2` | `build_rag_prompt_v2` | Template specifico **seedemu** con requisiti espliciti | RAG: aggiunge retrieved + lista requisiti libreria | Solo **codice eseguibile**, niente testo extra |\n",
    "| **v3**   | `build_baseline_prompt_v3` | `build_rag_prompt_v3` | “Senior engineer”: type hints, docstring, test | RAG: retrieved come ispirazione prima del task | Output in fenced block \\`\\`\\`python con parti richieste |\n",
    "| **v4**   | `build_baseline_prompt_v4` | `build_rag_prompt_v4` | **Raw answer only** (nessun boilerplate) | RAG: retrieved + stesse regole; vietato copiare snippet | Restituisce **solo** la risposta grezza |\n",
    "| **v5**   | `build_baseline_prompt_v5` | `build_rag_prompt_v5` | Implementazione completa + test; enfasi qualità | RAG: retrieved **troncati** con `truncate_to_n_tokens` | Output in \\`\\`\\`python, niente spiegazioni |\n",
    "| **v6**   | `build_baseline_prompt_v6` | `build_rag_prompt_v6` | **Long Code Arena**: singolo file `.py`, regole strette | RAG: sezione “Retrieved Library Snippets” con snippet troncati | Solo **codice** in fenced block, niente chain-of-thought |\n",
    "| **v6_2** | `build_baseline_prompt_v6_2` | `build_rag_prompt_v6_2` | Variante **UPPERCASE** del v6 (stesso spirito) | RAG: snippet troncati (stesso schema v6) | Output solo codice, marker \\`\\`\\`PYTHON |\n",
    "| **v6_3** | `build_baseline_prompt_v6_3` | `build_rag_prompt_v6_3` | Come v6_2 ma stress su “no errors / clean code” | RAG: idem v6_2 con note di qualità | Output solo codice, marker \\`\\`\\`PYTHON |\n",
    "| **v7**   | `build_baseline_prompt_v7` | `build_rag_prompt_v7` | Script standalone, uso idiomatico libreria, alta qualità | RAG: retrieved come contesto (troncabili) | Solo codice, niente markdown/extra |\n",
    "| **v8**   | `build_baseline_prompt_v8` | `build_rag_prompt_v8` | Checklist esplicita (imports, docstring, tests, robustness) | RAG: retrieved troncati e integrati | Output solo codice dopo \\`\\`\\`python |\n",
    "| **v9**   | `build_baseline_prompt_v9` | `build_rag_prompt_v9` | Focus su **API-recall** + self-review interna | RAG: “verified excerpts” troncati per ancorare le API | Output solo codice; niente spiegazioni esterne |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6dc7e92f-6389-44a2-bfb0-9516d98a7e09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt builders (v1–v9, baseline e RAG) e utility caricati correttamente.\n"
     ]
    }
   ],
   "source": [
    "# --- Importa il package prompts modularizzato ---\n",
    "from prompts import PROMPT_REGISTRY, truncate_to_n_tokens\n",
    "print(\"Prompt builders (v1–v9, baseline e RAG) e utility caricati correttamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71def82c",
   "metadata": {},
   "source": [
    "### 1.3 Settig API for LLMs cloud Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a90d0a-6cab-4979-8898-1b9478d133b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- API KEYS (kept in notebook; don't commit this cell) ---\n",
    "%env GOOGLE_API_KEY=not stupid enough to leave this here, sorry\n",
    "%env OPENROUTER_API_KEY=sk-or-v1-not stupid enough to leave this here, sorry\n",
    "\n",
    "# Runtime niceties\n",
    "import os, gc, torch\n",
    "os.environ.setdefault(\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\", \"python\")\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "\n",
    "# If you already imported clients earlier, reload so they see the env.\n",
    "from importlib import reload\n",
    "import models.llm_clients as llm; reload(llm)\n",
    "\n",
    "print(\"GOOGLE_API_KEY set:\", bool(os.getenv(\"GOOGLE_API_KEY\")))\n",
    "print(\"OPENROUTER_API_KEY set:\", bool(os.getenv(\"OPENROUTER_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2450d4-f3bb-41ca-8797-8cc6dbdeda90",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 2: LLM &Tokenizer Loading with 4-bit Quantization Model + Save in cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a31469cf-f7a5-4bca-95ff-e68756ff95f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8c914f0d22448698b624e223b57efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models.loader import load_model_and_tokenizer\n",
    "from models.config import load_config\n",
    "\n",
    "cfg = load_config()\n",
    "cfg.model.model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "cfg.cache.root = \"./cache\"\n",
    "tokenizer, model, used_cache = load_model_and_tokenizer(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92b6bc6-346c-4994-8116-a0bc60ac0685",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 3: Dataset and Knowledge Base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a24fc7b-7746-45b5-888b-8c8125923ef0",
   "metadata": {
    "tags": []
   },
   "source": [
    "### -> 3.1:  Uploading Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8fab863e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Loading dataset 'JetBrains-Research/lca-library-based-code-generation' (split='test')…\n",
      "Dataset loaded with 150 examples across all libraries.\n",
      "Filtering to repos: ['seed-emulator', 'pyscf__pyscf']\n",
      "Filtered to 23 examples.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b87aea0fbe4841880a1835b7d61bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4e3de3d8d74be3bbb11ac4a4796e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HuggingFace cache dir: C:\\Users\\drugm\\.cache\\huggingface\\datasets\n",
      "cache_files (full split): [{'filename': 'C:\\\\Users\\\\drugm\\\\.cache\\\\huggingface\\\\datasets\\\\JetBrains-Research___lca-library-based-code-generation\\\\default\\\\0.0.0\\\\dc460b7e403f63e58ec7128b80eaf2c9c95344f8\\\\lca-library-based-code-generation-test.arrow'}]\n",
      "\n",
      "===== SUMMARY =====\n",
      "Split: test\n",
      "Total in split: 150\n",
      "Filter repos: ['seed-emulator', 'pyscf__pyscf']\n",
      "Total after filter: 23\n",
      "\n",
      "Saved files:\n",
      " - Full split   : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\data\\lca_test_all.jsonl\n",
      " - Filtered     : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\data\\lca_test_filtered.jsonl\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Load + (optional) Filter + Save + Print paths (usando pipeline.dataset_ops)\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "from pipeline.dataset_ops import (\n",
    "    ensure_out_dir,\n",
    "    save_dataset_jsonl,\n",
    "    load_and_optionally_filter,\n",
    "    print_summary,\n",
    ")\n",
    "\n",
    "# Parametri\n",
    "DATASET_NAME = \"JetBrains-Research/lca-library-based-code-generation\"\n",
    "SPLIT = \"test\"  # \"train\" | \"validation\" | \"test\"\n",
    "TARGET_REPOS = [\"seed-emulator\", \"pyscf__pyscf\"]  # [] per non filtrare\n",
    "\n",
    "# Output\n",
    "OUT_DIR = Path(\"data\")\n",
    "OUT_ALL = OUT_DIR / f\"lca_{SPLIT}_all.jsonl\"\n",
    "OUT_FILT = OUT_DIR / f\"lca_{SPLIT}_filtered.jsonl\"\n",
    "\n",
    "# Esecuzione\n",
    "ensure_out_dir(OUT_DIR)\n",
    "\n",
    "ds_full, ds_filt = load_and_optionally_filter(\n",
    "    dataset_name=DATASET_NAME,\n",
    "    split=SPLIT,\n",
    "    target_repos=TARGET_REPOS,\n",
    ")\n",
    "\n",
    "save_dataset_jsonl(ds_full, OUT_ALL)\n",
    "save_dataset_jsonl(ds_filt, OUT_FILT)\n",
    "\n",
    "print_summary(\n",
    "    ds_full=ds_full,\n",
    "    ds_filt=ds_filt,\n",
    "    split=SPLIT,\n",
    "    target_repos=TARGET_REPOS,\n",
    "    out_all=OUT_ALL,\n",
    "    out_filt=OUT_FILT,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5fd5be-365b-4854-8f4b-2c29657fa34c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### -> 3.2 Uploading KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "be3ab82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- RAG Knowledge Base Configuration (GitHub) ---\n",
      "  KBs will be downloaded from GitHub base URL: https://raw.githubusercontent.com/PatrizioAcquadro/RAG_Project_SE2/main/knowledge_bases_prod/kb_LIBRARY_KEY.json\n",
      "  Downloaded KBs will be temporarily stored in: ./temp_downloaded_kbs\n",
      "Fetching KB files from GitHub…\n",
      "\n",
      "Found 64 KB files on remote\n",
      "\n",
      "--- RAG Knowledge Base Configuration (GitHub) ---\n",
      "  Base URL: https://raw.githubusercontent.com/PatrizioAcquadro/RAG_Project_SE2/main/knowledge_bases_prod/kb_LIBRARY_KEY.json\n",
      "  Local directory: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\temp_downloaded_kbs\n",
      "\n",
      "Target KBs:\n",
      " - seed-emulator: kb_seed-labs__seed-emulator.json -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\temp_downloaded_kbs\\kb_seed-labs__seed-emulator.json (exists=True, size=885251)\n",
      " - pyscf: kb_pyscf__pyscf.json -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\temp_downloaded_kbs\\kb_pyscf__pyscf.json (exists=True, size=8315405)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# KB configuration + filter/download 2 specific KBs + print paths\n",
    "# ============================================================\n",
    "from pipeline.kb_ops import (\n",
    "    configure_kb,\n",
    "    list_kbs_from_remote,\n",
    "    fetch_target_kbs,\n",
    "    print_kb_summary,\n",
    ")\n",
    "from utils.kb_manager import kb_base_url\n",
    "\n",
    "# Parametri di configurazione (adatta se necessario)\n",
    "CFG = configure_kb(\n",
    "    username=\"PatrizioAcquadro\",\n",
    "    repo_name=\"RAG_Project_SE2\",\n",
    "    branch=\"main\",\n",
    "    kb_folder=\"knowledge_bases_prod\",\n",
    "    local_dir=\"./temp_downloaded_kbs\",\n",
    ")\n",
    "\n",
    "# Stampa info base come nella tua cella originale\n",
    "print(\"--- RAG Knowledge Base Configuration (GitHub) ---\")\n",
    "print(f\"  KBs will be downloaded from GitHub base URL: {kb_base_url().rstrip('/')}/kb_LIBRARY_KEY.json\")\n",
    "print(f\"  Downloaded KBs will be temporarily stored in: {CFG.local_dir}\")\n",
    "\n",
    "# Elenco dei KB remoti disponibili (primi 20 per non stampare troppo)\n",
    "try:\n",
    "    remote_kbs = list_kbs_from_remote()\n",
    "    print(f\"\\nFound {len(remote_kbs)} KB files on remote\")\n",
    "    for x in remote_kbs[:20]:\n",
    "        size_info = f\" (size: {x.get('size')} bytes)\" if x.get(\"size\") is not None else \"\"\n",
    "except Exception as e:\n",
    "    print(\"Error fetching KB metadata from GitHub:\", e)\n",
    "\n",
    "# Filtriamo per le due librerie di interesse e assicuriamoci che siano presenti localmente\n",
    "TARGET_REPOS = [\"seed-emulator\", \"pyscf\"]  # puoi usare anche \"pyscf__pyscf\"\n",
    "downloaded = fetch_target_kbs(CFG, TARGET_REPOS)\n",
    "\n",
    "# Riepilogo finale con i path locali dove trovarli§\n",
    "print()\n",
    "print_kb_summary(CFG, TARGET_REPOS, downloaded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213c0fac-7bd3-4020-bfe2-64baf9bdc32e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### -> 3.3 Check presence of Dataset and KB in the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b7ac3dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching KB files from GitHub…\n",
      "=== Remote (GitHub) KB check ===\n",
      "Target repos: ['seed-emulator', 'pyscf']\n",
      "Known KB mapping (default_kb_name_map): {'seed-emulator': 'kb_seed-labs__seed-emulator.json', 'pyscf': 'kb_pyscf__pyscf.json', 'pyscf__pyscf': 'kb_pyscf__pyscf.json'}\n",
      "KB required (resolved): {'seed-emulator': 'kb_seed-labs__seed-emulator.json', 'pyscf': 'kb_pyscf__pyscf.json'}\n",
      "KB present on remote: ['kb_seed-labs__seed-emulator.json', 'kb_pyscf__pyscf.json']\n",
      "KB missing on remote: []\n",
      "\n",
      "=== Local cache check ===\n",
      "Ready for retrieval:\n",
      " - seed-emulator: kb_seed-labs__seed-emulator.json -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\temp_downloaded_kbs\\kb_seed-labs__seed-emulator.json (size=885251)\n",
      " - pyscf: kb_pyscf__pyscf.json -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\temp_downloaded_kbs\\kb_pyscf__pyscf.json (size=8315405)\n",
      "Not ready:\n",
      "\n",
      "Summary:\n",
      "  Target repos            : 2\n",
      "  Remote KB available     : 2\n",
      "  Remote KB missing       : 0\n",
      "  Locally ready (download): 2\n",
      "  Locally missing         : 0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Verifica KB per le repo target: presenza su GitHub e in locale\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "\n",
    "# Import dai moduli di utilità\n",
    "try:\n",
    "    CFG\n",
    "except NameError:\n",
    "    CFG = None\n",
    "\n",
    "from pipeline.kb_ops import (\n",
    "    configure_kb,\n",
    "    default_kb_name_map,\n",
    "    resolve_kb_names_for_repos,\n",
    "    list_kbs_from_remote,\n",
    "    fetch_target_kbs,\n",
    ")\n",
    "\n",
    "# Parametri\n",
    "TARGET_REPOS = [\"seed-emulator\", \"pyscf\"]  # usa \"pyscf__pyscf\" se preferisci lo stesso alias del dataset\n",
    "\n",
    "# Configurazione (se non già presente)\n",
    "if CFG is None:\n",
    "    CFG = configure_kb(\n",
    "        username=\"PatrizioAcquadro\",\n",
    "        repo_name=\"RAG_Project_SE2\",\n",
    "        branch=\"main\",\n",
    "        kb_folder=\"knowledge_bases_prod\",\n",
    "        local_dir=\"./temp_downloaded_kbs\",\n",
    "    )\n",
    "\n",
    "# 1) Risolviamo i nomi dei file KB attesi per le repo target\n",
    "repo_to_kb = resolve_kb_names_for_repos(TARGET_REPOS)  # es.: {\"seed-emulator\": \"kb_seed-labs__seed-emulator.json\", \"pyscf\": \"kb_pyscf__pyscf.json\"}\n",
    "\n",
    "# 2) Elenco remoto (GitHub) e verifica esistenza dei KB richiesti\n",
    "remote = list_kbs_from_remote()\n",
    "remote_names = {item.get(\"name\") for item in remote if \"name\" in item}\n",
    "\n",
    "missing_remote = {repo: fname for repo, fname in repo_to_kb.items() if fname not in remote_names}\n",
    "present_remote = {repo: fname for repo, fname in repo_to_kb.items() if fname in remote_names}\n",
    "\n",
    "print(\"=== Remote (GitHub) KB check ===\")\n",
    "print(f\"Target repos: {TARGET_REPOS}\")\n",
    "print(f\"Known KB mapping (default_kb_name_map): {default_kb_name_map()}\")\n",
    "print(f\"KB required (resolved): {repo_to_kb}\")\n",
    "print(f\"KB present on remote: {list(present_remote.values())}\")\n",
    "print(f\"KB missing on remote: {list(missing_remote.values())}\")\n",
    "\n",
    "# 3) Se esistono su remoto, assicuriamoci di averli in locale e stampiamo i path\n",
    "downloaded_rows = []\n",
    "if present_remote:\n",
    "    # scarica/valida solo quelli presenti su remoto\n",
    "    present_repos = list(present_remote.keys())\n",
    "    downloaded_rows = fetch_target_kbs(CFG, present_repos)\n",
    "\n",
    "# 4) Riepilogo finale: per ogni repo target indichiamo se è pronta al retrieval\n",
    "print(\"\\n=== Local cache check ===\")\n",
    "ready, not_ready = [], []\n",
    "rows_by_repo = {row[\"repo_name\"]: row for row in downloaded_rows}\n",
    "\n",
    "for repo in TARGET_REPOS:\n",
    "    kb_fname = repo_to_kb.get(repo)\n",
    "    remote_ok = kb_fname in remote_names\n",
    "    row = rows_by_repo.get(repo)\n",
    "    local_ok = bool(row and row[\"exists\"] and (row[\"size_bytes\"] or 0) > 0)\n",
    "    local_path = Path(row[\"local_path\"]).resolve() if row else None\n",
    "\n",
    "    status = {\n",
    "        \"repo\": repo,\n",
    "        \"kb_filename\": kb_fname,\n",
    "        \"remote_found\": remote_ok,\n",
    "        \"local_found\": local_ok,\n",
    "        \"local_path\": str(local_path) if local_path else None,\n",
    "        \"size_bytes\": (row[\"size_bytes\"] if row else None),\n",
    "    }\n",
    "    if remote_ok and local_ok:\n",
    "        ready.append(status)\n",
    "    else:\n",
    "        not_ready.append(status)\n",
    "\n",
    "# Stampa dettagli\n",
    "print(\"Ready for retrieval:\")\n",
    "for s in ready:\n",
    "    print(f\" - {s['repo']}: {s['kb_filename']} -> {s['local_path']} (size={s['size_bytes']})\")\n",
    "\n",
    "print(\"Not ready:\")\n",
    "for s in not_ready:\n",
    "    print(f\" - {s['repo']}: remote_found={s['remote_found']} local_found={s['local_found']} kb_filename={s['kb_filename']} local_path={s['local_path']}\")\n",
    "\n",
    "# Esito sintetico\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"  Target repos            : {len(TARGET_REPOS)}\")\n",
    "print(f\"  Remote KB available     : {len(present_remote)}\")\n",
    "print(f\"  Remote KB missing       : {len(missing_remote)}\")\n",
    "print(f\"  Locally ready (download): {len(ready)}\")\n",
    "print(f\"  Locally missing         : {len(not_ready)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b301f3-32c0-48fe-a516-59f8da833f8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 4: RETRIVAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a72c5cc-cffc-4121-b6d2-eb7a0a82eee3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 -> BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "dc33482e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cache hit: found existing results for K=3 at 'BM25\\retrieved_k3_samples.json'.\n",
      "BM25 summary:\n",
      "- num_queries_payload: 150\n",
      "- num_kbs_payload: 62\n",
      "- raw_json_path: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\BM25\\retrieved_k3_samples.json\n",
      "- normalized_jsonl_path: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\bm25_topk_k3.jsonl\n",
      "- exported_queries: 0\n",
      "- k: 3\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BM25: esecuzione filtrata su seed-emulator / pyscf + export JSONL normalizzato\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "from BM25.pipeline import run_bm25_and_export\n",
    "\n",
    "# Assunto: lca_dataset_split è già definito (dataset filtrato o completo)\n",
    "# Se non lo è, caricalo prima (es. dalla cella che abbiamo scritto in precedenza)\n",
    "\n",
    "TARGET_REPOS = [\"seed-emulator\", \"pyscf__pyscf\"]  # usa \"pyscf\" se è questo l'alias nel tuo dataset\n",
    "TOP_K = 3\n",
    "\n",
    "summary = run_bm25_and_export(\n",
    "    dataset=lca_dataset_split,\n",
    "    target_repos=TARGET_REPOS,\n",
    "    top_k=TOP_K,\n",
    "    out_dir=Path(\"outputs/retrieval\"),\n",
    "    bm25_k1=1.5,\n",
    "    bm25_b=0.75,\n",
    "    overwrite_kb_download=False,\n",
    "    max_samples=None,       # imposta un int per test veloci\n",
    "    force_rebuild=False,\n",
    ")\n",
    "\n",
    "print(\"BM25 summary:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"- {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a88634-9590-4f6b-8b3b-76c11c78b386",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2  -> CodeBert Embedding & Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "7c073be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COSINE] queries selezionate: 23 | k=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COSINE: 100%|██████████| 23/23 [00:47<00:00,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COSINE] Saved files:\n",
      " - RAW          : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\cosine_raw_k3.json\n",
      " - JSONL        : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\cosine_topk_k3.jsonl\n",
      " - JSON (array) : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\cosine_topk_k3.json\n",
      "[COSINE] Exported queries: 23 | k = 3 | metric = cosine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# COSINE — esecuzione batch\n",
    "# ============================\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "import COSINE.batch as cosine_batch; reload(cosine_batch)\n",
    "\n",
    "# Parametri principali\n",
    "TARGET_REPOS = [\"seed-emulator\", \"pyscf\", \"pyscf__pyscf\"]\n",
    "TOP_K = 3\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "BATCH_SIZE = 32\n",
    "RRF_K = 60\n",
    "BM25_K1 = 1.5\n",
    "BM25_B  = 0.75\n",
    "MAX_SAMPLES = None  # usa None per tutte le query già filtrate nel tuo lca_dataset_split\n",
    "\n",
    "# Esecuzione\n",
    "result = cosine_batch.run_cosine_retrieval(\n",
    "    dataset=lca_dataset_split,\n",
    "    target_repos=TARGET_REPOS,\n",
    "    top_k=TOP_K,\n",
    "    model_name=MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    rrf_k=RRF_K,\n",
    "    bm25_k1=BM25_K1,\n",
    "    bm25_b=BM25_B,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    out_dir=Path(\"outputs/retrieval\"),\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f3989d",
   "metadata": {},
   "source": [
    "### 4.3  -> Hybrid Approach Cosine Similarity & BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d8cb7be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HYBRID] queries selezionate: 23 | k=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HYBRID: 100%|██████████| 23/23 [00:46<00:00,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HYBRID] Saved files:\n",
      " - RAW          : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\hybrid_raw_k3.json\n",
      " - JSONL        : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\hybrid_topk_k3.jsonl\n",
      " - JSON (array) : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\hybrid_topk_k3.json\n",
      "[HYBRID] Exported queries: 23 | k = 3 | metric = hybrid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# HYBRID — esecuzione batch\n",
    "# ============================\n",
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "import HYBRID.batch as hybrid_batch; reload(hybrid_batch)\n",
    "\n",
    "# Parametri principali\n",
    "TARGET_REPOS = [\"seed-emulator\", \"pyscf\", \"pyscf__pyscf\"]\n",
    "TOP_K = 3\n",
    "MODEL_NAME = \"microsoft/codebert-base\"\n",
    "BATCH_SIZE = 32\n",
    "RRF_K = 60\n",
    "BM25_K1 = 1.5\n",
    "BM25_B  = 0.75\n",
    "MAX_SAMPLES = None  # None => tutte le query filtrate\n",
    "\n",
    "# Esecuzione\n",
    "result = hybrid_batch.run_hybrid_retrieval(\n",
    "    dataset=lca_dataset_split,\n",
    "    target_repos=TARGET_REPOS,\n",
    "    top_k=TOP_K,\n",
    "    model_name=MODEL_NAME,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    rrf_k=RRF_K,\n",
    "    bm25_k1=BM25_K1,\n",
    "    bm25_b=BM25_B,\n",
    "    max_samples=MAX_SAMPLES,\n",
    "    out_dir=Path(\"outputs/retrieval\"),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2101493-c356-4467-a695-b77d64d4820a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.4 Multi-Hop: Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852120d",
   "metadata": {},
   "source": [
    "#### 4.4.0 Multi-Hop Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7fd207c4-62bb-4680-866a-40083fcde125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Cell 1: Imports / reloads ===\n",
    "from importlib import reload\n",
    "import os, json, time, pandas as pd\n",
    "\n",
    "# Project helpers and multihop engine\n",
    "import S6.mh_helpers as MH; reload(MH)\n",
    "from S6.mh_helpers import discover_libs, filters_for, resolve_instructions  # add resolve_instructions\n",
    "import S6.multihop as mh; reload(mh)\n",
    "\n",
    "from models.retrieval_providers import make_provider  # retriever factory\n",
    "from data.datasets import load_lca_dataset            # HF dataset loader for 'instruction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "6ac86ad3-f409-4790-80d8-07cd0876cf0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Loading dataset 'JetBrains-Research/lca-library-based-code-generation' (split='test')…\n",
      "Dataset loaded with 150 examples across all libraries.\n",
      "[TEST] mode=dataset  #items=1\n",
      "Discovered 62 libs\n",
      "Sample: ['1200wd__bitcoinlib', 'aidasoft__dd4hep', 'ansys__pyaedt', 'ansys__pydpf-core', 'ansys__pymapdl']\n",
      "[INFO] retrieval_providers: BM25 indexed docs: 133530 across 62 libs (local)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Variables / config (robust version) ===\n",
    "\n",
    "# Defensive import: make sure discover_libs / filters_for are available even if Cell 1 wasn't run\n",
    "try:\n",
    "    discover_libs  # type: ignore[name-defined]\n",
    "    filters_for    # type: ignore[name-defined]\n",
    "    resolve_instructions  # type: ignore[name-defined]\n",
    "except NameError:\n",
    "    from importlib import reload\n",
    "    import sys, os\n",
    "    ROOT = os.path.abspath(\".\")\n",
    "    if ROOT not in sys.path:\n",
    "        sys.path.insert(0, ROOT)\n",
    "    import S6.mh_helpers as MH; reload(MH)\n",
    "    from S6.mh_helpers import discover_libs, filters_for, resolve_instructions\n",
    "\n",
    "# --- SINGLE SWITCH for instruction selection ---\n",
    "# Choices:\n",
    "#   \"custom\" | \"all\" | 5 (random) | \"idx:12\" | [0, 7, 12]\n",
    "INSTRUCTION_SELECTION = \"idx:0\"     # change just this one knob\n",
    "\n",
    "# Load dataset only if needed ( internet required for first HF download )\n",
    "DS = load_lca_dataset(split=\"test\") if INSTRUCTION_SELECTION != \"custom\" else None\n",
    "\n",
    "# Materialize questions according to the single switch\n",
    "# When selection == \"custom\", we feed your two hand-written questions (below)\n",
    "TEST_SET = resolve_instructions(\n",
    "    INSTRUCTION_SELECTION,\n",
    "    dataset=DS,\n",
    "    custom=[\n",
    "        \"How does the library create a BIP39 mnemonic and derive a seed?\",\n",
    "        \"Where is BIP32 key derivation implemented?\",\n",
    "    ],\n",
    "    seed=42,\n",
    ")\n",
    "print(f\"[TEST] mode={TEST_SET['mode']}  #items={len(TEST_SET['items'])}\")\n",
    "\n",
    "# KB (libs) parameters\n",
    "KB_DIR = \"temp_downloaded_kbs\"\n",
    "\n",
    "# Discover and index ALL libraries under KB_DIR (always index everything)\n",
    "LIBS = discover_libs(KB_DIR)\n",
    "\n",
    "# Runtime selection:\n",
    "# None = open-world; str = single repo; list[str] = multiple repos\n",
    "SELECTED_LIBS = None                     # default: search across all indexed libs\n",
    "F = filters_for(SELECTED_LIBS)           # None | {\"repo\": str} | {\"repo\": [..,..]}\n",
    "REPO_FILTER = SELECTED_LIBS              # back-compat alias\n",
    "\n",
    "print(f\"Discovered {len(LIBS)} libs\")\n",
    "print(\"Sample:\", LIBS[:5])\n",
    "\n",
    "# Retriever selector\n",
    "RETRIEVER = globals().get(\"RETRIEVER\", \"bm25\")  # \"bm25\" | \"knn\" | \"hybrid\"\n",
    "\n",
    "# LLM settings\n",
    "BACKEND = \"gemini\"                        # \"gemini\" | \"openrouter\" | \"local\"\n",
    "MODEL   = \"gemini-2.0-flash\"              # or \"gemini-2.0-flash\" | \"deepseek/deepseek-chat-v3.1:free\" | \"cache/Qwen_Qwen2.5-Coder-7B-Instruct_4bit_nf4\"\n",
    "\n",
    "# Evaluation & output\n",
    "top_k      = 5\n",
    "CSV_DIR    = \"results/multihop_csv\"\n",
    "COMPARE_FN = \"multihop_compare.csv\"\n",
    "IMPACT_FN  = \"multihop_topk_impact.csv\"\n",
    "PLANNER_CTX_DOCS  = 3\n",
    "PLANNER_CTX_CHARS = 500\n",
    "PLAN_MAX_TOKENS   = 64\n",
    "DECOMP_MAX_TOKENS = 96\n",
    "SHOW_TOP          = min(5, top_k)\n",
    "\n",
    "# Questions used across runs (kept for readability; selection is driven by TEST_SET)\n",
    "Q_DECOMP = \"How does the library create a BIP39 mnemonic and derive a seed?\"\n",
    "Q_IR     = \"Where is BIP32 key derivation implemented?\"\n",
    "\n",
    "# Output scores normalized (for better visualization)\n",
    "MH.NORMALIZE_SCORES = True\n",
    "MH.NORMALIZE_MAX    = 100.0\n",
    "\n",
    "# --- Provider built via your retriever switch; always index the full KB for BM25 ---\n",
    "if RETRIEVER == \"bm25\":\n",
    "    provider = make_provider(\n",
    "        \"bm25\",\n",
    "        local_kb_dir=KB_DIR,\n",
    "        library_filter=LIBS,\n",
    "        tokenizer=None,  # or pass your bm25 tokenizer\n",
    "    )\n",
    "elif RETRIEVER == \"knn\":\n",
    "    # Requires retrievers/KNN.py implemented + a built index\n",
    "    provider = make_provider(\n",
    "        \"knn\",\n",
    "        index_dir=\"indexes/knn\",       # adjust to your KNN index path\n",
    "        index_type=\"hnsw\",             # or \"flat\"\n",
    "        ef_search=128,\n",
    "        nprobe=16,\n",
    "        # embedder=...                 # optional: your SentenceTransformer etc.\n",
    "    )\n",
    "elif RETRIEVER == \"hybrid\":\n",
    "    # Requires retrievers/HYBRID.py and KNN available\n",
    "    provider = make_provider(\n",
    "        \"hybrid\",\n",
    "        bm25_kwargs=dict(local_kb_dir=KB_DIR, library_filter=LIBS),\n",
    "        knn_kwargs=dict(index_dir=\"indexes/knn\", index_type=\"hnsw\", ef_search=128, nprobe=16),\n",
    "        fusion=\"rrf\", alpha=0.5, rrf_k=60,\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(f\"Unknown RETRIEVER: {RETRIEVER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "5b0176b3-5ebd-4136-a58c-b448380bcd35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Decomposition-first ===\n",
      "Question           : Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n",
      "\n",
      "The base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n",
      "\n",
      "The domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n",
      "\n",
      "The eBGP layer should add private peerings between different autonomous systems. \n",
      "\n",
      "Finally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file.\n",
      "Strategy           : decomposition_first\n",
      "k_sub / k_final    : 2 / 5\n",
      "max_hops           : 1\n",
      "Backend / Model    : gemini / gemini-2.0-flash\n",
      "\n",
      "Final top-k (after the strategy):\n",
      "Top-5 hits (normalized 0..100):\n",
      "   1. [seed-labs__seed-emulator] doc_id=507  score=100.00\n",
      "   2. [seed-labs__seed-emulator] doc_id=569  score=97.17\n",
      "   3. [seed-labs__seed-emulator] doc_id=641  score=92.60\n",
      "   4. [seed-labs__seed-emulator] doc_id=544  score=92.03\n",
      "   5. [seed-labs__seed-emulator] doc_id=12  score=57.24\n",
      "Latency: 275 ms | Total final hits: 5\n",
      "\n",
      "--- Per-subquery retrieval (top_k each) ---\n",
      "  Q0: \"seedemu\"\n",
      "     1. doc_id=12  score=100.000  [seed-labs__seed-emulator]\n",
      "     2. doc_id=569  score=95.610  [seed-labs__seed-emulator]\n",
      "     3. doc_id=544  score=91.010  [seed-labs__seed-emulator]\n",
      "     4. doc_id=13  score=90.490  [seed-labs__seed-emulator]\n",
      "     5. doc_id=419  score=84.430  [seed-labs__seed-emulator]\n",
      "  Q1: \"seedemu\" AND \"emulator\" AND \"layers\"\n",
      "     1. doc_id=507  score=100.000  [seed-labs__seed-emulator]\n",
      "     2. doc_id=569  score=97.170  [seed-labs__seed-emulator]\n",
      "     3. doc_id=641  score=92.600  [seed-labs__seed-emulator]\n",
      "     4. doc_id=611  score=92.180  [seed-labs__seed-emulator]\n",
      "     5. doc_id=544  score=92.030  [seed-labs__seed-emulator]\n",
      "\n",
      "=== Iterative-refine ===\n",
      "Question           : Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n",
      "\n",
      "The base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n",
      "\n",
      "The domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n",
      "\n",
      "The eBGP layer should add private peerings between different autonomous systems. \n",
      "\n",
      "Finally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file.\n",
      "Strategy           : iterative_refine\n",
      "k_sub / k_final    : 1 / 5\n",
      "max_hops           : 3\n",
      "Backend / Model    : gemini / gemini-2.0-flash\n",
      "\n",
      "Final top-k (after the strategy):\n",
      "Top-5 hits (normalized 0..100):\n",
      "   1. [seed-labs__seed-emulator] doc_id=988  score=100.00\n",
      "   2. [seed-labs__seed-emulator] doc_id=987  score=96.93\n",
      "   3. [seed-labs__seed-emulator] doc_id=87  score=90.93\n",
      "   4. [seed-labs__seed-emulator] doc_id=84  score=89.52\n",
      "   5. [seed-labs__seed-emulator] doc_id=88  score=68.96\n",
      "Latency: 7211 ms | Total final hits: 5\n",
      "\n",
      "--- Per-query retrieval progression (top_k each) ---\n",
      "  Q0: Generate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n",
      "\n",
      "The base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n",
      "\n",
      "The domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n",
      "\n",
      "The eBGP layer should add private peerings between different autonomous systems. \n",
      "\n",
      "Finally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file.\n",
      "     1. doc_id=985  score=100.000  [seed-labs__seed-emulator]\n",
      "     2. doc_id=988  score=88.940  [seed-labs__seed-emulator]\n",
      "     3. doc_id=987  score=82.830  [seed-labs__seed-emulator]\n",
      "     4. doc_id=607  score=78.620  [seed-labs__seed-emulator]\n",
      "     5. doc_id=721  score=75.350  [seed-labs__seed-emulator]\n",
      "  Q1: seedemu Emulator Base Routing Ebgp createInternetExchange \"stub AS\" Service\n",
      "     1. doc_id=988  score=100.000  [seed-labs__seed-emulator]\n",
      "     2. doc_id=987  score=95.760  [seed-labs__seed-emulator]\n",
      "     3. doc_id=87  score=67.990  [seed-labs__seed-emulator]\n",
      "     4. doc_id=84  score=66.490  [seed-labs__seed-emulator]\n",
      "     5. doc_id=611  score=63.240  [seed-labs__seed-emulator]\n",
      "  Q2: seedemu Emulator Base Routing Ebgp createInternetExchange getLayer dump binary file\n",
      "     1. doc_id=988  score=100.000  [seed-labs__seed-emulator]\n",
      "     2. doc_id=987  score=96.930  [seed-labs__seed-emulator]\n",
      "     3. doc_id=87  score=90.930  [seed-labs__seed-emulator]\n",
      "     4. doc_id=84  score=89.520  [seed-labs__seed-emulator]\n",
      "     5. doc_id=88  score=68.960  [seed-labs__seed-emulator]\n",
      "\n",
      "Compare CSV appended: results/multihop_csv\\multihop_compare.csv\n",
      "\n",
      "Compact comparison table (preview):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_iso</th>\n",
       "      <th>strategy</th>\n",
       "      <th>backend</th>\n",
       "      <th>model</th>\n",
       "      <th>retriever</th>\n",
       "      <th>k_final</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>unique_repos</th>\n",
       "      <th>diversity</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>jaccard_vs_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-20T12:21:10</td>\n",
       "      <td>decomposition_first</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>bm25</td>\n",
       "      <td>5</td>\n",
       "      <td>275.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>87.81</td>\n",
       "      <td>100.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-20T12:21:23</td>\n",
       "      <td>iterative_refine</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>bm25</td>\n",
       "      <td>5</td>\n",
       "      <td>7211.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>89.27</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp_iso             strategy backend             model  \\\n",
       "0  2025-09-20T12:21:10  decomposition_first  gemini  gemini-2.0-flash   \n",
       "1  2025-09-20T12:21:23     iterative_refine  gemini  gemini-2.0-flash   \n",
       "\n",
       "  retriever  k_final  latency_ms  unique_repos  diversity  mean_score  \\\n",
       "0      bm25        5       275.0             1        0.2       87.81   \n",
       "1      bm25        5      7211.0             1        0.2       89.27   \n",
       "\n",
       "   max_score  jaccard_vs_baseline  \n",
       "0      100.0                  NaN  \n",
       "1      100.0                  0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 3: Detailed multihop run, compare CSV, compact table preview ===\n",
    "# Prints: initial question, model-generated (sub)queries, FINAL top-k, and per-(sub)query top-k.\n",
    "# Appends rows to results/multihop_csv/multihop_compare.csv and previews a compact table.\n",
    "\n",
    "def _filters():\n",
    "    \"\"\"Return the normalized filters dict computed in Cell 2.\"\"\"\n",
    "    return F\n",
    "\n",
    "def _per_query_print(per_q_hits: dict, show_top: int):\n",
    "    \"\"\"Pretty-print per-query compact hits (doc_id, score, repo).\"\"\"\n",
    "    if not per_q_hits:\n",
    "        print(\"  (no per-query results)\")\n",
    "        return\n",
    "    for i, (q, hits) in enumerate(per_q_hits.items()):\n",
    "        print(f\"  Q{i}: {q}\")\n",
    "        if not hits:\n",
    "            print(\"    (no hits)\")\n",
    "            continue\n",
    "        for j, h in enumerate(hits[:show_top], 1):\n",
    "            print(f\"    {j:>2}. doc_id={h['doc_id']}  score={h['score']:.3f}  [{h['repo']}]\")\n",
    "\n",
    "rows = []  # rows that will be written to compare CSV and shown in preview\n",
    "\n",
    "def run_detailed_for_question(question: str):\n",
    "    # ----- Run decomposition_first (detailed) -----\n",
    "    pack1, hits1, cfg1, lat1 = MH.run_mode(\n",
    "        provider=provider,\n",
    "        question=question,\n",
    "        strategy_key=\"decomposition_first\",\n",
    "        top_k=top_k,\n",
    "        repo_filter=REPO_FILTER,                   # legacy arg; normalized internally\n",
    "        backend=BACKEND,\n",
    "        model=MODEL,\n",
    "        cache_dir=\".cache/mh_detailed_decomposition_first\",\n",
    "        decomposer_max_tokens=DECOMP_MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "    MH.print_header(\"Decomposition-first\", question, cfg1, _filters())\n",
    "    print(\"\\nFinal top-k (after the strategy):\")\n",
    "    MH.print_hits(hits1, top_k)\n",
    "    print(f\"Latency: {lat1:.0f} ms | Total final hits: {len(hits1)}\")\n",
    "\n",
    "    # Per-subquery: re-run retrieval for each subquery (same provider/search) and print compact lists\n",
    "    subs1 = MH.extract_subqueries(pack1)\n",
    "    print(\"\\n--- Per-subquery retrieval (top_k each) ---\")\n",
    "    per_q_hits_1 = MH.per_query_hits_dict(subs1, provider, top_k, _filters())\n",
    "    _per_query_print(per_q_hits_1, SHOW_TOP)\n",
    "\n",
    "    # Build + collect the compare row\n",
    "    row1 = MH.build_compare_row(\n",
    "        strategy=\"decomposition_first\", backend=BACKEND, model=MODEL, retriever=RETRIEVER,\n",
    "        cfg=cfg1, repo_filter=REPO_FILTER, latency_ms=lat1,\n",
    "        hits=hits1, question=question, kb_dir=KB_DIR, libs=LIBS,\n",
    "        jaccard_vs_baseline=None,\n",
    "    )\n",
    "    row1[\"subqueries\"] = \" || \".join(subs1)\n",
    "    rows.append(row1)\n",
    "\n",
    "    # ----- Run iterative_refine (detailed) -----\n",
    "    pack2, hits2, cfg2, lat2 = MH.run_mode(\n",
    "        provider=provider,\n",
    "        question=question,\n",
    "        strategy_key=\"iterative_refine\",\n",
    "        top_k=top_k,\n",
    "        repo_filter=REPO_FILTER,\n",
    "        backend=BACKEND,\n",
    "        model=MODEL,\n",
    "        cache_dir=\".cache/mh_detailed_iterative_refine\",\n",
    "        planner_ctx_docs=PLANNER_CTX_DOCS,\n",
    "        planner_ctx_chars=PLANNER_CTX_CHARS,\n",
    "        planner_max_tokens=PLAN_MAX_TOKENS,\n",
    "    )\n",
    "\n",
    "    MH.print_header(\"Iterative-refine\", question, cfg2, _filters())\n",
    "    print(\"\\nFinal top-k (after the strategy):\")\n",
    "    MH.print_hits(hits2, top_k)\n",
    "    print(f\"Latency: {lat2:.0f} ms | Total final hits: {len(hits2)}\")\n",
    "\n",
    "    # Progression: ensure initial question appears first if not already in subqueries\n",
    "    subs2 = MH.extract_subqueries(pack2)\n",
    "    if not subs2 or subs2[0] != question:\n",
    "        subs2 = [question] + subs2\n",
    "\n",
    "    print(\"\\n--- Per-query retrieval progression (top_k each) ---\")\n",
    "    per_q_hits_2 = MH.per_query_hits_dict(subs2, provider, top_k, _filters())\n",
    "    _per_query_print(per_q_hits_2, SHOW_TOP)\n",
    "\n",
    "    # Build + collect the compare row (compute Jaccard vs first run as a quick diff signal)\n",
    "    jacc = MH.jaccard_hits(hits2, hits1) if hits1 else None\n",
    "    row2 = MH.build_compare_row(\n",
    "        strategy=\"iterative_refine\", backend=BACKEND, model=MODEL, retriever=RETRIEVER,\n",
    "        cfg=cfg2, repo_filter=REPO_FILTER, latency_ms=lat2,\n",
    "        hits=hits2, question=question, kb_dir=KB_DIR, libs=LIBS,\n",
    "        jaccard_vs_baseline=jacc,\n",
    "    )\n",
    "    row2[\"subqueries\"] = \" || \".join(subs2)\n",
    "    rows.append(row2)\n",
    "\n",
    "# ---- Drive it using the single selection (NO dependency on Q_DECOMP/Q_IR) ----\n",
    "for item in TEST_SET[\"items\"]:\n",
    "    run_detailed_for_question(item[\"instruction\"])\n",
    "\n",
    "# Persist both compare rows\n",
    "compare_csv_path = MH.save_compare_csv(CSV_DIR, rows, filename=COMPARE_FN)\n",
    "print(\"\\nCompare CSV appended:\", compare_csv_path)\n",
    "\n",
    "# ---- Preview the compact comparison table (final-result metrics only) ----\n",
    "import pandas as _pd\n",
    "df_cmp = _pd.DataFrame(rows)\n",
    "cols = [\"timestamp_iso\",\"strategy\",\"backend\",\"model\",\"retriever\",\"k_final\",\"latency_ms\",\n",
    "        \"unique_repos\",\"diversity\",\"mean_score\",\"max_score\",\"jaccard_vs_baseline\"]\n",
    "print(\"\\nCompact comparison table (preview):\")\n",
    "display(df_cmp[cols].sort_values([\"strategy\",\"backend\",\"model\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "635c24a8-2ba0-4b55-810a-a0a3bf8df0b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare CSV appended: results/multihop_csv\\multihop_compare.csv\n",
      "Impact CSV appended: results/multihop_csv\\multihop_topk_impact.csv\n",
      "\n",
      "Compact comparison table (preview):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_iso</th>\n",
       "      <th>strategy</th>\n",
       "      <th>backend</th>\n",
       "      <th>model</th>\n",
       "      <th>retriever</th>\n",
       "      <th>k_final</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>unique_repos</th>\n",
       "      <th>diversity</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>max_score</th>\n",
       "      <th>jaccard_vs_baseline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-09-20T12:21:23</td>\n",
       "      <td>decomposition_first</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>bm25</td>\n",
       "      <td>3</td>\n",
       "      <td>280.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>84.80</td>\n",
       "      <td>100.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-09-20T12:21:24</td>\n",
       "      <td>decomposition_first</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>bm25</td>\n",
       "      <td>5</td>\n",
       "      <td>282.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>87.81</td>\n",
       "      <td>100.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-09-20T12:21:24</td>\n",
       "      <td>decomposition_first</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>bm25</td>\n",
       "      <td>8</td>\n",
       "      <td>638.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>90.50</td>\n",
       "      <td>100.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-09-20T12:21:32</td>\n",
       "      <td>iterative_refine</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>bm25</td>\n",
       "      <td>3</td>\n",
       "      <td>7060.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>97.27</td>\n",
       "      <td>100.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-09-20T12:21:45</td>\n",
       "      <td>iterative_refine</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>bm25</td>\n",
       "      <td>5</td>\n",
       "      <td>7218.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>92.24</td>\n",
       "      <td>100.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2025-09-20T12:21:58</td>\n",
       "      <td>iterative_refine</td>\n",
       "      <td>gemini</td>\n",
       "      <td>gemini-2.0-flash</td>\n",
       "      <td>bm25</td>\n",
       "      <td>8</td>\n",
       "      <td>7318.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>95.02</td>\n",
       "      <td>100.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         timestamp_iso             strategy backend             model  \\\n",
       "0  2025-09-20T12:21:23  decomposition_first  gemini  gemini-2.0-flash   \n",
       "1  2025-09-20T12:21:24  decomposition_first  gemini  gemini-2.0-flash   \n",
       "2  2025-09-20T12:21:24  decomposition_first  gemini  gemini-2.0-flash   \n",
       "3  2025-09-20T12:21:32     iterative_refine  gemini  gemini-2.0-flash   \n",
       "4  2025-09-20T12:21:45     iterative_refine  gemini  gemini-2.0-flash   \n",
       "5  2025-09-20T12:21:58     iterative_refine  gemini  gemini-2.0-flash   \n",
       "\n",
       "  retriever  k_final  latency_ms  unique_repos  diversity  mean_score  \\\n",
       "0      bm25        3       280.0             1     0.3333       84.80   \n",
       "1      bm25        5       282.0             1     0.2000       87.81   \n",
       "2      bm25        8       638.0             1     0.1250       90.50   \n",
       "3      bm25        3      7060.0             1     0.3333       97.27   \n",
       "4      bm25        5      7218.0             1     0.2000       92.24   \n",
       "5      bm25        8      7318.0             1     0.1250       95.02   \n",
       "\n",
       "   max_score jaccard_vs_baseline  \n",
       "0      100.0                None  \n",
       "1      100.0                None  \n",
       "2      100.0                None  \n",
       "3      100.0                None  \n",
       "4      100.0                None  \n",
       "5      100.0                None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 4: top_k summary sweep (final-only), impact CSV, compact comparison table preview ===\n",
    "# Focuses on final results and compact metrics across k values.\n",
    "\n",
    "TOPK_GRID = [3, 5, 8]\n",
    "SUMMARY_STRATEGIES = [\"decomposition_first\", \"iterative_refine\"]\n",
    "\n",
    "def _filters():\n",
    "    \"\"\"Return the normalized filters dict computed in Cell 2.\"\"\"\n",
    "    return F\n",
    "\n",
    "rows_summary = []   # for compact preview (final-only)\n",
    "impact_paths = []   # each row saved to top_k impact CSV\n",
    "\n",
    "def run_summary_for_question(question: str):\n",
    "    for strat in SUMMARY_STRATEGIES:\n",
    "        for k_val in TOPK_GRID:\n",
    "            pack, hits, cfg, latency_ms = MH.run_mode(\n",
    "                provider=provider,\n",
    "                question=question,\n",
    "                strategy_key=strat,\n",
    "                top_k=k_val,\n",
    "                repo_filter=REPO_FILTER,   # legacy arg; normalized internally\n",
    "                backend=BACKEND,\n",
    "                model=MODEL,\n",
    "                cache_dir=f\".cache/mh_topk_summary_{strat}_k{k_val}\",\n",
    "            )\n",
    "\n",
    "            # Build a compact compare row (final metrics only)\n",
    "            row = MH.build_compare_row(\n",
    "                strategy=strat, backend=BACKEND, model=MODEL, retriever=RETRIEVER,\n",
    "                cfg=cfg, repo_filter=REPO_FILTER, latency_ms=latency_ms,\n",
    "                hits=hits, question=question, kb_dir=KB_DIR, libs=LIBS,\n",
    "                jaccard_vs_baseline=None,\n",
    "            )\n",
    "            row[\"subqueries\"] = \" || \".join(MH.extract_subqueries(pack))\n",
    "            rows_summary.append(row)\n",
    "\n",
    "            # Save an impact row (keeps per-query hits in CSV for offline analysis)\n",
    "            subs = MH.extract_subqueries(pack)\n",
    "            if strat == \"iterative_refine\" and (not subs or subs[0] != question):\n",
    "                subs = [question] + subs\n",
    "            per_q_hits = MH.per_query_hits_dict(subs, provider, k_val, _filters())\n",
    "            path = MH.save_topk_impact_csv(\n",
    "                CSV_DIR,\n",
    "                strategy=strat, backend=BACKEND, model=MODEL, retriever=RETRIEVER,\n",
    "                top_k=k_val, question=question, repo_filter=REPO_FILTER,\n",
    "                subqueries=subs, per_query_hits=per_q_hits,\n",
    "                pack_hits=hits, latency_ms=latency_ms, kb_dir=KB_DIR, libs=LIBS,\n",
    "                filename=IMPACT_FN,\n",
    "            )\n",
    "            impact_paths.append(path)\n",
    "\n",
    "# Drive the sweep using the same selection (NO dependency on Q_DECOMP/Q_IR)\n",
    "for item in TEST_SET[\"items\"]:\n",
    "    run_summary_for_question(item[\"instruction\"])\n",
    "\n",
    "# Also append these rows to the compare CSV for unified analysis\n",
    "compare_csv_path = MH.save_compare_csv(CSV_DIR, rows_summary, filename=COMPARE_FN)\n",
    "print(\"Compare CSV appended:\", compare_csv_path)\n",
    "print(\"Impact CSV appended:\", impact_paths[-1] if impact_paths else \"(none)\")\n",
    "\n",
    "# ---- Preview the compact comparison table (final-only metrics) ----\n",
    "import pandas as _pd\n",
    "df_cmp_summary = _pd.DataFrame(rows_summary)\n",
    "cols = [\"timestamp_iso\",\"strategy\",\"backend\",\"model\",\"retriever\",\"k_final\",\"latency_ms\",\n",
    "        \"unique_repos\",\"diversity\",\"mean_score\",\"max_score\",\"jaccard_vs_baseline\"]\n",
    "print(\"\\nCompact comparison table (preview):\")\n",
    "display(df_cmp_summary[cols].sort_values([\"strategy\",\"k_final\",\"backend\",\"model\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d18e7159-990f-492a-817d-2001576125e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] retrieval_providers: BM25 indexed docs: 133530 across 62 libs (local)\n",
      "[MH] strategy=iterative_refine k=5 libs=all latency_ms=2290\n",
      "[MH] subqueries: ['Replace me with the instruction you want to run', 'kivy BuilderException \"not isinstance(instr, Instruction)\" Factory.get', 'kivy Factory.get Instruction BuilderException canvas.clear']\n",
      "[MH] final_query: kivy Factory.get Instruction BuilderException canvas.clear\n",
      "[MH] snippets_used=5\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Single-mode multi-hop (no visualization) ===\n",
    "from importlib import reload\n",
    "import S6.mh_helpers as MH; reload(MH)\n",
    "from S6.mh_helpers import discover_libs, filters_for, extract_subqueries, snippets_from_hits\n",
    "from models.retrieval_providers import make_provider\n",
    "\n",
    "# --------------------\n",
    "# Config (edit as needed)\n",
    "# --------------------\n",
    "KB_DIR      = \"temp_downloaded_kbs\"\n",
    "RETRIEVER   = \"bm25\"                  # \"bm25\" | \"knn\" | \"hybrid\"\n",
    "STRATEGY    = \"iterative_refine\"      # \"decomposition_first\" | \"iterative_refine\"\n",
    "QUESTION    = \"Replace me with the instruction you want to run\"\n",
    "TOP_K       = 5\n",
    "\n",
    "# LLM settings (used only by multi-hop planning/decomposition)\n",
    "BACKEND     = \"gemini\"                # \"gemini\" | \"openrouter\" | \"local\"\n",
    "MODEL       = \"gemini-2.0-flash\"\n",
    "\n",
    "# Strategy params\n",
    "PLANNER_CTX_DOCS  = 3\n",
    "PLANNER_CTX_CHARS = 500\n",
    "PLAN_MAX_TOKENS   = 64\n",
    "DECOMP_MAX_TOKENS = 96\n",
    "\n",
    "# Library selection: None = all; or \"lib_name\"; or [\"libA\",\"libB\"]\n",
    "SELECTED_LIBS = None\n",
    "\n",
    "# --------------------\n",
    "# Build provider (indexes all KBs under KB_DIR)\n",
    "# --------------------\n",
    "LIBS = discover_libs(KB_DIR)\n",
    "F = filters_for(SELECTED_LIBS)\n",
    "provider = make_provider(RETRIEVER, local_kb_dir=KB_DIR, library_filter=LIBS)\n",
    "\n",
    "# Ensure IR \"final\" uses last refined subquery (matches our earlier fix)\n",
    "force_last = (STRATEGY == \"iterative_refine\")\n",
    "\n",
    "# --------------------\n",
    "# Run the single mode\n",
    "# --------------------\n",
    "pack, hits, cfg, latency_ms = MH.run_mode(\n",
    "    provider=provider,\n",
    "    question=QUESTION,\n",
    "    strategy_key=STRATEGY,\n",
    "    top_k=TOP_K,\n",
    "    repo_filter=SELECTED_LIBS,                  # legacy alias supported by run_mode\n",
    "    backend=BACKEND,\n",
    "    model=MODEL,\n",
    "    cache_dir=f\".cache/mh_single_{STRATEGY}\",\n",
    "    planner_ctx_docs=PLANNER_CTX_DOCS,\n",
    "    planner_ctx_chars=PLANNER_CTX_CHARS,\n",
    "    planner_max_tokens=PLAN_MAX_TOKENS,\n",
    "    decomposer_max_tokens=DECOMP_MAX_TOKENS,\n",
    "    force_last_subquery_final=force_last,\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Extract ALL top_k snippets (no truncation)\n",
    "# --------------------\n",
    "SNIPPETS = snippets_from_hits(hits, provider, dedupe=True)\n",
    "\n",
    "# Lean trace; keep for reproducibility (optional)\n",
    "print(f\"[MH] strategy={STRATEGY} k={TOP_K} libs={'all' if SELECTED_LIBS is None else SELECTED_LIBS} \"\n",
    "      f\"latency_ms={int(latency_ms)}\")\n",
    "print(f\"[MH] subqueries: {extract_subqueries(pack)}\")\n",
    "print(f\"[MH] final_query: {pack.get('meta',{}).get('final_query_text')}\")\n",
    "print(f\"[MH] snippets_used={len(SNIPPETS)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60321aa",
   "metadata": {},
   "source": [
    "#### 4.4.1 Multi-Hop: Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0fa05368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] retrieval_providers: BM25 indexed docs: 6618 across 1 libs (local)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[decomposition_first] building JSON: 100%|██████████| 23/23 [01:06<00:00,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " - JSONL       : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_decomposition_topk_k3.jsonl\n",
      " - JSON (array): C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_decomposition_topk_k3.json\n",
      "Exported queries: 23 | k = 3 | strategy = decomposition_first\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multihop \"decomposition_first\" (top-4) su 23 query selezionate\n",
    "# KB limitate a: seed-emulator, pyscf (pyscf__pyscf)\n",
    "# Output: outputs/multihop/mh_decomposition_topk_k4.{jsonl,json}\n",
    "# ============================================================\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helpers multihop / provider\n",
    "from importlib import reload\n",
    "import S6.mh_helpers as MH; reload(MH)\n",
    "from S6.mh_helpers import discover_libs, filters_for, snippets_from_hits\n",
    "from models.retrieval_providers import make_provider\n",
    "\n",
    "# --- Parametri run ---\n",
    "strategy_key     = \"decomposition_first\"\n",
    "top_k            = 3\n",
    "retriever_name   = \"bm25\"  # \"bm25\" | \"knn\" | \"hybrid\"\n",
    "kb_dir           = \"temp_downloaded_kbs\"\n",
    "selected_libs    = [\"seed-emulator\", \"pyscf\", \"pyscf__pyscf\"]\n",
    "backend          = \"gemini\"\n",
    "model_name       = \"gemini-2.0-flash\"\n",
    "cache_dir        = f\".cache/mh_json_{strategy_key}_k{top_k}\"\n",
    "max_samples      = 23  # numero di query da processare\n",
    "\n",
    "# --- Output ---\n",
    "OUT_DIR   = Path(\"outputs/multihop\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSONL = OUT_DIR / f\"mh_decomposition_topk_k{top_k}.jsonl\"\n",
    "OUT_JSON  = OUT_DIR / f\"mh_decomposition_topk_k{top_k}.json\"\n",
    "\n",
    "# --- Normalizzazione hits (robusta, compatibile con sezioni precedenti) ---\n",
    "def norm_hit(hit: Union[Dict[str, Any], str], i: int) -> Dict[str, Any]:\n",
    "    if isinstance(hit, str):\n",
    "        return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": hit, \"metadata\": {}}\n",
    "    if isinstance(hit, dict):\n",
    "        doc_id = hit.get(\"doc_id\") or hit.get(\"id\") or hit.get(\"path\") or f\"doc_{i}\"\n",
    "        text   = hit.get(\"text\") or hit.get(\"content\") or hit.get(\"snippet\") or \"\"\n",
    "        score  = hit.get(\"score\") or hit.get(\"similarity\") or hit.get(\"bm25_score\") or 0.0\n",
    "        path   = hit.get(\"path\")\n",
    "        md     = hit.get(\"metadata\") if isinstance(hit.get(\"metadata\"), dict) else {}\n",
    "        try: score = float(score)\n",
    "        except Exception: score = 0.0\n",
    "        return {\"doc_id\": doc_id, \"score\": score, \"path\": path, \"text\": text, \"metadata\": md}\n",
    "    return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": str(hit), \"metadata\": {}}\n",
    "\n",
    "def norm_repo_name(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"repo_name\", \"repo_full_name\"):\n",
    "        if k in item and item[k]:\n",
    "            return str(item[k]).strip().lower()\n",
    "    return \"\"\n",
    "\n",
    "def norm_instruction(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"instruction\", \"query\", \"prompt\"):\n",
    "        if k in item and item[k]:\n",
    "            return str(item[k])\n",
    "    return \"\"\n",
    "\n",
    "def norm_query_id(item: Dict[str, Any], idx: int) -> str:\n",
    "    for k in (\"id\", \"query_id\", \"qid\"):\n",
    "        if k in item and item[k]:\n",
    "            return str(item[k])\n",
    "    return f\"{norm_repo_name(item) or 'repo'}__{idx:06d}\"\n",
    "\n",
    "# --- Provider limitato alle due KB ---\n",
    "# Nota: passiamo il filtro direttamente al provider e anche a run_mode.\n",
    "provider = make_provider(\n",
    "    retriever_name,\n",
    "    local_kb_dir=kb_dir,\n",
    "    library_filter=selected_libs,\n",
    ")\n",
    "\n",
    "F = filters_for(selected_libs)\n",
    "\n",
    "# --- Costruisci l'iteratore di 23 esempi dal dataset già filtrato a monte ---\n",
    "# lca_dataset_split deve essere definito (come nelle sezioni precedenti).\n",
    "data_iter = list(lca_dataset_split)[:max_samples]\n",
    "\n",
    "# --- Esecuzione e serializzazione ---\n",
    "rows = []\n",
    "for i, item in enumerate(tqdm(data_iter, desc=f\"[{strategy_key}] building JSON\")):\n",
    "    q    = norm_instruction(item)\n",
    "    repo = norm_repo_name(item)\n",
    "    try:\n",
    "        pack, hits, cfg, latency_ms = MH.run_mode(\n",
    "            provider=provider,\n",
    "            question=q,\n",
    "            strategy_key=strategy_key,\n",
    "            top_k=top_k,\n",
    "            repo_filter=selected_libs,   # applica il filtro anche qui\n",
    "            backend=backend,\n",
    "            model=model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            # Token/ctx knobs usano i default interni di MH.run_mode se omessi\n",
    "        )\n",
    "        # Estraggo i final snippets (dedupe True per sicurezza)\n",
    "        # NB: se hits già contiene i testi completi, va bene; altrimenti snippets_from_hits\n",
    "        # recupera i contenuti dal provider.\n",
    "        final_snippets = snippets_from_hits(hits, provider, dedupe=True)\n",
    "        hits_norm = [norm_hit(h, j) for j, h in enumerate(final_snippets)]\n",
    "        row = {\n",
    "            \"query_id\": norm_query_id(item, i),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": q,\n",
    "            \"retrieval_method\": f\"multihop:{strategy_key}\",\n",
    "            \"k\": top_k,\n",
    "            \"retrieval_params\": {\n",
    "                \"retriever\": retriever_name,\n",
    "                \"backend\": backend,\n",
    "                \"model\": model_name,\n",
    "                \"repo_filter\": selected_libs,\n",
    "                \"strategy\": strategy_key,\n",
    "            },\n",
    "            \"results\": hits_norm\n",
    "        }\n",
    "    except Exception as e:\n",
    "        row = {\n",
    "            \"query_id\": norm_query_id(item, i),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": q,\n",
    "            \"retrieval_method\": f\"multihop:{strategy_key}\",\n",
    "            \"k\": top_k,\n",
    "            \"retrieval_params\": {\n",
    "                \"retriever\": retriever_name,\n",
    "                \"backend\": backend,\n",
    "                \"model\": model_name,\n",
    "                \"repo_filter\": selected_libs,\n",
    "                \"strategy\": strategy_key,\n",
    "            },\n",
    "            \"results\": [],\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "    rows.append(row)\n",
    "\n",
    "# ---- Salvataggio JSONL + JSON (array) ----\n",
    "with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in rows:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with OUT_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rows, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(\" - JSONL       :\", OUT_JSONL.resolve())\n",
    "print(\" - JSON (array):\", OUT_JSON.resolve())\n",
    "print(\"Exported queries:\", len(rows), \"| k =\", top_k, \"| strategy =\", strategy_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0082ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] retrieval_providers: BM25 indexed docs: 6618 across 1 libs (local)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iterative_refine] building JSON: 100%|██████████| 23/23 [06:04<00:00, 15.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " - JSONL       : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_iterative_topk_k3.jsonl\n",
      " - JSON (array): C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_iterative_topk_k3.json\n",
      "Exported queries: 23 | k = 3 | strategy = iterative_refine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multihop \"iterative_refine\" (top-4) su 23 query selezionate\n",
    "# KB limitate a: seed-emulator, pyscf (pyscf__pyscf)\n",
    "# Output: outputs/multihop/mh_iterative_topk_k4.{jsonl,json}\n",
    "# ============================================================\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helpers multihop / provider\n",
    "from importlib import reload\n",
    "import S6.mh_helpers as MH; reload(MH)\n",
    "from S6.mh_helpers import discover_libs, filters_for, snippets_from_hits\n",
    "from models.retrieval_providers import make_provider\n",
    "\n",
    "# --- Parametri run ---\n",
    "strategy_key     = \"iterative_refine\"\n",
    "top_k            = 3\n",
    "retriever_name   = \"bm25\"  # \"bm25\" | \"knn\" | \"hybrid\"\n",
    "kb_dir           = \"temp_downloaded_kbs\"\n",
    "selected_libs    = [\"seed-emulator\", \"pyscf\", \"pyscf__pyscf\"]\n",
    "backend          = \"gemini\"\n",
    "model_name       = \"gemini-2.0-flash\"\n",
    "cache_dir        = f\".cache/mh_json_{strategy_key}_k{top_k}\"\n",
    "max_samples      = 23  # numero di query da processare\n",
    "\n",
    "# --- Output ---\n",
    "OUT_DIR   = Path(\"outputs/multihop\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSONL = OUT_DIR / f\"mh_iterative_topk_k{top_k}.jsonl\"\n",
    "OUT_JSON  = OUT_DIR / f\"mh_iterative_topk_k{top_k}.json\"\n",
    "\n",
    "# --- Normalizzazione hits (come sopra) ---\n",
    "def norm_hit(hit: Union[Dict[str, Any], str], i: int) -> Dict[str, Any]:\n",
    "    if isinstance(hit, str):\n",
    "        return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": hit, \"metadata\": {}}\n",
    "    if isinstance(hit, dict):\n",
    "        doc_id = hit.get(\"doc_id\") or hit.get(\"id\") or hit.get(\"path\") or f\"doc_{i}\"\n",
    "        text   = hit.get(\"text\") or hit.get(\"content\") or hit.get(\"snippet\") or \"\"\n",
    "        score  = hit.get(\"score\") or hit.get(\"similarity\") or hit.get(\"bm25_score\") or 0.0\n",
    "        path   = hit.get(\"path\")\n",
    "        md     = hit.get(\"metadata\") if isinstance(hit.get(\"metadata\"), dict) else {}\n",
    "        try: score = float(score)\n",
    "        except Exception: score = 0.0\n",
    "        return {\"doc_id\": doc_id, \"score\": score, \"path\": path, \"text\": text, \"metadata\": md}\n",
    "    return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": str(hit), \"metadata\": {}}\n",
    "\n",
    "def norm_repo_name(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"repo_name\", \"repo_full_name\"):\n",
    "        if k in item and item[k]:\n",
    "            return str(item[k]).strip().lower()\n",
    "    return \"\"\n",
    "\n",
    "def norm_instruction(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"instruction\", \"query\", \"prompt\"):\n",
    "        if k in item and item[k]:\n",
    "            return str(item[k])\n",
    "    return \"\"\n",
    "\n",
    "def norm_query_id(item: Dict[str, Any], idx: int) -> str:\n",
    "    for k in (\"id\", \"query_id\", \"qid\"):\n",
    "        if k in item and item[k]:\n",
    "            return str(item[k])\n",
    "    return f\"{norm_repo_name(item) or 'repo'}__{idx:06d}\"\n",
    "\n",
    "# --- Provider limitato alle due KB ---\n",
    "provider = make_provider(\n",
    "    retriever_name,\n",
    "    local_kb_dir=kb_dir,\n",
    "    library_filter=selected_libs,\n",
    ")\n",
    "\n",
    "F = filters_for(selected_libs)\n",
    "\n",
    "# --- Costruisci l'iteratore di 23 esempi dal dataset già filtrato a monte ---\n",
    "data_iter = list(lca_dataset_split)[:max_samples]\n",
    "\n",
    "# --- Esecuzione e serializzazione ---\n",
    "rows = []\n",
    "for i, item in enumerate(tqdm(data_iter, desc=f\"[{strategy_key}] building JSON\")):\n",
    "    q    = norm_instruction(item)\n",
    "    repo = norm_repo_name(item)\n",
    "    try:\n",
    "        pack, hits, cfg, latency_ms = MH.run_mode(\n",
    "            provider=provider,\n",
    "            question=q,\n",
    "            strategy_key=strategy_key,\n",
    "            top_k=top_k,\n",
    "            repo_filter=selected_libs,\n",
    "            backend=backend,\n",
    "            model=model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            # Forza l'uso dell'ULTIMA sub-query come finale (coerente con la tua Cell 5)\n",
    "            force_last_subquery_final=True,\n",
    "        )\n",
    "        final_snippets = snippets_from_hits(hits, provider, dedupe=True)\n",
    "        hits_norm = [norm_hit(h, j) for j, h in enumerate(final_snippets)]\n",
    "        row = {\n",
    "            \"query_id\": norm_query_id(item, i),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": q,\n",
    "            \"retrieval_method\": f\"multihop:{strategy_key}\",\n",
    "            \"k\": top_k,\n",
    "            \"retrieval_params\": {\n",
    "                \"retriever\": retriever_name,\n",
    "                \"backend\": backend,\n",
    "                \"model\": model_name,\n",
    "                \"repo_filter\": selected_libs,\n",
    "                \"strategy\": strategy_key,\n",
    "                \"force_last_subquery_final\": True,\n",
    "            },\n",
    "            \"results\": hits_norm\n",
    "        }\n",
    "    except Exception as e:\n",
    "        row = {\n",
    "            \"query_id\": norm_query_id(item, i),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": q,\n",
    "            \"retrieval_method\": f\"multihop:{strategy_key}\",\n",
    "            \"k\": top_k,\n",
    "            \"retrieval_params\": {\n",
    "                \"retriever\": retriever_name,\n",
    "                \"backend\": backend,\n",
    "                \"model\": model_name,\n",
    "                \"repo_filter\": selected_libs,\n",
    "                \"strategy\": strategy_key,\n",
    "                \"force_last_subquery_final\": True,\n",
    "            },\n",
    "            \"results\": [],\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "    rows.append(row)\n",
    "\n",
    "# ---- Salvataggio JSONL + JSON (array) ----\n",
    "with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in rows:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with OUT_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rows, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(\" - JSONL       :\", OUT_JSONL.resolve())\n",
    "print(\" - JSON (array):\", OUT_JSON.resolve())\n",
    "print(\"Exported queries:\", len(rows), \"| k =\", top_k, \"| strategy =\", strategy_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12700425",
   "metadata": {},
   "source": [
    "#### 4.4.2 Multi-Hop: iterative_refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3047c01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] retrieval_providers: BM25 indexed docs: 6618 across 1 libs (local)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[iterative_refine] building JSON: 100%|██████████| 23/23 [06:10<00:00, 16.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      " - JSONL       : C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_iterative_topk_k3.jsonl\n",
      " - JSON (array): C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_iterative_topk_k3.json\n",
      "Exported queries: 23 | k = 3 | strategy = iterative_refine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Multihop \"iterative_refine\" (top-4) su 23 query selezionate\n",
    "# KB limitate a: seed-emulator, pyscf (pyscf__pyscf)\n",
    "# Output: outputs/multihop/mh_iterative_topk_k4.{jsonl,json}\n",
    "# ============================================================\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Union\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helpers multihop / provider\n",
    "from importlib import reload\n",
    "import S6.mh_helpers as MH; reload(MH)\n",
    "from S6.mh_helpers import discover_libs, filters_for, snippets_from_hits\n",
    "from models.retrieval_providers import make_provider\n",
    "\n",
    "# --- Parametri run ---\n",
    "strategy_key     = \"iterative_refine\"\n",
    "top_k            = 3\n",
    "retriever_name   = \"bm25\"  # \"bm25\" | \"knn\" | \"hybrid\"\n",
    "kb_dir           = \"temp_downloaded_kbs\"\n",
    "selected_libs    = [\"seed-emulator\", \"pyscf\", \"pyscf__pyscf\"]\n",
    "backend          = \"gemini\"\n",
    "model_name       = \"gemini-2.0-flash\"\n",
    "cache_dir        = f\".cache/mh_json_{strategy_key}_k{top_k}\"\n",
    "max_samples      = 23  # numero di query da processare\n",
    "\n",
    "# --- Output ---\n",
    "OUT_DIR   = Path(\"outputs/multihop\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUT_JSONL = OUT_DIR / f\"mh_iterative_topk_k{top_k}.jsonl\"\n",
    "OUT_JSON  = OUT_DIR / f\"mh_iterative_topk_k{top_k}.json\"\n",
    "\n",
    "# --- Normalizzazione hits (come sopra) ---\n",
    "def norm_hit(hit: Union[Dict[str, Any], str], i: int) -> Dict[str, Any]:\n",
    "    if isinstance(hit, str):\n",
    "        return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": hit, \"metadata\": {}}\n",
    "    if isinstance(hit, dict):\n",
    "        doc_id = hit.get(\"doc_id\") or hit.get(\"id\") or hit.get(\"path\") or f\"doc_{i}\"\n",
    "        text   = hit.get(\"text\") or hit.get(\"content\") or hit.get(\"snippet\") or \"\"\n",
    "        score  = hit.get(\"score\") or hit.get(\"similarity\") or hit.get(\"bm25_score\") or 0.0\n",
    "        path   = hit.get(\"path\")\n",
    "        md     = hit.get(\"metadata\") if isinstance(hit.get(\"metadata\"), dict) else {}\n",
    "        try: score = float(score)\n",
    "        except Exception: score = 0.0\n",
    "        return {\"doc_id\": doc_id, \"score\": score, \"path\": path, \"text\": text, \"metadata\": md}\n",
    "    return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": str(hit), \"metadata\": {}}\n",
    "\n",
    "def norm_repo_name(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"repo_name\", \"repo_full_name\"):\n",
    "        if k in item and item[k]:\n",
    "            return str(item[k]).strip().lower()\n",
    "    return \"\"\n",
    "\n",
    "def norm_instruction(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"instruction\", \"query\", \"prompt\"):\n",
    "        if k in item and item[k]:\n",
    "            return str(item[k])\n",
    "    return \"\"\n",
    "\n",
    "def norm_query_id(item: Dict[str, Any], idx: int) -> str:\n",
    "    for k in (\"id\", \"query_id\", \"qid\"):\n",
    "        if k in item and item[k]:\n",
    "            return str(item[k])\n",
    "    return f\"{norm_repo_name(item) or 'repo'}__{idx:06d}\"\n",
    "\n",
    "# --- Provider limitato alle due KB ---\n",
    "provider = make_provider(\n",
    "    retriever_name,\n",
    "    local_kb_dir=kb_dir,\n",
    "    library_filter=selected_libs,\n",
    ")\n",
    "\n",
    "F = filters_for(selected_libs)\n",
    "\n",
    "# --- Costruisci l'iteratore di 23 esempi dal dataset già filtrato a monte ---\n",
    "data_iter = list(lca_dataset_split)[:max_samples]\n",
    "\n",
    "# --- Esecuzione e serializzazione ---\n",
    "rows = []\n",
    "for i, item in enumerate(tqdm(data_iter, desc=f\"[{strategy_key}] building JSON\")):\n",
    "    q    = norm_instruction(item)\n",
    "    repo = norm_repo_name(item)\n",
    "    try:\n",
    "        pack, hits, cfg, latency_ms = MH.run_mode(\n",
    "            provider=provider,\n",
    "            question=q,\n",
    "            strategy_key=strategy_key,\n",
    "            top_k=top_k,\n",
    "            repo_filter=selected_libs,\n",
    "            backend=backend,\n",
    "            model=model_name,\n",
    "            cache_dir=cache_dir,\n",
    "            # Forza l'uso dell'ULTIMA sub-query come finale (coerente con la tua Cell 5)\n",
    "            force_last_subquery_final=True,\n",
    "        )\n",
    "        final_snippets = snippets_from_hits(hits, provider, dedupe=True)\n",
    "        hits_norm = [norm_hit(h, j) for j, h in enumerate(final_snippets)]\n",
    "        row = {\n",
    "            \"query_id\": norm_query_id(item, i),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": q,\n",
    "            \"retrieval_method\": f\"multihop:{strategy_key}\",\n",
    "            \"k\": top_k,\n",
    "            \"retrieval_params\": {\n",
    "                \"retriever\": retriever_name,\n",
    "                \"backend\": backend,\n",
    "                \"model\": model_name,\n",
    "                \"repo_filter\": selected_libs,\n",
    "                \"strategy\": strategy_key,\n",
    "                \"force_last_subquery_final\": True,\n",
    "            },\n",
    "            \"results\": hits_norm\n",
    "        }\n",
    "    except Exception as e:\n",
    "        row = {\n",
    "            \"query_id\": norm_query_id(item, i),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": q,\n",
    "            \"retrieval_method\": f\"multihop:{strategy_key}\",\n",
    "            \"k\": top_k,\n",
    "            \"retrieval_params\": {\n",
    "                \"retriever\": retriever_name,\n",
    "                \"backend\": backend,\n",
    "                \"model\": model_name,\n",
    "                \"repo_filter\": selected_libs,\n",
    "                \"strategy\": strategy_key,\n",
    "                \"force_last_subquery_final\": True,\n",
    "            },\n",
    "            \"results\": [],\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "    rows.append(row)\n",
    "\n",
    "# ---- Salvataggio JSONL + JSON (array) ----\n",
    "with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in rows:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "with OUT_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rows, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(\" - JSONL       :\", OUT_JSONL.resolve())\n",
    "print(\" - JSON (array):\", OUT_JSON.resolve())\n",
    "print(\"Exported queries:\", len(rows), \"| k =\", top_k, \"| strategy =\", strategy_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8740de89",
   "metadata": {},
   "source": [
    "## 5 Prompt Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20dd395",
   "metadata": {},
   "source": [
    "### -> 5.0 BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "08f50a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] baseline_make_prompts qid=seed-emulator__000000: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n",
      "\n",
      "The base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n",
      "\n",
      "The domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n",
      "\n",
      "The eBGP layer should add private peerings between different autonomous systems. \n",
      "\n",
      "Finally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulation using the seedemu library. The emulation should include three layers: base, routing, and eBGP. It should also include a domain name caching service. \n",
      "\n",
      "The base layer should create multiple autonomous systems and internet exchanges. Each autonomous system should have multiple hosts and a router. The hosts and the router should join a network within the autonomous system and the router should also join an internet exchange. \n",
      "\n",
      "The domain name caching service should be installed on specific hosts within the autonomous systems and bindings should be added for these installations. \n",
      "\n",
      "The eBGP layer should add private peerings between different autonomous systems. \n",
      "\n",
      "Finally, all the layers and the domain name caching service should be added to the emulator and the state of the emulator should be dumped to a binary file._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000001: Retrieval file non trovato: BM25\\retrieved_kGenerate code that calculates the effective electronic coupling based on single determinant diabatic states using the pyscf library. The code should first define a molecule with specific atoms and basis. Then, it should perform two state calculations with DFT, storing molecular orbital information into separate chkfiles. The code should then read the MO coefficients and occupation numbers from these chkfiles. Afterwards, it should calculate the overlap between two determinants, construct density matrices, calculate one-electron and two-electron part contributions, and calculate new total energy. Finally, the code should calculate the effective electronic coupling and print the results. The code should also remove the chkfiles at the end._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that calculates the effective electronic coupling based on single determinant diabatic states using the pyscf library. The code should first define a molecule with specific atoms and basis. Then, it should perform two state calculations with DFT, storing molecular orbital information into separate chkfiles. The code should then read the MO coefficients and occupation numbers from these chkfiles. Afterwards, it should calculate the overlap between two determinants, construct density matrices, calculate one-electron and two-electron part contributions, and calculate new total energy. Finally, the code should calculate the effective electronic coupling and print the results. The code should also remove the chkfiles at the end._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000002: Retrieval file non trovato: BM25\\retrieved_kGenerate code that initializes an emulator and several layers using the seedemu library. The code should create an Internet Exchange with a specific ID and set its display name and description. Then, it should create three Autonomous Systems with different IDs. For each Autonomous System, the code should create a network, a router that joins two networks, and a host that joins a network. It should also install a web service on a virtual node and bind this node to a host. The code should set display names and descriptions for the networks, routers, and Autonomous Systems. After creating the Autonomous Systems, the code should peer them with the Internet Exchange. Finally, the code should add all the layers to the emulator, render the emulator, and compile it with Docker, enabling the internet map._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that initializes an emulator and several layers using the seedemu library. The code should create an Internet Exchange with a specific ID and set its display name and description. Then, it should create three Autonomous Systems with different IDs. For each Autonomous System, the code should create a network, a router that joins two networks, and a host that joins a network. It should also install a web service on a virtual node and bind this node to a host. The code should set display names and descriptions for the networks, routers, and Autonomous Systems. After creating the Autonomous Systems, the code should peer them with the Internet Exchange. Finally, the code should add all the layers to the emulator, render the emulator, and compile it with Docker, enabling the internet map._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000003: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulation using the seed-emulator library. The emulation should include three layers: base, routing, and eBGP. The base layer should create three autonomous systems with specific routers and networks. The first autonomous system should create five hosts and a router, all of which join a network. The second autonomous system should create three routers, each joining a different network. The third autonomous system should create two routers, both joining the same network. The eBGP layer should add private peering between different autonomous systems. Finally, the code should add all layers to the emulator and dump the emulator state to a binary file._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulation using the seed-emulator library. The emulation should include three layers: base, routing, and eBGP. The base layer should create three autonomous systems with specific routers and networks. The first autonomous system should create five hosts and a router, all of which join a network. The second autonomous system should create three routers, each joining a different network. The third autonomous system should create two routers, both joining the same network. The eBGP layer should add private peering between different autonomous systems. Finally, the code should add all layers to the emulator and dump the emulator state to a binary file._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000004: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulation environment using the seedemu library. The environment should include multiple layers such as Base, Routing, Ebgp, Ibgp, Ospf, and WebService. It should create several Internet Exchanges with custom display names. It should also create Transit Autonomous Systems and single-homed stub Autonomous Systems with various services. The code should also add a host with a customized IP address to one of the Autonomous Systems and create a real-world Autonomous System. It should enable remote access to one of the Autonomous System's network. The code should also set up peering via a route server and private peering with different peer relationships. Finally, the code should add all the layers to the emulator, save the emulator to a component file, and render and compile the emulator._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulation environment using the seedemu library. The environment should include multiple layers such as Base, Routing, Ebgp, Ibgp, Ospf, and WebService. It should create several Internet Exchanges with custom display names. It should also create Transit Autonomous Systems and single-homed stub Autonomous Systems with various services. The code should also add a host with a customized IP address to one of the Autonomous Systems and create a real-world Autonomous System. It should enable remote access to one of the Autonomous System's network. The code should also set up peering via a route server and private peering with different peer relationships. Finally, the code should add all the layers to the emulator, save the emulator to a component file, and render and compile the emulator._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000005: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulation environment using the seed-emulator library. The environment should include three types of autonomous systems (AS): transit, stub, and utility. \n",
      "\n",
      "For the transit AS, create two internet exchanges with specific display names, three internal networks, and four routers linked in a linear structure. \n",
      "\n",
      "For the stub AS, create three different systems. The first two should have an internal network, a router, and two host nodes. The first host node should have additional software installed and a new account created. The third system should be created using a utility function and further customized.\n",
      "\n",
      "Establish BGP peering by creating an Ebgp layer and setting up the transit AS as the internet service provider for all the stub ASes. Also, set up direct peering between two of the stub ASes.\n",
      "\n",
      "Create a web service layer with two web service nodes and bind these virtual nodes to physical nodes. \n",
      "\n",
      "Add all the created layers to the emulator and save it to a component file. Render the emulator and change the display names for the nodes hosting the web services.\n",
      "\n",
      "Finally, compile the emulator using Docker, specifying custom images from DockerHub and local sources. Generate Docker files and copy the base container image to the output folder._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulation environment using the seed-emulator library. The environment should include three types of autonomous systems (AS): transit, stub, and utility. \n",
      "\n",
      "For the transit AS, create two internet exchanges with specific display names, three internal networks, and four routers linked in a linear structure. \n",
      "\n",
      "For the stub AS, create three different systems. The first two should have an internal network, a router, and two host nodes. The first host node should have additional software installed and a new account created. The third system should be created using a utility function and further customized.\n",
      "\n",
      "Establish BGP peering by creating an Ebgp layer and setting up the transit AS as the internet service provider for all the stub ASes. Also, set up direct peering between two of the stub ASes.\n",
      "\n",
      "Create a web service layer with two web service nodes and bind these virtual nodes to physical nodes. \n",
      "\n",
      "Add all the created layers to the emulator and save it to a component file. Render the emulator and change the display names for the nodes hosting the web services.\n",
      "\n",
      "Finally, compile the emulator using Docker, specifying custom images from DockerHub and local sources. Generate Docker files and copy the base container image to the output folder._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000006: Retrieval file non trovato: BM25\\retrieved_kGenerate code that performs two tasks using the pyscf library. The first task is to transform a Full Configuration Interaction (FCI) wavefunction with respect to orbital rotation\\transformation. This involves creating two molecules with different atomic configurations, calculating their FCI energies, and then transforming the wavefunction of the first molecule to match the second one. The second task is to transfer a FCI wavefunction from a smaller orbital space to a larger one. This involves creating a molecule with a specific atomic configuration, calculating its FCI energy, and then expanding the wavefunction to a larger orbital space. The code should also compare the transformed wavefunction with the one obtained from the FCI solver and check if they are close. Finally, the code should transform the FCI wavefunction using a different method and compare the results with the previous transformation._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that performs two tasks using the pyscf library. The first task is to transform a Full Configuration Interaction (FCI) wavefunction with respect to orbital rotation/transformation. This involves creating two molecules with different atomic configurations, calculating their FCI energies, and then transforming the wavefunction of the first molecule to match the second one. The second task is to transfer a FCI wavefunction from a smaller orbital space to a larger one. This involves creating a molecule with a specific atomic configuration, calculating its FCI energy, and then expanding the wavefunction to a larger orbital space. The code should also compare the transformed wavefunction with the one obtained from the FCI solver and check if they are close. Finally, the code should transform the FCI wavefunction using a different method and compare the results with the previous transformation._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000007: Retrieval file non trovato: BM25\\retrieved_kGenerate code that performs a CCSD (Coupled Cluster with Single and Double excitations) calculation with k-point sampling for a system of two carbon atoms in a cell using the pyscf library. The code should first build the cell with the given atomic coordinates, basis, pseudopotential, lattice vectors, and unit. Then, it should perform KHF and KCCSD calculations with 2x2x2 k-points and print the total energy per unit cell. Next, it should perform KHF and KCCSD calculations for a single k-point and print the total energy per unit cell. \n",
      "\n",
      "Afterwards, the code should perform a single k-point calculation using the RHF method, run RCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the RCCSD energy based on CCSD density matrices. \n",
      "\n",
      "Next, the code should convert the RHF object to a UHF object, run UCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the UCCSD energy based on CCSD density matrices. \n",
      "\n",
      "Finally, the code should convert the UHF object to a GHF object, run GCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the GCCSD energy based on CCSD density matrices._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that performs a CCSD (Coupled Cluster with Single and Double excitations) calculation with k-point sampling for a system of two carbon atoms in a cell using the pyscf library. The code should first build the cell with the given atomic coordinates, basis, pseudopotential, lattice vectors, and unit. Then, it should perform KHF and KCCSD calculations with 2x2x2 k-points and print the total energy per unit cell. Next, it should perform KHF and KCCSD calculations for a single k-point and print the total energy per unit cell. \n",
      "\n",
      "Afterwards, the code should perform a single k-point calculation using the RHF method, run RCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the RCCSD energy based on CCSD density matrices. \n",
      "\n",
      "Next, the code should convert the RHF object to a UHF object, run UCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the UCCSD energy based on CCSD density matrices. \n",
      "\n",
      "Finally, the code should convert the UHF object to a GHF object, run GCCSD, and print the total energy per unit cell at the k-point. It should also calculate and print the GCCSD energy based on CCSD density matrices._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000008: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS150, AS151, AS152) with their respective networks and routers. AS150 should be a transit AS with four routers and three networks. AS151 and AS152 should each have a web host and a router, and they should each join a network. AS151 and AS152 should also join an internet exchange. The code should also set up BGP peering between AS150 and AS151, and between AS150 and AS152. Finally, the code should add all the layers to the emulator and dump the emulator's state to a binary file._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS150, AS151, AS152) with their respective networks and routers. AS150 should be a transit AS with four routers and three networks. AS151 and AS152 should each have a web host and a router, and they should each join a network. AS151 and AS152 should also join an internet exchange. The code should also set up BGP peering between AS150 and AS151, and between AS150 and AS152. Finally, the code should add all the layers to the emulator and dump the emulator's state to a binary file._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000009: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates a network topology using the seedemu library. The topology should consist of three autonomous systems (AS): AS150, AS2, and AS151. AS150 and AS151 should each have one router and one network, while AS2 should have two routers and one network. AS150 and AS2 should be connected through an internet exchange (IX) 100, and AS2 and AS151 should be connected through IX 101. \n",
      "\n",
      "Additionally, create a BGP attacker component that hijacks the prefix of AS151 and joins IX 100. Merge this component with the main simulation. \n",
      "\n",
      "Finally, establish private peering relationships: between AS150 and AS2 at IX 100, between AS151 and AS2 at IX 101, and between the attacker and AS2 at IX 100. Render and compile the simulation with Docker, managing the network internally._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates a network topology using the seedemu library. The topology should consist of three autonomous systems (AS): AS150, AS2, and AS151. AS150 and AS151 should each have one router and one network, while AS2 should have two routers and one network. AS150 and AS2 should be connected through an internet exchange (IX) 100, and AS2 and AS151 should be connected through IX 101. \n",
      "\n",
      "Additionally, create a BGP attacker component that hijacks the prefix of AS151 and joins IX 100. Merge this component with the main simulation. \n",
      "\n",
      "Finally, establish private peering relationships: between AS150 and AS2 at IX 100, between AS151 and AS2 at IX 101, and between the attacker and AS2 at IX 100. Render and compile the simulation with Docker, managing the network internally._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000010: Retrieval file non trovato: BM25\\retrieved_kGenerate code that calculates the triplet and quintet energy gap of an Iron-Porphyrin molecule using DMRG-CASSCF and DMRG-NEVPT2 methods from the pyscf library. The code should first define the DMET active space, then calculate the quintet and triplet energies separately. The active space should include the Fe double d-shell, 4s shell, and the ligand N 2pz orbitals to describe metal-ligand pi bond and pi backbond. The code should also output the active space orbitals to molden format._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that calculates the triplet and quintet energy gap of an Iron-Porphyrin molecule using DMRG-CASSCF and DMRG-NEVPT2 methods from the pyscf library. The code should first define the DMET active space, then calculate the quintet and triplet energies separately. The active space should include the Fe double d-shell, 4s shell, and the ligand N 2pz orbitals to describe metal-ligand pi bond and pi backbond. The code should also output the active space orbitals to molden format._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000011: Retrieval file non trovato: BM25\\retrieved_kGenerate code that performs a restricted AGF2 calculation with density fitting using the PySCF library. The code should define a molecule with a specific atomic structure and basis, and then run a RHF calculation with a specified convergence tolerance and auxiliary basis. After running the AGF2 calculation with a specified convergence tolerance, the code should print the first three ionization potentials and electron affinities. Then, it should calculate the MO-basis density matrix and dipole moments, transforming dipole moment integrals into MO basis and adding the nuclear component. Finally, the code should print the calculated dipole moment._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that performs a restricted AGF2 calculation with density fitting using the PySCF library. The code should define a molecule with a specific atomic structure and basis, and then run a RHF calculation with a specified convergence tolerance and auxiliary basis. After running the AGF2 calculation with a specified convergence tolerance, the code should print the first three ionization potentials and electron affinities. Then, it should calculate the MO-basis density matrix and dipole moments, transforming dipole moment integrals into MO basis and adding the nuclear component. Finally, the code should print the calculated dipole moment._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000012: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS) with AS numbers 150, 151, and 152. Each AS should have a host named 'web' and a router named 'router0'. The 'web' host in each AS should have a web service installed. Each AS should also have a network named 'net0' which both the 'web' host and 'router0' join. AS150 and AS152 should have a cross connection between their routers. An internet exchange with the number 100 should be created and AS150 and AS151 should be peers on this exchange. AS150 should also be a provider for AS152. The emulation should be rendered and compiled using Docker with self-managed network. The compiled emulation should be saved in the directory '.\\cross-connect'._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulation using the seed-emulator library. The emulation should include three autonomous systems (AS) with AS numbers 150, 151, and 152. Each AS should have a host named 'web' and a router named 'router0'. The 'web' host in each AS should have a web service installed. Each AS should also have a network named 'net0' which both the 'web' host and 'router0' join. AS150 and AS152 should have a cross connection between their routers. An internet exchange with the number 100 should be created and AS150 and AS151 should be peers on this exchange. AS150 should also be a provider for AS152. The emulation should be rendered and compiled using Docker with self-managed network. The compiled emulation should be saved in the directory './cross-connect'._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000013: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulator with 10 stub Autonomous Systems (AS) and hosts. Then, create an Ethereum service with saveState set to True and override set to True. Create two blockchains, one based on Proof of Work (POW) and the other on Proof of Authority (POA). For each blockchain, create four nodes and set the first two nodes of each blockchain as bootnodes and start mining on them. For the third node of each blockchain, create accounts with a certain balance. For the fourth node of each blockchain, set custom geth command options. Enable HTTP and WebSocket connections on certain nodes and set custom geth binary file on one of the nodes. Customize the display names of the nodes for visualization purposes. Bind the virtual nodes to physical nodes using filters. Add the Ethereum layer to the emulator and save the component to a file. Finally, compile the emulator with Docker and save the output to a directory._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulator with 10 stub Autonomous Systems (AS) and hosts. Then, create an Ethereum service with saveState set to True and override set to True. Create two blockchains, one based on Proof of Work (POW) and the other on Proof of Authority (POA). For each blockchain, create four nodes and set the first two nodes of each blockchain as bootnodes and start mining on them. For the third node of each blockchain, create accounts with a certain balance. For the fourth node of each blockchain, set custom geth command options. Enable HTTP and WebSocket connections on certain nodes and set custom geth binary file on one of the nodes. Customize the display names of the nodes for visualization purposes. Bind the virtual nodes to physical nodes using filters. Add the Ethereum layer to the emulator and save the component to a file. Finally, compile the emulator with Docker and save the output to a directory._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000014: Retrieval file non trovato: BM25\\retrieved_kGenerate code that constructs Maximally Localized Wannier Functions (MLWFs) using the pywannier90 tool from the pyscf library. The code should define a unit cell, perform a PBE calculation, save and load the kks object, construct MLWFs, export the MLWFs in xsf format for plotting, export certain matrices and run a wannier90 using these, interpolate the Fock or band structure using the Slater-Koster scheme, print the difference in the eigenvalues interpolated by scf.get_bands function and by pywannier90, and plot the band structure using mcu._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that constructs Maximally Localized Wannier Functions (MLWFs) using the pywannier90 tool from the pyscf library. The code should define a unit cell, perform a PBE calculation, save and load the kks object, construct MLWFs, export the MLWFs in xsf format for plotting, export certain matrices and run a wannier90 using these, interpolate the Fock or band structure using the Slater-Koster scheme, print the difference in the eigenvalues interpolated by scf.get_bands function and by pywannier90, and plot the band structure using mcu._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000015: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulator base with 10 Stub AS and 3 hosts per stub AS using the seedemu library. Then, create an Ethereum service layer and a sub-layer of it, a blockchain with the name \"pos\" and consensus mechanism set to POS. Set the terminal total difficulty of the blockchain to 30. \n",
      "\n",
      "For each host in the AS, create a blockchain virtual node, a Docker container label, and enable Geth to communicate with the geth node via http. Set specific hosts as BeaconSetupNode, BootNode, and validator nodes with different conditions. Also, customize the display names of the nodes and bind the virtual node to the physical node. \n",
      "\n",
      "Finally, add the Ethereum layer to the emulator, render it, and compile it with Docker with internetMap and etherView enabled. The output should be saved in the '.\\output' directory and existing files should be overridden._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulator base with 10 Stub AS and 3 hosts per stub AS using the seedemu library. Then, create an Ethereum service layer and a sub-layer of it, a blockchain with the name \"pos\" and consensus mechanism set to POS. Set the terminal total difficulty of the blockchain to 30. \n",
      "\n",
      "For each host in the AS, create a blockchain virtual node, a Docker container label, and enable Geth to communicate with the geth node via http. Set specific hosts as BeaconSetupNode, BootNode, and validator nodes with different conditions. Also, customize the display names of the nodes and bind the virtual node to the physical node. \n",
      "\n",
      "Finally, add the Ethereum layer to the emulator, render it, and compile it with Docker with internetMap and etherView enabled. The output should be saved in the './output' directory and existing files should be overridden._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000016: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulation using the seedemu library. The emulation should include three layers: Base, Routing, and Ebgp. It should create multiple autonomous systems, each with their own hosts and routers. The routers should join different networks. The autonomous systems should be connected through internet exchanges. The code should also define a function to create a stub autonomous system with a specified ASN and exchange. The function should create hosts and a router for the autonomous system, and join them to a network. The router should also join the specified exchange. The code should also add private peering relationships between different autonomous systems. Finally, the code should add the layers to the emulator and dump the emulator state to a binary file._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulation using the seedemu library. The emulation should include three layers: Base, Routing, and Ebgp. It should create multiple autonomous systems, each with their own hosts and routers. The routers should join different networks. The autonomous systems should be connected through internet exchanges. The code should also define a function to create a stub autonomous system with a specified ASN and exchange. The function should create hosts and a router for the autonomous system, and join them to a network. The router should also join the specified exchange. The code should also add private peering relationships between different autonomous systems. Finally, the code should add the layers to the emulator and dump the emulator state to a binary file._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000017: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulation using the seed-emulator library. The emulation should include base, routing, eBGP, iBGP, OSPF, and web service layers. It should define a function to create a stub autonomous system with a web server and a router that join a network and an internet exchange. The code should create three internet exchanges and multiple stub autonomous systems that join these exchanges. It should also create two autonomous systems with routers that join different networks and internet exchanges. The code should define private peerings between different autonomous systems. Finally, it should add a BGP attacker component that hijacks certain prefixes and joins an internet exchange. The code should merge the BGP attacker with the emulator and render the new emulator. The code should compile the new emulator using Docker and output the result to a specified directory._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulation using the seed-emulator library. The emulation should include base, routing, eBGP, iBGP, OSPF, and web service layers. It should define a function to create a stub autonomous system with a web server and a router that join a network and an internet exchange. The code should create three internet exchanges and multiple stub autonomous systems that join these exchanges. It should also create two autonomous systems with routers that join different networks and internet exchanges. The code should define private peerings between different autonomous systems. Finally, it should add a BGP attacker component that hijacks certain prefixes and joins an internet exchange. The code should merge the BGP attacker with the emulator and render the new emulator. The code should compile the new emulator using Docker and output the result to a specified directory._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000018: Retrieval file non trovato: BM25\\retrieved_kGenerate code that demonstrates the use of the parallelized CCSD with K-point sampling in the pyscf library. The code should create a supercell composed of replicated units and run a molecular Hartree-Fock program using integrals between periodic gaussians. It should then call a molecular CC method for gamma point calculation and perform k-point calculations for the same system. The code should also calculate the differences between gamma\\k-point mean-field, ccsd, ip-eomccsd, and ea-eomccsd calculations and print these differences._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that demonstrates the use of the parallelized CCSD with K-point sampling in the pyscf library. The code should create a supercell composed of replicated units and run a molecular Hartree-Fock program using integrals between periodic gaussians. It should then call a molecular CC method for gamma point calculation and perform k-point calculations for the same system. The code should also calculate the differences between gamma/k-point mean-field, ccsd, ip-eomccsd, and ea-eomccsd calculations and print these differences._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=seed-emulator__000019: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates an emulation environment using the seed-emulator library. The environment should include a ransomware service, a Tor service, and a DNS layer. \n",
      "\n",
      "For the ransomware service, create a ransomware attacker and 16 ransomware victims. The attacker should be installed on a host in an autonomous system and should not support botnet or Tor. The victims should be installed on hosts and should not support botnet. \n",
      "\n",
      "For the Tor service, create different types of Tor nodes including directory authorities, clients, relays, exits, and a hidden service. The hidden service should be linked to the ransomware attacker. \n",
      "\n",
      "For the DNS layer, create a root server, TLD and ccTLD servers, second-level zone servers, and a local DNS server. The servers should have appropriate zones and records. \n",
      "\n",
      "Finally, compile the emulator using a Docker compiler with custom base images for the victim and attacker nodes. Copy necessary files to the output directory and make a script executable._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates an emulation environment using the seed-emulator library. The environment should include a ransomware service, a Tor service, and a DNS layer. \n",
      "\n",
      "For the ransomware service, create a ransomware attacker and 16 ransomware victims. The attacker should be installed on a host in an autonomous system and should not support botnet or Tor. The victims should be installed on hosts and should not support botnet. \n",
      "\n",
      "For the Tor service, create different types of Tor nodes including directory authorities, clients, relays, exits, and a hidden service. The hidden service should be linked to the ransomware attacker. \n",
      "\n",
      "For the DNS layer, create a root server, TLD and ccTLD servers, second-level zone servers, and a local DNS server. The servers should have appropriate zones and records. \n",
      "\n",
      "Finally, compile the emulator using a Docker compiler with custom base images for the victim and attacker nodes. Copy necessary files to the output directory and make a script executable._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000020: Retrieval file non trovato: BM25\\retrieved_kGenerate code that creates a cell using the pyscf.pbc.gto.Cell() function, with specified atom positions, basis, pseudo, a, unit, and verbosity. Then, perform KHF and KMP2 calculations with 2x2x2 k-points, and print the KMP2 energy per unit cell. Repeat the KHF and KMP2 calculations for a single k-point calculation. Then, perform a single k-point calculation using the RHF method, and print the RMP2 energy per unit cell at the k-point. Also, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Convert the RHF object to UHF and GHF objects, and for each, perform a UMP2 and GMP2 calculation respectively, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Print the UMP2 and GMP2 energy per unit cell at the k-point, and the total energy based on the MP2 density matrices._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that creates a cell using the pyscf.pbc.gto.Cell() function, with specified atom positions, basis, pseudo, a, unit, and verbosity. Then, perform KHF and KMP2 calculations with 2x2x2 k-points, and print the KMP2 energy per unit cell. Repeat the KHF and KMP2 calculations for a single k-point calculation. Then, perform a single k-point calculation using the RHF method, and print the RMP2 energy per unit cell at the k-point. Also, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Convert the RHF object to UHF and GHF objects, and for each, perform a UMP2 and GMP2 calculation respectively, generate the first and second order reduced density matrices, and calculate the total energy based on these matrices. Print the UMP2 and GMP2 energy per unit cell at the k-point, and the total energy based on the MP2 density matrices._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000021: Retrieval file non trovato: BM25\\retrieved_kGenerate code that calculates the coupling matrix for singlet energy transfer (SET) and triplet energy transfer (TET) between two molecules using the pyscf library. The code should perform CIS calculations for the excited states of two molecules, calculate the intermolecular 2e integrals, transform these integrals to MO basis, and compute the J-type and K-type coupling. The code should also include functions to compute the Coulomb integrals and exchange integrals across the two molecules, and to evaluate the coupling term including J, K and DFT XC contributions. The code should finally evaluate the overall coupling term._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that calculates the coupling matrix for singlet energy transfer (SET) and triplet energy transfer (TET) between two molecules using the pyscf library. The code should perform CIS calculations for the excited states of two molecules, calculate the intermolecular 2e integrals, transform these integrals to MO basis, and compute the J-type and K-type coupling. The code should also include functions to compute the Coulomb integrals and exchange integrals across the two molecules, and to evaluate the coupling term including J, K and DFT XC contributions. The code should finally evaluate the overall coupling term._samples.json (batch Sez. 5).\n",
      "[WARN] baseline_make_prompts qid=pyscf__000022: Retrieval file non trovato: BM25\\retrieved_kGenerate code that calculates the force from Quantum Mechanics (QM) region acting on the background Molecular Mechanics (MM) particles. The code should define a molecule using the pyscf library, generate random coordinates and charges for MM particles, and define a function to calculate the force. The force calculation should include the interaction between QM atoms and MM particles, and the interaction between electron density and MM particles. The code should then calculate the force from Hartree-Fock (HF) electron density and verify it. \n",
      "\n",
      "Next, the code should consider the response of HF orbitals in the analytical gradients for post-HF methods. As an example, it should use MP2 gradients to demonstrate how to include the orbital response effects in the force for MM particles. The code should define a function to make the reduced density matrix (rdm1) with orbital response, calculate the force from MP2 electron density (including orbital response), and verify it._samples.json.\n",
      "Genera prima BM25/retrieved_kGenerate code that calculates the force from Quantum Mechanics (QM) region acting on the background Molecular Mechanics (MM) particles. The code should define a molecule using the pyscf library, generate random coordinates and charges for MM particles, and define a function to calculate the force. The force calculation should include the interaction between QM atoms and MM particles, and the interaction between electron density and MM particles. The code should then calculate the force from Hartree-Fock (HF) electron density and verify it. \n",
      "\n",
      "Next, the code should consider the response of HF orbitals in the analytical gradients for post-HF methods. As an example, it should use MP2 gradients to demonstrate how to include the orbital response effects in the force for MM particles. The code should define a function to make the reduced density matrix (rdm1) with orbital response, calculate the force from MP2 electron density (including orbital response), and verify it._samples.json (batch Sez. 5).\n",
      "[OK] baseline_make_prompts: salvati 0 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\baseline_make_prompts_baseline.jsonl\n",
      "[SKIP] prompts.bm25_make_prompts: Nessuna funzione baseline trovata in prompts.bm25_make_prompts. Attesi uno tra: build_baseline_prompt, build_baseline_prompt_bm25_make_prompts, build_baseline, baseline_prompt, make_baseline_prompt\n",
      "[WARN] templates qid=seed-emulator__000000: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000001: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000002: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000003: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000004: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000005: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000006: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000007: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000008: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000009: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000010: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000011: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000012: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000013: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000014: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000015: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000016: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000017: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000018: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=seed-emulator__000019: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000020: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000021: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[WARN] templates qid=pyscf__000022: v1_baseline() missing 1 required positional argument: 'repo'\n",
      "[OK] templates: salvati 0 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\templates_baseline.jsonl\n",
      "[SKIP] prompts.utils: Nessuna funzione baseline trovata in prompts.utils. Attesi uno tra: build_baseline_prompt, build_baseline_prompt_utils, build_baseline, baseline_prompt, make_baseline_prompt\n",
      "[OK] v1: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v1_baseline.jsonl\n",
      "[OK] v2: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v2_baseline.jsonl\n",
      "[OK] v3: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v3_baseline.jsonl\n",
      "[OK] v4: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v4_baseline.jsonl\n",
      "[OK] v5: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v5_baseline.jsonl\n",
      "[OK] v6: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v6_baseline.jsonl\n",
      "[OK] v6_2: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v6_2_baseline.jsonl\n",
      "[OK] v6_3: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v6_3_baseline.jsonl\n",
      "[OK] v7: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v7_baseline.jsonl\n",
      "[OK] v8: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v8_baseline.jsonl\n",
      "[OK] v9: salvati 23 prompt -> C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v9_baseline.jsonl\n",
      "\n",
      "File aggregato: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\_all_baseline.jsonl\n",
      "Totale prompt baseline creati: 253\n",
      "\n",
      "File per-template:\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\baseline_make_prompts_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\templates_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v1_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v2_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v3_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v4_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v5_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v6_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v6_2_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v6_3_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v7_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v8_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\v9_baseline.jsonl\n",
      "Aggregato: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\baseline\\_all_baseline.jsonl\n",
      "Totale: 253\n"
     ]
    }
   ],
   "source": [
    "from promptgen import generate_baseline_prompts\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_iter = list(lca_dataset_split)  # le 23 query già filtrate\n",
    "\n",
    "per_template_paths, aggregated_path, total = generate_baseline_prompts(\n",
    "    dataset_like=dataset_iter,\n",
    "    prompts_dir=Path(\"prompts\"),\n",
    "    out_dir=Path(\"outputs/prompts/baseline\"),\n",
    "    aggregate_filename=\"_all_baseline.jsonl\",\n",
    "    fail_fast=False,\n",
    ")\n",
    "\n",
    "print(\"\\nFile per-template:\")\n",
    "for p in per_template_paths:\n",
    "    print(\" -\", p.resolve())\n",
    "print(\"Aggregato:\", aggregated_path.resolve())\n",
    "print(\"Totale:\", total)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f45dcc2-1753-4cb7-b82e-007366f968a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### -> 5.1 Retrivial augmented generations prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c0d72-bdfe-42a9-ad76-56852576dba5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### -- 5.1.1 BM 25 prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "5ce0a98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 bundle: items totali=150 | righe utili dopo filtro=23\n",
      "Template caricati: v1, v2, v3, v4, v5, v6, v6_2, v6_3, v7, v8, v9\n",
      "\n",
      "Scritti i file RAG (BM25, top-3):\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v1_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v2_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v3_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v4_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v5_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v6_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v7_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v8_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v9_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v6_2_rag_bm25_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\v6_3_rag_bm25_top3.jsonl\n",
      "Aggregato:\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\RAG_rag_bm25_top3.jsonl\n",
      "Totale prompt (aggregato): 253\n"
     ]
    }
   ],
   "source": [
    "# === RAG BM25 prompt builder (adatto al tuo file bundle meta+results) ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# --- Percorso del TUO file BM25 (bundle con meta+results) ---\n",
    "BM25_BUNDLE_PATH = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\BM25\\retrieved_k3_samples.json\")\n",
    "\n",
    "# --- Output ---\n",
    "TOP_K_SNIPPETS = 3\n",
    "METHOD_TAG     = \"rag_bm25_top3\"\n",
    "OUT_DIR        = Path(\"outputs/prompts\") / METHOD_TAG\n",
    "AGG_JSONL      = OUT_DIR / f\"RAG_{METHOD_TAG}.jsonl\"\n",
    "\n",
    "# Repo target (case-insensitive). Se non matcha nulla, il filtro viene disattivato automaticamente.\n",
    "TARGET_REPOS = {\"seed-emulator\", \"seed-labs__seed-emulator\", \"pyscf\", \"pyscf__pyscf\"}\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _norm_repo_name(d: Dict[str, Any]) -> str:\n",
    "    for k in (\"repo_name\", \"repo_full_name\", \"repo\"):\n",
    "        v = d.get(k)\n",
    "        if v:\n",
    "            return str(v).strip().lower()\n",
    "    return \"\"\n",
    "\n",
    "def _norm_instruction(d: Dict[str, Any]) -> str:\n",
    "    for k in (\"instruction\", \"query\", \"prompt\"):\n",
    "        v = d.get(k)\n",
    "        if v:\n",
    "            return str(v)\n",
    "    return \"\"\n",
    "\n",
    "def _norm_query_id(repo: str, idx: int) -> str:\n",
    "    repo = (repo or \"repo\").lower()\n",
    "    return f\"{repo}__{idx:06d}\"\n",
    "\n",
    "def _norm_hit_from_topk(hit: Dict[str, Any], i: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Adatta un elemento della lista 'topk' del tuo bundle:\n",
    "      {rank, library_key, snippet, snippet_len}\n",
    "    in un risultato standardizzato con doc_id/text/score/path.\n",
    "    \"\"\"\n",
    "    rank = hit.get(\"rank\", i+1)\n",
    "    lib  = hit.get(\"library_key\") or \"\"\n",
    "    text = hit.get(\"snippet\") or \"\"\n",
    "    # Score artificiale: più basso il rank, più alto lo score\n",
    "    try:\n",
    "        score = 1.0 / float(rank)\n",
    "    except Exception:\n",
    "        score = 0.0\n",
    "    return {\n",
    "        \"doc_id\": f\"{lib}::rank{rank}\",\n",
    "        \"score\": score,\n",
    "        \"path\": None,\n",
    "        \"text\": text,\n",
    "    }\n",
    "\n",
    "def _take_top_k(results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:\n",
    "    if not isinstance(results, list):\n",
    "        return []\n",
    "    sorted_res = sorted(\n",
    "        results,\n",
    "        key=lambda x: (x.get(\"score\") if isinstance(x.get(\"score\"), (int, float)) else -1.0),\n",
    "        reverse=True,\n",
    "    )\n",
    "    return sorted_res[:k]\n",
    "\n",
    "# ---------- Caricamento del bundle BM25 ----------\n",
    "if not BM25_BUNDLE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"File BM25 non trovato: {BM25_BUNDLE_PATH}\")\n",
    "\n",
    "bundle = json.loads(BM25_BUNDLE_PATH.read_text(encoding=\"utf-8\"))\n",
    "items  = bundle.get(\"results\") or []\n",
    "\n",
    "rows_raw: List[Dict[str, Any]] = []\n",
    "for item in items:\n",
    "    repo = (_norm_repo_name(item) or item.get(\"repo_full_name\") or \"\").lower()\n",
    "    instr = _norm_instruction(item)\n",
    "    idx = int(item.get(\"idx\", len(rows_raw)))\n",
    "    qid = _norm_query_id(repo, idx)\n",
    "\n",
    "    # Converti topk -> results normalizzati\n",
    "    topk_list = item.get(\"topk\") or []\n",
    "    results_norm = [_norm_hit_from_topk(h, j) for j, h in enumerate(topk_list)]\n",
    "\n",
    "    rows_raw.append({\n",
    "        \"query_id\": qid,\n",
    "        \"repo_name\": repo,\n",
    "        \"instruction\": instr,\n",
    "        \"results\": results_norm,\n",
    "    })\n",
    "\n",
    "# ---------- Filtro repo (disattiva se svuota tutto) ----------\n",
    "rows = [r for r in rows_raw if (r.get(\"repo_name\") or \"\") in TARGET_REPOS]\n",
    "if not rows:\n",
    "    print(\"[WARN] Filtro repo ha rimosso tutto. Disattivo filtro e uso tutti gli items.\")\n",
    "    rows = rows_raw\n",
    "\n",
    "print(f\"BM25 bundle: items totali={len(items)} | righe utili dopo filtro={len(rows)}\")\n",
    "\n",
    "# ---------- Costruzione prompt ----------\n",
    "from prompts_common.templates import load_all_prompt_builders\n",
    "from prompts_common.rag_prompt_maker import make_rag_prompt, save_jsonl\n",
    "\n",
    "builders = load_all_prompt_builders()\n",
    "print(\"Template caricati:\", \", \".join(sorted(builders.keys())))\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "agg_rows: List[Dict[str, Any]] = []\n",
    "per_template_buffers: Dict[str, List[Dict[str, Any]]] = {name: [] for name in builders.keys()}\n",
    "\n",
    "for r in rows:\n",
    "    qid   = str(r.get(\"query_id\"))\n",
    "    repo  = (r.get(\"repo_name\") or \"\").lower()\n",
    "    instr = r.get(\"instruction\") or \"\"\n",
    "    results_topk = _take_top_k(r.get(\"results\") or [], TOP_K_SNIPPETS)\n",
    "\n",
    "    for templ_name, builder in builders.items():\n",
    "        prompt_text = make_rag_prompt(\n",
    "            base_builder=builder,\n",
    "            instruction=instr,\n",
    "            snippets=results_topk,\n",
    "            repo_name=repo,\n",
    "            method=\"bm25\",\n",
    "            k=TOP_K_SNIPPETS,\n",
    "        )\n",
    "        row_out = {\n",
    "            \"query_id\": qid,\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": instr,\n",
    "            \"template\": templ_name,\n",
    "            \"variant\": \"rag_bm25\",\n",
    "            \"k_snippets\": TOP_K_SNIPPETS,\n",
    "            \"retrieval_method\": \"bm25\",\n",
    "            \"snippets\": results_topk,\n",
    "            \"prompt\": prompt_text,\n",
    "        }\n",
    "        per_template_buffers[templ_name].append(row_out)\n",
    "        agg_rows.append(row_out)\n",
    "\n",
    "# ---------- Salvataggi ----------\n",
    "written = []\n",
    "for templ_name, rows_buf in per_template_buffers.items():\n",
    "    path = OUT_DIR / f\"{templ_name}_rag_bm25_top3.jsonl\"\n",
    "    save_jsonl(path, rows_buf)\n",
    "    written.append(path)\n",
    "\n",
    "save_jsonl(AGG_JSONL, agg_rows)\n",
    "\n",
    "print(\"\\nScritti i file RAG (BM25, top-3):\")\n",
    "for p in written: print(\" -\", p.resolve())\n",
    "print(\"Aggregato:\")\n",
    "print(\" -\", AGG_JSONL.resolve())\n",
    "print(f\"Totale prompt (aggregato): {len(agg_rows)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf0529",
   "metadata": {},
   "source": [
    "#### -- 5.1.2 Cosine prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "03f17b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carico JSONL normalizzato: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\cosine_topk_k3.jsonl\n",
      "COSINE rows (filtrate su repo target): 23\n",
      "Template caricati: v1, v2, v3, v4, v5, v6, v6_2, v6_3, v7, v8, v9\n",
      "\n",
      "Scritti i file RAG (COSINE, top-3):\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v1_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v2_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v3_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v4_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v5_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v6_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v7_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v8_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v9_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v6_2_rag_cosine_top3.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\v6_3_rag_cosine_top3.jsonl\n",
      "Aggregato:\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\RAG_rag_cosine_top3.jsonl\n"
     ]
    }
   ],
   "source": [
    "# === RAG prompts con COSINE (top-3) usando i file indicati ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# File COSINE (metti i tuoi path qui)\n",
    "RAW_JSON_PATH        = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\cosine_raw_k3.json\")\n",
    "NORM_JSONL_PATH      = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\cosine_topk_k3.jsonl\")\n",
    "NORM_JSON_ARRAY_PATH = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\cosine_topk_k3.json\")\n",
    "\n",
    "# Output\n",
    "TOP_K_SNIPPETS = 3\n",
    "METHOD_TAG     = \"rag_cosine_top3\"\n",
    "OUT_DIR        = Path(\"outputs/prompts/rag_cosine_top3\")\n",
    "AGG_JSONL      = OUT_DIR / f\"RAG_{METHOD_TAG}.jsonl\"\n",
    "\n",
    "TARGET_REPOS = {\"seed-emulator\", \"seed-labs__seed-emulator\", \"pyscf\", \"pyscf__pyscf\"}\n",
    "\n",
    "# ---------- Normalizzazione hits ----------\n",
    "def _norm_hit(hit: Any, i: int) -> Dict[str, Any]:\n",
    "    if isinstance(hit, str):\n",
    "        return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": hit}\n",
    "    if isinstance(hit, dict):\n",
    "        doc_id = hit.get(\"doc_id\") or hit.get(\"id\") or hit.get(\"path\") or f\"doc_{i}\"\n",
    "        text   = hit.get(\"text\") or hit.get(\"content\") or hit.get(\"snippet\") or \"\"\n",
    "        # cosine di solito è 'similarity', ma supportiamo anche 'score' e 'bm25_score'\n",
    "        score  = hit.get(\"similarity\", hit.get(\"score\", hit.get(\"bm25_score\", 0.0)))\n",
    "        path   = hit.get(\"path\")\n",
    "        try: score = float(score)\n",
    "        except Exception: score = 0.0\n",
    "        return {\"doc_id\": doc_id, \"score\": score, \"path\": path, \"text\": text}\n",
    "    return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": str(hit)}\n",
    "\n",
    "def _take_top_k(results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:\n",
    "    if not isinstance(results, list):\n",
    "        return []\n",
    "    sorted_res = sorted(\n",
    "        results,\n",
    "        key=lambda x: (x.get(\"score\") if isinstance(x.get(\"score\"), (int, float)) else -1.0),\n",
    "        reverse=True\n",
    "    )\n",
    "    return sorted_res[:k]\n",
    "\n",
    "def _norm_repo_name(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"repo_name\", \"repo_full_name\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v).strip().lower()\n",
    "    return \"\"\n",
    "\n",
    "def _norm_instruction(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"instruction\", \"query\", \"prompt\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v)\n",
    "    return \"\"\n",
    "\n",
    "def _norm_query_id(item: Dict[str, Any], fallback_idx: int) -> str:\n",
    "    for k in (\"id\", \"query_id\", \"qid\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v)\n",
    "    repo = _norm_repo_name(item) or \"repo\"\n",
    "    return f\"{repo}__{fallback_idx:06d}\"\n",
    "\n",
    "# ---------- Loader: preferisci JSONL normalizzato, poi JSON array, infine RAW JSON ----------\n",
    "def _load_normalized_jsonl(p: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            obj = json.loads(line)\n",
    "            repo = _norm_repo_name(obj)\n",
    "            if repo not in TARGET_REPOS:\n",
    "                continue\n",
    "            res  = [_norm_hit(h, j) for j, h in enumerate(obj.get(\"results\") or [])]\n",
    "            obj[\"results\"] = res\n",
    "            rows.append(obj)\n",
    "    return rows\n",
    "\n",
    "def _load_normalized_json_array(p: Path) -> List[Dict[str, Any]]:\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    rows = []\n",
    "    for obj in data:\n",
    "        repo = _norm_repo_name(obj)\n",
    "        if repo not in TARGET_REPOS:\n",
    "            continue\n",
    "        res = [_norm_hit(h, j) for j, h in enumerate(obj.get(\"results\") or [])]\n",
    "        rows.append({\n",
    "            \"query_id\": obj.get(\"query_id\"),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": _norm_instruction(obj) or obj.get(\"instruction\") or \"\",\n",
    "            \"results\": res,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def _load_raw_json(p: Path) -> List[Dict[str, Any]]:\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    rows = []\n",
    "    for i, item in enumerate(data):\n",
    "        repo = _norm_repo_name(item)\n",
    "        if repo not in TARGET_REPOS:\n",
    "            continue\n",
    "        instr = _norm_instruction(item)\n",
    "        hits  = item.get(\"retrieved_snippets\") or []\n",
    "        res   = [_norm_hit(h, j) for j, h in enumerate(hits)]\n",
    "        rows.append({\n",
    "            \"query_id\": _norm_query_id(item, i),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": instr,\n",
    "            \"results\": res,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def load_cosine_rows() -> List[Dict[str, Any]]:\n",
    "    if NORM_JSONL_PATH.exists():\n",
    "        print(f\"Carico JSONL normalizzato: {NORM_JSONL_PATH}\")\n",
    "        return _load_normalized_jsonl(NORM_JSONL_PATH)\n",
    "    if NORM_JSON_ARRAY_PATH.exists():\n",
    "        print(f\"JSONL non trovato. Carico JSON (array): {NORM_JSON_ARRAY_PATH}\")\n",
    "        return _load_normalized_json_array(NORM_JSON_ARRAY_PATH)\n",
    "    if RAW_JSON_PATH.exists():\n",
    "        print(f\"Normalizzati non trovati. Carico RAW JSON: {RAW_JSON_PATH}\")\n",
    "        return _load_raw_json(RAW_JSON_PATH)\n",
    "    raise FileNotFoundError(\"Nessuno dei file COSINE esiste nei path indicati.\")\n",
    "\n",
    "# ---------- Costruzione prompt ----------\n",
    "from prompts_common.templates import load_all_prompt_builders\n",
    "from prompts_common.rag_prompt_maker import make_rag_prompt, save_jsonl\n",
    "\n",
    "cosine_rows = load_cosine_rows()\n",
    "print(f\"COSINE rows (filtrate su repo target): {len(cosine_rows)}\")\n",
    "\n",
    "builders = load_all_prompt_builders()\n",
    "print(\"Template caricati:\", \", \".join(sorted(builders.keys())))\n",
    "\n",
    "per_template_buffers: Dict[str, List[Dict[str, Any]]] = {name: [] for name in builders.keys()}\n",
    "agg_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for r in cosine_rows:\n",
    "    qid   = str(r.get(\"query_id\"))\n",
    "    repo  = (r.get(\"repo_name\") or \"\").lower()\n",
    "    instr = r.get(\"instruction\") or \"\"\n",
    "    results_topk = _take_top_k((r.get(\"results\") or []), TOP_K_SNIPPETS)\n",
    "\n",
    "    for templ_name, builder in builders.items():\n",
    "        prompt_text = make_rag_prompt(\n",
    "            base_builder=builder,\n",
    "            instruction=instr,\n",
    "            snippets=results_topk,\n",
    "            repo_name=repo,\n",
    "            method=\"cosine\",\n",
    "            k=TOP_K_SNIPPETS,\n",
    "        )\n",
    "        row_out = {\n",
    "            \"query_id\": qid,\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": instr,\n",
    "            \"template\": templ_name,\n",
    "            \"variant\": \"rag_cosine\",\n",
    "            \"k_snippets\": TOP_K_SNIPPETS,\n",
    "            \"retrieval_method\": \"cosine\",\n",
    "            \"snippets\": results_topk,\n",
    "            \"prompt\": prompt_text,\n",
    "        }\n",
    "        per_template_buffers[templ_name].append(row_out)\n",
    "        agg_rows.append(row_out)\n",
    "\n",
    "# ---------- Salvataggi ----------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "written = []\n",
    "for templ_name, rows in per_template_buffers.items():\n",
    "    path = OUT_DIR / f\"{templ_name}_rag_cosine_top3.jsonl\"\n",
    "    save_jsonl(path, rows)\n",
    "    written.append(path)\n",
    "\n",
    "save_jsonl(AGG_JSONL, agg_rows)\n",
    "\n",
    "print(\"\\nScritti i file RAG (COSINE, top-3):\")\n",
    "for p in written: print(\" -\", p.resolve())\n",
    "print(\"Aggregato:\")\n",
    "print(\" -\", AGG_JSONL.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8abf8c2-eaaf-4b61-b0fc-f5b0fa21c6c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### -- 5.1.3 Hybrid prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ee4d5a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carico JSONL normalizzato: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\hybrid_topk_k5.jsonl\n",
      "HYBRID rows (filtrate su repo target): 23\n",
      "Template caricati: v1, v2, v3, v4, v5, v6, v6_2, v6_3, v7, v8, v9\n",
      "\n",
      "Scritti i file RAG (HYBRID, top-5):\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v1_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v2_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v3_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v4_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v5_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v6_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v7_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v8_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v9_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v6_2_rag_hybrid_top5.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\v6_3_rag_hybrid_top5.jsonl\n",
      "Aggregato:\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\RAG_rag_hybrid_top5.jsonl\n"
     ]
    }
   ],
   "source": [
    "# === RAG prompts con HYBRID (top-5) usando i file indicati ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# File HYBRID (tuoi)\n",
    "RAW_JSON_PATH = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\hybrid_raw_k5.json\")\n",
    "NORM_JSONL_PATH = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\hybrid_topk_k5.jsonl\")\n",
    "NORM_JSON_ARRAY_PATH = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\retrieval\\hybrid_topk_k5.json\")\n",
    "\n",
    "# Output\n",
    "TOP_K_SNIPPETS = 3\n",
    "METHOD_TAG = \"rag_hybrid_top5\"\n",
    "OUT_DIR = Path(\"outputs/prompts/rag_hybrid_top3\")\n",
    "AGG_JSONL = OUT_DIR / f\"RAG_{METHOD_TAG}.jsonl\"\n",
    "\n",
    "TARGET_REPOS = {\"seed-emulator\", \"seed-labs__seed-emulator\", \"pyscf\", \"pyscf__pyscf\"}\n",
    "\n",
    "# ---------- Normalizzazione hits ----------\n",
    "def _norm_hit(hit: Any, i: int) -> Dict[str, Any]:\n",
    "    if isinstance(hit, str):\n",
    "        return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": hit}\n",
    "    if isinstance(hit, dict):\n",
    "        doc_id = hit.get(\"doc_id\") or hit.get(\"id\") or hit.get(\"path\") or f\"doc_{i}\"\n",
    "        text   = hit.get(\"text\") or hit.get(\"content\") or hit.get(\"snippet\") or \"\"\n",
    "        score  = hit.get(\"score\") or hit.get(\"similarity\") or hit.get(\"bm25_score\") or 0.0\n",
    "        path   = hit.get(\"path\")\n",
    "        try: score = float(score)\n",
    "        except Exception: score = 0.0\n",
    "        return {\"doc_id\": doc_id, \"score\": score, \"path\": path, \"text\": text}\n",
    "    return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": str(hit)}\n",
    "\n",
    "def _take_top_k(results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:\n",
    "    if not isinstance(results, list):\n",
    "        return []\n",
    "    sorted_res = sorted(\n",
    "        results,\n",
    "        key=lambda x: (x.get(\"score\") if isinstance(x.get(\"score\"), (int, float)) else -1.0),\n",
    "        reverse=True\n",
    "    )\n",
    "    return sorted_res[:k]\n",
    "\n",
    "def _norm_repo_name(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"repo_name\", \"repo_full_name\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v).strip().lower()\n",
    "    return \"\"\n",
    "\n",
    "def _norm_instruction(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"instruction\", \"query\", \"prompt\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v)\n",
    "    return \"\"\n",
    "\n",
    "def _norm_query_id(item: Dict[str, Any], fallback_idx: int) -> str:\n",
    "    for k in (\"id\", \"query_id\", \"qid\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v)\n",
    "    repo = _norm_repo_name(item) or \"repo\"\n",
    "    return f\"{repo}__{fallback_idx:06d}\"\n",
    "\n",
    "# ---------- Loader: preferisci JSONL normalizzato, poi JSON array, infine RAW JSON ----------\n",
    "def _load_normalized_jsonl(p: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            obj = json.loads(line)\n",
    "            repo = _norm_repo_name(obj)\n",
    "            if repo not in TARGET_REPOS:\n",
    "                continue\n",
    "            res  = [_norm_hit(h, j) for j, h in enumerate(obj.get(\"results\") or [])]\n",
    "            obj[\"results\"] = res\n",
    "            rows.append(obj)\n",
    "    return rows\n",
    "\n",
    "def _load_normalized_json_array(p: Path) -> List[Dict[str, Any]]:\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    rows = []\n",
    "    for obj in data:\n",
    "        repo = _norm_repo_name(obj)\n",
    "        if repo not in TARGET_REPOS:\n",
    "            continue\n",
    "        res = [_norm_hit(h, j) for j, h in enumerate(obj.get(\"results\") or [])]\n",
    "        rows.append({\n",
    "            \"query_id\": obj.get(\"query_id\"),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": _norm_instruction(obj) or obj.get(\"instruction\") or \"\",\n",
    "            \"results\": res,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def _load_raw_json(p: Path) -> List[Dict[str, Any]]:\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    rows = []\n",
    "    for i, item in enumerate(data):\n",
    "        repo = _norm_repo_name(item)\n",
    "        if repo not in TARGET_REPOS:\n",
    "            continue\n",
    "        instr = _norm_instruction(item)\n",
    "        hits  = item.get(\"retrieved_snippets\") or []\n",
    "        res   = [_norm_hit(h, j) for j, h in enumerate(hits)]\n",
    "        rows.append({\n",
    "            \"query_id\": _norm_query_id(item, i),\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": instr,\n",
    "            \"results\": res,\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def load_hybrid_rows() -> List[Dict[str, Any]]:\n",
    "    if NORM_JSONL_PATH.exists():\n",
    "        print(f\"Carico JSONL normalizzato: {NORM_JSONL_PATH}\")\n",
    "        return _load_normalized_jsonl(NORM_JSONL_PATH)\n",
    "    if NORM_JSON_ARRAY_PATH.exists():\n",
    "        print(f\"JSONL non trovato. Carico JSON (array): {NORM_JSON_ARRAY_PATH}\")\n",
    "        return _load_normalized_json_array(NORM_JSON_ARRAY_PATH)\n",
    "    if RAW_JSON_PATH.exists():\n",
    "        print(f\"Normalizzati non trovati. Carico RAW JSON: {RAW_JSON_PATH}\")\n",
    "        return _load_raw_json(RAW_JSON_PATH)\n",
    "    raise FileNotFoundError(\"Nessuno dei file HYBRID esiste nei path indicati.\")\n",
    "\n",
    "# ---------- Costruzione prompt ----------\n",
    "from prompts_common.templates import load_all_prompt_builders\n",
    "from prompts_common.rag_prompt_maker import make_rag_prompt, save_jsonl\n",
    "\n",
    "hybrid_rows = load_hybrid_rows()\n",
    "print(f\"HYBRID rows (filtrate su repo target): {len(hybrid_rows)}\")\n",
    "\n",
    "builders = load_all_prompt_builders()\n",
    "print(\"Template caricati:\", \", \".join(sorted(builders.keys())))\n",
    "\n",
    "per_template_buffers: Dict[str, List[Dict[str, Any]]] = {name: [] for name in builders.keys()}\n",
    "agg_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for r in hybrid_rows:\n",
    "    qid   = str(r.get(\"query_id\"))\n",
    "    repo  = (r.get(\"repo_name\") or \"\").lower()\n",
    "    instr = r.get(\"instruction\") or \"\"\n",
    "    results_topk = _take_top_k((r.get(\"results\") or []), TOP_K_SNIPPETS)\n",
    "\n",
    "    for templ_name, builder in builders.items():\n",
    "        prompt_text = make_rag_prompt(\n",
    "            base_builder=builder,\n",
    "            instruction=instr,\n",
    "            snippets=results_topk,\n",
    "            repo_name=repo,\n",
    "            method=\"hybrid\",\n",
    "            k=TOP_K_SNIPPETS,\n",
    "        )\n",
    "        row_out = {\n",
    "            \"query_id\": qid,\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": instr,\n",
    "            \"template\": templ_name,\n",
    "            \"variant\": \"rag_hybrid\",\n",
    "            \"k_snippets\": TOP_K_SNIPPETS,\n",
    "            \"retrieval_method\": \"hybrid\",\n",
    "            \"snippets\": results_topk,\n",
    "            \"prompt\": prompt_text,\n",
    "        }\n",
    "        per_template_buffers[templ_name].append(row_out)\n",
    "        agg_rows.append(row_out)\n",
    "\n",
    "# ---------- Salvataggi ----------\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "written = []\n",
    "for templ_name, rows in per_template_buffers.items():\n",
    "    path = OUT_DIR / f\"{templ_name}_rag_hybrid_top5.jsonl\"\n",
    "    save_jsonl(path, rows)\n",
    "    written.append(path)\n",
    "\n",
    "save_jsonl(AGG_JSONL, agg_rows)\n",
    "\n",
    "print(\"\\nScritti i file RAG (HYBRID, top-5):\")\n",
    "for p in written: print(\" -\", p.resolve())\n",
    "print(\"Aggregato:\")\n",
    "print(\" -\", AGG_JSONL.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cae33d-3e45-4078-9eec-2d374f0f14ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### -- 5.1.3 Multihope optione 1 prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "554e3a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carico MultiHop JSONL: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_decomposition_topk_k3.jsonl\n",
      "MultiHop rows: 23  | strategy=decomposition_first | k=3\n",
      "Template caricati: v1, v2, v3, v4, v5, v6, v6_2, v6_3, v7, v8, v9\n",
      "\n",
      "Scritti i file RAG (MultiHop decomposition, top-4):\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v1_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v2_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v3_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v4_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v5_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v6_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v7_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v8_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v9_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v6_2_rag_multihop_decomposition_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\v6_3_rag_multihop_decomposition_top4.jsonl\n",
      "Aggregato:\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\rag_multihop_decomposition_top4.jsonl\n"
     ]
    }
   ],
   "source": [
    "# === RAG prompts con MULTIHOP \"decomposition_first\" (top-4) ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# ---- INPUT MultiHop (i tuoi file) ----\n",
    "MH_JSONL = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_decomposition_topk_k3.jsonl\")\n",
    "MH_JSON  = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_decomposition_topk_k3.json\")\n",
    "\n",
    "# ---- PARAMS / OUTPUT ----\n",
    "TOP_K_SNIPPETS = 3\n",
    "STRATEGY = \"decomposition_first\"\n",
    "METHOD_TAG = \"rag_multihop_decomposition_top4\"\n",
    "OUT_DIR = Path(\"outputs/prompts\") / METHOD_TAG\n",
    "AGG_JSONL = OUT_DIR / f\"{METHOD_TAG}.jsonl\"\n",
    "TARGET_REPOS = {\"seed-emulator\", \"seed-labs__seed-emulator\", \"pyscf\", \"pyscf__pyscf\"}\n",
    "\n",
    "# ---------- Utilità di normalizzazione ----------\n",
    "def _norm_repo_name(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"repo_name\", \"repo_full_name\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v).strip().lower()\n",
    "    return \"\"\n",
    "\n",
    "def _norm_instruction(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"instruction\", \"query\", \"prompt\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v)\n",
    "    return \"\"\n",
    "\n",
    "def _norm_hit(hit: Any, i: int) -> Dict[str, Any]:\n",
    "    if isinstance(hit, str):\n",
    "        return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": hit}\n",
    "    if isinstance(hit, dict):\n",
    "        doc_id = hit.get(\"doc_id\") or hit.get(\"id\") or hit.get(\"path\") or f\"doc_{i}\"\n",
    "        text   = hit.get(\"text\") or hit.get(\"content\") or hit.get(\"snippet\") or \"\"\n",
    "        score  = hit.get(\"score\") or hit.get(\"similarity\") or hit.get(\"bm25_score\") or 0.0\n",
    "        path   = hit.get(\"path\")\n",
    "        try: score = float(score)\n",
    "        except Exception: score = 0.0\n",
    "        return {\"doc_id\": doc_id, \"score\": score, \"path\": path, \"text\": text}\n",
    "    return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": str(hit)}\n",
    "\n",
    "def _take_top_k(results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:\n",
    "    if not isinstance(results, list):\n",
    "        return []\n",
    "    sorted_res = sorted(\n",
    "        results,\n",
    "        key=lambda x: (x.get(\"score\") if isinstance(x.get(\"score\"), (int, float)) else -1.0),\n",
    "        reverse=True\n",
    "    )\n",
    "    return sorted_res[:k]\n",
    "\n",
    "# ---------- Loader: preferisci JSONL poi JSON ----------\n",
    "def _load_mh_jsonl(p: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            obj = json.loads(line)\n",
    "            repo = _norm_repo_name(obj)\n",
    "            if repo and repo not in TARGET_REPOS:\n",
    "                continue\n",
    "            # compat: risultati possono stare in \"results\" o \"retrieved_snippets\"\n",
    "            res = obj.get(\"results\")\n",
    "            if res is None:\n",
    "                res = obj.get(\"retrieved_snippets\") or []\n",
    "            res = [_norm_hit(h, j) for j, h in enumerate(res)]\n",
    "            rows.append({\n",
    "                \"query_id\": obj.get(\"query_id\"),\n",
    "                \"repo_name\": repo or obj.get(\"repo_name\"),\n",
    "                \"instruction\": _norm_instruction(obj) or obj.get(\"instruction\") or \"\",\n",
    "                \"results\": res\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def _load_mh_json(p: Path) -> List[Dict[str, Any]]:\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    rows = []\n",
    "    for i, obj in enumerate(data):\n",
    "        repo = _norm_repo_name(obj)\n",
    "        if repo and repo not in TARGET_REPOS:\n",
    "            continue\n",
    "        res = obj.get(\"results\")\n",
    "        if res is None:\n",
    "            res = obj.get(\"retrieved_snippets\") or []\n",
    "        res = [_norm_hit(h, j) for j, h in enumerate(res)]\n",
    "        rows.append({\n",
    "            \"query_id\": obj.get(\"query_id\") or f\"{repo or 'repo'}__{i:06d}\",\n",
    "            \"repo_name\": repo or obj.get(\"repo_name\"),\n",
    "            \"instruction\": _norm_instruction(obj) or obj.get(\"instruction\") or \"\",\n",
    "            \"results\": res\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def load_multihop_rows() -> List[Dict[str, Any]]:\n",
    "    if MH_JSONL.exists():\n",
    "        print(f\"Carico MultiHop JSONL: {MH_JSONL}\")\n",
    "        return _load_mh_jsonl(MH_JSONL)\n",
    "    if MH_JSON.exists():\n",
    "        print(f\"JSONL non trovato. Carico MultiHop JSON: {MH_JSON}\")\n",
    "        return _load_mh_json(MH_JSON)\n",
    "    raise FileNotFoundError(\"File MultiHop non trovati nei path indicati.\")\n",
    "\n",
    "# ---------- Costruzione prompt ----------\n",
    "from prompts_common.templates import load_all_prompt_builders\n",
    "from prompts_common.rag_prompt_maker import make_rag_prompt, save_jsonl\n",
    "\n",
    "rows = load_multihop_rows()\n",
    "print(f\"MultiHop rows: {len(rows)}  | strategy={STRATEGY} | k={TOP_K_SNIPPETS}\")\n",
    "\n",
    "builders = load_all_prompt_builders()\n",
    "print(\"Template caricati:\", \", \".join(sorted(builders.keys())))\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "per_template_buffers: Dict[str, List[Dict[str, Any]]] = {name: [] for name in builders.keys()}\n",
    "agg_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for r in rows:\n",
    "    qid   = str(r.get(\"query_id\"))\n",
    "    repo  = (r.get(\"repo_name\") or \"\").lower()\n",
    "    instr = r.get(\"instruction\") or \"\"\n",
    "    topk  = _take_top_k((r.get(\"results\") or []), TOP_K_SNIPPETS)\n",
    "\n",
    "    for templ_name, builder in builders.items():\n",
    "        prompt_text = make_rag_prompt(\n",
    "            base_builder=builder,\n",
    "            instruction=instr,\n",
    "            snippets=topk,\n",
    "            repo_name=repo,\n",
    "            method=\"multihop_decomposition\",\n",
    "            k=TOP_K_SNIPPETS,\n",
    "        )\n",
    "        row_out = {\n",
    "            \"query_id\": qid,\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": instr,\n",
    "            \"template\": templ_name,\n",
    "            \"variant\": \"rag_multihop_decomposition\",\n",
    "            \"strategy\": STRATEGY,\n",
    "            \"k_snippets\": TOP_K_SNIPPETS,\n",
    "            \"retrieval_method\": \"multihop\",\n",
    "            \"snippets\": topk,\n",
    "            \"prompt\": prompt_text,\n",
    "        }\n",
    "        per_template_buffers[templ_name].append(row_out)\n",
    "        agg_rows.append(row_out)\n",
    "\n",
    "# ---------- Salvataggi ----------\n",
    "written = []\n",
    "for templ_name, rows_out in per_template_buffers.items():\n",
    "    path = OUT_DIR / f\"{templ_name}_{METHOD_TAG}.jsonl\"\n",
    "    save_jsonl(path, rows_out)\n",
    "    written.append(path)\n",
    "\n",
    "save_jsonl(AGG_JSONL, agg_rows)\n",
    "\n",
    "print(\"\\nScritti i file RAG (MultiHop decomposition, top-4):\")\n",
    "for p in written: print(\" -\", p.resolve())\n",
    "print(\"Aggregato:\")\n",
    "print(\" -\", AGG_JSONL.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560bf98-869e-487b-9087-ac1ab445a110",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### -- 5.1.4 Multihope opzione 2 prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "670f1d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carico MultiHop JSONL: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_iterative_topk_k3.jsonl\n",
      "MultiHop rows: 23  | strategy=iterative_refine | k=3\n",
      "Template caricati: v1, v2, v3, v4, v5, v6, v6_2, v6_3, v7, v8, v9\n",
      "\n",
      "Scritti i file RAG (MultiHop iterative_refine, top-4):\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v1_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v2_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v3_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v4_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v5_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v6_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v7_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v8_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v9_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v6_2_rag_multihop_iterative_top4.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\v6_3_rag_multihop_iterative_top4.jsonl\n",
      "Aggregato:\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\rag_multihop_iterative_top4.jsonl\n"
     ]
    }
   ],
   "source": [
    "# === RAG prompts con MULTIHOP \"iterative_refine\" (top-4) ===\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# ---- INPUT MultiHop (iterative_refine) ----\n",
    "MH_JSONL = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_iterative_topk_k3.jsonl\")\n",
    "MH_JSON  = Path(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_iterative_topk_k3.json\")\n",
    "\n",
    "# ---- PARAMS / OUTPUT ----\n",
    "TOP_K_SNIPPETS = 3\n",
    "STRATEGY = \"iterative_refine\"\n",
    "METHOD_TAG = \"rag_multihop_iterative_top4\"\n",
    "OUT_DIR = Path(\"outputs/prompts\") / METHOD_TAG\n",
    "AGG_JSONL = OUT_DIR / f\"{METHOD_TAG}.jsonl\"\n",
    "TARGET_REPOS = {\"seed-emulator\", \"seed-labs__seed-emulator\", \"pyscf\", \"pyscf__pyscf\"}\n",
    "\n",
    "# ---------- Utilità di normalizzazione ----------\n",
    "def _norm_repo_name(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"repo_name\", \"repo_full_name\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v).strip().lower()\n",
    "    return \"\"\n",
    "\n",
    "def _norm_instruction(item: Dict[str, Any]) -> str:\n",
    "    for k in (\"instruction\", \"query\", \"prompt\"):\n",
    "        v = item.get(k)\n",
    "        if v: return str(v)\n",
    "    return \"\"\n",
    "\n",
    "def _norm_hit(hit: Any, i: int) -> Dict[str, Any]:\n",
    "    if isinstance(hit, str):\n",
    "        return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": hit}\n",
    "    if isinstance(hit, dict):\n",
    "        doc_id = hit.get(\"doc_id\") or hit.get(\"id\") or hit.get(\"path\") or f\"doc_{i}\"\n",
    "        text   = hit.get(\"text\") or hit.get(\"content\") or hit.get(\"snippet\") or \"\"\n",
    "        score  = hit.get(\"score\") or hit.get(\"similarity\") or hit.get(\"bm25_score\") or 0.0\n",
    "        path   = hit.get(\"path\")\n",
    "        try: score = float(score)\n",
    "        except Exception: score = 0.0\n",
    "        return {\"doc_id\": doc_id, \"score\": score, \"path\": path, \"text\": text}\n",
    "    return {\"doc_id\": f\"doc_{i}\", \"score\": 0.0, \"path\": None, \"text\": str(hit)}\n",
    "\n",
    "def _take_top_k(results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:\n",
    "    if not isinstance(results, list):\n",
    "        return []\n",
    "    sorted_res = sorted(\n",
    "        results,\n",
    "        key=lambda x: (x.get(\"score\") if isinstance(x.get(\"score\"), (int, float)) else -1.0),\n",
    "        reverse=True\n",
    "    )\n",
    "    return sorted_res[:k]\n",
    "\n",
    "# ---------- Loader: preferisci JSONL poi JSON ----------\n",
    "def _load_mh_jsonl(p: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with p.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            obj = json.loads(line)\n",
    "            repo = _norm_repo_name(obj)\n",
    "            if repo and repo not in TARGET_REPOS:\n",
    "                continue\n",
    "            res = obj.get(\"results\")\n",
    "            if res is None:\n",
    "                res = obj.get(\"retrieved_snippets\") or []\n",
    "            res = [_norm_hit(h, j) for j, h in enumerate(res)]\n",
    "            rows.append({\n",
    "                \"query_id\": obj.get(\"query_id\"),\n",
    "                \"repo_name\": repo or obj.get(\"repo_name\"),\n",
    "                \"instruction\": _norm_instruction(obj) or obj.get(\"instruction\") or \"\",\n",
    "                \"results\": res\n",
    "            })\n",
    "    return rows\n",
    "\n",
    "def _load_mh_json(p: Path) -> List[Dict[str, Any]]:\n",
    "    data = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "    rows = []\n",
    "    for i, obj in enumerate(data):\n",
    "        repo = _norm_repo_name(obj)\n",
    "        if repo and repo not in TARGET_REPOS:\n",
    "            continue\n",
    "        res = obj.get(\"results\")\n",
    "        if res is None:\n",
    "            res = obj.get(\"retrieved_snippets\") or []\n",
    "        res = [_norm_hit(h, j) for j, h in enumerate(res)]\n",
    "        rows.append({\n",
    "            \"query_id\": obj.get(\"query_id\") or f\"{repo or 'repo'}__{i:06d}\",\n",
    "            \"repo_name\": repo or obj.get(\"repo_name\"),\n",
    "            \"instruction\": _norm_instruction(obj) or obj.get(\"instruction\") or \"\",\n",
    "            \"results\": res\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def load_multihop_rows() -> List[Dict[str, Any]]:\n",
    "    if MH_JSONL.exists():\n",
    "        print(f\"Carico MultiHop JSONL: {MH_JSONL}\")\n",
    "        return _load_mh_jsonl(MH_JSONL)\n",
    "    if MH_JSON.exists():\n",
    "        print(f\"JSONL non trovato. Carico MultiHop JSON: {MH_JSON}\")\n",
    "        return _load_mh_json(MH_JSON)\n",
    "    raise FileNotFoundError(\"File MultiHop (iterative_refine) non trovati nei path indicati.\")\n",
    "\n",
    "# ---------- Costruzione prompt ----------\n",
    "from prompts_common.templates import load_all_prompt_builders\n",
    "from prompts_common.rag_prompt_maker import make_rag_prompt, save_jsonl\n",
    "\n",
    "rows = load_multihop_rows()\n",
    "print(f\"MultiHop rows: {len(rows)}  | strategy={STRATEGY} | k={TOP_K_SNIPPETS}\")\n",
    "\n",
    "builders = load_all_prompt_builders()\n",
    "print(\"Template caricati:\", \", \".join(sorted(builders.keys())))\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "per_template_buffers: Dict[str, List[Dict[str, Any]]] = {name: [] for name in builders.keys()}\n",
    "agg_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for r in rows:\n",
    "    qid   = str(r.get(\"query_id\"))\n",
    "    repo  = (r.get(\"repo_name\") or \"\").lower()\n",
    "    instr = r.get(\"instruction\") or \"\"\n",
    "    topk  = _take_top_k((r.get(\"results\") or []), TOP_K_SNIPPETS)\n",
    "\n",
    "    for templ_name, builder in builders.items():\n",
    "        prompt_text = make_rag_prompt(\n",
    "            base_builder=builder,\n",
    "            instruction=instr,\n",
    "            snippets=topk,\n",
    "            repo_name=repo,\n",
    "            method=\"multihop_iterative\",\n",
    "            k=TOP_K_SNIPPETS,\n",
    "        )\n",
    "        row_out = {\n",
    "            \"query_id\": qid,\n",
    "            \"repo_name\": repo,\n",
    "            \"instruction\": instr,\n",
    "            \"template\": templ_name,\n",
    "            \"variant\": \"rag_multihop_iterative\",\n",
    "            \"strategy\": STRATEGY,\n",
    "            \"k_snippets\": TOP_K_SNIPPETS,\n",
    "            \"retrieval_method\": \"multihop\",\n",
    "            \"snippets\": topk,\n",
    "            \"prompt\": prompt_text,\n",
    "        }\n",
    "        per_template_buffers[templ_name].append(row_out)\n",
    "        agg_rows.append(row_out)\n",
    "\n",
    "# ---------- Salvataggi ----------\n",
    "written = []\n",
    "for templ_name, rows_out in per_template_buffers.items():\n",
    "    path = OUT_DIR / f\"{templ_name}_{METHOD_TAG}.jsonl\"\n",
    "    save_jsonl(path, rows_out)\n",
    "    written.append(path)\n",
    "\n",
    "save_jsonl(AGG_JSONL, agg_rows)\n",
    "\n",
    "print(\"\\nScritti i file RAG (MultiHop iterative_refine, top-4):\")\n",
    "for p in written: print(\" -\", p.resolve())\n",
    "print(\"Aggregato:\")\n",
    "print(\" -\", AGG_JSONL.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95599596-20c9-4ed7-88e2-24ab718a6d36",
   "metadata": {},
   "source": [
    "## 6: CODE GENERATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab327e8-43b6-4894-907c-ac8c458fc8e0",
   "metadata": {},
   "source": [
    "### 6.1   BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6ded1202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f54cedcaa3b4e4c84ca7bbd9d320631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trovati 14 file di prompt:\n",
      " - outputs\\prompts\\baseline\\_all_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\baseline_make_prompts_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\templates_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v1_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v2_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v3_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v4_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v5_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v6_2_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v6_3_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v6_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v7_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v8_baseline.jsonl\n",
      " - outputs\\prompts\\baseline\\v9_baseline.jsonl\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\_all_baseline.jsonl | items: 253 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> _all_baseline:   0%|          | 0/253 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   0%|          | 1/253 [00:42<2:58:55, 42.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   1%|          | 2/253 [01:24<2:57:40, 42.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   1%|          | 3/253 [02:07<2:56:12, 42.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   2%|▏         | 4/253 [02:49<2:55:33, 42.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   2%|▏         | 5/253 [03:31<2:54:48, 42.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   2%|▏         | 6/253 [04:14<2:54:49, 42.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   3%|▎         | 7/253 [04:56<2:53:54, 42.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   3%|▎         | 8/253 [05:39<2:53:54, 42.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   4%|▎         | 9/253 [06:21<2:52:42, 42.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   4%|▍         | 10/253 [07:04<2:52:03, 42.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   4%|▍         | 11/253 [07:46<2:50:49, 42.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   5%|▍         | 12/253 [08:28<2:49:44, 42.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   5%|▌         | 13/253 [09:10<2:49:13, 42.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   6%|▌         | 14/253 [09:53<2:48:43, 42.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   6%|▌         | 15/253 [10:35<2:47:44, 42.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   6%|▋         | 16/253 [11:18<2:47:14, 42.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   7%|▋         | 17/253 [12:00<2:46:25, 42.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   7%|▋         | 18/253 [12:42<2:45:45, 42.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   8%|▊         | 19/253 [13:24<2:44:44, 42.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   8%|▊         | 20/253 [14:07<2:44:24, 42.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   8%|▊         | 21/253 [14:49<2:43:58, 42.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   9%|▊         | 22/253 [15:31<2:42:53, 42.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   9%|▉         | 23/253 [16:14<2:42:21, 42.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:   9%|▉         | 24/253 [16:42<2:25:47, 38.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  10%|▉         | 25/253 [17:11<2:14:47, 35.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  10%|█         | 26/253 [17:30<1:55:17, 30.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  11%|█         | 27/253 [18:11<2:06:28, 33.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  11%|█         | 28/253 [18:38<1:58:21, 31.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  11%|█▏        | 29/253 [19:22<2:11:40, 35.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  12%|█▏        | 30/253 [19:49<2:01:56, 32.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  12%|█▏        | 31/253 [20:33<2:13:58, 36.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  13%|█▎        | 32/253 [21:16<2:21:16, 38.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  13%|█▎        | 33/253 [21:51<2:16:03, 37.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  13%|█▎        | 34/253 [22:34<2:22:03, 38.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  14%|█▍        | 35/253 [23:17<2:26:00, 40.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  14%|█▍        | 36/253 [24:00<2:28:58, 41.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  15%|█▍        | 37/253 [24:44<2:30:53, 41.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  15%|█▌        | 38/253 [25:27<2:31:35, 42.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  15%|█▌        | 39/253 [26:11<2:32:12, 42.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  16%|█▌        | 40/253 [26:32<2:08:04, 36.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  16%|█▌        | 41/253 [27:15<2:15:18, 38.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  17%|█▋        | 42/253 [27:35<1:54:54, 32.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  17%|█▋        | 43/253 [28:18<2:05:57, 35.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  17%|█▋        | 44/253 [29:02<2:13:27, 38.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  18%|█▊        | 45/253 [29:45<2:17:50, 39.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  18%|█▊        | 46/253 [30:29<2:21:08, 40.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  19%|█▊        | 47/253 [31:12<2:22:45, 41.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  19%|█▉        | 48/253 [31:55<2:23:28, 41.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  19%|█▉        | 49/253 [32:38<2:23:53, 42.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  20%|█▉        | 50/253 [33:21<2:23:46, 42.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  20%|██        | 51/253 [34:04<2:23:40, 42.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  21%|██        | 52/253 [34:48<2:24:00, 42.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  21%|██        | 53/253 [35:31<2:23:26, 43.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  21%|██▏       | 54/253 [36:15<2:23:34, 43.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  22%|██▏       | 55/253 [36:58<2:22:37, 43.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  22%|██▏       | 56/253 [37:41<2:22:02, 43.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  23%|██▎       | 57/253 [38:24<2:20:56, 43.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  23%|██▎       | 58/253 [39:07<2:19:56, 43.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  23%|██▎       | 59/253 [39:50<2:19:20, 43.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  24%|██▎       | 60/253 [40:33<2:18:48, 43.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  24%|██▍       | 61/253 [41:16<2:17:54, 43.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  25%|██▍       | 62/253 [42:00<2:17:22, 43.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  25%|██▍       | 63/253 [42:43<2:16:29, 43.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  25%|██▌       | 64/253 [43:26<2:15:47, 43.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  26%|██▌       | 65/253 [44:08<2:14:47, 43.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  26%|██▌       | 66/253 [44:52<2:14:27, 43.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  26%|██▋       | 67/253 [45:35<2:13:59, 43.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  27%|██▋       | 68/253 [46:18<2:12:54, 43.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  27%|██▋       | 69/253 [47:01<2:12:23, 43.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  28%|██▊       | 70/253 [47:44<2:11:03, 42.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  28%|██▊       | 71/253 [48:26<2:09:46, 42.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  28%|██▊       | 72/253 [49:09<2:08:43, 42.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  29%|██▉       | 73/253 [49:51<2:07:40, 42.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  29%|██▉       | 74/253 [50:33<2:06:51, 42.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  30%|██▉       | 75/253 [51:16<2:06:34, 42.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  30%|███       | 76/253 [51:59<2:05:43, 42.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  30%|███       | 77/253 [52:42<2:05:29, 42.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  31%|███       | 78/253 [53:25<2:04:27, 42.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  31%|███       | 79/253 [54:07<2:03:45, 42.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  32%|███▏      | 80/253 [54:49<2:02:40, 42.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  32%|███▏      | 81/253 [55:32<2:01:42, 42.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  32%|███▏      | 82/253 [56:09<1:56:32, 40.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  33%|███▎      | 83/253 [56:52<1:57:21, 41.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  33%|███▎      | 84/253 [57:34<1:57:24, 41.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  34%|███▎      | 85/253 [57:45<1:30:48, 32.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  34%|███▍      | 86/253 [58:27<1:38:36, 35.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  34%|███▍      | 87/253 [59:10<1:43:51, 37.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  35%|███▍      | 88/253 [59:52<1:47:04, 38.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  35%|███▌      | 89/253 [1:00:35<1:49:31, 40.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  36%|███▌      | 90/253 [1:01:17<1:51:02, 40.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  36%|███▌      | 91/253 [1:02:00<1:51:29, 41.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  36%|███▋      | 92/253 [1:02:42<1:51:55, 41.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  37%|███▋      | 93/253 [1:03:25<1:52:00, 42.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  37%|███▋      | 94/253 [1:04:07<1:51:42, 42.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  38%|███▊      | 95/253 [1:04:50<1:51:22, 42.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  38%|███▊      | 96/253 [1:05:33<1:50:51, 42.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  38%|███▊      | 97/253 [1:06:15<1:50:24, 42.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  39%|███▊      | 98/253 [1:06:59<1:50:18, 42.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  39%|███▉      | 99/253 [1:07:41<1:49:39, 42.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  40%|███▉      | 100/253 [1:08:25<1:49:29, 42.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  40%|███▉      | 101/253 [1:09:07<1:48:33, 42.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  40%|████      | 102/253 [1:09:50<1:47:54, 42.88s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  41%|████      | 103/253 [1:10:33<1:46:53, 42.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  41%|████      | 104/253 [1:11:15<1:45:55, 42.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  42%|████▏     | 105/253 [1:11:58<1:45:18, 42.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  42%|████▏     | 106/253 [1:12:41<1:44:41, 42.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  42%|████▏     | 107/253 [1:13:23<1:43:48, 42.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  43%|████▎     | 108/253 [1:14:06<1:43:14, 42.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  43%|████▎     | 109/253 [1:14:49<1:42:24, 42.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  43%|████▎     | 110/253 [1:15:32<1:41:45, 42.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  44%|████▍     | 111/253 [1:16:14<1:40:50, 42.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  44%|████▍     | 112/253 [1:16:57<1:40:23, 42.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  45%|████▍     | 113/253 [1:17:40<1:39:54, 42.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  45%|████▌     | 114/253 [1:18:22<1:38:56, 42.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  45%|████▌     | 115/253 [1:19:05<1:38:18, 42.74s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  46%|████▌     | 116/253 [1:19:50<1:38:53, 43.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  46%|████▌     | 117/253 [1:20:07<1:20:22, 35.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  47%|████▋     | 118/253 [1:20:51<1:25:51, 38.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  47%|████▋     | 119/253 [1:21:27<1:23:44, 37.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  47%|████▋     | 120/253 [1:22:12<1:27:46, 39.60s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  48%|████▊     | 121/253 [1:22:57<1:30:50, 41.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  48%|████▊     | 122/253 [1:23:42<1:32:16, 42.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  49%|████▊     | 123/253 [1:24:27<1:33:38, 43.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  49%|████▉     | 124/253 [1:25:12<1:33:42, 43.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  49%|████▉     | 125/253 [1:25:56<1:33:49, 43.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  50%|████▉     | 126/253 [1:26:13<1:15:26, 35.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  50%|█████     | 127/253 [1:26:41<1:09:59, 33.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  51%|█████     | 128/253 [1:27:25<1:16:31, 36.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  51%|█████     | 129/253 [1:27:59<1:14:07, 35.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  51%|█████▏    | 130/253 [1:28:43<1:18:42, 38.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  52%|█████▏    | 131/253 [1:29:28<1:21:59, 40.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  52%|█████▏    | 132/253 [1:30:13<1:23:47, 41.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  53%|█████▎    | 133/253 [1:30:57<1:24:54, 42.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  53%|█████▎    | 134/253 [1:31:16<1:09:51, 35.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  53%|█████▎    | 135/253 [1:32:01<1:15:00, 38.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  54%|█████▍    | 136/253 [1:32:45<1:18:18, 40.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  54%|█████▍    | 137/253 [1:33:30<1:20:02, 41.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  55%|█████▍    | 138/253 [1:34:15<1:21:19, 42.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  55%|█████▍    | 139/253 [1:35:01<1:22:43, 43.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  55%|█████▌    | 140/253 [1:35:47<1:23:21, 44.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  56%|█████▌    | 141/253 [1:36:33<1:23:37, 44.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  56%|█████▌    | 142/253 [1:37:19<1:23:28, 45.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  57%|█████▋    | 143/253 [1:38:05<1:23:18, 45.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  57%|█████▋    | 144/253 [1:38:51<1:23:14, 45.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  57%|█████▋    | 145/253 [1:39:38<1:22:40, 45.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  58%|█████▊    | 146/253 [1:40:25<1:22:27, 46.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  58%|█████▊    | 147/253 [1:41:11<1:21:35, 46.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  58%|█████▊    | 148/253 [1:41:57<1:20:55, 46.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  59%|█████▉    | 149/253 [1:42:43<1:19:55, 46.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  59%|█████▉    | 150/253 [1:43:29<1:19:00, 46.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  60%|█████▉    | 151/253 [1:44:15<1:18:21, 46.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  60%|██████    | 152/253 [1:45:01<1:17:41, 46.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  60%|██████    | 153/253 [1:45:47<1:16:47, 46.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  61%|██████    | 154/253 [1:46:10<1:04:49, 39.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  61%|██████▏   | 155/253 [1:46:57<1:07:28, 41.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  62%|██████▏   | 156/253 [1:47:43<1:09:08, 42.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  62%|██████▏   | 157/253 [1:48:28<1:09:52, 43.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  62%|██████▏   | 158/253 [1:49:15<1:10:28, 44.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  63%|██████▎   | 159/253 [1:50:01<1:10:39, 45.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  63%|██████▎   | 160/253 [1:50:47<1:10:15, 45.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  64%|██████▎   | 161/253 [1:51:34<1:09:56, 45.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  64%|██████▍   | 162/253 [1:52:20<1:09:29, 45.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  64%|██████▍   | 163/253 [1:53:06<1:08:51, 45.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  65%|██████▍   | 164/253 [1:53:52<1:08:14, 46.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  65%|██████▌   | 165/253 [1:54:38<1:07:29, 46.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  66%|██████▌   | 166/253 [1:55:25<1:06:50, 46.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  66%|██████▌   | 167/253 [1:56:11<1:06:25, 46.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  66%|██████▋   | 168/253 [1:56:58<1:05:37, 46.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  67%|██████▋   | 169/253 [1:57:45<1:05:09, 46.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  67%|██████▋   | 170/253 [1:58:31<1:04:14, 46.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  68%|██████▊   | 171/253 [1:59:18<1:03:32, 46.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  68%|██████▊   | 172/253 [2:00:04<1:02:34, 46.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  68%|██████▊   | 173/253 [2:00:50<1:01:40, 46.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  69%|██████▉   | 174/253 [2:01:36<1:00:58, 46.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  69%|██████▉   | 175/253 [2:02:23<1:00:15, 46.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  70%|██████▉   | 176/253 [2:03:09<59:21, 46.25s/it]  The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  70%|██████▉   | 177/253 [2:03:31<49:40, 39.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  70%|███████   | 178/253 [2:04:17<51:36, 41.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  71%|███████   | 179/253 [2:05:04<52:45, 42.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  71%|███████   | 180/253 [2:05:50<53:12, 43.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  72%|███████▏  | 181/253 [2:06:36<53:30, 44.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  72%|███████▏  | 182/253 [2:07:23<53:29, 45.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  72%|███████▏  | 183/253 [2:08:09<53:01, 45.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  73%|███████▎  | 184/253 [2:08:55<52:38, 45.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  73%|███████▎  | 185/253 [2:09:40<51:25, 45.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  74%|███████▎  | 186/253 [2:10:24<50:18, 45.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  74%|███████▍  | 187/253 [2:11:09<49:20, 44.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  74%|███████▍  | 188/253 [2:11:53<48:22, 44.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  75%|███████▍  | 189/253 [2:12:37<47:34, 44.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  75%|███████▌  | 190/253 [2:13:22<46:58, 44.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  75%|███████▌  | 191/253 [2:14:07<46:08, 44.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  76%|███████▌  | 192/253 [2:14:52<45:35, 44.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  76%|███████▋  | 193/253 [2:15:36<44:41, 44.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  77%|███████▋  | 194/253 [2:16:21<43:57, 44.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  77%|███████▋  | 195/253 [2:17:05<43:04, 44.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  77%|███████▋  | 196/253 [2:17:50<42:13, 44.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  78%|███████▊  | 197/253 [2:18:34<41:32, 44.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  78%|███████▊  | 198/253 [2:19:19<40:50, 44.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  79%|███████▊  | 199/253 [2:20:03<40:01, 44.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  79%|███████▉  | 200/253 [2:20:48<39:19, 44.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  79%|███████▉  | 201/253 [2:21:32<38:32, 44.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  80%|███████▉  | 202/253 [2:22:17<37:47, 44.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  80%|████████  | 203/253 [2:23:01<36:58, 44.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  81%|████████  | 204/253 [2:23:46<36:20, 44.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  81%|████████  | 205/253 [2:24:30<35:39, 44.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  81%|████████▏ | 206/253 [2:25:15<34:50, 44.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  82%|████████▏ | 207/253 [2:25:59<34:08, 44.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  82%|████████▏ | 208/253 [2:26:43<33:18, 44.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  83%|████████▎ | 209/253 [2:27:11<28:50, 39.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  83%|████████▎ | 210/253 [2:27:55<29:13, 40.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  83%|████████▎ | 211/253 [2:28:39<29:12, 41.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  84%|████████▍ | 212/253 [2:29:23<29:00, 42.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  84%|████████▍ | 213/253 [2:30:08<28:45, 43.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  85%|████████▍ | 214/253 [2:30:52<28:14, 43.45s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  85%|████████▍ | 215/253 [2:31:37<27:48, 43.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  85%|████████▌ | 216/253 [2:32:21<27:06, 43.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  86%|████████▌ | 217/253 [2:33:06<26:28, 44.12s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  86%|████████▌ | 218/253 [2:33:49<25:41, 44.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  87%|████████▋ | 219/253 [2:34:33<24:56, 44.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  87%|████████▋ | 220/253 [2:35:18<24:14, 44.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  87%|████████▋ | 221/253 [2:36:02<23:33, 44.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  88%|████████▊ | 222/253 [2:36:46<22:46, 44.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  88%|████████▊ | 223/253 [2:37:30<22:04, 44.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  89%|████████▊ | 224/253 [2:38:14<21:19, 44.14s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  89%|████████▉ | 225/253 [2:38:58<20:36, 44.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  89%|████████▉ | 226/253 [2:39:42<19:50, 44.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  90%|████████▉ | 227/253 [2:40:27<19:09, 44.21s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  90%|█████████ | 228/253 [2:41:11<18:27, 44.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  91%|█████████ | 229/253 [2:41:55<17:40, 44.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  91%|█████████ | 230/253 [2:42:40<16:56, 44.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  91%|█████████▏| 231/253 [2:43:24<16:14, 44.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  92%|█████████▏| 232/253 [2:44:08<15:30, 44.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  92%|█████████▏| 233/253 [2:44:53<14:46, 44.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  92%|█████████▏| 234/253 [2:45:37<14:01, 44.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  93%|█████████▎| 235/253 [2:46:21<13:18, 44.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  93%|█████████▎| 236/253 [2:47:07<12:37, 44.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  94%|█████████▎| 237/253 [2:47:51<11:52, 44.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  94%|█████████▍| 238/253 [2:48:36<11:11, 44.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  94%|█████████▍| 239/253 [2:49:21<10:25, 44.68s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  95%|█████████▍| 240/253 [2:50:06<09:41, 44.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  95%|█████████▌| 241/253 [2:50:50<08:54, 44.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  96%|█████████▌| 242/253 [2:51:17<07:11, 39.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  96%|█████████▌| 243/253 [2:52:01<06:48, 40.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  96%|█████████▋| 244/253 [2:52:46<06:17, 41.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  97%|█████████▋| 245/253 [2:53:30<05:41, 42.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  97%|█████████▋| 246/253 [2:54:15<05:02, 43.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  98%|█████████▊| 247/253 [2:54:59<04:21, 43.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  98%|█████████▊| 248/253 [2:55:44<03:39, 43.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  98%|█████████▊| 249/253 [2:56:28<02:55, 43.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  99%|█████████▉| 250/253 [2:57:13<02:12, 44.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline:  99%|█████████▉| 251/253 [2:57:57<01:28, 44.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline: 100%|█████████▉| 252/253 [2:58:42<00:44, 44.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> _all_baseline: 100%|██████████| 253/253 [2:59:26<00:00, 42.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\_all_baseline.jsonl (+253)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\baseline_make_prompts_baseline.jsonl | items: 0 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> baseline_make_prompts_baseline: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\baseline_make_prompts_baseline.jsonl (+0)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\templates_baseline.jsonl | items: 0 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> templates_baseline: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\templates_baseline.jsonl (+0)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v1_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v1_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:   4%|▍         | 1/23 [00:42<15:34, 42.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:   9%|▊         | 2/23 [01:24<14:48, 42.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  13%|█▎        | 3/23 [02:06<14:05, 42.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  17%|█▋        | 4/23 [02:49<13:22, 42.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  22%|██▏       | 5/23 [03:31<12:40, 42.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  26%|██▌       | 6/23 [04:14<12:01, 42.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  30%|███       | 7/23 [04:56<11:18, 42.39s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  35%|███▍      | 8/23 [05:39<10:38, 42.58s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  39%|███▉      | 9/23 [06:21<09:54, 42.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  43%|████▎     | 10/23 [07:04<09:12, 42.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  48%|████▊     | 11/23 [07:46<08:27, 42.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  52%|█████▏    | 12/23 [08:27<07:44, 42.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  57%|█████▋    | 13/23 [09:10<07:02, 42.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  61%|██████    | 14/23 [09:52<06:20, 42.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  65%|██████▌   | 15/23 [10:34<05:37, 42.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  70%|██████▉   | 16/23 [11:16<04:55, 42.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  74%|███████▍  | 17/23 [11:58<04:13, 42.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  78%|███████▊  | 18/23 [12:41<03:30, 42.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  83%|████████▎ | 19/23 [13:23<02:48, 42.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  87%|████████▋ | 20/23 [14:05<02:06, 42.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  91%|█████████▏| 21/23 [14:47<01:24, 42.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline:  96%|█████████▌| 22/23 [15:29<00:42, 42.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v1_baseline: 100%|██████████| 23/23 [16:12<00:00, 42.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v1_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v2_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v2_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:   4%|▍         | 1/23 [00:28<10:23, 28.35s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:   9%|▊         | 2/23 [00:57<10:01, 28.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  13%|█▎        | 3/23 [01:15<08:01, 24.09s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  17%|█▋        | 4/23 [01:56<09:40, 30.57s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  22%|██▏       | 5/23 [02:23<08:44, 29.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  26%|██▌       | 6/23 [03:06<09:39, 34.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  30%|███       | 7/23 [03:33<08:27, 31.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  35%|███▍      | 8/23 [04:17<08:53, 35.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  39%|███▉      | 9/23 [05:00<08:49, 37.85s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  43%|████▎     | 10/23 [05:34<07:56, 36.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  48%|████▊     | 11/23 [06:16<07:41, 38.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  52%|█████▏    | 12/23 [06:59<07:17, 39.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  57%|█████▋    | 13/23 [07:42<06:47, 40.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  61%|██████    | 14/23 [08:25<06:13, 41.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  65%|██████▌   | 15/23 [09:08<05:35, 41.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  70%|██████▉   | 16/23 [09:51<04:55, 42.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  74%|███████▍  | 17/23 [10:12<03:34, 35.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  78%|███████▊  | 18/23 [10:55<03:09, 37.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  83%|████████▎ | 19/23 [11:14<02:09, 32.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  87%|████████▋ | 20/23 [11:57<01:46, 35.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  91%|█████████▏| 21/23 [12:41<01:15, 37.96s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline:  96%|█████████▌| 22/23 [13:24<00:39, 39.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v2_baseline: 100%|██████████| 23/23 [14:07<00:00, 36.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v2_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v3_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v3_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:   4%|▍         | 1/23 [00:42<15:39, 42.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:   9%|▊         | 2/23 [01:25<14:54, 42.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  13%|█▎        | 3/23 [02:07<14:12, 42.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  17%|█▋        | 4/23 [02:50<13:29, 42.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  22%|██▏       | 5/23 [03:33<12:47, 42.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  26%|██▌       | 6/23 [04:16<12:08, 42.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  30%|███       | 7/23 [04:59<11:25, 42.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  35%|███▍      | 8/23 [05:42<10:45, 43.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  39%|███▉      | 9/23 [06:25<10:00, 42.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  43%|████▎     | 10/23 [07:08<09:18, 42.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  48%|████▊     | 11/23 [07:50<08:33, 42.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  52%|█████▏    | 12/23 [08:33<07:49, 42.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  57%|█████▋    | 13/23 [09:16<07:07, 42.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  61%|██████    | 14/23 [09:59<06:25, 42.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  65%|██████▌   | 15/23 [10:41<05:41, 42.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  70%|██████▉   | 16/23 [11:24<04:59, 42.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  74%|███████▍  | 17/23 [12:07<04:16, 42.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  78%|███████▊  | 18/23 [12:49<03:33, 42.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  83%|████████▎ | 19/23 [13:32<02:50, 42.63s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  87%|████████▋ | 20/23 [14:15<02:08, 42.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  91%|█████████▏| 21/23 [14:58<01:25, 42.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline:  96%|█████████▌| 22/23 [15:40<00:42, 42.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v3_baseline: 100%|██████████| 23/23 [16:23<00:00, 42.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v3_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v4_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v4_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:   4%|▍         | 1/23 [00:42<15:26, 42.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:   9%|▊         | 2/23 [01:24<14:42, 42.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  13%|█▎        | 3/23 [02:06<14:00, 42.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  17%|█▋        | 4/23 [02:47<13:17, 41.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  22%|██▏       | 5/23 [03:30<12:36, 42.01s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  26%|██▌       | 6/23 [04:12<11:57, 42.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  30%|███       | 7/23 [04:54<11:14, 42.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  35%|███▍      | 8/23 [05:37<10:35, 42.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  39%|███▉      | 9/23 [06:19<09:51, 42.26s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  43%|████▎     | 10/23 [07:01<09:09, 42.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  48%|████▊     | 11/23 [07:43<08:25, 42.15s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  52%|█████▏    | 12/23 [08:25<07:42, 42.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  57%|█████▋    | 13/23 [09:02<06:44, 40.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  61%|██████    | 14/23 [09:44<06:09, 41.03s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  65%|██████▌   | 15/23 [10:26<05:30, 41.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  70%|██████▉   | 16/23 [10:37<03:44, 32.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  74%|███████▍  | 17/23 [11:19<03:30, 35.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  78%|███████▊  | 18/23 [12:01<03:05, 37.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  83%|████████▎ | 19/23 [12:43<02:34, 38.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  87%|████████▋ | 20/23 [13:25<01:59, 39.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  91%|█████████▏| 21/23 [14:07<01:20, 40.49s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline:  96%|█████████▌| 22/23 [14:49<00:40, 40.90s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v4_baseline: 100%|██████████| 23/23 [15:32<00:00, 40.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v4_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v5_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v5_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:   4%|▍         | 1/23 [00:42<15:31, 42.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:   9%|▊         | 2/23 [01:24<14:46, 42.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  13%|█▎        | 3/23 [02:06<14:04, 42.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  17%|█▋        | 4/23 [02:48<13:21, 42.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  22%|██▏       | 5/23 [03:31<12:40, 42.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  26%|██▌       | 6/23 [04:13<12:01, 42.43s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  30%|███       | 7/23 [04:56<11:18, 42.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  35%|███▍      | 8/23 [05:39<10:39, 42.62s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  39%|███▉      | 9/23 [06:21<09:55, 42.51s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  43%|████▎     | 10/23 [07:04<09:12, 42.52s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  48%|████▊     | 11/23 [07:46<08:28, 42.38s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  52%|█████▏    | 12/23 [08:28<07:45, 42.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  57%|█████▋    | 13/23 [09:10<07:03, 42.33s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  61%|██████    | 14/23 [09:53<06:21, 42.36s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  65%|██████▌   | 15/23 [10:35<05:38, 42.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  70%|██████▉   | 16/23 [11:17<04:56, 42.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  74%|███████▍  | 17/23 [11:59<04:13, 42.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  78%|███████▊  | 18/23 [12:42<03:31, 42.30s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  83%|████████▎ | 19/23 [13:24<02:48, 42.23s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  87%|████████▋ | 20/23 [14:06<02:07, 42.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  91%|█████████▏| 21/23 [14:49<01:24, 42.42s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline:  96%|█████████▌| 22/23 [15:31<00:42, 42.32s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v5_baseline: 100%|██████████| 23/23 [16:14<00:00, 42.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v5_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v6_2_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v6_2_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:   4%|▍         | 1/23 [00:45<16:45, 45.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:   9%|▊         | 2/23 [01:31<15:56, 45.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  13%|█▎        | 3/23 [02:16<15:11, 45.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  17%|█▋        | 4/23 [03:02<14:25, 45.54s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  22%|██▏       | 5/23 [03:47<13:40, 45.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  26%|██▌       | 6/23 [04:34<12:58, 45.82s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  30%|███       | 7/23 [05:19<12:12, 45.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  35%|███▍      | 8/23 [06:06<11:30, 46.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  39%|███▉      | 9/23 [06:52<10:42, 45.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  43%|████▎     | 10/23 [07:38<09:57, 45.93s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  48%|████▊     | 11/23 [08:23<09:09, 45.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  52%|█████▏    | 12/23 [09:08<08:22, 45.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  57%|█████▋    | 13/23 [09:54<07:37, 45.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  61%|██████    | 14/23 [10:40<06:51, 45.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  65%|██████▌   | 15/23 [11:26<06:05, 45.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  70%|██████▉   | 16/23 [11:49<04:32, 38.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  74%|███████▍  | 17/23 [12:34<04:05, 40.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  78%|███████▊  | 18/23 [13:20<03:31, 42.37s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  83%|████████▎ | 19/23 [14:05<02:53, 43.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  87%|████████▋ | 20/23 [14:52<02:12, 44.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  91%|█████████▏| 21/23 [15:38<01:29, 44.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline:  96%|█████████▌| 22/23 [16:23<00:44, 44.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_2_baseline: 100%|██████████| 23/23 [17:09<00:00, 44.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v6_2_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v6_3_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v6_3_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:   4%|▍         | 1/23 [00:45<16:49, 45.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:   9%|▊         | 2/23 [01:31<16:00, 45.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  13%|█▎        | 3/23 [02:17<15:15, 45.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  17%|█▋        | 4/23 [03:02<14:28, 45.72s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  22%|██▏       | 5/23 [03:48<13:43, 45.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  26%|██▌       | 6/23 [04:35<13:01, 45.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  30%|███       | 7/23 [05:21<12:15, 45.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  35%|███▍      | 8/23 [06:07<11:32, 46.18s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  39%|███▉      | 9/23 [06:53<10:44, 46.06s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  43%|████▎     | 10/23 [07:39<09:59, 46.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  48%|████▊     | 11/23 [08:25<09:11, 45.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  52%|█████▏    | 12/23 [09:10<08:24, 45.83s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  57%|█████▋    | 13/23 [09:56<07:38, 45.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  61%|██████    | 14/23 [10:42<06:53, 45.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  65%|██████▌   | 15/23 [11:28<06:06, 45.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  70%|██████▉   | 16/23 [11:51<04:31, 38.84s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  74%|███████▍  | 17/23 [12:36<04:05, 40.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  78%|███████▊  | 18/23 [13:22<03:32, 42.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  83%|████████▎ | 19/23 [14:08<02:53, 43.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  87%|████████▋ | 20/23 [14:54<02:12, 44.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  91%|█████████▏| 21/23 [15:40<01:29, 44.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline:  96%|█████████▌| 22/23 [16:26<00:45, 45.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_3_baseline: 100%|██████████| 23/23 [17:12<00:00, 44.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v6_3_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v6_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v6_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:   4%|▍         | 1/23 [00:44<16:13, 44.25s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:   9%|▊         | 2/23 [01:01<09:52, 28.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  13%|█▎        | 3/23 [01:45<11:49, 35.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  17%|█▋        | 4/23 [02:21<11:15, 35.55s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  22%|██▏       | 5/23 [03:05<11:35, 38.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  26%|██▌       | 6/23 [03:49<11:32, 40.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  30%|███       | 7/23 [04:34<11:09, 41.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  35%|███▍      | 8/23 [05:19<10:43, 42.87s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  39%|███▉      | 9/23 [06:03<10:05, 43.24s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  43%|████▎     | 10/23 [06:47<09:27, 43.64s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  48%|████▊     | 11/23 [07:03<07:02, 35.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  52%|█████▏    | 12/23 [07:31<06:01, 32.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  57%|█████▋    | 13/23 [08:15<06:03, 36.34s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  61%|██████    | 14/23 [08:49<05:19, 35.50s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  65%|██████▌   | 15/23 [09:33<05:04, 38.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  70%|██████▉   | 16/23 [10:17<04:39, 39.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  74%|███████▍  | 17/23 [11:01<04:07, 41.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  78%|███████▊  | 18/23 [11:45<03:30, 42.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  83%|████████▎ | 19/23 [12:04<02:19, 34.91s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  87%|████████▋ | 20/23 [12:48<01:53, 37.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  91%|█████████▏| 21/23 [13:33<01:19, 39.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline:  96%|█████████▌| 22/23 [14:16<00:41, 41.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v6_baseline: 100%|██████████| 23/23 [15:01<00:00, 39.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v6_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v7_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v7_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:   4%|▍         | 1/23 [00:44<16:09, 44.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:   9%|▊         | 2/23 [01:27<15:23, 43.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  13%|█▎        | 3/23 [02:11<14:39, 43.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  17%|█▋        | 4/23 [02:55<13:54, 43.92s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  22%|██▏       | 5/23 [03:39<13:11, 43.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  26%|██▌       | 6/23 [04:24<12:31, 44.20s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  30%|███       | 7/23 [05:08<11:46, 44.17s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  35%|███▍      | 8/23 [05:53<11:05, 44.40s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  39%|███▉      | 9/23 [06:37<10:19, 44.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  43%|████▎     | 10/23 [07:21<09:35, 44.27s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  48%|████▊     | 11/23 [08:05<08:49, 44.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  52%|█████▏    | 12/23 [08:49<08:04, 44.02s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  57%|█████▋    | 13/23 [09:33<07:20, 44.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  61%|██████    | 14/23 [10:17<06:37, 44.13s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  65%|██████▌   | 15/23 [11:01<05:52, 44.04s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  70%|██████▉   | 16/23 [11:45<05:08, 44.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  74%|███████▍  | 17/23 [12:29<04:24, 44.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  78%|███████▊  | 18/23 [13:13<03:40, 44.07s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  83%|████████▎ | 19/23 [13:57<02:55, 43.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  87%|████████▋ | 20/23 [14:42<02:12, 44.11s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  91%|█████████▏| 21/23 [15:26<01:28, 44.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline:  96%|█████████▌| 22/23 [16:10<00:44, 44.08s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v7_baseline: 100%|██████████| 23/23 [16:54<00:00, 44.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v7_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v8_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v8_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:   4%|▍         | 1/23 [00:43<16:03, 43.78s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:   9%|▊         | 2/23 [01:11<11:55, 34.05s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  13%|█▎        | 3/23 [01:54<12:49, 38.46s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  17%|█▋        | 4/23 [02:38<12:48, 40.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  22%|██▏       | 5/23 [03:22<12:29, 41.66s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  26%|██▌       | 6/23 [04:06<12:03, 42.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  30%|███       | 7/23 [04:50<11:27, 42.97s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  35%|███▍      | 8/23 [05:34<10:52, 43.48s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  39%|███▉      | 9/23 [06:18<10:09, 43.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  43%|████▎     | 10/23 [07:02<09:28, 43.71s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  48%|████▊     | 11/23 [07:46<08:43, 43.65s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  52%|█████▏    | 12/23 [08:29<07:59, 43.61s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  57%|█████▋    | 13/23 [09:13<07:16, 43.69s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  61%|██████    | 14/23 [09:57<06:33, 43.77s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  65%|██████▌   | 15/23 [10:40<05:49, 43.70s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  70%|██████▉   | 16/23 [11:24<05:06, 43.76s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  74%|███████▍  | 17/23 [12:08<04:22, 43.73s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  78%|███████▊  | 18/23 [12:52<03:38, 43.75s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  83%|████████▎ | 19/23 [13:35<02:54, 43.67s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  87%|████████▋ | 20/23 [14:19<02:11, 43.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  91%|█████████▏| 21/23 [15:03<01:27, 43.89s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline:  96%|█████████▌| 22/23 [15:47<00:43, 43.79s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v8_baseline: 100%|██████████| 23/23 [16:31<00:00, 43.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v8_baseline.jsonl (+23)\n",
      "\n",
      "[CODEGEN] file: outputs\\prompts\\baseline\\v9_baseline.jsonl | items: 23 | resume hits: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating -> v9_baseline:   0%|          | 0/23 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:   4%|▍         | 1/23 [00:44<16:10, 44.10s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:   9%|▊         | 2/23 [01:28<15:23, 43.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  13%|█▎        | 3/23 [02:12<14:40, 44.00s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  17%|█▋        | 4/23 [02:55<13:54, 43.95s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  22%|██▏       | 5/23 [03:39<13:11, 43.99s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  26%|██▌       | 6/23 [04:24<12:31, 44.22s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  30%|███       | 7/23 [05:08<11:46, 44.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  35%|███▍      | 8/23 [05:53<11:06, 44.41s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  39%|███▉      | 9/23 [06:37<10:20, 44.29s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  43%|████▎     | 10/23 [07:21<09:35, 44.31s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  48%|████▊     | 11/23 [08:05<08:49, 44.16s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  52%|█████▏    | 12/23 [08:32<07:06, 38.80s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  57%|█████▋    | 13/23 [09:16<06:44, 40.44s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  61%|██████    | 14/23 [10:00<06:14, 41.59s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  65%|██████▌   | 15/23 [10:44<05:38, 42.28s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  70%|██████▉   | 16/23 [11:28<05:00, 42.86s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  74%|███████▍  | 17/23 [12:12<04:19, 43.19s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  78%|███████▊  | 18/23 [12:56<03:37, 43.47s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  83%|████████▎ | 19/23 [13:40<02:54, 43.56s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  87%|████████▋ | 20/23 [14:25<02:11, 43.81s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  91%|█████████▏| 21/23 [15:09<01:27, 43.98s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline:  96%|█████████▌| 22/23 [15:53<00:43, 43.94s/it]The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Generating -> v9_baseline: 100%|██████████| 23/23 [16:37<00:00, 43.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] -> outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v9_baseline.jsonl (+23)\n",
      "\n",
      "Output per-prompt aggiornati:\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\_all_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\baseline_make_prompts_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\templates_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v1_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v2_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v3_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v4_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v5_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v6_2_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v6_3_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v6_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v7_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v8_baseline.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\v9_baseline.jsonl\n",
      "\n",
      "Aggregato globale:\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\all_generations.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Code generation per tutti i prompt disponibili ===\n",
    "from pathlib import Path\n",
    "from models.loader import load_model_and_tokenizer\n",
    "from models.config import load_config\n",
    "\n",
    "# 1) Carica il modello locale (come facevi tu)\n",
    "cfg = load_config()\n",
    "cfg.model.model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "cfg.cache.root = \"./cache\"\n",
    "tokenizer, model, used_cache = load_model_and_tokenizer(cfg)\n",
    "\n",
    "# 2) Scansiona i file di prompt (al momento solo baseline)\n",
    "from codegen import scan_prompt_files, run_codegen_over_prompt_files\n",
    "\n",
    "PROMPT_DIRS = [\n",
    "    Path(\"outputs/prompts/baseline\"),\n",
    "    # in futuro aggiungi qui:\n",
    "    # Path(\"outputs/prompts/bm25\"),\n",
    "    # Path(\"outputs/prompts/cosine\"),\n",
    "    # Path(\"outputs/prompts/hybrid\"),\n",
    "    # Path(\"outputs/prompts/multihop_decomposition_first\"),\n",
    "    # Path(\"outputs/prompts/multihop_iterative_refine\"),\n",
    "]\n",
    "\n",
    "prompt_files = scan_prompt_files(PROMPT_DIRS)\n",
    "print(f\"Trovati {len(prompt_files)} file di prompt:\")\n",
    "for p in prompt_files: print(\" -\", p)\n",
    "\n",
    "# 3) Parametri di generazione (tunabili)\n",
    "GEN_CFG = dict(\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    do_sample=False,  # deterministico per confronti\n",
    ")\n",
    "\n",
    "# 4) Esegui codegen con resume (non rigenera se già presente)\n",
    "written = run_codegen_over_prompt_files(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    prompt_files=prompt_files,\n",
    "    out_root=Path(\"outputs/codegen\"),\n",
    "    model_name=cfg.model.model_name,\n",
    "    **GEN_CFG\n",
    ")\n",
    "\n",
    "print(\"\\nOutput per-prompt aggiornati:\")\n",
    "for w in written: print(\" -\", w.resolve())\n",
    "\n",
    "print(\"\\nAggregato globale:\")\n",
    "from codegen.io_utils import result_paths\n",
    "if prompt_files:\n",
    "    _, agg = result_paths(Path(\"outputs/codegen\"), cfg.model.model_name, prompt_files[0])\n",
    "    print(\" -\", agg.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaadda2-c0b9-463a-81b1-85eef167569b",
   "metadata": {},
   "source": [
    "### -> 6.2 RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9f176",
   "metadata": {},
   "source": [
    "#### 6.2.1  RAG WITH BM25 - RISULTATI PRONTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "58210f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2281d8446f4d048ff9f01e0b18e82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_bm25_top3] prompt letti: 253\n",
      "[rag_bm25_top3] resume: 0 già presenti (aggregato)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b312df173fa4436196459de9c99c5c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rag_bm25_top3:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_bm25_top3] completato in 4519.6s | nuove generazioni: 253\n",
      "Risultati: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_bm25_top3\n"
     ]
    }
   ],
   "source": [
    "# === CODEGEN SOLO per RAG BM25 top-3 — OOM-safe + resume + progress (clonato da COSINE) ===\n",
    "import os, gc, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models.loader import load_model_and_tokenizer\n",
    "from models.config import load_config\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Carica modello locale\n",
    "# -----------------------------\n",
    "cfg = load_config()\n",
    "cfg.model.model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "cfg.cache.root = \"./cache\"\n",
    "tokenizer, model, used_cache = load_model_and_tokenizer(cfg)\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Utility I/O\n",
    "# -----------------------------\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def append_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    if not rows: return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_done_keys(path: Path) -> set:\n",
    "    done = set()\n",
    "    if path.exists():\n",
    "        for obj in read_jsonl(path):\n",
    "            done.add( (str(obj.get(\"query_id\")), str(obj.get(\"template\")), str(obj.get(\"variant\"))) )\n",
    "    return done\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Config generazione + guard-rails (identico a COSINE)\n",
    "# -----------------------------\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=200,     # identico al runner COSINE\n",
    "    do_sample=False,        # deterministico come COSINE\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_time=30.0,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "MODEL_CTX = getattr(model.config, \"max_position_embeddings\", None) or getattr(tokenizer, \"model_max_length\", 2048)\n",
    "MAX_INPUT_TOKENS = min(1024, max(512, int(MODEL_CTX - GEN_KW[\"max_new_tokens\"] - 64)))\n",
    "\n",
    "def truncate_prompt_tokens(prompt: str) -> str:\n",
    "    ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(ids) <= MAX_INPUT_TOKENS:\n",
    "        return prompt\n",
    "    ids = ids[-MAX_INPUT_TOKENS:]  # tieni la coda come nel COSINE\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def generate_safe(prompt: str) -> Tuple[str, Dict[str, Any]]:\n",
    "    stats = {\"attempts\": 0, \"oom_retries\": 0, \"used_max_new_tokens\": GEN_KW[\"max_new_tokens\"]}\n",
    "    prompt_use = truncate_prompt_tokens(prompt)\n",
    "    kw = dict(GEN_KW)\n",
    "    for _ in range(3):\n",
    "        stats[\"attempts\"] += 1\n",
    "        try:\n",
    "            inputs = tokenizer(prompt_use, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                out_ids = model.generate(**inputs, **kw)\n",
    "            text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "            if text.startswith(prompt_use):\n",
    "                text = text[len(prompt_use):].lstrip()\n",
    "            del inputs, out_ids\n",
    "            return text, stats\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            stats[\"oom_retries\"] += 1\n",
    "            kw[\"max_new_tokens\"] = max(128, int(kw[\"max_new_tokens\"] * 0.8))\n",
    "            stats[\"used_max_new_tokens\"] = kw[\"max_new_tokens\"]\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                stats[\"oom_retries\"] += 1\n",
    "                kw[\"max_new_tokens\"] = max(128, int(kw[\"max_new_tokens\"] * 0.8))\n",
    "                stats[\"used_max_new_tokens\"] = kw[\"max_new_tokens\"]\n",
    "                torch.cuda.empty_cache(); gc.collect()\n",
    "                continue\n",
    "            raise\n",
    "    # fallback compatto (come COSINE)\n",
    "    short_prompt = prompt_use[-800:]\n",
    "    inputs = tokenizer(short_prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(**inputs, **{**GEN_KW, \"max_new_tokens\": 160, \"max_time\": 25.0})\n",
    "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    if text.startswith(short_prompt):\n",
    "        text = text[len(short_prompt):].lstrip()\n",
    "    del inputs, out_ids\n",
    "    return text, {**stats, \"used_max_new_tokens\": 160, \"fallback\": True}\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Sorgente RAG BM25 e cartelle output\n",
    "# -----------------------------\n",
    "BM25_RAG_FILE = Path(\n",
    "    r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_bm25_top3\\RAG_rag_bm25_top3.jsonl\"\n",
    ")\n",
    "if not BM25_RAG_FILE.exists():\n",
    "    raise FileNotFoundError(f\"File prompt BM25 non trovato: {BM25_RAG_FILE}\")\n",
    "\n",
    "TAG = \"rag_bm25_top3\"\n",
    "RESULTS_ROOT = Path(\"results/rag_generations\")\n",
    "OUT_DIR = RESULTS_ROOT / TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGG_OUT = OUT_DIR / \"AGGREGATED_results.jsonl\"\n",
    "\n",
    "rows = read_jsonl(BM25_RAG_FILE)\n",
    "print(f\"[{TAG}] prompt letti: {len(rows)}\")\n",
    "\n",
    "# Resume\n",
    "done_agg = load_done_keys(AGG_OUT)\n",
    "print(f\"[{TAG}] resume: {len(done_agg)} già presenti (aggregato)\")\n",
    "\n",
    "templates = sorted({r.get(\"template\", \"unknown\") for r in rows})\n",
    "per_template_targets: Dict[str, Path] = {t: OUT_DIR / f\"{t}_results.jsonl\" for t in templates}\n",
    "per_template_done: Dict[str, set] = {t: load_done_keys(p) for t, p in per_template_targets.items()}\n",
    "per_template_buffers: Dict[str, List[Dict[str, Any]]] = {t: [] for t in templates}\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Loop di generazione (BM25 clonato)\n",
    "# -----------------------------\n",
    "total_new = 0\n",
    "new_agg_rows: List[Dict[str, Any]] = []\n",
    "t0 = time.time()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pbar = tqdm(rows, desc=TAG, dynamic_ncols=True)\n",
    "for r in pbar:\n",
    "    qid = str(r.get(\"query_id\"))\n",
    "    templ = str(r.get(\"template\"))\n",
    "    variant = str(r.get(\"variant\") or TAG)  # \"rag_bm25_top3\"\n",
    "    prompt = r.get(\"prompt\") or \"\"\n",
    "\n",
    "    key = (qid, templ, variant)\n",
    "    if key in done_agg or key in per_template_done.get(templ, set()):\n",
    "        continue\n",
    "\n",
    "    if not r.get(\"snippets\"):\n",
    "        print(f\"[WARN] {templ} qid={qid}: campo 'snippets' mancante/vuoto nel prompt BM25.\")\n",
    "\n",
    "    gen_text, stats = generate_safe(prompt)\n",
    "\n",
    "    row_out = {\n",
    "        \"query_id\": qid,\n",
    "        \"repo_name\": r.get(\"repo_name\"),\n",
    "        \"instruction\": r.get(\"instruction\"),\n",
    "        \"template\": templ,\n",
    "        \"variant\": variant,                 # \"rag_bm25_top3\"\n",
    "        \"retrieval_method\": r.get(\"retrieval_method\"),\n",
    "        \"k_snippets\": r.get(\"k_snippets\"),\n",
    "        \"snippets\": r.get(\"snippets\"),\n",
    "        \"prompt\": prompt,\n",
    "        \"generation\": gen_text,\n",
    "        \"gen_stats\": stats,\n",
    "        \"model_name\": cfg.model.model_name,\n",
    "    }\n",
    "\n",
    "    per_template_buffers[templ].append(row_out)\n",
    "    new_agg_rows.append(row_out)\n",
    "    total_new += 1\n",
    "\n",
    "    try:\n",
    "        alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        pbar.set_postfix(done=total_new, mem=f\"{alloc:.2f}G/{reserved:.2f}G\")\n",
    "    except Exception:\n",
    "        pbar.set_postfix(done=total_new)\n",
    "\n",
    "    if sum(len(v) for v in per_template_buffers.values()) >= 8:\n",
    "        for tname, buf in per_template_buffers.items():\n",
    "            if buf:\n",
    "                append_jsonl(per_template_targets[tname], buf)\n",
    "                per_template_buffers[tname].clear()\n",
    "        if new_agg_rows:\n",
    "            append_jsonl(AGG_OUT, new_agg_rows)\n",
    "            new_agg_rows.clear()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Flush finale\n",
    "for tname, buf in per_template_buffers.items():\n",
    "    if buf:\n",
    "        append_jsonl(per_template_targets[tname], buf)\n",
    "        per_template_buffers[tname].clear()\n",
    "\n",
    "if new_agg_rows:\n",
    "    append_jsonl(AGG_OUT, new_agg_rows)\n",
    "    new_agg_rows.clear()\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"[{TAG}] completato in {dt:.1f}s | nuove generazioni: {total_new}\")\n",
    "print(\"Risultati:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3efc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1f248c5de64468bb87a12a95aea727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_bm25_top3] prompt letti: 253\n",
      "[rag_bm25_top3] resume: 253 già presenti (aggregato)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a45071892f754a87ba8327a23e07e73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rag_bm25_top3:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_bm25_top3] completato in 0.0s | nuove generazioni: 0\n",
      "Risultati: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_bm25_top3\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# === CODEGEN RAG (SOLO BM25) con resume, OOM-safe e progress bar ===\n",
    "import os, gc, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 1) Carica il modello locale (stessa logica della baseline)\n",
    "from models.loader import load_model_and_tokenizer\n",
    "from models.config import load_config\n",
    "\n",
    "cfg = load_config()\n",
    "cfg.model.model_name = \"codellama/CodeLlama-7b-Instruct-hf\"  # cambia qui se vuoi altro modello\n",
    "cfg.cache.root = \"./cache\"\n",
    "tokenizer, model, used_cache = load_model_and_tokenizer(cfg)\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "# 2) Parametri di generazione (sampling on per evitare warning \"temperature/top_p ignorati\")\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=384,   # riduci a 256 se vuoi più veloce/meno OOM\n",
    "    temperature=0.2,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# 3) Utility I/O\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def append_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    if not rows: return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_done_keys(path: Path) -> set:\n",
    "    \"\"\"\n",
    "    Chiave di resume: (query_id, template, variant).\n",
    "    \"\"\"\n",
    "    done = set()\n",
    "    if path.exists():\n",
    "        try:\n",
    "            for obj in read_jsonl(path):\n",
    "                done.add((str(obj.get(\"query_id\")), str(obj.get(\"template\")), str(obj.get(\"variant\"))))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return done\n",
    "\n",
    "# 4) Limiti contesto e troncamento input\n",
    "MODEL_CTX = getattr(model.config, \"max_position_embeddings\", None) or getattr(tokenizer, \"model_max_length\", 2048)\n",
    "MAX_INPUT_TOKENS = max(256, int(MODEL_CTX - GEN_KW[\"max_new_tokens\"] - 32))\n",
    "\n",
    "def truncate_input_for_ctx(prompt: str, max_input_tokens: int) -> str:\n",
    "    ids = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    if len(ids) <= max_input_tokens:\n",
    "        return prompt\n",
    "    keep_head = max(64, int(max_input_tokens * 0.25))\n",
    "    keep_tail = max_input_tokens - keep_head\n",
    "    ids_new = ids[:keep_head] + ids[-keep_tail:]\n",
    "    return tokenizer.decode(ids_new, skip_special_tokens=True)\n",
    "\n",
    "def generate_safe(prompt: str, gen_kw: Dict[str, Any]) -> Tuple[str, Dict[str, Any]]:\n",
    "    stats = {\"attempts\": 0, \"used_max_new_tokens\": gen_kw.get(\"max_new_tokens\"), \"oom_retries\": 0}\n",
    "    prompt_trim = truncate_input_for_ctx(prompt, MAX_INPUT_TOKENS)\n",
    "    kw = dict(gen_kw)\n",
    "    for _ in range(3):\n",
    "        stats[\"attempts\"] += 1\n",
    "        try:\n",
    "            inputs = tokenizer(prompt_trim, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                out_ids = model.generate(**inputs, **kw)\n",
    "            text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "            # taglia l'eco dell'input se presente\n",
    "            if text.startswith(prompt_trim):\n",
    "                text = text[len(prompt_trim):].lstrip()\n",
    "            return text, stats\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            stats[\"oom_retries\"] += 1\n",
    "            kw[\"max_new_tokens\"] = max(128, int(kw[\"max_new_tokens\"] * 0.6))\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                stats[\"oom_retries\"] += 1\n",
    "                kw[\"max_new_tokens\"] = max(128, int(kw[\"max_new_tokens\"] * 0.6))\n",
    "                torch.cuda.empty_cache(); gc.collect()\n",
    "                continue\n",
    "            raise\n",
    "    return \"\", stats\n",
    "\n",
    "# 5) Solo BM25: sorgente e destinazioni\n",
    "BM25_PROMPTS = Path(\"outputs/prompts/rag_bm25_top3/RAG_rag_bm25_top3.jsonl\")\n",
    "if not BM25_PROMPTS.exists():\n",
    "    raise FileNotFoundError(f\"File prompt BM25 non trovato: {BM25_PROMPTS.resolve()}\")\n",
    "\n",
    "RESULTS_ROOT = Path(\"results/rag_generations\")\n",
    "OUT_DIR = RESULTS_ROOT / \"rag_bm25_top3\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGG_OUT = OUT_DIR / \"AGGREGATED_results.jsonl\"\n",
    "\n",
    "# 6) Carica prompt e prepara resume\n",
    "rows = read_jsonl(BM25_PROMPTS)\n",
    "done_agg = load_done_keys(AGG_OUT)\n",
    "print(f\"[rag_bm25_top3] prompt letti: {len(rows)}\")\n",
    "print(f\"[rag_bm25_top3] resume: {len(done_agg)} già presenti (aggregato)\")\n",
    "\n",
    "templates = sorted({str(r.get(\"template\", \"unknown\")) for r in rows})\n",
    "per_template_targets = {t: OUT_DIR / f\"{t}_results.jsonl\" for t in templates}\n",
    "per_template_done = {t: load_done_keys(per_template_targets[t]) for t in templates}\n",
    "buffers = {t: [] for t in templates}\n",
    "new_agg: List[Dict[str, Any]] = []\n",
    "\n",
    "# 7) Loop di generazione\n",
    "local_count = 0\n",
    "t0 = time.time()\n",
    "pbar = tqdm(rows, total=len(rows), desc=\"rag_bm25_top3\", dynamic_ncols=True)\n",
    "\n",
    "for r in pbar:\n",
    "    qid = str(r.get(\"query_id\"))\n",
    "    templ = str(r.get(\"template\"))\n",
    "    variant = str(r.get(\"variant\", \"rag_bm25_top3\"))\n",
    "    prompt = r.get(\"prompt\") or \"\"\n",
    "    key = (qid, templ, variant)\n",
    "\n",
    "    if key in done_agg or key in per_template_done.get(templ, set()):\n",
    "        pbar.set_postfix(done=local_count); continue\n",
    "\n",
    "    gen_text, stats = generate_safe(prompt, GEN_KW)\n",
    "\n",
    "    out_row = {\n",
    "        \"query_id\": qid,\n",
    "        \"repo_name\": r.get(\"repo_name\"),\n",
    "        \"instruction\": r.get(\"instruction\"),\n",
    "        \"template\": templ,\n",
    "        \"variant\": variant,\n",
    "        \"retrieval_method\": r.get(\"retrieval_method\"),\n",
    "        \"k_snippets\": r.get(\"k_snippets\"),\n",
    "        \"snippets\": r.get(\"snippets\"),\n",
    "        \"prompt\": prompt,\n",
    "        \"generation\": gen_text,\n",
    "        \"gen_stats\": stats,\n",
    "        \"model_name\": cfg.model.model_name,\n",
    "    }\n",
    "\n",
    "    buffers[templ].append(out_row)\n",
    "    new_agg.append(out_row)\n",
    "    local_count += 1\n",
    "\n",
    "    # feedback memoria GPU (se disponibile)\n",
    "    try:\n",
    "        alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        pbar.set_postfix(done=local_count, mem=f\"{alloc:.2f}G/{reserved:.2f}G\")\n",
    "    except Exception:\n",
    "        pbar.set_postfix(done=local_count)\n",
    "\n",
    "    # flush periodico per non accumulare in RAM\n",
    "    if sum(len(v) for v in buffers.values()) >= 16:\n",
    "        for tname, buf in buffers.items():\n",
    "            if buf:\n",
    "                append_jsonl(per_template_targets[tname], buf)\n",
    "                buf.clear()\n",
    "        if new_agg:\n",
    "            append_jsonl(AGG_OUT, new_agg); new_agg.clear()\n",
    "        torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# flush finale\n",
    "for tname, buf in buffers.items():\n",
    "    if buf:\n",
    "        append_jsonl(per_template_targets[tname], buf)\n",
    "if new_agg:\n",
    "    append_jsonl(AGG_OUT, new_agg)\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"[rag_bm25_top3] completato in {dt:.1f}s | nuove generazioni: {local_count}\")\n",
    "print(\"Risultati:\", OUT_DIR.resolve())\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba39c7",
   "metadata": {},
   "source": [
    "#### 6.2.2  RAG WITH COSINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "627b6e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ce3c3baf474099a0717d742b0072a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_cosine_top3] prompt letti: 253\n",
      "[rag_cosine_top3] resume: 8 già presenti (aggregato)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fce86f5412746da89c6016444d34915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rag_cosine_top3:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_cosine_top3] completato in 7890.2s | nuove generazioni: 245\n",
      "Risultati: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_cosine_top3\n"
     ]
    }
   ],
   "source": [
    "# === CODEGEN SOLO per RAG COSINE top-3 — OOM-safe + resume + progress ===\n",
    "import os, gc, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models.loader import load_model_and_tokenizer\n",
    "from models.config import load_config\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Carica modello locale\n",
    "# -----------------------------\n",
    "cfg = load_config()\n",
    "cfg.model.model_name = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "cfg.cache.root = \"./cache\"\n",
    "tokenizer, model, used_cache = load_model_and_tokenizer(cfg)\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Utility I/O\n",
    "# -----------------------------\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if not line: \n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def append_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    if not rows: return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_done_keys(path: Path) -> set:\n",
    "    done = set()\n",
    "    if path.exists():\n",
    "        for obj in read_jsonl(path):\n",
    "            done.add( (str(obj.get(\"query_id\")), str(obj.get(\"template\")), str(obj.get(\"variant\"))) )\n",
    "    return done\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Config generazione + guard-rails\n",
    "# -----------------------------\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=200,     # ridotto per velocità/VRAM\n",
    "    do_sample=False,        # deterministico\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_time=30.0,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "MODEL_CTX = getattr(model.config, \"max_position_embeddings\", None) or getattr(tokenizer, \"model_max_length\", 2048)\n",
    "MAX_INPUT_TOKENS = min(1024, max(512, int(MODEL_CTX - GEN_KW[\"max_new_tokens\"] - 64)))\n",
    "\n",
    "def truncate_prompt_tokens(prompt: str) -> str:\n",
    "    ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(ids) <= MAX_INPUT_TOKENS:\n",
    "        return prompt\n",
    "    ids = ids[-MAX_INPUT_TOKENS:]  # tieni la coda (snippet)\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def generate_safe(prompt: str) -> Tuple[str, Dict[str, Any]]:\n",
    "    stats = {\"attempts\": 0, \"oom_retries\": 0, \"used_max_new_tokens\": GEN_KW[\"max_new_tokens\"]}\n",
    "    prompt_use = truncate_prompt_tokens(prompt)\n",
    "    kw = dict(GEN_KW)\n",
    "    for _ in range(3):\n",
    "        stats[\"attempts\"] += 1\n",
    "        try:\n",
    "            inputs = tokenizer(prompt_use, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                out_ids = model.generate(**inputs, **kw)\n",
    "            text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "            if text.startswith(prompt_use):\n",
    "                text = text[len(prompt_use):].lstrip()\n",
    "            del inputs, out_ids\n",
    "            return text, stats\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            stats[\"oom_retries\"] += 1\n",
    "            kw[\"max_new_tokens\"] = max(128, int(kw[\"max_new_tokens\"] * 0.8))\n",
    "            stats[\"used_max_new_tokens\"] = kw[\"max_new_tokens\"]\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                stats[\"oom_retries\"] += 1\n",
    "                kw[\"max_new_tokens\"] = max(128, int(kw[\"max_new_tokens\"] * 0.8))\n",
    "                stats[\"used_max_new_tokens\"] = kw[\"max_new_tokens\"]\n",
    "                torch.cuda.empty_cache(); gc.collect()\n",
    "                continue\n",
    "            raise\n",
    "    # fallback compatto\n",
    "    short_prompt = prompt_use[-800:]\n",
    "    inputs = tokenizer(short_prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(**inputs, **{**GEN_KW, \"max_new_tokens\": 160, \"max_time\": 25.0})\n",
    "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    if text.startswith(short_prompt):\n",
    "        text = text[len(short_prompt):].lstrip()\n",
    "    del inputs, out_ids\n",
    "    return text, {**stats, \"used_max_new_tokens\": 160, \"fallback\": True}\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Sorgente RAG COSINE e cartelle output\n",
    "# -----------------------------\n",
    "COSINE_RAG_FILE = Path(\n",
    "    r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_cosine_top3\\RAG_rag_cosine_top3.jsonl\"\n",
    ")\n",
    "if not COSINE_RAG_FILE.exists():\n",
    "    raise FileNotFoundError(f\"File prompt COSINE non trovato: {COSINE_RAG_FILE}\")\n",
    "\n",
    "TAG = \"rag_cosine_top3\"\n",
    "RESULTS_ROOT = Path(\"results/rag_generations\")\n",
    "OUT_DIR = RESULTS_ROOT / TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGG_OUT = OUT_DIR / \"AGGREGATED_results.jsonl\"\n",
    "\n",
    "rows = read_jsonl(COSINE_RAG_FILE)\n",
    "print(f\"[{TAG}] prompt letti: {len(rows)}\")\n",
    "\n",
    "# Resume\n",
    "done_agg = load_done_keys(AGG_OUT)\n",
    "print(f\"[{TAG}] resume: {len(done_agg)} già presenti (aggregato)\")\n",
    "\n",
    "templates = sorted({r.get(\"template\", \"unknown\") for r in rows})\n",
    "per_template_targets: Dict[str, Path] = {t: OUT_DIR / f\"{t}_results.jsonl\" for t in templates}\n",
    "per_template_done: Dict[str, set] = {t: load_done_keys(p) for t, p in per_template_targets.items()}\n",
    "per_template_buffers: Dict[str, List[Dict[str, Any]]] = {t: [] for t in templates}\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Loop di generazione (solo COSINE)\n",
    "# -----------------------------\n",
    "total_new = 0\n",
    "new_agg_rows: List[Dict[str, Any]] = []\n",
    "t0 = time.time()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pbar = tqdm(rows, desc=TAG, dynamic_ncols=True)\n",
    "for r in pbar:\n",
    "    qid = str(r.get(\"query_id\"))\n",
    "    templ = str(r.get(\"template\"))\n",
    "    variant = str(r.get(\"variant\") or TAG)  # \"rag_cosine_top3\"\n",
    "    prompt = r.get(\"prompt\") or \"\"\n",
    "\n",
    "    key = (qid, templ, variant)\n",
    "    if key in done_agg or key in per_template_done.get(templ, set()):\n",
    "        continue\n",
    "\n",
    "    # Avviso se mancano gli snippet (devono esserci!)\n",
    "    if not r.get(\"snippets\"):\n",
    "        print(f\"[WARN] {templ} qid={qid}: campo 'snippets' mancante/vuoto nel prompt COSINE.\")\n",
    "\n",
    "    gen_text, stats = generate_safe(prompt)\n",
    "\n",
    "    row_out = {\n",
    "        \"query_id\": qid,\n",
    "        \"repo_name\": r.get(\"repo_name\"),\n",
    "        \"instruction\": r.get(\"instruction\"),\n",
    "        \"template\": templ,\n",
    "        \"variant\": variant,                 # \"rag_cosine_top3\"\n",
    "        \"retrieval_method\": r.get(\"retrieval_method\"),\n",
    "        \"k_snippets\": r.get(\"k_snippets\"),\n",
    "        \"snippets\": r.get(\"snippets\"),\n",
    "        \"prompt\": prompt,\n",
    "        \"generation\": gen_text,\n",
    "        \"gen_stats\": stats,\n",
    "        \"model_name\": cfg.model.model_name,\n",
    "    }\n",
    "\n",
    "    per_template_buffers[templ].append(row_out)\n",
    "    new_agg_rows.append(row_out)\n",
    "    total_new += 1\n",
    "\n",
    "    # Telemetria memoria & flush periodico\n",
    "    try:\n",
    "        alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        pbar.set_postfix(done=total_new, mem=f\"{alloc:.2f}G/{reserved:.2f}G\")\n",
    "    except Exception:\n",
    "        pbar.set_postfix(done=total_new)\n",
    "\n",
    "    if sum(len(v) for v in per_template_buffers.values()) >= 8:\n",
    "        for tname, buf in per_template_buffers.items():\n",
    "            if buf:\n",
    "                append_jsonl(per_template_targets[tname], buf)\n",
    "                per_template_buffers[tname].clear()\n",
    "        if new_agg_rows:\n",
    "            append_jsonl(AGG_OUT, new_agg_rows)\n",
    "            new_agg_rows.clear()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# Flush finale\n",
    "for tname, buf in per_template_buffers.items():\n",
    "    if buf:\n",
    "        append_jsonl(per_template_targets[tname], buf)\n",
    "        per_template_buffers[tname].clear()\n",
    "\n",
    "if new_agg_rows:\n",
    "    append_jsonl(AGG_OUT, new_agg_rows)\n",
    "    new_agg_rows.clear()\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"[{TAG}] completato in {dt:.1f}s | nuove generazioni: {total_new}\")\n",
    "print(\"Risultati:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0bd6ca",
   "metadata": {},
   "source": [
    "#### 6.2.3  RAG WITH HYBRID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "a524c052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f3aa5171064efd9845a5058b89f602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_hybrid_top5] prompt letti: 253\n",
      "[rag_hybrid_top5] resume: 0 già presenti (aggregato)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27d2ec1c402043738ff2874a9a9ea7cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rag_hybrid_top5:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_hybrid_top5] completato in 4349.7s | nuove generazioni: 253\n",
      "Risultati: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_hybrid_top5\n"
     ]
    }
   ],
   "source": [
    "# === CODEGEN SOLO per RAG HYBRID — OOM-safe + resume + progress ===\n",
    "import os, gc, json, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Carica modello locale (come baseline)\n",
    "# -----------------------------\n",
    "from models.loader import load_model_and_tokenizer\n",
    "from models.config import load_config\n",
    "\n",
    "cfg = load_config()\n",
    "cfg.model.model_name = \"codellama/CodeLlama-7b-Instruct-hf\"   # cambia se vuoi\n",
    "cfg.cache.root = \"./cache\"\n",
    "tokenizer, model, used_cache = load_model_and_tokenizer(cfg)\n",
    "device = next(model.parameters()).device\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Utility I/O\n",
    "# -----------------------------\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def append_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    if not rows: return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_done_keys(path: Path) -> set:\n",
    "    \"\"\"Chiave resume: (query_id, template, variant).\"\"\"\n",
    "    done = set()\n",
    "    if path.exists():\n",
    "        try:\n",
    "            for obj in read_jsonl(path):\n",
    "                done.add((str(obj.get(\"query_id\")), str(obj.get(\"template\")), str(obj.get(\"variant\"))))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return done\n",
    "\n",
    "def method_tag_from_filename(p: Path) -> str:\n",
    "    # \"RAG_rag_hybrid_top5.jsonl\" -> \"rag_hybrid_top5\"\n",
    "    name = p.stem\n",
    "    return name[len(\"RAG_\"):] if name.startswith(\"RAG_\") else name\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Config generazione + guard-rails (ridotti per velocità/VRAM)\n",
    "# -----------------------------\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=200,      # più corto per velocità/VRAM\n",
    "    do_sample=False,         # deterministico\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_time=30.0,           # timeout per sample\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "MODEL_CTX = getattr(model.config, \"max_position_embeddings\", None) or getattr(tokenizer, \"model_max_length\", 2048)\n",
    "MAX_INPUT_TOKENS = min(1024, max(512, int(MODEL_CTX - GEN_KW[\"max_new_tokens\"] - 64)))\n",
    "\n",
    "def truncate_prompt_tokens(prompt: str) -> str:\n",
    "    ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(ids) <= MAX_INPUT_TOKENS:\n",
    "        return prompt\n",
    "    # tieni la coda: di solito contiene gli snippet\n",
    "    ids = ids[-MAX_INPUT_TOKENS:]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def generate_safe(prompt: str) -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"Retry se OOM abbassando max_new_tokens; fallback compatto se serve.\"\"\"\n",
    "    stats = {\"attempts\": 0, \"oom_retries\": 0, \"used_max_new_tokens\": GEN_KW[\"max_new_tokens\"]}\n",
    "    prompt_use = truncate_prompt_tokens(prompt)\n",
    "    kw = dict(GEN_KW)\n",
    "    for _ in range(3):\n",
    "        stats[\"attempts\"] += 1\n",
    "        try:\n",
    "            inputs = tokenizer(prompt_use, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                out_ids = model.generate(**inputs, **kw)\n",
    "            text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "            if text.startswith(prompt_use):\n",
    "                text = text[len(prompt_use):].lstrip()\n",
    "            del inputs, out_ids\n",
    "            return text, stats\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            stats[\"oom_retries\"] += 1\n",
    "            kw[\"max_new_tokens\"] = max(128, int(kw[\"max_new_tokens\"] * 0.8))\n",
    "            stats[\"used_max_new_tokens\"] = kw[\"max_new_tokens\"]\n",
    "            torch.cuda.empty_cache(); gc.collect()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                stats[\"oom_retries\"] += 1\n",
    "                kw[\"max_new_tokens\"] = max(128, int(kw[\"max_new_tokens\"] * 0.8))\n",
    "                stats[\"used_max_new_tokens\"] = kw[\"max_new_tokens\"]\n",
    "                torch.cuda.empty_cache(); gc.collect()\n",
    "                continue\n",
    "            raise\n",
    "    # fallback ultra-compatto\n",
    "    short_prompt = prompt_use[-800:]\n",
    "    inputs = tokenizer(short_prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(**inputs, **{**GEN_KW, \"max_new_tokens\": 160, \"max_time\": 25.0})\n",
    "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    if text.startswith(short_prompt):\n",
    "        text = text[len(short_prompt):].lstrip()\n",
    "    del inputs, out_ids\n",
    "    return text, {**stats, \"used_max_new_tokens\": 160, \"fallback\": True}\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Sorgente RAG HYBRID e cartelle output\n",
    "# -----------------------------\n",
    "HYBRID_RAG_FILE = Path(\n",
    "    r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top3\\RAG_rag_hybrid_top5.jsonl\"\n",
    ")\n",
    "# In alternativa, se usi la variante \"top5\":\n",
    "# HYBRID_RAG_FILE = Path(\n",
    "#     r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_hybrid_top5\\RAG_rag_hybrid_top5.jsonl\"\n",
    "# )\n",
    "\n",
    "if not HYBRID_RAG_FILE.exists():\n",
    "    raise FileNotFoundError(f\"File prompt HYBRID non trovato: {HYBRID_RAG_FILE}\")\n",
    "\n",
    "TAG = method_tag_from_filename(HYBRID_RAG_FILE)  # es. \"rag_hybrid_top5\" o \"rag_hybrid_top3\"\n",
    "RESULTS_ROOT = Path(\"results/rag_generations\")\n",
    "OUT_DIR = RESULTS_ROOT / TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGG_OUT = OUT_DIR / \"AGGREGATED_results.jsonl\"\n",
    "\n",
    "rows = read_jsonl(HYBRID_RAG_FILE)\n",
    "print(f\"[{TAG}] prompt letti: {len(rows)}\")\n",
    "\n",
    "# Resume aggregato\n",
    "done_agg = load_done_keys(AGG_OUT)\n",
    "print(f\"[{TAG}] resume: {len(done_agg)} già presenti (aggregato)\")\n",
    "\n",
    "# Target per template + rispettivi resume\n",
    "templates = sorted({str(r.get(\"template\", \"unknown\")) for r in rows})\n",
    "per_template_targets: Dict[str, Path] = {t: OUT_DIR / f\"{t}_results.jsonl\" for t in templates}\n",
    "per_template_done: Dict[str, set] = {t: load_done_keys(p) for t, p in per_template_targets.items()}\n",
    "per_template_buffers: Dict[str, List[Dict[str, Any]]] = {t: [] for t in templates}\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Loop di generazione (solo HYBRID)\n",
    "# -----------------------------\n",
    "total_new = 0\n",
    "new_agg_rows: List[Dict[str, Any]] = []\n",
    "t0 = time.time()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pbar = tqdm(rows, desc=TAG, dynamic_ncols=True)\n",
    "for r in pbar:\n",
    "    qid = str(r.get(\"query_id\"))\n",
    "    templ = str(r.get(\"template\"))\n",
    "    variant = str(r.get(\"variant\") or TAG)  # dovrebbe essere \"rag_hybrid_*\"\n",
    "    prompt = r.get(\"prompt\") or \"\"\n",
    "    key = (qid, templ, variant)\n",
    "\n",
    "    # Avvisa se mancano gli snippet (devono esserci!)\n",
    "    if not r.get(\"snippets\"):\n",
    "        print(f\"[WARN] {templ} qid={qid}: campo 'snippets' mancante/vuoto nel prompt HYBRID.\")\n",
    "\n",
    "    # Resume\n",
    "    if key in done_agg or key in per_template_done.get(templ, set()):\n",
    "        continue\n",
    "\n",
    "    gen_text, stats = generate_safe(prompt)\n",
    "\n",
    "    row_out = {\n",
    "        \"query_id\": qid,\n",
    "        \"repo_name\": r.get(\"repo_name\"),\n",
    "        \"instruction\": r.get(\"instruction\"),\n",
    "        \"template\": templ,\n",
    "        \"variant\": variant,\n",
    "        \"retrieval_method\": r.get(\"retrieval_method\"),\n",
    "        \"k_snippets\": r.get(\"k_snippets\"),\n",
    "        \"snippets\": r.get(\"snippets\"),\n",
    "        \"prompt\": prompt,\n",
    "        \"generation\": gen_text,\n",
    "        \"gen_stats\": stats,\n",
    "        \"model_name\": cfg.model.model_name,\n",
    "    }\n",
    "\n",
    "    per_template_buffers[templ].append(row_out)\n",
    "    new_agg_rows.append(row_out)\n",
    "    total_new += 1\n",
    "\n",
    "    # feedback memoria GPU (se disponibile)\n",
    "    try:\n",
    "        alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "        pbar.set_postfix(done=total_new, mem=f\"{alloc:.2f}G/{reserved:.2f}G\")\n",
    "    except Exception:\n",
    "        pbar.set_postfix(done=total_new)\n",
    "\n",
    "    # flush periodico\n",
    "    if sum(len(v) for v in per_template_buffers.values()) >= 8:  # flush più frequente\n",
    "        for tname, buf in per_template_buffers.items():\n",
    "            if buf:\n",
    "                append_jsonl(per_template_targets[tname], buf)\n",
    "                per_template_buffers[tname].clear()\n",
    "        if new_agg_rows:\n",
    "            append_jsonl(AGG_OUT, new_agg_rows)\n",
    "            new_agg_rows.clear()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# flush finale\n",
    "for tname, buf in per_template_buffers.items():\n",
    "    if buf:\n",
    "        append_jsonl(per_template_targets[tname], buf)\n",
    "        per_template_buffers[tname].clear()\n",
    "\n",
    "if new_agg_rows:\n",
    "    append_jsonl(AGG_OUT, new_agg_rows)\n",
    "    new_agg_rows.clear()\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"[{TAG}] completato in {dt:.1f}s | nuove generazioni: {total_new}\")\n",
    "print(\"Risultati:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf98d84",
   "metadata": {},
   "source": [
    "#### 6.2.4  RAG WITH MULTIHOP V1: DECOMPOSITION-FIRST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bc92b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c937f8500b340f590a78be9347385a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_multihop_decomposition_top4] prompt letti: 253\n",
      "[rag_multihop_decomposition_top4] resume: 0 già presenti (aggregato)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rag_multihop_decomposition_top4: 100%|##########| 253/253 [3:06:38<00:00, 44.26s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_multihop_decomposition_top4] completato in 11198.9s | nuove generazioni: 253\n",
      "Risultati: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_multihop_decomposition_top4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === CODEGEN SOLO per RAG MultiHop: DECOMPOSITION-FIRST ===\n",
    "import json, time, gc\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from models.loader import load_model_and_tokenizer\n",
    "from models.config import load_config\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) PERCORSO PROMPT MULTIHOP (DECOMPOSITION)\n",
    "#    Dalla tua log: file aggregato senza prefisso \"RAG_\".\n",
    "# -------------------------------------------------\n",
    "RAG_FILE = Path(\n",
    "    r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_decomposition_top4\\rag_multihop_decomposition_top4.jsonl\"\n",
    ")\n",
    "if not RAG_FILE.exists():\n",
    "    raise FileNotFoundError(f\"File prompt MultiHop (decomposition) non trovato: {RAG_FILE}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Carica il modello (come baseline)\n",
    "# -------------------------------------------------\n",
    "cfg = load_config()\n",
    "cfg.model.model_name = \"codellama/CodeLlama-7b-Instruct-hf\"   # cambia se vuoi\n",
    "cfg.cache.root = \"./cache\"\n",
    "tokenizer, model, used_cache = load_model_and_tokenizer(cfg)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Utilità I/O\n",
    "# -------------------------------------------------\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def append_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_done_keys(path: Path) -> set:\n",
    "    done = set()\n",
    "    if path.exists():\n",
    "        for obj in read_jsonl(path):\n",
    "            done.add((str(obj.get(\"query_id\")), str(obj.get(\"template\")), str(obj.get(\"variant\"))))\n",
    "    return done\n",
    "\n",
    "def tag_from_filename(p: Path) -> str:\n",
    "    # Usa direttamente lo stem del file (qui non c'è prefisso \"RAG_\")\n",
    "    return p.stem\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Config generazione + guard-rails\n",
    "# -------------------------------------------------\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=320,\n",
    "    do_sample=False,  # deterministico\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_time=45.0,    # timeout per sample\n",
    ")\n",
    "\n",
    "MAX_INPUT_TOKENS = 1536  # lascia spazio all'output per evitare OOM\n",
    "\n",
    "def truncate_prompt_tokens(prompt: str) -> str:\n",
    "    ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(ids) <= MAX_INPUT_TOKENS:\n",
    "        return prompt\n",
    "    ids = ids[-MAX_INPUT_TOKENS:]  # tieni la coda (spesso contiene gli snippet)\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) Carica prompt & prepara output\n",
    "# -------------------------------------------------\n",
    "rows = read_jsonl(RAG_FILE)\n",
    "TAG = tag_from_filename(RAG_FILE)  # es. \"rag_multihop_decomposition_top4\"\n",
    "print(f\"[{TAG}] prompt letti: {len(rows)}\")\n",
    "\n",
    "RESULTS_ROOT = Path(\"results/rag_generations\")\n",
    "OUT_DIR = RESULTS_ROOT / TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGG_OUT = OUT_DIR / \"AGGREGATED_results.jsonl\"\n",
    "\n",
    "done_agg = load_done_keys(AGG_OUT)\n",
    "print(f\"[{TAG}] resume: {len(done_agg)} già presenti (aggregato)\")\n",
    "\n",
    "templates = sorted({r.get(\"template\", \"unknown\") for r in rows})\n",
    "per_template_targets = {t: OUT_DIR / f\"{t}_results.jsonl\" for t in templates}\n",
    "per_template_done = {t: load_done_keys(p) for t, p in per_template_targets.items()}\n",
    "per_template_buffers = {t: [] for t in templates}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) Loop generazione\n",
    "# -------------------------------------------------\n",
    "total_new = 0\n",
    "new_agg_rows: List[Dict[str, Any]] = []\n",
    "t0 = time.time()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "for r in tqdm(rows, desc=TAG, ascii=True):\n",
    "    qid = str(r.get(\"query_id\"))\n",
    "    templ = str(r.get(\"template\"))\n",
    "    variant = str(r.get(\"variant\") or TAG)  # dovrebbe essere \"rag_multihop_decomposition\"\n",
    "    prompt = r.get(\"prompt\") or \"\"\n",
    "\n",
    "    key = (qid, templ, variant)\n",
    "    if key in done_agg or key in per_template_done.get(templ, set()):\n",
    "        continue\n",
    "\n",
    "    prompt_use = truncate_prompt_tokens(prompt)\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(prompt_use, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out_ids = model.generate(**inputs, **GEN_KW)\n",
    "        gen_text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "        if gen_text.startswith(prompt_use):\n",
    "            gen_text = gen_text[len(prompt_use):].lstrip()\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        short_prompt = prompt_use[-800:]\n",
    "        inputs = tokenizer(short_prompt, return_tensors=\"pt\").to(model.device)\n",
    "        with torch.no_grad():\n",
    "            out_ids = model.generate(**inputs, **{**GEN_KW, \"max_new_tokens\": 200, \"max_time\": 30.0})\n",
    "        gen_text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "        if gen_text.startswith(short_prompt):\n",
    "            gen_text = gen_text[len(short_prompt):].lstrip()\n",
    "    except Exception as e:\n",
    "        gen_text = f\"<<GENERATION_ERROR: {e!r}>>\"\n",
    "\n",
    "    row_out = {\n",
    "        \"query_id\": qid,\n",
    "        \"repo_name\": r.get(\"repo_name\"),\n",
    "        \"instruction\": r.get(\"instruction\"),\n",
    "        \"template\": templ,\n",
    "        \"variant\": variant,\n",
    "        \"retrieval_method\": r.get(\"retrieval_method\"),\n",
    "        \"k_snippets\": r.get(\"k_snippets\"),\n",
    "        \"snippets\": r.get(\"snippets\"),\n",
    "        \"prompt\": prompt,\n",
    "        \"generation\": gen_text,\n",
    "        \"model_name\": cfg.model.model_name,\n",
    "    }\n",
    "\n",
    "    per_template_buffers[templ].append(row_out)\n",
    "    new_agg_rows.append(row_out)\n",
    "    total_new += 1\n",
    "\n",
    "    if total_new % 16 == 0:\n",
    "        for tname, buf in per_template_buffers.items():\n",
    "            if buf:\n",
    "                append_jsonl(per_template_targets[tname], buf)\n",
    "                per_template_buffers[tname].clear()\n",
    "        if new_agg_rows:\n",
    "            append_jsonl(AGG_OUT, new_agg_rows)\n",
    "            new_agg_rows.clear()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# flush finale\n",
    "for tname, buf in per_template_buffers.items():\n",
    "    if buf:\n",
    "        append_jsonl(per_template_targets[tname], buf)\n",
    "        per_template_buffers[tname].clear()\n",
    "if new_agg_rows:\n",
    "    append_jsonl(AGG_OUT, new_agg_rows)\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"[{TAG}] completato in {dt:.1f}s | nuove generazioni: {total_new}\")\n",
    "print(\"Risultati:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a460e",
   "metadata": {},
   "source": [
    "#### 6.2.5  RAG WITH MULTIHop: ITERATIVE-REFINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "7b16436b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb8b66c25c794bb3b9a4238a0d7c8ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_multihop_iterative_top4] prompt letti: 253\n",
      "[rag_multihop_iterative_top4] resume: 0 già presenti (aggregato)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576eee778cf24cedb1d93a0f24e4e7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "rag_multihop_iterative_top4:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rag_multihop_iterative_top4] completato in 11657.9s | nuove generazioni: 253\n",
      "Risultati: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_multihop_iterative_top4\n"
     ]
    }
   ],
   "source": [
    "# === CODEGEN SOLO per RAG MultiHop: ITERATIVE-REFINE (con fix snippets mancanti) ===\n",
    "import json, time, gc, re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "from models.loader import load_model_and_tokenizer\n",
    "from models.config import load_config\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 1) PERCORSI FILE\n",
    "# -------------------------------------------------\n",
    "PROMPTS_JSONL = Path(\n",
    "    r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\prompts\\rag_multihop_iterative_top4\\rag_multihop_iterative_top4.jsonl\"\n",
    ")\n",
    "RETRIEVAL_JSONL = Path(\n",
    "    r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\multihop\\mh_iterative_topk_k3.jsonl\"\n",
    ")\n",
    "\n",
    "if not PROMPTS_JSONL.exists():\n",
    "    raise FileNotFoundError(f\"File prompt MultiHop (iterative_refine) non trovato: {PROMPTS_JSONL}\")\n",
    "if not RETRIEVAL_JSONL.exists():\n",
    "    raise FileNotFoundError(f\"File retrieval MultiHop (iterative_refine) non trovato: {RETRIEVAL_JSONL}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) Carica il modello (come baseline/BM25)\n",
    "# -------------------------------------------------\n",
    "cfg = load_config()\n",
    "cfg.model.model_name = \"codellama/CodeLlama-7b-Instruct-hf\"   # cambia se vuoi\n",
    "cfg.cache.root = \"./cache\"\n",
    "tokenizer, model, used_cache = load_model_and_tokenizer(cfg)\n",
    "device = next(model.parameters()).device\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Utilità I/O + normalizzazione\n",
    "# -------------------------------------------------\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def append_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    if not rows: return\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def load_done_keys(path: Path) -> set:\n",
    "    done = set()\n",
    "    if path.exists():\n",
    "        for obj in read_jsonl(path):\n",
    "            done.add((str(obj.get(\"query_id\")), str(obj.get(\"template\")), str(obj.get(\"variant\"))))\n",
    "    return done\n",
    "\n",
    "def take_top_k(results: List[Dict[str, Any]], k: int) -> List[Dict[str, Any]]:\n",
    "    if not isinstance(results, list): return []\n",
    "    # ordina per score decrescente se disponibile, altrimenti lascia l'ordine\n",
    "    try:\n",
    "        return sorted(results, key=lambda x: float(x.get(\"score\", 0.0)), reverse=True)[:k]\n",
    "    except Exception:\n",
    "        return results[:k]\n",
    "\n",
    "def clean_text(t: str) -> str:\n",
    "    # Evita markdown eccessivo negli snippet iniettati\n",
    "    return t.strip()\n",
    "\n",
    "# Inietta gli snippet alla fine del prompt se non presenti\n",
    "def inject_snippets_into_prompt(prompt: str, snippets: List[Dict[str, Any]]) -> str:\n",
    "    if not snippets:\n",
    "        return prompt\n",
    "    # Se il prompt contiene già una sezione \"Context Snippets\", non duplicare\n",
    "    if re.search(r\"(?i)context\\s+snippets\", prompt):\n",
    "        return prompt\n",
    "    blocks = []\n",
    "    for i, h in enumerate(snippets, 1):\n",
    "        txt = clean_text(h.get(\"text\", \"\"))\n",
    "        doc = str(h.get(\"doc_id\") or \"\")\n",
    "        sc  = h.get(\"score\")\n",
    "        head = f\"[{i}] doc_id={doc}\" + (f\" | score={sc:.4f}\" if isinstance(sc, (int,float)) else \"\")\n",
    "        blocks.append(head + \"\\n\" + txt)\n",
    "    ctx = \"\\n\\n### Context Snippets\\n\" + \"\\n\\n---\\n\\n\".join(blocks) + \"\\n\"\n",
    "    # Aggiungi in coda: manteniamo il prompt originale e poi il contesto\n",
    "    return prompt + ctx\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Carica retrieval MultiHop e prepara lookup by query_id\n",
    "# -------------------------------------------------\n",
    "retrieval_rows = read_jsonl(RETRIEVAL_JSONL)\n",
    "# attesi campi: query_id, repo_name, instruction, results=[{doc_id,text,score,...}]\n",
    "lookup_hits: Dict[str, List[Dict[str, Any]]] = {}\n",
    "for obj in retrieval_rows:\n",
    "    qid = str(obj.get(\"query_id\"))\n",
    "    res = obj.get(\"results\") or []\n",
    "    lookup_hits[qid] = res\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 5) Config generazione + guard-rails\n",
    "# -------------------------------------------------\n",
    "GEN_KW = dict(\n",
    "    max_new_tokens=320,\n",
    "    do_sample=False,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_time=45.0,\n",
    ")\n",
    "MODEL_CTX = getattr(model.config, \"max_position_embeddings\", None) or getattr(tokenizer, \"model_max_length\", 2048)\n",
    "MAX_INPUT_TOKENS = max(256, int(MODEL_CTX - GEN_KW[\"max_new_tokens\"] - 32))\n",
    "\n",
    "def truncate_prompt_tokens(prompt: str) -> str:\n",
    "    ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(ids) <= MAX_INPUT_TOKENS:\n",
    "        return prompt\n",
    "    # tieni la coda (gli snippet sono tipicamente in fondo)\n",
    "    ids = ids[-MAX_INPUT_TOKENS:]\n",
    "    return tokenizer.decode(ids, skip_special_tokens=True)\n",
    "\n",
    "def generate_safe(prompt: str) -> Tuple[str, Dict[str, Any]]:\n",
    "    stats = {\"attempts\": 0, \"oom_retries\": 0, \"used_max_new_tokens\": GEN_KW[\"max_new_tokens\"]}\n",
    "    prompt_use = truncate_prompt_tokens(prompt)\n",
    "    kw = dict(GEN_KW)\n",
    "    for _ in range(3):\n",
    "        stats[\"attempts\"] += 1\n",
    "        try:\n",
    "            inputs = tokenizer(prompt_use, return_tensors=\"pt\").to(device)\n",
    "            with torch.no_grad():\n",
    "                out_ids = model.generate(**inputs, **kw)\n",
    "            text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "            if text.startswith(prompt_use):\n",
    "                text = text[len(prompt_use):].lstrip()\n",
    "            del inputs, out_ids\n",
    "            return text, stats\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            stats[\"oom_retries\"] += 1\n",
    "            kw[\"max_new_tokens\"] = max(160, int(kw[\"max_new_tokens\"] * 0.6))\n",
    "            stats[\"used_max_new_tokens\"] = kw[\"max_new_tokens\"]\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e).lower():\n",
    "                stats[\"oom_retries\"] += 1\n",
    "                kw[\"max_new_tokens\"] = max(160, int(kw[\"max_new_tokens\"] * 0.6))\n",
    "                stats[\"used_max_new_tokens\"] = kw[\"max_new_tokens\"]\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "                continue\n",
    "            raise\n",
    "    # fallback ultra-compatto\n",
    "    short_prompt = prompt_use[-800:]\n",
    "    inputs = tokenizer(short_prompt, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        out_ids = model.generate(**inputs, **{**GEN_KW, \"max_new_tokens\": 160, \"max_time\": 30.0})\n",
    "    text = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    if text.startswith(short_prompt):\n",
    "        text = text[len(short_prompt):].lstrip()\n",
    "    del inputs, out_ids\n",
    "    return text, {**stats, \"used_max_new_tokens\": 160, \"fallback\": True}\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 6) Carica prompt, ripristina snippets mancanti, genera\n",
    "# -------------------------------------------------\n",
    "rows = read_jsonl(PROMPTS_JSONL)\n",
    "TAG = PROMPTS_JSONL.stem  # \"rag_multihop_iterative_top4\"\n",
    "print(f\"[{TAG}] prompt letti: {len(rows)}\")\n",
    "\n",
    "RESULTS_ROOT = Path(\"results/rag_generations\")\n",
    "OUT_DIR = RESULTS_ROOT / TAG\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGG_OUT = OUT_DIR / \"AGGREGATED_results.jsonl\"\n",
    "\n",
    "done_agg = load_done_keys(AGG_OUT)\n",
    "print(f\"[{TAG}] resume: {len(done_agg)} già presenti (aggregato)\")\n",
    "\n",
    "templates = sorted({str(r.get(\"template\", \"unknown\")) for r in rows})\n",
    "per_template_targets = {t: OUT_DIR / f\"{t}_results.jsonl\" for t in templates}\n",
    "per_template_done = {t: load_done_keys(p) for t, p in per_template_targets.items()}\n",
    "buffers = {t: [] for t in templates}\n",
    "\n",
    "total_new = 0\n",
    "new_agg_rows: List[Dict[str, Any]] = []\n",
    "t0 = time.time()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "pbar = tqdm(rows, desc=TAG, dynamic_ncols=True)\n",
    "for r in pbar:\n",
    "    qid   = str(r.get(\"query_id\"))\n",
    "    templ = str(r.get(\"template\"))\n",
    "    variant = str(r.get(\"variant\") or TAG)  # es. \"rag_multihop_iterative_top4\"\n",
    "    prompt = r.get(\"prompt\") or \"\"\n",
    "    k_snip = int(r.get(\"k_snippets\") or 4)\n",
    "\n",
    "    key = (qid, templ, variant)\n",
    "    if key in done_agg or key in per_template_done.get(templ, set()):\n",
    "        continue\n",
    "\n",
    "    # --- FIX: ripristina 'snippets' se mancanti/vuoti ---\n",
    "    snippets = r.get(\"snippets\") or []\n",
    "    if not snippets:\n",
    "        src_hits = lookup_hits.get(qid) or []\n",
    "        snippets = take_top_k(src_hits, k_snip)\n",
    "        # inietta gli snippet anche nel prompt, così il modello li vede\n",
    "        prompt = inject_snippets_into_prompt(prompt, snippets)\n",
    "\n",
    "    gen_text, stats = generate_safe(prompt)\n",
    "\n",
    "    row_out = {\n",
    "        \"query_id\": qid,\n",
    "        \"repo_name\": r.get(\"repo_name\"),\n",
    "        \"instruction\": r.get(\"instruction\"),\n",
    "        \"template\": templ,\n",
    "        \"variant\": variant,\n",
    "        \"retrieval_method\": r.get(\"retrieval_method\"),\n",
    "        \"k_snippets\": k_snip,\n",
    "        \"snippets\": snippets,          # <- garantiti qui\n",
    "        \"prompt\": prompt,              # <- prompt con sezione Context Snippets se serviva\n",
    "        \"generation\": gen_text,\n",
    "        \"gen_stats\": stats,\n",
    "        \"model_name\": cfg.model.model_name,\n",
    "    }\n",
    "\n",
    "    buffers[templ].append(row_out)\n",
    "    new_agg_rows.append(row_out)\n",
    "    total_new += 1\n",
    "\n",
    "    # flush periodico\n",
    "    if sum(len(v) for v in buffers.values()) >= 16:\n",
    "        for tname, buf in buffers.items():\n",
    "            if buf:\n",
    "                append_jsonl(per_template_targets[tname], buf)\n",
    "                buffers[tname].clear()\n",
    "        if new_agg_rows:\n",
    "            append_jsonl(AGG_OUT, new_agg_rows)\n",
    "            new_agg_rows.clear()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# flush finale\n",
    "for tname, buf in buffers.items():\n",
    "    if buf:\n",
    "        append_jsonl(per_template_targets[tname], buf)\n",
    "        buffers[tname].clear()\n",
    "\n",
    "if new_agg_rows:\n",
    "    append_jsonl(AGG_OUT, new_agg_rows)\n",
    "    new_agg_rows.clear()\n",
    "\n",
    "dt = time.time() - t0\n",
    "print(f\"[{TAG}] completato in {dt:.1f}s | nuove generazioni: {total_new}\")\n",
    "print(\"Risultati:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "139f596b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Salvati:\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\baseline\\codebleu_per_instance.jsonl\n",
      " - C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\baseline\\summary_by_prompt.json\n",
      "\n",
      "Esempio per_instance:\n",
      "{'query_id': 'seed-emulator__000000', 'repo_name': 'seed-emulator', 'prompt_type': 'v1', 'model_name': 'codellama/CodeLlama-7b-Instruct-hf', 'codebleu': 12.706925473630774, 'bleu': 12.706925473630774, 'w_bleu': None, 'ast': None, 'df': None, 'error_reason': None}\n",
      "\n",
      "Esempio summary:\n",
      "{'prompt_type': 'v1', 'count': 23, 'mean': 2.633914445340188, 'min': 1.8548169447666272, 'max': 18.519668477565148, 'std': 3.9425483532272927}\n"
     ]
    }
   ],
   "source": [
    "# === Scrive tabelle finali in percentuale ===\n",
    "import os, sys, json, math\n",
    "from pathlib import Path\n",
    "\n",
    "# Assicurati che il pacchetto sia importabile\n",
    "if os.getcwd() not in sys.path:\n",
    "    sys.path.insert(0, os.getcwd())\n",
    "try:\n",
    "    from Code_evaluation.codebleu_eval import compute_codebleu_for_generations, export_summary_csv\n",
    "except Exception:\n",
    "    from code_evaluation.codebleu_eval import compute_codebleu_for_generations, export_summary_csv\n",
    "\n",
    "# ---- PATH (adatta se serve) ----\n",
    "GENERATIONS_PATH = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\_all_baseline.jsonl\"\n",
    "DATASET_PATH     = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\data\\lca_test_filtered.jsonl\"\n",
    "OUT_DIR_BASE     = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\baseline\"\n",
    "\n",
    "Path(OUT_DIR_BASE).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Esegui / ri-esegui valutazione (filtra solo baseline) ----\n",
    "per_instance_raw, summary_raw = compute_codebleu_for_generations(\n",
    "    generations_path=GENERATIONS_PATH,\n",
    "    dataset_path=DATASET_PATH,\n",
    "    lang=\"python\",\n",
    "    prompt_field=\"template\",\n",
    "    variant_field=\"variant\",\n",
    "    variant_value=\"baseline\",\n",
    "    out_dir=None  # qui non salviamo, salviamo noi sotto con formato richiesto\n",
    ")\n",
    "\n",
    "# ---- Helper: conversione robusta a percentuale ----\n",
    "def to_percent(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    try:\n",
    "        v = float(x)\n",
    "        if math.isnan(v):\n",
    "            return None\n",
    "        # se è chiaramente in [0,1], porta a %\n",
    "        if 0.0 <= v <= 1.0:\n",
    "            return v * 100.0\n",
    "        # se è >100 probabilmente è già % \"sballata\": limitiamo a 100 ma NON dovrebbe capitare\n",
    "        return v\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# ---- Costruisci codebleu_per_instance.jsonl (253 righe attese) ----\n",
    "# Usiamo la combinazione 0.25/0.25/0.25/0.25 come \"codebleu\" principale.\n",
    "per_instance_final = []\n",
    "for r in per_instance_raw:\n",
    "    out = {\n",
    "        \"query_id\":   r.get(\"query_id\"),\n",
    "        \"repo_name\":  r.get(\"repo_name\"),\n",
    "        \"prompt_type\":r.get(\"prompt_type\"),\n",
    "        \"model_name\": r.get(\"model_name\"),\n",
    "        \"codebleu\":   to_percent(r.get(\"codebleu_a025\")),\n",
    "        \"bleu\":       to_percent(r.get(\"bleu_a025\")),\n",
    "        \"w_bleu\":     to_percent(r.get(\"w_bleu_a025\")),\n",
    "        \"ast\":        to_percent(r.get(\"ast_a025\")),\n",
    "        \"df\":         to_percent(r.get(\"df_a025\")),\n",
    "        \"error_reason\": r.get(\"error_reason\"),\n",
    "    }\n",
    "    per_instance_final.append(out)\n",
    "\n",
    "per_instance_path = os.path.join(OUT_DIR_BASE, \"codebleu_per_instance.jsonl\")\n",
    "with open(per_instance_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in per_instance_final:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# ---- Costruisci summary_by_prompt.json (11 righe) in percentuale ----\n",
    "summary_final = []\n",
    "for s in summary_raw:\n",
    "    summary_final.append({\n",
    "        \"prompt_type\": s.get(\"prompt_type\"),\n",
    "        \"count\":       s.get(\"count\"),\n",
    "        \"mean\":        to_percent(s.get(\"mean_codebleu_025\")),\n",
    "        \"min\":         to_percent(s.get(\"min_codebleu_025\")),\n",
    "        \"max\":         to_percent(s.get(\"max_codebleu_025\")),\n",
    "        \"std\":         to_percent(s.get(\"std_codebleu_025\")),\n",
    "    })\n",
    "\n",
    "summary_path = os.path.join(OUT_DIR_BASE, \"summary_by_prompt.json\")\n",
    "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary_final, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"[OK] Salvati:\")\n",
    "print(\" -\", per_instance_path)\n",
    "print(\" -\", summary_path)\n",
    "\n",
    "# ---- (Opzionale) stampa due righe di esempio ----\n",
    "print(\"\\nEsempio per_instance:\")\n",
    "print(per_instance_final[0] if per_instance_final else None)\n",
    "\n",
    "print(\"\\nEsempio summary:\")\n",
    "print(summary_final[0] if summary_final else None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe2048e",
   "metadata": {},
   "source": [
    "Per ogni query che sono 23 in totale abbiamo diverse strategie retrivial. Ogni combinazione di query e topksnippets trovati con la specfica strategia di retrivial viene combinata con differenti versioni di prompt. Serve trovsre risultati per ogni categoria di retrvial (BASELINE, BM25, COSINE, HYBRID , MULTIHOPEV1, MULTIHOPEV2) e per ognuna serve vedere quale è la migliore strategia di prompt. Poi fare riflessioni sui risultti. \n",
    "\n",
    "PASSARE ALCUNE RIGHE DEL FILE JSONL CON CODE GENERATO , PASSARE ALCUNE RIGHE DATASET COSI SA COME SONO LE COLONNE DA USARE, PASSARE LA PATH DEL DATASET E LA PATH DEL JSONL CON CODICE GENERATO. POI CHIEDERE DI TROVARE LA MEDIA PER LE METRICHE, IL RISULTATO PIU ALTO E PIU BASSO. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d979f26",
   "metadata": {},
   "source": [
    "## Section 7: Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "243c0d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n",
      "WARNING:root:WARNING: There is no reference data-flows extracted from the whole corpus, and the data-flow match score degenerates to 0. Please consider ignoring this score.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvato: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_hybrid_top5\\codebleu_summary_by_variant_prompt.csv\n"
     ]
    }
   ],
   "source": [
    "# === CELLA: calcolo CodeBLEU normalizzato per tutte le tipologie ===\n",
    "# Requisiti:\n",
    "#   - cartella/libreria `codebleu/` presente nel PYTHONPATH (quella che abbiamo creato)\n",
    "#   - pip: `codebleu` installato (fork che espone `calc_codebleu`)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Se necessario, aggiungi il path della tua cartella progetto, es:\n",
    "# sys.path.append(r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\")\n",
    "\n",
    "from codebleu_F import compute_grouped_codebleu  # dal nostro pacchetto modulare\n",
    "\n",
    "# -----------------------------\n",
    "# PATH forniti (puliti)\n",
    "# -----------------------------\n",
    "BASELINE_FILE = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\outputs\\codegen\\codellama_CodeLlama-7b-Instruct-hf\\by_prompt\\_all_baseline.jsonl\"\n",
    "DATASET_FILE  = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\data\\lca_test_filtered.jsonl\"\n",
    "\n",
    "BM25_FILE     = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_bm25_top3\\AGGREGATED_results.jsonl\"\n",
    "COSINE_FILE   = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_cosine_top3\\AGGREGATED_results.jsonl\"\n",
    "HYBRID_FILE   = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_hybrid_top5\\AGGREGATED_results.jsonl\"\n",
    "\n",
    "MULTIHOP_DECOMP_DIR = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_multihop_decomposition_top4\"\n",
    "MULTIHOP_ITER_DIR   = r\"C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_multihop_iterative_top4\"\n",
    "\n",
    "# -----------------------------\n",
    "# Utility di parsing\n",
    "# -----------------------------\n",
    "\n",
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(line))\n",
    "            except Exception:\n",
    "                # prova a ripulire eventuali BOM/escape strani\n",
    "                try:\n",
    "                    rows.append(json.loads(line.encode(\"utf-8\", \"ignore\").decode(\"utf-8\")))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return rows\n",
    "\n",
    "def load_dataset(df_path: Path) -> pd.DataFrame:\n",
    "    data = read_jsonl(df_path)\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # 👈 ADATTA QUI se la reference sta in un altro campo\n",
    "    # Proviamo vari alias comuni:\n",
    "    candidate_ref_cols = [\"references\", \"reference\", \"ground_truth\", \"solution\", \"target\", \"code\"]\n",
    "    ref_col = None\n",
    "    for c in candidate_ref_cols:\n",
    "        if c in df.columns:\n",
    "            ref_col = c\n",
    "            break\n",
    "    if ref_col is None:\n",
    "        raise ValueError(f\"Non trovo una colonna reference tra {candidate_ref_cols} in {df_path}\")\n",
    "\n",
    "    # normalizza in lista di stringhe (multi-ref OK)\n",
    "    def to_list_refs(x):\n",
    "        if isinstance(x, list):\n",
    "            return [s for s in x if isinstance(s, str)]\n",
    "        if isinstance(x, str):\n",
    "            return [x]\n",
    "        return []\n",
    "\n",
    "    df[\"references_norm\"] = df[ref_col].apply(to_list_refs)\n",
    "\n",
    "    # 👈 ADATTA QUI: id della riga (preferiamo chiavi stabili)\n",
    "    id_col = None\n",
    "    for c in [\"id\", \"qid\", \"sample_id\", \"problem_id\", \"uid\"]:\n",
    "        if c in df.columns:\n",
    "            id_col = c\n",
    "            break\n",
    "    if id_col is None:\n",
    "        # se non c'è un ID, creiamo un indice posizionale\n",
    "        df[\"__row_index__\"] = np.arange(len(df))\n",
    "        id_col = \"__row_index__\"\n",
    "\n",
    "    df = df[[id_col, \"references_norm\"]].rename(columns={id_col: \"join_id\"})\n",
    "    return df\n",
    "\n",
    "def extract_pred_and_prompt(rows: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Estrae prediction e (se presente) prompt_type da una lista di dict JSON.\n",
    "    \"\"\"\n",
    "    def pick_pred(d: Dict[str, Any]) -> str:\n",
    "        # 👈 ADATTA QUI se la predizione sta sotto un altro nome\n",
    "        for k in [\"prediction\", \"generated\", \"code\", \"output\", \"generation\"]:\n",
    "            if k in d and isinstance(d[k], str):\n",
    "                return d[k]\n",
    "        # in alcuni formati, c'è un oggetto {\"text\": \"...\"}\n",
    "        for k in [\"prediction\", \"generated\", \"output\", \"generation\"]:\n",
    "            if k in d and isinstance(d[k], dict) and isinstance(d[k].get(\"text\"), str):\n",
    "                return d[k][\"text\"]\n",
    "        return \"\"\n",
    "\n",
    "    def pick_prompt_type(d: Dict[str, Any]) -> Optional[str]:\n",
    "        for k in [\"prompt_type\", \"promptVersion\", \"prompt\", \"template_id\"]:\n",
    "            if k in d and isinstance(d[k], str):\n",
    "                return d[k]\n",
    "        return None\n",
    "\n",
    "    def pick_id(d: Dict[str, Any]) -> Optional[str]:\n",
    "        for k in [\"id\", \"qid\", \"sample_id\", \"problem_id\", \"uid\"]:\n",
    "            if k in d:\n",
    "                return str(d[k])\n",
    "        return None\n",
    "\n",
    "    recs = []\n",
    "    for d in rows:\n",
    "        recs.append({\n",
    "            \"join_id\": pick_id(d),\n",
    "            \"prediction\": pick_pred(d),\n",
    "            \"prompt_type\": pick_prompt_type(d)\n",
    "        })\n",
    "    df = pd.DataFrame(recs)\n",
    "    # se manca join_id, usa indice posizionale\n",
    "    if df[\"join_id\"].isna().any():\n",
    "        df[\"__row_index__\"] = np.arange(len(df))\n",
    "        df[\"join_id\"] = df[\"join_id\"].fillna(df[\"__row_index__\"].astype(str))\n",
    "        df.drop(columns=[\"__row_index__\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_generation_file(file_path: Path, variant_hint: str) -> pd.DataFrame:\n",
    "    rows = read_jsonl(file_path)\n",
    "    dfp = extract_pred_and_prompt(rows)\n",
    "    dfp[\"variant\"] = variant_hint\n",
    "    return dfp\n",
    "\n",
    "def load_generation_dir(dir_path: Path, variant_hint: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Carica tutte le .jsonl nella cartella e prova a inferire prompt_type dal nome file se non presente dentro.\n",
    "    \"\"\"\n",
    "    all_parts = []\n",
    "    for fp in sorted(dir_path.glob(\"*.jsonl\")):\n",
    "        rows = read_jsonl(fp)\n",
    "        dfp = extract_pred_and_prompt(rows)\n",
    "        # se manca prompt_type, deducilo dal nome file (pattern tipo ..._v7.jsonl)\n",
    "        if dfp[\"prompt_type\"].isna().all():\n",
    "            m = re.search(r\"(v\\d+(_\\d+)?)\", fp.stem.lower())\n",
    "            if m:\n",
    "                dfp[\"prompt_type\"] = m.group(1)\n",
    "            else:\n",
    "                dfp[\"prompt_type\"] = \"unknown\"\n",
    "        dfp[\"variant\"] = variant_hint\n",
    "        dfp[\"__source_file__\"] = str(fp)\n",
    "        all_parts.append(dfp)\n",
    "    return pd.concat(all_parts, ignore_index=True) if all_parts else pd.DataFrame(columns=[\"join_id\",\"prediction\",\"prompt_type\",\"variant\"])\n",
    "\n",
    "# -----------------------------\n",
    "# Carica dataset (riferimenti)\n",
    "# -----------------------------\n",
    "dataset_df = load_dataset(Path(DATASET_FILE))\n",
    "\n",
    "# -----------------------------\n",
    "# Carica tutte le generazioni\n",
    "# -----------------------------\n",
    "frames = []\n",
    "\n",
    "# baseline (file singolo)\n",
    "frames.append(load_generation_file(Path(BASELINE_FILE), variant_hint=\"baseline\"))\n",
    "\n",
    "# RAG aggregati (file singoli)\n",
    "frames.append(load_generation_file(Path(BM25_FILE),   variant_hint=\"bm25\"))\n",
    "frames.append(load_generation_file(Path(COSINE_FILE), variant_hint=\"cosine\"))\n",
    "frames.append(load_generation_file(Path(HYBRID_FILE), variant_hint=\"hybrid\"))\n",
    "\n",
    "# Multihop (cartelle con più file)\n",
    "frames.append(load_generation_dir(Path(MULTIHOP_DECOMP_DIR), variant_hint=\"multihop_decomposition\"))\n",
    "frames.append(load_generation_dir(Path(MULTIHOP_ITER_DIR),   variant_hint=\"multihop_iterative\"))\n",
    "\n",
    "gens_df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# assicurati che prompt_type esista\n",
    "if \"prompt_type\" not in gens_df.columns:\n",
    "    gens_df[\"prompt_type\"] = \"unknown\"\n",
    "else:\n",
    "    gens_df[\"prompt_type\"] = gens_df[\"prompt_type\"].fillna(\"unknown\")\n",
    "\n",
    "# -----------------------------\n",
    "# Join con dataset per ottenere le references\n",
    "# -----------------------------\n",
    "# Se join_id è numerico stringa in uno dei due, allinea i tipi\n",
    "dataset_df[\"join_id\"] = dataset_df[\"join_id\"].astype(str)\n",
    "gens_df[\"join_id\"]    = gens_df[\"join_id\"].astype(str)\n",
    "\n",
    "merged = gens_df.merge(dataset_df, on=\"join_id\", how=\"left\")\n",
    "\n",
    "# Eventuali mancati match: fallback per posizione (solo dove references mancanti)\n",
    "if merged[\"references_norm\"].isna().any():\n",
    "    # prendi le righe bucate e rimpiazza references dalla posizione\n",
    "    mask = merged[\"references_norm\"].isna()\n",
    "    # mappa per variant/prompt_type l'ordine, per allineare con dataset se serve\n",
    "    # (fallback semplice: usa l'ordine del dataset)\n",
    "    fallback_refs = dataset_df[\"references_norm\"].tolist()\n",
    "    merged.loc[mask, \"references_norm\"] = merged.loc[mask].index.map(\n",
    "        lambda i: fallback_refs[i % len(fallback_refs)]\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Calcolo CodeBLEU per gruppo\n",
    "# -----------------------------\n",
    "# compute_grouped_codebleu richiede:\n",
    "#   - prediction_col\n",
    "#   - references_col (lista di stringhe)\n",
    "#   - group_cols\n",
    "summary = compute_grouped_codebleu(\n",
    "    merged,\n",
    "    prediction_col=\"prediction\",\n",
    "    references_col=\"references_norm\",\n",
    "    group_cols=(\"variant\",\"prompt_type\"),\n",
    "    # puoi cambiare average=\"corpus\" se preferisci corpus-level\n",
    "    average=\"macro\",\n",
    ")\n",
    "\n",
    "# Ordina per variant/prompt_type\n",
    "summary = summary.sort_values([\"variant\",\"prompt_type\"]).reset_index(drop=True)\n",
    "\n",
    "# Salva CSV\n",
    "out_dir = Path(HYBRID_FILE).parent  # salva vicino a uno dei risultati\n",
    "out_csv = out_dir / \"codebleu_summary_by_variant_prompt.csv\"\n",
    "summary.to_csv(out_csv, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Salvato:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "58ab5591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>codebleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>0.163410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bm25</td>\n",
       "      <td>0.128609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cosine</td>\n",
       "      <td>0.250732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>0.138757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>multihop_decomposition</td>\n",
       "      <td>0.200912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>multihop_iterative</td>\n",
       "      <td>0.224805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  variant  codebleu\n",
       "0                baseline  0.163410\n",
       "1                    bm25  0.128609\n",
       "2                  cosine  0.250732\n",
       "3                  hybrid  0.138757\n",
       "4  multihop_decomposition  0.200912\n",
       "5      multihop_iterative  0.224805"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary_by_variant = summary.groupby(\"variant\", as_index=False)[\"codebleu\"].mean()\n",
    "display(summary_by_variant)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "4f3edb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# --- 1) definisci le \"firme\" testuali per riconoscere ogni versione ---\n",
    "PATTERNS = [\n",
    "    (\"v9\",    r\"\\(v9\\b|Workflow \\(do not output steps 1–3\\)|Verified code excerpts\"),\n",
    "    (\"v8\",    r\"\\(v8\\)|Library-Centric Python Code Generation \\(v8\\)\"),\n",
    "    (\"v7\",    r\"Python Code Generation Task \\(Library-Focused\\)\"),\n",
    "    (\"v6_3\",  r\"^# PYTHON LIBRARY-BASED CODE GENERATION TASK|```PYTHON\"),\n",
    "    (\"v6\",    r\"Long Code Arena · Library-Based Code Generation\"),\n",
    "    (\"v5\",    r\"Write a complete Python 3 implementation for the following task\"),\n",
    "    (\"v4\",    r\"Return exactly and only the raw answer|^You are a senior Python engineer\\.\\s+Use the retrieved examples\"),\n",
    "    (\"v3\",    r\"^You are a senior Python engineer\\.\\s+Fulfill the following task|Implementation\\*\\*:\\s*```python\"),\n",
    "    (\"v2\",    r\"\\bseedemu\\b|Use the `seedemu` Python library\"),\n",
    "    (\"v1\",    r\"^### Task:\\s|^### Retrieved Examples:\\s\"),\n",
    "]\n",
    "\n",
    "FALLBACK_VERSION = \"v6_2\"   # come richiesto\n",
    "\n",
    "# --- 2) funzione di riconoscimento su una singola stringa ---\n",
    "def detect_version(prompt_text: str) -> str:\n",
    "    if not isinstance(prompt_text, str) or not prompt_text.strip():\n",
    "        return FALLBACK_VERSION\n",
    "    txt = prompt_text.strip()\n",
    "    for ver, pat in PATTERNS:\n",
    "        if re.search(pat, txt, flags=re.IGNORECASE | re.MULTILINE):\n",
    "            return ver\n",
    "    return FALLBACK_VERSION\n",
    "\n",
    "# --- 3) applica al tuo DataFrame summary ---\n",
    "# summary ha colonne: ['variant', 'prompt_type', 'codebleu']\n",
    "summary = summary.copy()\n",
    "summary[\"prompt_version\"] = summary[\"prompt_type\"].apply(detect_version)\n",
    "\n",
    "# (opzionale) rimuovi il testo lungo\n",
    "summary = summary.drop(columns=[\"prompt_type\"])\n",
    "\n",
    "# ordina e mostra\n",
    "summary = summary.sort_values([\"variant\", \"prompt_version\"]).reset_index(drop=True)\n",
    "\n",
    "# --- 4) (opzionale) salva anche il mapping trovato per tracciabilità ---\n",
    "# NB: qui il mapping è \"dinamico\" (pattern->versione). Se vuoi anche\n",
    "# un mapping esplicito dai TESTI LUNGHI visti a 'vX', fai così:\n",
    "seen = {}\n",
    "for s in summary[\"prompt_version\"].unique():\n",
    "    seen[s] = []  # placeholder per completezza\n",
    "\n",
    "# popola (solo le prime occorrenze per non appesantire)\n",
    "for row in summary.itertuples(index=False):\n",
    "    pv = row.prompt_version\n",
    "    if len(seen[pv]) < 3:  # salva qualche esempio\n",
    "        # ATTENZIONE: questo richiede il prompt_type originale. Se lo hai già droppato, usa una copia\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "a2b98711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>prompt_version</th>\n",
       "      <th>codebleu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.185871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baseline</td>\n",
       "      <td>v2</td>\n",
       "      <td>0.171419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>baseline</td>\n",
       "      <td>v6_2</td>\n",
       "      <td>0.141880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>baseline</td>\n",
       "      <td>v6_3</td>\n",
       "      <td>0.164756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baseline</td>\n",
       "      <td>v7</td>\n",
       "      <td>0.144842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>baseline</td>\n",
       "      <td>v8</td>\n",
       "      <td>0.153567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>baseline</td>\n",
       "      <td>v9</td>\n",
       "      <td>0.172220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bm25</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.161507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bm25</td>\n",
       "      <td>v2</td>\n",
       "      <td>0.142972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>bm25</td>\n",
       "      <td>v6_2</td>\n",
       "      <td>0.096764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>bm25</td>\n",
       "      <td>v6_3</td>\n",
       "      <td>0.122803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bm25</td>\n",
       "      <td>v7</td>\n",
       "      <td>0.133774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bm25</td>\n",
       "      <td>v8</td>\n",
       "      <td>0.168210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>bm25</td>\n",
       "      <td>v9</td>\n",
       "      <td>0.087831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cosine</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.253125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>cosine</td>\n",
       "      <td>v2</td>\n",
       "      <td>0.256719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>cosine</td>\n",
       "      <td>v6_2</td>\n",
       "      <td>0.250018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>cosine</td>\n",
       "      <td>v6_3</td>\n",
       "      <td>0.249077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>cosine</td>\n",
       "      <td>v7</td>\n",
       "      <td>0.250010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>cosine</td>\n",
       "      <td>v8</td>\n",
       "      <td>0.250002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>cosine</td>\n",
       "      <td>v9</td>\n",
       "      <td>0.250112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.155605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>v2</td>\n",
       "      <td>0.154330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>v6_2</td>\n",
       "      <td>0.112124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>v6_3</td>\n",
       "      <td>0.129295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>v7</td>\n",
       "      <td>0.137169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>v8</td>\n",
       "      <td>0.215327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>hybrid</td>\n",
       "      <td>v9</td>\n",
       "      <td>0.094614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>multihop_decomposition</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.133053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>multihop_decomposition</td>\n",
       "      <td>v2</td>\n",
       "      <td>0.182133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>multihop_decomposition</td>\n",
       "      <td>v6_2</td>\n",
       "      <td>0.153410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>multihop_decomposition</td>\n",
       "      <td>v6_3</td>\n",
       "      <td>0.206845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>multihop_decomposition</td>\n",
       "      <td>v7</td>\n",
       "      <td>0.191376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>multihop_decomposition</td>\n",
       "      <td>v8</td>\n",
       "      <td>0.250543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>multihop_decomposition</td>\n",
       "      <td>v9</td>\n",
       "      <td>0.244997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>multihop_iterative</td>\n",
       "      <td>v1</td>\n",
       "      <td>0.177532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>multihop_iterative</td>\n",
       "      <td>v2</td>\n",
       "      <td>0.213739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>multihop_iterative</td>\n",
       "      <td>v6_2</td>\n",
       "      <td>0.201527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>multihop_iterative</td>\n",
       "      <td>v6_3</td>\n",
       "      <td>0.223354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>multihop_iterative</td>\n",
       "      <td>v7</td>\n",
       "      <td>0.250354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>multihop_iterative</td>\n",
       "      <td>v8</td>\n",
       "      <td>0.250143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>multihop_iterative</td>\n",
       "      <td>v9</td>\n",
       "      <td>0.250154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   variant prompt_version  codebleu\n",
       "0                 baseline             v1  0.185871\n",
       "1                 baseline             v2  0.171419\n",
       "2                 baseline           v6_2  0.141880\n",
       "3                 baseline           v6_3  0.164756\n",
       "4                 baseline             v7  0.144842\n",
       "5                 baseline             v8  0.153567\n",
       "6                 baseline             v9  0.172220\n",
       "7                     bm25             v1  0.161507\n",
       "8                     bm25             v2  0.142972\n",
       "9                     bm25           v6_2  0.096764\n",
       "10                    bm25           v6_3  0.122803\n",
       "11                    bm25             v7  0.133774\n",
       "12                    bm25             v8  0.168210\n",
       "13                    bm25             v9  0.087831\n",
       "14                  cosine             v1  0.253125\n",
       "15                  cosine             v2  0.256719\n",
       "16                  cosine           v6_2  0.250018\n",
       "17                  cosine           v6_3  0.249077\n",
       "18                  cosine             v7  0.250010\n",
       "19                  cosine             v8  0.250002\n",
       "20                  cosine             v9  0.250112\n",
       "21                  hybrid             v1  0.155605\n",
       "22                  hybrid             v2  0.154330\n",
       "23                  hybrid           v6_2  0.112124\n",
       "24                  hybrid           v6_3  0.129295\n",
       "25                  hybrid             v7  0.137169\n",
       "26                  hybrid             v8  0.215327\n",
       "27                  hybrid             v9  0.094614\n",
       "28  multihop_decomposition             v1  0.133053\n",
       "29  multihop_decomposition             v2  0.182133\n",
       "30  multihop_decomposition           v6_2  0.153410\n",
       "31  multihop_decomposition           v6_3  0.206845\n",
       "32  multihop_decomposition             v7  0.191376\n",
       "33  multihop_decomposition             v8  0.250543\n",
       "34  multihop_decomposition             v9  0.244997\n",
       "35      multihop_iterative             v1  0.177532\n",
       "36      multihop_iterative             v2  0.213739\n",
       "37      multihop_iterative           v6_2  0.201527\n",
       "38      multihop_iterative           v6_3  0.223354\n",
       "39      multihop_iterative             v7  0.250354\n",
       "40      multihop_iterative             v8  0.250143\n",
       "41      multihop_iterative             v9  0.250154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvato: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_hybrid_top5\\codebleu_avg_by_variant_prompt.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prompt_version</th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v6_2</th>\n",
       "      <th>v6_3</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>variant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>baseline</th>\n",
       "      <td>0.185871</td>\n",
       "      <td>0.171419</td>\n",
       "      <td>0.141880</td>\n",
       "      <td>0.164756</td>\n",
       "      <td>0.144842</td>\n",
       "      <td>0.153567</td>\n",
       "      <td>0.172220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bm25</th>\n",
       "      <td>0.161507</td>\n",
       "      <td>0.142972</td>\n",
       "      <td>0.096764</td>\n",
       "      <td>0.122803</td>\n",
       "      <td>0.133774</td>\n",
       "      <td>0.168210</td>\n",
       "      <td>0.087831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosine</th>\n",
       "      <td>0.253125</td>\n",
       "      <td>0.256719</td>\n",
       "      <td>0.250018</td>\n",
       "      <td>0.249077</td>\n",
       "      <td>0.250010</td>\n",
       "      <td>0.250002</td>\n",
       "      <td>0.250112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hybrid</th>\n",
       "      <td>0.155605</td>\n",
       "      <td>0.154330</td>\n",
       "      <td>0.112124</td>\n",
       "      <td>0.129295</td>\n",
       "      <td>0.137169</td>\n",
       "      <td>0.215327</td>\n",
       "      <td>0.094614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multihop_decomposition</th>\n",
       "      <td>0.133053</td>\n",
       "      <td>0.182133</td>\n",
       "      <td>0.153410</td>\n",
       "      <td>0.206845</td>\n",
       "      <td>0.191376</td>\n",
       "      <td>0.250543</td>\n",
       "      <td>0.244997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multihop_iterative</th>\n",
       "      <td>0.177532</td>\n",
       "      <td>0.213739</td>\n",
       "      <td>0.201527</td>\n",
       "      <td>0.223354</td>\n",
       "      <td>0.250354</td>\n",
       "      <td>0.250143</td>\n",
       "      <td>0.250154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "prompt_version                v1        v2      v6_2      v6_3        v7  \\\n",
       "variant                                                                    \n",
       "baseline                0.185871  0.171419  0.141880  0.164756  0.144842   \n",
       "bm25                    0.161507  0.142972  0.096764  0.122803  0.133774   \n",
       "cosine                  0.253125  0.256719  0.250018  0.249077  0.250010   \n",
       "hybrid                  0.155605  0.154330  0.112124  0.129295  0.137169   \n",
       "multihop_decomposition  0.133053  0.182133  0.153410  0.206845  0.191376   \n",
       "multihop_iterative      0.177532  0.213739  0.201527  0.223354  0.250354   \n",
       "\n",
       "prompt_version                v8        v9  \n",
       "variant                                     \n",
       "baseline                0.153567  0.172220  \n",
       "bm25                    0.168210  0.087831  \n",
       "cosine                  0.250002  0.250112  \n",
       "hybrid                  0.215327  0.094614  \n",
       "multihop_decomposition  0.250543  0.244997  \n",
       "multihop_iterative      0.250143  0.250154  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvato pivot: C:\\Users\\drugm\\Documents\\RP_PCTITO\\JACK_17_EDITS\\results\\rag_generations\\rag_hybrid_top5\\codebleu_pivot_by_variant_prompt.csv\n"
     ]
    }
   ],
   "source": [
    "# Calcola la media per ogni coppia (variant, prompt_version)\n",
    "avg_summary = (\n",
    "    summary\n",
    "    .groupby([\"variant\", \"prompt_version\"], as_index=False)\n",
    "    .mean(numeric_only=True)  # prende solo le colonne numeriche\n",
    ")\n",
    "\n",
    "# Ordina per chiarezza\n",
    "avg_summary = avg_summary.sort_values([\"variant\", \"prompt_version\"]).reset_index(drop=True)\n",
    "\n",
    "# Mostra\n",
    "display(avg_summary)\n",
    "\n",
    "# (Opzionale) salva in CSV\n",
    "out_csv_avg = Path(HYBRID_FILE).parent / \"codebleu_avg_by_variant_prompt.csv\"\n",
    "avg_summary.to_csv(out_csv_avg, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Salvato:\", out_csv_avg)\n",
    "\n",
    "# Pivot table: variant sulle righe, prompt_version sulle colonne\n",
    "pivot = summary.pivot_table(\n",
    "    index=\"variant\",\n",
    "    columns=\"prompt_version\",\n",
    "    values=\"codebleu\",   # <-- metti qui la metrica che vuoi mediare\n",
    "    aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "# Ordina le colonne per avere v1..v11 in ordine naturale\n",
    "pivot = pivot.reindex(sorted(pivot.columns, key=lambda x: (x[0]!=\"v\", x)), axis=1)\n",
    "\n",
    "# Mostra la tabella\n",
    "display(pivot)\n",
    "\n",
    "# (opzionale) salva in CSV\n",
    "out_csv_pivot = Path(HYBRID_FILE).parent / \"codebleu_pivot_by_variant_prompt.csv\"\n",
    "pivot.to_csv(out_csv_pivot, encoding=\"utf-8-sig\")\n",
    "print(\"Salvato pivot:\", out_csv_pivot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f006e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
